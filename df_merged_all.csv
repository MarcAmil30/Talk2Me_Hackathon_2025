paper_id,title,authors,url,text,filename,methodology,future_work,methodology_clean,future_work_clean,text_clean,combined,cluster
f3aad42fb7a93a0a80417c4bf1486c090f4651a0,PutnamBench: A Multilingual Competition-Mathematics Benchmark for Formal Theorem-Proving,"['George Tsoukalas', 'Jasper Lee', 'John Jennings', 'Jimmy Xin', 'Michelle Ding', 'Michael Jennings', 'Amitayush Thakur', 'Swarat Chaudhuri']",https://openreview.net/pdf/f3aad42fb7a93a0a80417c4bf1486c090f4651a0.pdf,"PutnamBench: A Multilingual Competition-Mathematics Benchmark for Formal Theorem-Proving We present PutnamBench, a benchmark of 1337 formalizations of Putnam competition problems in Lean 4, Isabelle, and Coq. We present PutnamBench, a new multilingual evaluation benchmark for formal theorem-proving. PutnamBench consists of formalizations of problems sourced from the William Lowell Putnam Mathematical Competition, the premier undergraduate-level mathematics competition in North America. All the problem statements come with formalizations in Lean 4 and Isabelle; a substantial subset have Coq formalizations as well. PutnamBench consists of 1337 hand-written formalizations across the three proof assistants, and aims to benchmark the next generation of theorem-proving algorithms for competition mathematics. Proving the theorems requires significant problem-solving ability and proficiency in a broad range of topics taught in undergraduate mathematics courses. We evaluate several established neural and symbolic theorem provers using PutnamBench. These approaches can only solve a handful of the problems, establishing our benchmark as a difficult open challenge for research on formal theorem-proving. PutnamBench is available at https://github.com/trishullab/PUTNAM.",f3aad42fb7a93a0a80417c4bf1486c090f4651a0.pdf,"methodology which
has seen significant application for theorem-proving in
MINIF2F. We run DSP with pass@10, using temperature
T= 0.1and GPT-4 as the underlying language model. Our
evaluation yields two successful proofs: one of Putnam 2001
A1, a problem involving magmas (sets with a binary opera-
tion), and one of Putnam 1995 A1, a problem involving a
closed-under-multiplication subset of the reals. In particular,
Putnam 1995 A1 cannot be solved by Sledgehammer alone.Putnam 2001 A1. Consider a set Sand a binary op-
eration ⋆, i.e., for each a, b∈S,a ⋆ b∈S. Assume
(a⋆b)⋆a=bfor all a, b∈S. Prove that a⋆(b⋆a) =b
for all a, b∈S.
theorem putnam_2001_a1:
fixes op :: "" 'a⇒'a⇒'a""
assumes hop : "" ∀a b :: 'a.
op (op a b) a = b""
shows "" ∀a b :: 'a. op a (op b a) = b""
proof -
{
fix a b :: 'a
have ""op (op a (op b a)) a = op b a"" using
hop by simp
then have ""op a (op b a) = b"" using hop by
metis
}
then show ?thesis by simp
qed
Figure 3: A formalization of Putnam 2001 A1 in Isabelle
and the corresponding proof discovered by our evaluation
with DSP. Sledgehammer alone can also produce a success-
ful proof to this theorem.
The generated proof is included in Figure 3.
We run a baseline using Sledgehammer, a powerful automa-
tion tool in Isabelle which makes calls to external SMT
solvers to prove a given goal. With a set timeout of t= 120
seconds, we run Sledgehammer on each Isabelle formaliza-
tion. The result of this evaluation is 3 successfully proven
problems: Putnam 1971 B1, 2001 A1, and 2012 A2. No-
tably, all of these problems are statements about sets with
binary operations. We include the statements of 1971 B1
and 2012 A2 in Figure 20.
Coq. We run GPT-4 with a Coq-based prompt on our Coq
formalizations using the same configuration as in Lean and
Isabelle. The result of the experiment is 1 solved problem,
namely Putnam 1988 B1, which was also solved in Lean 4.
The proof, which we include in Figure 13, generally follows
5
PUTNAM BENCH : A Multilingual Competition-Mathematics Be","Conclusion
We presented PUTNAM BENCH , a benchmark for neural
theorem-proving consisting of formalizations of Putnam
competition problems. A distinctive feature of PUTNAM -
BENCH is that it spans a broad range of undergraduate-level
mathematical topics, including algebra, analysis, and num-
ber theory. Another unique benefit is that it includes prob-
lems in Lean 4, Isabelle, and Coq, the three most popular
formal proof frameworks.
As our experiments show, PUTNAM BENCH is a challeng-
ing benchmark: all current theorem-proving approaches
fail to solve more than a handful of its problems. We be-
lieve that these failures have two root causes: (i) While
current theorem-provers can effectively stitch together stan-
dard proof steps well-represented in the training corpus,
they often fail at synthesizing new lemmas and orchestrat-
ing these lemmas into complex proofs. (ii) Current methods
often fail to leverage the deep knowledge available in math-
ematics repositories. Developing a new generation of neural
theorem-provers in which these weaknesses are at least
partly addressed is an exciting direction of future research.
7
PUTNAM BENCH : A Multilingual Competition-Mathematics Benchmark for Formal Theorem-Proving
7. Impact Statement
This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal
consequences of our work, none which we feel must be
specifically highlighted here.
References
Archive of Formal Proofs — isa-afp.org. https://www.
isa-afp.org/ . [Accessed 25-05-2024].
Alexanderson, G., Klosinski, L., and Larson, L. The William
Lowell Putnam Mathematical Competition: Problems
and Solutions, 1965-1984 . MAA problem books se-
ries. Mathematical Association of America, 1985. ISBN
9780883854419. URL https://books.google.com/
books?id=mv0oAQAAMAAJ .
Azerbayev, Z., Piotrowski, B., Schoelkopf, H., Ayers, E. W.,
Radev, D., and Avigad, J. Proofnet: Autoformalizing and
formally proving undergraduate-level mathematics, 202","methodology which has seen significant application for theorem-proving in MINIF2F. We run DSP with pass@10, using temperature T= 0.1and GPT-4 as the underlying language model. Our evaluation yields two successful Figure 3: A formalization of Putnam 2001 A1 in Isabelle and the corresponding proof discovered by our evaluation with DSP. Sledgehammer alone can also produce a success- ful proof to this theorem. The generated proof is included in Figure 3. We run a baseline using Sledgehammer, a powerful automa- tion tool in Isabelle which makes calls to external SMT solvers to prove a given goal. With a set timeout of t= 120 seconds, we run Sledgehammer on each Isabelle formaliza- tion. The result of this evaluation is 3 successfully proven problems: Putnam 1971 B1, 2001 A1, and 2012 A2. No- tably, all of these problems are statements about sets with binary operations. We include the statements of 1971 B1 and 2012 A2 in Figure 20. Coq. We run GPT-4 with a Coq-based prompt on our Coq formalizations using the same configuration as in Lean and Isabelle. The result of the experiment is 1 solved problem, namely Putnam 1988 B1, which was also solved in Lean 4. The proof, which we include in Figure 13, generally follows 5 PUTNAM BENCH : A Multilingual Competition-Mathematics Be","Conclusion We presented PUTNAM BENCH , a benchmark for neural theorem-proving consisting of formalizations of Putnam competition problems. A distinctive feature of PUTNAM - BENCH is that it spans a broad range of undergraduate-level mathematical topics, including algebra, analysis, and num- ber theory. Another unique benefit is that it includes prob- lems in Lean 4, Isabelle, and Coq, the three most popular formal proof frameworks. As our experiments show, PUTNAM BENCH is a challeng- ing benchmark: all current theorem-proving approaches fail to solve more than a handful of its problems. We be- lieve that these failures have two root causes: (i) While current theorem-provers can effectively stitch together stan- dard proof steps well-represented in the training corpus, they often fail at synthesizing new lemmas and orchestrat- ing these lemmas into complex proofs. (ii) Current methods often fail to leverage the deep knowledge available in math- ematics repositories. Developing a new generation of neural theorem-provers in which these weaknesses are at least partly addressed is an exciting direction of future research. 7 PUTNAM BENCH : A Multilingual Competition-Mathematics Benchmark for Formal Theorem-Proving 7. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.","PutnamBench: A Multilingual Competition-Mathematics Benchmark for Formal Theorem-Proving We present PutnamBench, a benchmark of 1337 formalizations of Putnam competition problems in Lean 4, Isabelle, and Coq. We present PutnamBench, a new multilingual evaluation benchmark for formal theorem-proving. PutnamBench consists of formalizations of problems sourced from the William Lowell Putnam Mathematical Competition, the premier undergraduate-level mathematics competition in North America. All the problem statements come with formalizations in Lean 4 and Isabelle; a substantial subset have Coq formalizations as well. PutnamBench consists of 1337 hand-written formalizations across the three proof assistants, and aims to benchmark the next generation of theorem-proving algorithms for competition mathematics. Proving the theorems requires significant problem-solving ability and proficiency in a broad range of topics taught in undergraduate mathematics courses. We evaluate several established neural and symbolic theorem provers using PutnamBench. These approaches can only solve a handful of the problems, establishing our benchmark as a difficult open challenge for research on formal theorem-proving. PutnamBench is available at https://github.com/trishullab/PUTNAM.","PutnamBench: A Multilingual Competition-Mathematics Benchmark for Formal Theorem-Proving We present PutnamBench, a benchmark of 1337 formalizations of Putnam competition problems in Lean 4, Isabelle, and Coq. We present PutnamBench, a new multilingual evaluation benchmark for formal theorem-proving. PutnamBench consists of formalizations of problems sourced from the William Lowell Putnam Mathematical Competition, the premier undergraduate-level mathematics competition in North America. All the problem statements come with formalizations in Lean 4 and Isabelle; a substantial subset have Coq formalizations as well. PutnamBench consists of 1337 hand-written formalizations across the three proof assistants, and aims to benchmark the next generation of theorem-proving algorithms for competition mathematics. Proving the theorems requires significant problem-solving ability and proficiency in a broad range of topics taught in undergraduate mathematics courses. We evaluate several established neural and symbolic theorem provers using PutnamBench. These approaches can only solve a handful of the problems, establishing our benchmark as a difficult open challenge for research on formal theorem-proving. PutnamBench is available at https://github.com/trishullab/PUTNAM. Conclusion We presented PUTNAM BENCH , a benchmark for neural theorem-proving consisting of formalizations of Putnam competition problems. A distinctive feature of PUTNAM - BENCH is that it spans a broad range of undergraduate-level mathematical topics, including algebra, analysis, and num- ber theory. Another unique benefit is that it includes prob- lems in Lean 4, Isabelle, and Coq, the three most popular formal proof frameworks. As our experiments show, PUTNAM BENCH is a challeng- ing benchmark: all current theorem-proving approaches fail to solve more than a handful of its problems. We be- lieve that these failures have two root causes: (i) While current theorem-provers can effectively stitch together stan- dard proof steps well-represented in the training corpus, they often fail at synthesizing new lemmas and orchestrat- ing these lemmas into complex proofs. (ii) Current methods often fail to leverage the deep knowledge available in math- ematics repositories. Developing a new generation of neural theorem-provers in which these weaknesses are at least partly addressed is an exciting direction of future research. 7 PUTNAM BENCH : A Multilingual Competition-Mathematics Benchmark for Formal Theorem-Proving 7. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. methodology which has seen significant application for theorem-proving in MINIF2F. We run DSP with pass@10, using temperature T= 0.1and GPT-4 as the underlying language model. Our evaluation yields two successful Figure 3: A formalization of Putnam 2001 A1 in Isabelle and the corresponding proof discovered by our evaluation with DSP. Sledgehammer alone can also produce a success- ful proof to this theorem. The generated proof is included in Figure 3. We run a baseline using Sledgehammer, a powerful automa- tion tool in Isabelle which makes calls to external SMT solvers to prove a given goal. With a set timeout of t= 120 seconds, we run Sledgehammer on each Isabelle formaliza- tion. The result of this evaluation is 3 successfully proven problems: Putnam 1971 B1, 2001 A1, and 2012 A2. No- tably, all of these problems are statements about sets with binary operations. We include the statements of 1971 B1 and 2012 A2 in Figure 20. Coq. We run GPT-4 with a Coq-based prompt on our Coq formalizations using the same configuration as in Lean and Isabelle. The result of the experiment is 1 solved problem, namely Putnam 1988 B1, which was also solved in Lean 4. The proof, which we include in Figure 13, generally follows 5 PUTNAM BENCH : A Multilingual Competition-Mathematics Be",0
aa31d8eb1de7fb2f55359bbe2174056de902fe7d,Learning to Reason by Failing: Offline RL on Sub-optimal Rollouts Scales Synthetic Data by 8x,"['Amrith Setlur', 'Saurabh Garg', 'Xinyang Geng', 'Naman Garg', 'Virginia Smith', 'Aviral Kumar']",https://openreview.net/pdf/aa31d8eb1de7fb2f55359bbe2174056de902fe7d.pdf,"Learning to Reason by Failing: Offline RL on Sub-optimal Rollouts Scales Synthetic Data by 8x Training on model-generated synthetic data is a promising approach for finetuning LLMs, but it remains unclear when it helps or hurts. In this paper, we investigate this for reasoning problems via an empirical study, followed by a theoretical formalization. First, we find that while the typical approach of finetuning a model on synthetic correct or *positive* problem-solution pairs generated by capable models offers modest performance gains, sampling more correct solutions from the finetuned learner **doubles** the sample efficiency of synthetic data. At the same time, training on model-generated positives can amplify spurious correlations, resulting in flat or even inverse scaling trends as the amount of data increases. Surprisingly, we find that several of these issues can be addressed if we also utilize *negative* responses, \ie model-generated responses that are deemed incorrect via final answer checking. Crucially, these negatives must be constructed such that the training can appropriately recover the utility or credit of each intermediate step in the negative response. With this \emph{per-step} scheme, we are able to attain consistent gains over only positive data, attaining performance similar to amplifying the amount of synthetic data by **8x**. We show that training on per-step negatives can help to unlearn spurious correlations in the positive data, and is equivalent to advantage-weighted reinforcement learning (RL), implying that it inherits benefits of RL over imitating positive data alone.",aa31d8eb1de7fb2f55359bbe2174056de902fe7d.pdf,"approach for finetuning LLMs, but it
remains unclear when it helps or hurts. In this
paper, we investigate this for reasoning problems
via an empirical study, followed by a theoretical
formalization. First, we find that while the typical
approach of finetuning a model on synthetic cor-
rect or positive problem-solution pairs generated
by capable models offers modest performance
gains, sampling more correct solutions from the
finetuned learner doubles the sample efficiency
of synthetic data. At the same time, training on
model-generated positives can amplify spurious
correlations, resulting in flat or even inverse scal-
ing trends as the amount of data increases. Sur-
prisingly, we find that several of these issues can
be addressed if we also utilize negative responses,
i.e., model-generated responses that are deemed
incorrect via final answer checking. Crucially,
these negatives must be constructed such that the
training can appropriately recover the utility or
credit of each intermediate step in the negative
response. With this per-step scheme, we are able
to attain consistent gains over only positive data,
attaining performance similar to amplifying the
amount of synthetic data by 8×. We show that
training on per-step negatives can help to unlearn
spurious correlations in the positive data, and is
equivalent to advantage-weighted reinforcement
learning (RL), implying that it inherits benefits of
RL over imitating positive data alone.
1. Introduction
Training large language models (LLMs) relies on the ability
to train on large amounts of high-quality data. It is pre-
dicted that we will run out of high-quality internet data by
1Anonymous Institution, Anonymous City, Anonymous Region,
Anonymous Country. Correspondence to: Anonymous Author
<anon.email@domain.com>.
Preliminary work. Under review by the International Conference
on Machine Learning (ICML). Do not distribute.2026 (Villalobos et al., 2022; Liu et al., 2024), necessitat-
ing training on model-generated dat","future work should focus on.
Broader impact. Our work focuses purely on understanding the role of synthetic data in improving reasoning capabilities of
LLMs. While excessive use of synthetic data can have unintended side effects upon deployment (e.g., fitting onto spurious
correlations as we illustrate in Section 4) and advanced reasoning capabilities may have the potential to affect economy,
human life, and society in both good and bad ways, we believe that these societal impacts are not unique or special to our
work when compared to other works studying similar problems. In fact, with capabilities in foundation models improving
day-by-day, future research and policy decisions would only benefit from more conceptual models to understand how
algorithms operate and how data affects performance, which our work attempts to study.
C. Proof of Theorem 5.1
We first restate the theorem statement and then provide a proof for this below. Our main goal in this theorem is to show that
training with per-step DPO is equivalent to running advantage-weighted RL shown in the theoretical result.
Theorem C.1 (Equivalence of advantage-weighted RL and DPO with per-step pairs) .The optimal policy from Equation 1
withD±
πsftgiven by (x,[y1∶i,+yi+1],[y1∶i,−yi+1])where the positive and negative traces share prefix y1∶i∼πsft, and
−yi+1∼πsft(⋅∣x,y1∶i),+yi+1∼σ(A˜π(x,y1∶i;⋅)−A˜π(x,y1∶i;−yi+1)), is identical to the optima of the advantage-
weighted RL objective:
max
πEx∼psyn(x),y∼πsft(⋅∣x)[L
∑
i=1logπ(yi∣x,y0∶i−1)⋅exp(A˜π(x,y0∶i−1,yi)/β)]. (5)
Proof. To prove this statement, we make the following observation: DPO (Rafailov et al., 2023) is equivalent to optimizing
a KL-divergence penalized expected reward objective in an induced Bradly-Terry model of preferences defined by the
reward function. That is, for any reward function r(x,y)over contexts x∼µand responses y, the optimal solution to the
following RL objective:
max
πEx∼µ,y∼π(⋅∣x)[r(x,y)]−βDKL(π(⋅∣x)∣∣πsft(⋅∣x)), (6)
is given by the followi","approach for finetuning LLMs, but it remains unclear when it helps or hurts. In this paper, we investigate this for reasoning problems via an empirical study, followed by a theoretical formalization. First, we find that while the typical approach of finetuning a model on synthetic cor- rect or positive problem-solution pairs generated by capable models offers modest performance gains, sampling more correct solutions from the finetuned learner doubles the sample efficiency of synthetic data. At the same time, training on model-generated positives can amplify spurious correlations, resulting in flat or even inverse scal- ing trends as the amount of data increases. Sur- prisingly, we find that several of these issues can be addressed if we also utilize negative responses, i.e., model-generated responses that are deemed incorrect via final answer checking. Crucially, these negatives must be constructed such that the training can appropriately recover the utility or credit of each intermediate step in the negative response. With this per-step scheme, we are able to attain consistent gains over only positive data, attaining performance similar to amplifying the amount of synthetic data by 8 . We show that training on per-step negatives can help to unlearn spurious correlations in the positive data, and is equivalent to advantage-weighted reinforcement learning (RL), implying that it inherits benefits of RL over imitating positive data alone. 1. Introduction Training large language models (LLMs) relies on the ability to train on large amounts of high-quality data. It is pre- dicted that we will run out of high-quality internet data by 1Anonymous Institution, Anonymous City, Anonymous Region, Anonymous Country. Correspondence to: Anonymous Author <anon.email@domain.com>. Preliminary work. Under review by the International Conference on Machine Learning (ICML). Do not distribute.2026 (Villalobos et al., 2022; Liu et al., 2024), necessitat- ing training on model-generated dat","future work should focus on. Broader impact. Our work focuses purely on understanding the role of synthetic data in improving reasoning capabilities of LLMs. While excessive use of synthetic data can have unintended side effects upon deployment (e.g., fitting onto spurious correlations as we illustrate in Section 4) and advanced reasoning capabilities may have the potential to affect economy, human life, and society in both good and bad ways, we believe that these societal impacts are not unique or special to our work when compared to other works studying similar problems. In fact, with capabilities in foundation models improving day-by-day, future research and policy decisions would only benefit from more conceptual models to understand how algorithms operate and how data affects performance, which our work attempts to study. C. Proof of Theorem 5.1 We first restate the theorem statement and then provide a proof for this below. Our main goal in this theorem is to show that training with per-step DPO is equivalent to running advantage-weighted RL shown in the theoretical result. Theorem C.1 (Equivalence of advantage-weighted RL and DPO with per-step pairs) .The optimal policy from Equation 1 withD sftgiven by (x,[y1 i,+yi+1],[y1 i, yi+1])where the positive and negative traces share prefix y1 i sft, and yi+1 sft( x,y1 i),+yi+1 (A (x,y1 i; ) A (x,y1 i; yi+1)), is identical to the optima of the advantage- weighted RL objective: max Ex psyn(x),y sft( x)[L i=1log (yi x,y0 i 1) exp(A (x,y0 i 1,yi)/ )]. (5) Proof. To prove this statement, we make the following observation: DPO (Rafailov et al., 2023) is equivalent to optimizing a KL-divergence penalized expected reward objective in an induced Bradly-Terry model of preferences defined by the reward function. That is, for any reward function r(x,y)over contexts x and responses y, the optimal solution to the following RL objective: max Ex ,y ( x)[r(x,y)] DKL( ( x) sft( x)), (6) is given by the followi","Learning to Reason by Failing: Offline RL on Sub-optimal Rollouts Scales Synthetic Data by 8x Training on model-generated synthetic data is a promising approach for finetuning LLMs, but it remains unclear when it helps or hurts. In this paper, we investigate this for reasoning problems via an empirical study, followed by a theoretical formalization. First, we find that while the typical approach of finetuning a model on synthetic correct or *positive* problem-solution pairs generated by capable models offers modest performance gains, sampling more correct solutions from the finetuned learner **doubles** the sample efficiency of synthetic data. At the same time, training on model-generated positives can amplify spurious correlations, resulting in flat or even inverse scaling trends as the amount of data increases. Surprisingly, we find that several of these issues can be addressed if we also utilize *negative* responses, \ie model-generated responses that are deemed incorrect via final answer checking. Crucially, these negatives must be constructed such that the training can appropriately recover the utility or credit of each intermediate step in the negative response. With this \emph{per-step} scheme, we are able to attain consistent gains over only positive data, attaining performance similar to amplifying the amount of synthetic data by **8x**. We show that training on per-step negatives can help to unlearn spurious correlations in the positive data, and is equivalent to advantage-weighted reinforcement learning (RL), implying that it inherits benefits of RL over imitating positive data alone.","Learning to Reason by Failing: Offline RL on Sub-optimal Rollouts Scales Synthetic Data by 8x Training on model-generated synthetic data is a promising approach for finetuning LLMs, but it remains unclear when it helps or hurts. In this paper, we investigate this for reasoning problems via an empirical study, followed by a theoretical formalization. First, we find that while the typical approach of finetuning a model on synthetic correct or *positive* problem-solution pairs generated by capable models offers modest performance gains, sampling more correct solutions from the finetuned learner **doubles** the sample efficiency of synthetic data. At the same time, training on model-generated positives can amplify spurious correlations, resulting in flat or even inverse scaling trends as the amount of data increases. Surprisingly, we find that several of these issues can be addressed if we also utilize *negative* responses, \ie model-generated responses that are deemed incorrect via final answer checking. Crucially, these negatives must be constructed such that the training can appropriately recover the utility or credit of each intermediate step in the negative response. With this \emph{per-step} scheme, we are able to attain consistent gains over only positive data, attaining performance similar to amplifying the amount of synthetic data by **8x**. We show that training on per-step negatives can help to unlearn spurious correlations in the positive data, and is equivalent to advantage-weighted reinforcement learning (RL), implying that it inherits benefits of RL over imitating positive data alone. future work should focus on. Broader impact. Our work focuses purely on understanding the role of synthetic data in improving reasoning capabilities of LLMs. While excessive use of synthetic data can have unintended side effects upon deployment (e.g., fitting onto spurious correlations as we illustrate in Section 4) and advanced reasoning capabilities may have the potential to affect economy, human life, and society in both good and bad ways, we believe that these societal impacts are not unique or special to our work when compared to other works studying similar problems. In fact, with capabilities in foundation models improving day-by-day, future research and policy decisions would only benefit from more conceptual models to understand how algorithms operate and how data affects performance, which our work attempts to study. C. Proof of Theorem 5.1 We first restate the theorem statement and then provide a proof for this below. Our main goal in this theorem is to show that training with per-step DPO is equivalent to running advantage-weighted RL shown in the theoretical result. Theorem C.1 (Equivalence of advantage-weighted RL and DPO with per-step pairs) .The optimal policy from Equation 1 withD sftgiven by (x,[y1 i,+yi+1],[y1 i, yi+1])where the positive and negative traces share prefix y1 i sft, and yi+1 sft( x,y1 i),+yi+1 (A (x,y1 i; ) A (x,y1 i; yi+1)), is identical to the optima of the advantage- weighted RL objective: max Ex psyn(x),y sft( x)[L i=1log (yi x,y0 i 1) exp(A (x,y0 i 1,yi)/ )]. (5) Proof. To prove this statement, we make the following observation: DPO (Rafailov et al., 2023) is equivalent to optimizing a KL-divergence penalized expected reward objective in an induced Bradly-Terry model of preferences defined by the reward function. That is, for any reward function r(x,y)over contexts x and responses y, the optimal solution to the following RL objective: max Ex ,y ( x)[r(x,y)] DKL( ( x) sft( x)), (6) is given by the followi approach for finetuning LLMs, but it remains unclear when it helps or hurts. In this paper, we investigate this for reasoning problems via an empirical study, followed by a theoretical formalization. First, we find that while the typical approach of finetuning a model on synthetic cor- rect or positive problem-solution pairs generated by capable models offers modest performance gains, sampling more correct solutions from the finetuned learner doubles the sample efficiency of synthetic data. At the same time, training on model-generated positives can amplify spurious correlations, resulting in flat or even inverse scal- ing trends as the amount of data increases. Sur- prisingly, we find that several of these issues can be addressed if we also utilize negative responses, i.e., model-generated responses that are deemed incorrect via final answer checking. Crucially, these negatives must be constructed such that the training can appropriately recover the utility or credit of each intermediate step in the negative response. With this per-step scheme, we are able to attain consistent gains over only positive data, attaining performance similar to amplifying the amount of synthetic data by 8 . We show that training on per-step negatives can help to unlearn spurious correlations in the positive data, and is equivalent to advantage-weighted reinforcement learning (RL), implying that it inherits benefits of RL over imitating positive data alone. 1. Introduction Training large language models (LLMs) relies on the ability to train on large amounts of high-quality data. It is pre- dicted that we will run out of high-quality internet data by 1Anonymous Institution, Anonymous City, Anonymous Region, Anonymous Country. Correspondence to: Anonymous Author <anon.email@domain.com>. Preliminary work. Under review by the International Conference on Machine Learning (ICML). Do not distribute.2026 (Villalobos et al., 2022; Liu et al., 2024), necessitat- ing training on model-generated dat",0
7bc6e168a9fe100095a13b5221a503d8d173f3ab,Lean4trace: Data augmentation for neural theorem proving in Lean,"['Vasilii Nesterov', 'Yermek Kapushev', 'Mikhail Burtsev']",https://openreview.net/pdf/7bc6e168a9fe100095a13b5221a503d8d173f3ab.pdf,"Lean4trace: Data augmentation for neural theorem proving in Lean Integrating large language models as proof assistants with theorem provers has shown great promise. However, one of the major challenges in this field is the scarcity of training data. To address this, we release a new open-source tool, *Lean4trace*, for training data extraction from Lean 4 sources. Unlike previous approaches, *Lean4trace* is deeply integrated into the Lean elaborator, allowing us to modify proofs on-the-fly. Leveraging this feature, we propose two methods of data augmentation in Lean: (1) decomposing composite proof steps into multiple simpler steps; (2) testing existing proof automation tactics at each proof state and collecting the successful ones. Models trained on this augmented data are capable of proving 58.0% of theorems from a hold-out subset of Mathlib and 35.6% of the test subset of the MiniF2F benchmark.",7bc6e168a9fe100095a13b5221a503d8d173f3ab.pdf,"approaches,
Lean4trace is deeply integrated into the Lean elab-
orator, allowing us to modify proofs on-the-fly.
Leveraging this feature, we propose two methods
of data augmentation in Lean: (1) decomposing
composite proof steps into multiple simpler steps;
(2) testing existing proof automation tactics at
each proof state and collecting the successful ones.
Models trained on this augmented data are capa-
ble of proving 58.0% of theorems from a hold-out
subset of Mathlib and 35.6% of the test subset of
the MiniF2F benchmark.
1. Introduction
One of the advantages of mathematics over other sciences
is that the correctness of its results can, in principle, be
verified mechanically. This is particularly desirable because
the standard peer review process inevitably sometimes re-
sults in invalid proofs. However, in practice, formalizing
mathematics is a labor-intensive and time-consuming task.
The standard approach involves using interactive theorem
proving (ITP) systems, among which it is worth mentioning
Lean (de Moura et al., 2015), Isabelle (Nipkow et al., 2002),
Coq (Barras et al., 1997), and Metamath (Megill & Wheeler,
2019). The process of theorem proving in such a system is
similar to programming in an IDE: a user interacts with the
system via commands in a formal language, and the system
provides feedback on whether the proof is successful.
In recent years, significant efforts have been made to sim-
plify the formalization process. The most developed li-
1Moscow Institute for Physics and Technology2Yandex
3London Institute for Mathematical Sciences. Correspondence
to: Vasilii Nesterov <vas.nesterov63@google.com >.
AI for MATH Workshop at ICML 2024 . Copyright 2024 by the
author(s)braries of formalized mathematics now contain more than
100,000 theorems. One example is Mathlib (mathlib Com-
munity, 2020), a user-maintained mathematical library for
the Lean theorem prover, which covers a wide range of
mathematical fields.
From another perspective, formal theorem pro","Conclusion
In this paper, we present Lean4trace , a novel tool for data
extraction and augmentation tailored for training neural the-
orem provers in Lean. Our experimental results demonstrate
that models trained using our dataset achieve a 9% higher
performance on the MiniF2F benchmark compared to Re-
Prover (Yang et al., 2023), when trained and evaluated under
identical conditions. While proposed augmentations pro-
vides improvements on Mathlib dataset, they may degrade
the model when evaluated on a dataset from different distri-
bution. Nevertheless, our tool allows gathering a more com-
plete set of proof states in canonical setup and significantly
reduces computational resource requirements compared to
LeanDojo (Yang et al., 2023), making it feasible to run on a
modern PC. We believe that these advancements will lower
the barrier to entry in this field, fostering more accessible
and widespread research in neural theorem proving.
6. Acknowledgements
This work was supported by a grant for research centers in
the field of artificial intelligence, provided by the Analytical
Center for the Government of the Russian Federation in
accordance with the subsidy agreement (agreement identi-
fier 000000D730324P540002) and the agreement with the
Moscow Institute of Physics and Technology dated Novem-
ber 1, 2021 No. 70-2021-00138.
References
Azerbayev, Z., Schoelkopf, H., Paster, K., Dos Santos, M.,
McAleer, S., Jiang, A. Q., Deng, J., Biderman, S., and
Welleck, S. Llemma: An open language model for math-
ematics. arXiv preprint arXiv:2310.06786 , 2023.theorem mathd_numbertheory_135
(n A B C : Nat)
(h0: n = 3ˆ17 + 3ˆ10)
(h1: 11 | (n + 1))
(h2: [A,B,C].Pairwise ( · ̸=·))
(h3: {A,B,C} ⊆Finset.Icc 0 9)
(h4: Odd A ∧Odd C)
(h5:¬3 | B)
(h6: Nat.digits 10 n = [B,A,B,C,C,A,C,B,A]) :
100 *A + 10 *B + C = 129 := by
aesop -- general-purpose automatic tactic
theorem mathd_numbertheory_229 :
(5ˆ30) % 7 = 1 := by
decide -- tactic that proves some ""decidable"" goals
-- here it just compute","approaches, Lean4trace is deeply integrated into the Lean elab- orator, allowing us to modify proofs on-the-fly. Leveraging this feature, we propose two methods of data augmentation in Lean: (1) decomposing composite proof steps into multiple simpler steps; (2) testing existing proof automation tactics at each proof state and collecting the successful ones. Models trained on this augmented data are capa- ble of proving 58.0% of theorems from a hold-out subset of Mathlib and 35.6% of the test subset of the MiniF2F benchmark. 1. Introduction One of the advantages of mathematics over other sciences is that the correctness of its results can, in principle, be verified mechanically. This is particularly desirable because the standard peer review process inevitably sometimes re- sults in invalid proofs. However, in practice, formalizing mathematics is a labor-intensive and time-consuming task. The standard approach involves using interactive theorem proving (ITP) systems, among which it is worth mentioning Lean (de Moura et al., 2015), Isabelle (Nipkow et al., 2002), Coq (Barras et al., 1997), and Metamath (Megill & Wheeler, 2019). The process of theorem proving in such a system is similar to programming in an IDE: a user interacts with the system via commands in a formal language, and the system provides feedback on whether the proof is successful. In recent years, significant efforts have been made to sim- plify the formalization process. The most developed li- 1Moscow Institute for Physics and Technology2Yandex 3London Institute for Mathematical Sciences. Correspondence to: Vasilii Nesterov <vas.nesterov63@google.com >. AI for MATH Workshop at ICML 2024 . Copyright 2024 by the author(s)braries of formalized mathematics now contain more than 100,000 theorems. One example is Mathlib (mathlib Com- munity, 2020), a user-maintained mathematical library for the Lean theorem prover, which covers a wide range of mathematical fields. From another perspective, formal theorem pro","Conclusion In this paper, we present Lean4trace , a novel tool for data extraction and augmentation tailored for training neural the- orem provers in Lean. Our experimental results demonstrate that models trained using our dataset achieve a 9% higher performance on the MiniF2F benchmark compared to Re- Prover (Yang et al., 2023), when trained and evaluated under identical conditions. While proposed augmentations pro- vides improvements on Mathlib dataset, they may degrade the model when evaluated on a dataset from different distri- bution. Nevertheless, our tool allows gathering a more com- plete set of proof states in canonical setup and significantly reduces computational resource requirements compared to LeanDojo (Yang et al., 2023), making it feasible to run on a modern PC. We believe that these advancements will lower the barrier to entry in this field, fostering more accessible and widespread research in neural theorem proving. 6.","Lean4trace: Data augmentation for neural theorem proving in Lean Integrating large language models as proof assistants with theorem provers has shown great promise. However, one of the major challenges in this field is the scarcity of training data. To address this, we release a new open-source tool, *Lean4trace*, for training data extraction from Lean 4 sources. Unlike previous approaches, *Lean4trace* is deeply integrated into the Lean elaborator, allowing us to modify proofs on-the-fly. Leveraging this feature, we propose two methods of data augmentation in Lean: (1) decomposing composite proof steps into multiple simpler steps; (2) testing existing proof automation tactics at each proof state and collecting the successful ones. Models trained on this augmented data are capable of proving 58.0% of theorems from a hold-out subset of Mathlib and 35.6% of the test subset of the MiniF2F benchmark.","Lean4trace: Data augmentation for neural theorem proving in Lean Integrating large language models as proof assistants with theorem provers has shown great promise. However, one of the major challenges in this field is the scarcity of training data. To address this, we release a new open-source tool, *Lean4trace*, for training data extraction from Lean 4 sources. Unlike previous approaches, *Lean4trace* is deeply integrated into the Lean elaborator, allowing us to modify proofs on-the-fly. Leveraging this feature, we propose two methods of data augmentation in Lean: (1) decomposing composite proof steps into multiple simpler steps; (2) testing existing proof automation tactics at each proof state and collecting the successful ones. Models trained on this augmented data are capable of proving 58.0% of theorems from a hold-out subset of Mathlib and 35.6% of the test subset of the MiniF2F benchmark. Conclusion In this paper, we present Lean4trace , a novel tool for data extraction and augmentation tailored for training neural the- orem provers in Lean. Our experimental results demonstrate that models trained using our dataset achieve a 9% higher performance on the MiniF2F benchmark compared to Re- Prover (Yang et al., 2023), when trained and evaluated under identical conditions. While proposed augmentations pro- vides improvements on Mathlib dataset, they may degrade the model when evaluated on a dataset from different distri- bution. Nevertheless, our tool allows gathering a more com- plete set of proof states in canonical setup and significantly reduces computational resource requirements compared to LeanDojo (Yang et al., 2023), making it feasible to run on a modern PC. We believe that these advancements will lower the barrier to entry in this field, fostering more accessible and widespread research in neural theorem proving. 6. approaches, Lean4trace is deeply integrated into the Lean elab- orator, allowing us to modify proofs on-the-fly. Leveraging this feature, we propose two methods of data augmentation in Lean: (1) decomposing composite proof steps into multiple simpler steps; (2) testing existing proof automation tactics at each proof state and collecting the successful ones. Models trained on this augmented data are capa- ble of proving 58.0% of theorems from a hold-out subset of Mathlib and 35.6% of the test subset of the MiniF2F benchmark. 1. Introduction One of the advantages of mathematics over other sciences is that the correctness of its results can, in principle, be verified mechanically. This is particularly desirable because the standard peer review process inevitably sometimes re- sults in invalid proofs. However, in practice, formalizing mathematics is a labor-intensive and time-consuming task. The standard approach involves using interactive theorem proving (ITP) systems, among which it is worth mentioning Lean (de Moura et al., 2015), Isabelle (Nipkow et al., 2002), Coq (Barras et al., 1997), and Metamath (Megill & Wheeler, 2019). The process of theorem proving in such a system is similar to programming in an IDE: a user interacts with the system via commands in a formal language, and the system provides feedback on whether the proof is successful. In recent years, significant efforts have been made to sim- plify the formalization process. The most developed li- 1Moscow Institute for Physics and Technology2Yandex 3London Institute for Mathematical Sciences. Correspondence to: Vasilii Nesterov <vas.nesterov63@google.com >. AI for MATH Workshop at ICML 2024 . Copyright 2024 by the author(s)braries of formalized mathematics now contain more than 100,000 theorems. One example is Mathlib (mathlib Com- munity, 2020), a user-maintained mathematical library for the Lean theorem prover, which covers a wide range of mathematical fields. From another perspective, formal theorem pro",0
70ff41de083363020856e5381537edb482990dd0,Efficient Linear System Solver with Transformers,"['Max Vladymyrov', 'Johannes von Oswald', 'Nolan Andrew Miller', 'Mark Sandler']",https://openreview.net/pdf/70ff41de083363020856e5381537edb482990dd0.pdf,"Efficient Linear System Solver with Transformers A novel, efficient transformer-based approach to solving small linear systems. This paper investigates the potential of linear Transformers as solvers for systems of linear equations. We propose a novel approach where the Transformer encodes each equation as a separate token, allowing the model to process the system in a permutation-invariant manner. To enhance generalizability and reduce the parameter count, we introduce a block-wise re-parameterization technique for the attention weight matrices. This technique decouples the problem dimension from the model's parameter count, enabling the Transformer to effectively handle systems of varying sizes. Our experiments demonstrate the Transformer's competitive performance compared to established classical methods such as Conjugate Gradient, especially for systems with smaller sizes. We further explore the model's ability to extrapolate to larger systems, providing evidence for its potential as a versatile and efficient solver for linear equations.",70ff41de083363020856e5381537edb482990dd0.pdf,"approach where the
Transformer encodes each equation as a separate
token, allowing the model to process the system
in a permutation-invariant manner. To enhance
generalizability and reduce the parameter count,
we introduce a block-wise re-parameterization
technique for the attention weight matrices. This
technique decouples the problem dimension from
the model’s parameter count, enabling the Trans-
former to effectively handle systems of varying
sizes. Our experiments demonstrate the Trans-
former’s competitive performance compared to
established classical methods such as Conjugate
Gradient, especially for systems with smaller
sizes. We further explore the model’s ability to
extrapolate to larger systems, providing evidence
for its potential as a versatile and efﬁcient solver
for linear equations.
1. Introduction
Solving linear systems of equations is a fundamental prob-
lem in numerous ﬁelds, including scientiﬁc computing, ma-
chine learning, and engineering. While traditional methods
like Gaussian elimination and iterative solvers like Conju-
gate Gradient (Hestenes et al., 1952) are widely used, ex-
ploring alternative approaches holds the potential for more
efﬁcient and versatile solutions.
Transformers, originally developed for natural language pro-
cessing tasks (Vaswani et al., 2017), have demonstrated a
remarkable ability to capture complex relationships within
sequential data. This ability extends beyond natural lan-
guage processing, as evidenced by their successful applica-
tion in solving a variety of problems, ranging from noisy
linear regression and classiﬁcation (Garg et al., 2022) to the
1Google Research. Correspondence to: Max Vladymyrov
<mxv@google.com >.
The ﬁrst AI for MATH Workshop at the 41st International Confer-
ence on Machine Learning , Vienna, Austria. Copyright 2024 by
the author(s).traveling salesmen problem (Yang et al., 2023) and other
domains (Mirchandani et al., 2023). Transformers have
shown promise in various scientiﬁc computing ta","Conclusions
This paper explored the novel application of linear Trans-
formers for efﬁciently solving small systems of linear equa-
tions with symmetric and positive deﬁnite coefﬁcient ma-
trices. We demonstrated that by encoding each equation
as a distinct token and implementing a block-wise re-
parameterization technique, Transformers could achieve
accuracy comparable to 6-8 iterations of Conjugate Gradi-
ent, while being faster for small problem sizes. Our model
exhibited the ability to generalize beyond its training data,
effectively handling systems of varying sizes and unseen
condition numbers.
This research opens up exciting possibilities for utilizing
Transformers in numerical tasks. Further investigation into
architectural choices, training strategies, and the boundaries
of generalization could lead to the development of even
more efﬁcient and adaptable solvers. This work reinforces
the notion that Transformers, initially designed for natural
language processing, hold remarkable potential as powerful
tools across various scientiﬁc computing domains.
While our approach shows promise, it has several limitations.
Currently, we only handle positive deﬁnite symmetric matri-
ces, which restricts the applicability of our method. Future
work should explore extending this approach to general ma-
trices, including non-symmetric and indeﬁnite cases. Addi-
tionally, our method’s performance on very large systems or
highly ill-conditioned matrices needs further investigation.
4
Efﬁcient Linear System Solver with Transformers
Scaling the approach to handle sparse matrices efﬁciently is
another important direction for future research. Integrating
our Transformer-based solver with classical methods, po-
tentially as a preconditioner or in a hybrid algorithm, could
leverage the strengths of both approaches and is an exciting
area for further study.
References
Ahn, K., Cheng, X., Daneshmand, H., and Sra, S.
Transformers learn to implement preconditioned gra-
dient descent fo","approach where the Transformer encodes each equation as a separate token, allowing the model to process the system in a permutation-invariant manner. To enhance generalizability and reduce the parameter count, we introduce a block-wise re-parameterization technique for the attention weight matrices. This technique decouples the problem dimension from the model s parameter count, enabling the Trans- former to effectively handle systems of varying sizes. Our experiments demonstrate the Trans- former s competitive performance compared to established classical methods such as Conjugate Gradient, especially for systems with smaller sizes. We further explore the model s ability to extrapolate to larger systems, providing evidence for its potential as a versatile and ef cient solver for linear equations. 1. Introduction Solving linear systems of equations is a fundamental prob- lem in numerous elds, including scienti c computing, ma- chine learning, and engineering. While traditional methods like Gaussian elimination and iterative solvers like Conju- gate Gradient (Hestenes et al., 1952) are widely used, ex- ploring alternative approaches holds the potential for more ef cient and versatile solutions. Transformers, originally developed for natural language pro- cessing tasks (Vaswani et al., 2017), have demonstrated a remarkable ability to capture complex relationships within sequential data. This ability extends beyond natural lan- guage processing, as evidenced by their successful applica- tion in solving a variety of problems, ranging from noisy linear regression and classi cation (Garg et al., 2022) to the 1Google Research. Correspondence to: Max Vladymyrov <mxv@google.com >. The rst AI for MATH Workshop at the 41st International Confer- ence on Machine Learning , Vienna, Austria. Copyright 2024 by the author(s).traveling salesmen problem (Yang et al., 2023) and other domains (Mirchandani et al., 2023). Transformers have shown promise in various scienti c computing ta","Conclusions This paper explored the novel application of linear Trans- formers for ef ciently solving small systems of linear equa- tions with symmetric and positive de nite coef cient ma- trices. We demonstrated that by encoding each equation as a distinct token and implementing a block-wise re- parameterization technique, Transformers could achieve accuracy comparable to 6-8 iterations of Conjugate Gradi- ent, while being faster for small problem sizes. Our model exhibited the ability to generalize beyond its training data, effectively handling systems of varying sizes and unseen condition numbers. This research opens up exciting possibilities for utilizing Transformers in numerical tasks. Further investigation into architectural choices, training strategies, and the boundaries of generalization could lead to the development of even more ef cient and adaptable solvers. This work reinforces the notion that Transformers, initially designed for natural language processing, hold remarkable potential as powerful tools across various scienti c computing domains. While our approach shows promise, it has several limitations. Currently, we only handle positive de nite symmetric matri- ces, which restricts the applicability of our method. Future work should explore extending this approach to general ma- trices, including non-symmetric and inde nite cases. Addi- tionally, our method s performance on very large systems or highly ill-conditioned matrices needs further investigation. 4 Ef cient Linear System Solver with Transformers Scaling the approach to handle sparse matrices ef ciently is another important direction for future research. Integrating our Transformer-based solver with classical methods, po- tentially as a preconditioner or in a hybrid algorithm, could leverage the strengths of both approaches and is an exciting area for further study.","Efficient Linear System Solver with Transformers A novel, efficient transformer-based approach to solving small linear systems. This paper investigates the potential of linear Transformers as solvers for systems of linear equations. We propose a novel approach where the Transformer encodes each equation as a separate token, allowing the model to process the system in a permutation-invariant manner. To enhance generalizability and reduce the parameter count, we introduce a block-wise re-parameterization technique for the attention weight matrices. This technique decouples the problem dimension from the model's parameter count, enabling the Transformer to effectively handle systems of varying sizes. Our experiments demonstrate the Transformer's competitive performance compared to established classical methods such as Conjugate Gradient, especially for systems with smaller sizes. We further explore the model's ability to extrapolate to larger systems, providing evidence for its potential as a versatile and efficient solver for linear equations.","Efficient Linear System Solver with Transformers A novel, efficient transformer-based approach to solving small linear systems. This paper investigates the potential of linear Transformers as solvers for systems of linear equations. We propose a novel approach where the Transformer encodes each equation as a separate token, allowing the model to process the system in a permutation-invariant manner. To enhance generalizability and reduce the parameter count, we introduce a block-wise re-parameterization technique for the attention weight matrices. This technique decouples the problem dimension from the model's parameter count, enabling the Transformer to effectively handle systems of varying sizes. Our experiments demonstrate the Transformer's competitive performance compared to established classical methods such as Conjugate Gradient, especially for systems with smaller sizes. We further explore the model's ability to extrapolate to larger systems, providing evidence for its potential as a versatile and efficient solver for linear equations. Conclusions This paper explored the novel application of linear Trans- formers for ef ciently solving small systems of linear equa- tions with symmetric and positive de nite coef cient ma- trices. We demonstrated that by encoding each equation as a distinct token and implementing a block-wise re- parameterization technique, Transformers could achieve accuracy comparable to 6-8 iterations of Conjugate Gradi- ent, while being faster for small problem sizes. Our model exhibited the ability to generalize beyond its training data, effectively handling systems of varying sizes and unseen condition numbers. This research opens up exciting possibilities for utilizing Transformers in numerical tasks. Further investigation into architectural choices, training strategies, and the boundaries of generalization could lead to the development of even more ef cient and adaptable solvers. This work reinforces the notion that Transformers, initially designed for natural language processing, hold remarkable potential as powerful tools across various scienti c computing domains. While our approach shows promise, it has several limitations. Currently, we only handle positive de nite symmetric matri- ces, which restricts the applicability of our method. Future work should explore extending this approach to general ma- trices, including non-symmetric and inde nite cases. Addi- tionally, our method s performance on very large systems or highly ill-conditioned matrices needs further investigation. 4 Ef cient Linear System Solver with Transformers Scaling the approach to handle sparse matrices ef ciently is another important direction for future research. Integrating our Transformer-based solver with classical methods, po- tentially as a preconditioner or in a hybrid algorithm, could leverage the strengths of both approaches and is an exciting area for further study. approach where the Transformer encodes each equation as a separate token, allowing the model to process the system in a permutation-invariant manner. To enhance generalizability and reduce the parameter count, we introduce a block-wise re-parameterization technique for the attention weight matrices. This technique decouples the problem dimension from the model s parameter count, enabling the Trans- former to effectively handle systems of varying sizes. Our experiments demonstrate the Trans- former s competitive performance compared to established classical methods such as Conjugate Gradient, especially for systems with smaller sizes. We further explore the model s ability to extrapolate to larger systems, providing evidence for its potential as a versatile and ef cient solver for linear equations. 1. Introduction Solving linear systems of equations is a fundamental prob- lem in numerous elds, including scienti c computing, ma- chine learning, and engineering. While traditional methods like Gaussian elimination and iterative solvers like Conju- gate Gradient (Hestenes et al., 1952) are widely used, ex- ploring alternative approaches holds the potential for more ef cient and versatile solutions. Transformers, originally developed for natural language pro- cessing tasks (Vaswani et al., 2017), have demonstrated a remarkable ability to capture complex relationships within sequential data. This ability extends beyond natural lan- guage processing, as evidenced by their successful applica- tion in solving a variety of problems, ranging from noisy linear regression and classi cation (Garg et al., 2022) to the 1Google Research. Correspondence to: Max Vladymyrov <mxv@google.com >. The rst AI for MATH Workshop at the 41st International Confer- ence on Machine Learning , Vienna, Austria. Copyright 2024 by the author(s).traveling salesmen problem (Yang et al., 2023) and other domains (Mirchandani et al., 2023). Transformers have shown promise in various scienti c computing ta",0
0afed850bcc759f25fe19106cf23af0f53a7d57d,Large Language Models Can Self-Correct with Minimal Effort,"['Zhenyu Wu', 'Qingkai Zeng', 'Zhihan Zhang', 'Zhaoxuan Tan', 'Chao Shen', 'Meng Jiang']",https://openreview.net/pdf/0afed850bcc759f25fe19106cf23af0f53a7d57d.pdf,"Large Language Models Can Self-Correct with Minimal Effort Unleash inherent capabilities of LLMs to detect and rectify incorrect responses without external feedback. Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple yet effective verification method can unleash inherent capabilities of the LLMs. That is to mask a key condition in the question, add the current response to construct a verification question, and predict the condition to verify the response. The condition can be an entity in an open-domain question or a numeric value in a math question, which requires minimal effort (via prompting) to identify. We propose an iterative verify-then-correct framework to progressively identify and correct (probably) false responses, named ProCo. We conduct experiments on three reasoning tasks. On average, ProCo, with GPT-3.5-Turbo as the backend LLM, yields exact match on four open-domain question answering datasets, accuracy on three arithmetic reasoning datasets, and accuracy on a commonsense reasoning dataset, compared to Self-Correct.",0afed850bcc759f25fe19106cf23af0f53a7d57d.pdf,"approaches have
employed scalar reward functions as an alternative. For in-
stance, Rainer (Liu et al., 2022) used reinforcement learning
to generate contextual relevant knowledge in response to
queries. Self-Correction (Welleck et al., 2023) trained a cor-
rector to iteratively correct imperfect outputs. Other sources,
such as compilers (Chen et al., 2024) or search engines (Yu
et al., 2023b) can provide domain-specific feedback.
Recent research used LLMs to generate feedback. Self-
Correct (Kim et al., 2023) and Self-Refine (Madaan et al.,
2023) utilized LLMs to verify and refine their initial out-
puts. However, Huang et al. questioned the intrinsic self-
correcting capability of LLMs, indicating that without ex-
ternal feedback, LLMs struggle to correct their previous
responses. To unleash inherent capabilities of LLMs to
detect and rectify incorrect responses without external feed-
back, we introduce substitute verification . By providing
natural language feedback based on verification results, we
can steer LLMs away from incorrect answers, thus enhanc-
ing their performance in various reasoning tasks.
Verify Correctness of LLM Output Several studies
trained or fine-tuned language models to check the correct-
Numeric ValueKeith has 20 books. Jason has 21 books. How many 
books do they have together?Arithmetic Question
EntityWhen is the last time the minnesota  vikings  have 
been in the playoffs?Open -domain Question
ConceptWhat could happen to a paper  if you leave it outside 
even if it does not move?Commonsense QuestionFigure 2: Key conditions in complex reasoning tasks play
a crucial role in the problem-solving process. These condi-
tions can take various forms: a numeric value in arithmetic
questions, an entity in open-domain questions, or a concept
in commonsense questions.
ness of answers. Cobbe et al. fine-tuned GPT-3 as a verifier
to judge the correctness of solutions. Li et al. fine-tuned
DeBERTa-v3-large (He et al., 2021) to predict the proba-
bility","conclusions or judgements
(Huang & Chang, 2023). People have been exploiting and
improving the reasoning ability of large language mod-
els (LLMs). Wei et al. proposed chain-of-thought (CoT)
prompting and yielded promising results on several reason-
ing tasks, such as arithmetic reasoning (Kojima et al., 2022;
Zhou et al., 2023), commonsense reasoning (Wei et al.,
2022; Zhang et al., 2023; Wang et al., 2023b), and open-
domain question answering (Wang et al., 2023a), using only
1School of Cyber Science and Engineering, Xi’an Jiaotong Uni-
versity, China2Department of Computer Science and Engineering,
University of Notre Dame, IN, USA. Correspondence to: Chao
Shen<chaoshen@xjtu.edu.cn >.
The first AI for MATH Workshop at the 41stInternational Confer-
ence on Machine Learning , Vienna, Austria. Copyright 2024 by
the author(s).Method NQ CSQA AQuA
CoT 40.3 72 .9 51 .3
Self-Correct 40.1 65.9 48.7
PROCO(Ours) 48.0 75.5 65.2
Table 1: Performance comparison of different prompting
methods using GPT-3.5-Turbo as backend LLM.
a few or no reasoning exemplars. CoT guides LLMs to
generate intermediate reasoning steps instead of generating
the final answer directly, which helps the LLMs simulate
the human-like reasoning process.
Although CoT enables LLMs to handle some complex rea-
soning examples, it remains vulnerable to the negative im-
pact of individual errors in each step. Specifically, even a
minor error in one step can alter the trajectory of the en-
tire reasoning process, ultimately leading to an incorrect
conclusion. To address this issue, Dhuliawala et al.; Kim
et al. have explored the verification and correction on the
responses. For example, as shown in Figure 1 a, for a given
question and its initial LLM-generated answer, Self-Correct
(Kim et al., 2023) first instructs the LLM to criticize its
generated answer using the hint: “ Review previous answer
and find mistakes ”. Then, Self-Correct instructs the LLM to
refine initial answers based on the critique.
However, r","approaches have employed scalar reward functions as an alternative. For in- stance, Rainer (Liu et al., 2022) used reinforcement learning to generate contextual relevant knowledge in response to queries. Self-Correction (Welleck et al., 2023) trained a cor- rector to iteratively correct imperfect outputs. Other sources, such as compilers (Chen et al., 2024) or search engines (Yu et al., 2023b) can provide domain-specific feedback. Recent research used LLMs to generate feedback. Self- Correct (Kim et al., 2023) and Self-Refine (Madaan et al., 2023) utilized LLMs to verify and refine their initial out- puts. However, Huang et al. questioned the intrinsic self- correcting capability of LLMs, indicating that without ex- ternal feedback, LLMs struggle to correct their previous responses. To unleash inherent capabilities of LLMs to detect and rectify incorrect responses without external feed- back, we introduce substitute verification . By providing natural language feedback based on verification results, we can steer LLMs away from incorrect answers, thus enhanc- ing their performance in various reasoning tasks. Verify Correctness of LLM Output Several studies trained or fine-tuned language models to check the correct- Numeric ValueKeith has 20 books. Jason has 21 books. How many books do they have together?Arithmetic Question EntityWhen is the last time the minnesota vikings have been in the playoffs?Open -domain Question ConceptWhat could happen to a paper if you leave it outside even if it does not move?Commonsense QuestionFigure 2: Key conditions in complex reasoning tasks play a crucial role in the problem-solving process. These condi- tions can take various forms: a numeric value in arithmetic questions, an entity in open-domain questions, or a concept in commonsense questions. ness of answers. Cobbe et al. fine-tuned GPT-3 as a verifier to judge the correctness of solutions. Li et al. fine-tuned DeBERTa-v3-large (He et al., 2021) to predict the proba- bility","conclusions or judgements (Huang & Chang, 2023). People have been exploiting and improving the reasoning ability of large language mod- els (LLMs). Wei et al. proposed chain-of-thought (CoT) prompting and yielded promising results on several reason- ing tasks, such as arithmetic reasoning (Kojima et al., 2022; Zhou et al., 2023), commonsense reasoning (Wei et al., 2022; Zhang et al., 2023; Wang et al., 2023b), and open- domain question answering (Wang et al., 2023a), using only 1School of Cyber Science and Engineering, Xi an Jiaotong Uni- versity, China2Department of Computer Science and Engineering, University of Notre Dame, IN, USA. Correspondence to: Chao Shen<chaoshen@xjtu.edu.cn >. The first AI for MATH Workshop at the 41stInternational Confer- ence on Machine Learning , Vienna, Austria. Copyright 2024 by the author(s).Method NQ CSQA AQuA CoT 40.3 72 .9 51 .3 Self-Correct 40.1 65.9 48.7 PROCO(Ours) 48.0 75.5 65.2 Table 1: Performance comparison of different prompting methods using GPT-3.5-Turbo as backend LLM. a few or no reasoning exemplars. CoT guides LLMs to generate intermediate reasoning steps instead of generating the final answer directly, which helps the LLMs simulate the human-like reasoning process. Although CoT enables LLMs to handle some complex rea- soning examples, it remains vulnerable to the negative im- pact of individual errors in each step. Specifically, even a minor error in one step can alter the trajectory of the en- tire reasoning process, ultimately leading to an incorrect conclusion. To address this issue, Dhuliawala et al.; Kim et al. have explored the verification and correction on the responses. For example, as shown in Figure 1 a, for a given question and its initial LLM-generated answer, Self-Correct (Kim et al., 2023) first instructs the LLM to criticize its generated answer using the hint: Review previous answer and find mistakes . Then, Self-Correct instructs the LLM to refine initial answers based on the critique. However, r","Large Language Models Can Self-Correct with Minimal Effort Unleash inherent capabilities of LLMs to detect and rectify incorrect responses without external feedback. Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple yet effective verification method can unleash inherent capabilities of the LLMs. That is to mask a key condition in the question, add the current response to construct a verification question, and predict the condition to verify the response. The condition can be an entity in an open-domain question or a numeric value in a math question, which requires minimal effort (via prompting) to identify. We propose an iterative verify-then-correct framework to progressively identify and correct (probably) false responses, named ProCo. We conduct experiments on three reasoning tasks. On average, ProCo, with GPT-3.5-Turbo as the backend LLM, yields exact match on four open-domain question answering datasets, accuracy on three arithmetic reasoning datasets, and accuracy on a commonsense reasoning dataset, compared to Self-Correct.","Large Language Models Can Self-Correct with Minimal Effort Unleash inherent capabilities of LLMs to detect and rectify incorrect responses without external feedback. Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple yet effective verification method can unleash inherent capabilities of the LLMs. That is to mask a key condition in the question, add the current response to construct a verification question, and predict the condition to verify the response. The condition can be an entity in an open-domain question or a numeric value in a math question, which requires minimal effort (via prompting) to identify. We propose an iterative verify-then-correct framework to progressively identify and correct (probably) false responses, named ProCo. We conduct experiments on three reasoning tasks. On average, ProCo, with GPT-3.5-Turbo as the backend LLM, yields exact match on four open-domain question answering datasets, accuracy on three arithmetic reasoning datasets, and accuracy on a commonsense reasoning dataset, compared to Self-Correct. conclusions or judgements (Huang & Chang, 2023). People have been exploiting and improving the reasoning ability of large language mod- els (LLMs). Wei et al. proposed chain-of-thought (CoT) prompting and yielded promising results on several reason- ing tasks, such as arithmetic reasoning (Kojima et al., 2022; Zhou et al., 2023), commonsense reasoning (Wei et al., 2022; Zhang et al., 2023; Wang et al., 2023b), and open- domain question answering (Wang et al., 2023a), using only 1School of Cyber Science and Engineering, Xi an Jiaotong Uni- versity, China2Department of Computer Science and Engineering, University of Notre Dame, IN, USA. Correspondence to: Chao Shen<chaoshen@xjtu.edu.cn >. The first AI for MATH Workshop at the 41stInternational Confer- ence on Machine Learning , Vienna, Austria. Copyright 2024 by the author(s).Method NQ CSQA AQuA CoT 40.3 72 .9 51 .3 Self-Correct 40.1 65.9 48.7 PROCO(Ours) 48.0 75.5 65.2 Table 1: Performance comparison of different prompting methods using GPT-3.5-Turbo as backend LLM. a few or no reasoning exemplars. CoT guides LLMs to generate intermediate reasoning steps instead of generating the final answer directly, which helps the LLMs simulate the human-like reasoning process. Although CoT enables LLMs to handle some complex rea- soning examples, it remains vulnerable to the negative im- pact of individual errors in each step. Specifically, even a minor error in one step can alter the trajectory of the en- tire reasoning process, ultimately leading to an incorrect conclusion. To address this issue, Dhuliawala et al.; Kim et al. have explored the verification and correction on the responses. For example, as shown in Figure 1 a, for a given question and its initial LLM-generated answer, Self-Correct (Kim et al., 2023) first instructs the LLM to criticize its generated answer using the hint: Review previous answer and find mistakes . Then, Self-Correct instructs the LLM to refine initial answers based on the critique. However, r approaches have employed scalar reward functions as an alternative. For in- stance, Rainer (Liu et al., 2022) used reinforcement learning to generate contextual relevant knowledge in response to queries. Self-Correction (Welleck et al., 2023) trained a cor- rector to iteratively correct imperfect outputs. Other sources, such as compilers (Chen et al., 2024) or search engines (Yu et al., 2023b) can provide domain-specific feedback. Recent research used LLMs to generate feedback. Self- Correct (Kim et al., 2023) and Self-Refine (Madaan et al., 2023) utilized LLMs to verify and refine their initial out- puts. However, Huang et al. questioned the intrinsic self- correcting capability of LLMs, indicating that without ex- ternal feedback, LLMs struggle to correct their previous responses. To unleash inherent capabilities of LLMs to detect and rectify incorrect responses without external feed- back, we introduce substitute verification . By providing natural language feedback based on verification results, we can steer LLMs away from incorrect answers, thus enhanc- ing their performance in various reasoning tasks. Verify Correctness of LLM Output Several studies trained or fine-tuned language models to check the correct- Numeric ValueKeith has 20 books. Jason has 21 books. How many books do they have together?Arithmetic Question EntityWhen is the last time the minnesota vikings have been in the playoffs?Open -domain Question ConceptWhat could happen to a paper if you leave it outside even if it does not move?Commonsense QuestionFigure 2: Key conditions in complex reasoning tasks play a crucial role in the problem-solving process. These condi- tions can take various forms: a numeric value in arithmetic questions, an entity in open-domain questions, or a concept in commonsense questions. ness of answers. Cobbe et al. fine-tuned GPT-3 as a verifier to judge the correctness of solutions. Li et al. fine-tuned DeBERTa-v3-large (He et al., 2021) to predict the proba- bility",0
8b3bf32d71f7ec91ebdcfd798094623345755e15,Teaching Large Language Models to Reason with Reinforcement Learning,"['Alexander Havrilla', 'Yuqing Du', 'Sharath Chandra Raparthy', 'Christoforos Nalmpantis', 'Jane Dwivedi-Yu', 'Eric Hambro', 'Sainbayar Sukhbaatar', 'Roberta Raileanu']",https://openreview.net/pdf/8b3bf32d71f7ec91ebdcfd798094623345755e15.pdf,"Teaching Large Language Models to Reason with Reinforcement Learning We compare the performance of multiple RL algorithms across multiple setups for improving LLM reasoning Reinforcement Learning from Human Feedback (\textbf{RLHF}) has emerged as a dominant approach for aligning LLM outputs with human preferences. Inspired by the success of RLHF, we study the performance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization (\textbf{PPO}), Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model. We additionally start from multiple initializations with and without supervised fine-tuning (\textbf{SFT}) data. Overall, we find models fine-tuned with Expert Iteration to consistently achieve the highest task accuracy with PPO and RCRL close behind. Surprisingly, the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of samples to converge from a pretrained checkpoint. We investigate why this is the case, concluding that during RL training models fail to explore significantly beyond solutions already produced by SFT models. Additionally, we discuss a trade off between maj@1 and pass@96 metric performance during SFT training and how conversely RL training improves both simultaneously. We then conclude by discussing the implications of our findings for RLHF and the future role of RL in LLM fine-tuning.",8b3bf32d71f7ec91ebdcfd798094623345755e15.pdf,"approach for
aligning LLM outputs with human preferences.
Inspired by the success of RLHF, we study the per-
formance of multiple algorithms that learn from
feedback (Expert Iteration, Proximal Policy Op-
timization ( PPO ), Return-Conditioned RL) on
improving LLM reasoning capabilities. We in-
vestigate both sparse and dense rewards provided
to the LLM both heuristically and via a learned
reward model. We additionally start from mul-
tiple initializations with and without supervised
fine-tuning ( SFT) data. Overall, we find models
fine-tuned with Expert Iteration to consistently
achieve the highest task accuracy with PPO and
RCRL close behind. Surprisingly, the sample
complexity of Expert Iteration is similar to that
of PPO, requiring at most on the order of 106
samples to converge from a pretrained checkpoint.
We investigate why this is the case, concluding
that during RL training models fail to explore sig-
nificantly beyond solutions already produced by
SFT models. Additionally, we discuss a trade
off between maj@1 and pass@96 metric perfor-
mance during SFT training and how conversely
RL training improves both simultaneously. We
then conclude by discussing the implications of
our findings for RLHF and the future role of RL
in LLM fine-tuning.
1. Introduction
The reasoning abilities of large language models ( LLMs )
are rapidly improving as measured by their performance on
numerous math, science and code benchmarks (Cobbe et al.,
2021; Hendrycks et al., 2021b; Sawada et al., 2023; Liang
*Equal contribution1Meta2Georgia Tech3StabilityAI
4Anthropic. Correspondence to: Alex Havrilla
<ahavrilla3@gatech.edu >.
The first AI for MATH Workshop at the 41st International Confer-
ence on Machine Learning, Vienna, Austria. Copyright 2024 by
the author(s)et al., 2022; Srivastava et al., 2022; Rein et al., 2023; Mialon
et al., 2023; Chollet, 2019; Mishra et al., 2022; Hendrycks
et al., 2021a; Austin et al., 2021; Patel et al., 2021; Gao et al.,
2021). Simultaneously, Reinforc","future work.
Figure 16. Scores of synthetically backwards generated (Q, A )
pairs. Note: the score refers to the percentage of times the forward
student model MQ→Arecovers the intended final answer.
Question ”A school of 100 musicians goes on a skiing
trip. 40% are beginners, 30% are interme-
diate, and 50% are advanced. How many
people went on the skiing trip?”
Answer ”There are 100 * 0.4 = 40 beginner skiiers.
There are 100 * 0.3 = 30 intermediate ski-
iers. There are 100 * 0.5 = 50 advanced
skiiers. Therefore there are 40 + 30 + 50 =
120 skiiers total.”
F. RCRL Step-label Generating Process
Another natural candidate which could be used to iden-
tify mistakes at each step is a Process Based Reward
Model (PRM) (Lightman et al., 2023). A PRM es-
timates the probability of correctness of a step Si,
p(Sicorrect |Q, S 1, S2, ..., S i)independently of its im-
pact on the final answer. However, this would be expen-
sive, requiring collecting human annotated samples. Instead,
we propose to approximate the optimal value function V∗
of the reasoning task. V∗corresponds to the value func-
tion of the optimal policy which is able to successfully
solve the reasoning task from any logically valid interme-
diate state Sj. Such an optimal value function would have
V∗(Q, S 1, ..., S i) = 1 for a solution prefix with no mis-
takes, and V∗(Q, S 1, ..., S i) = 0 if the prefix already con-
tains a mistake which will result in an incorrect final an-
swer. Note however, V∗does not exactly correspond to
a PRM. This is because a partial solution S1, ..., S iwith
a mistake at step j̸=iand valid terminal step Siwill
haveV∗(Q, S 1, ..., S i) = 0 andPRM (Q, S 1, ..., S i) = 1 .
To make this distinction clear, we call models we train to
18
RL for LLM Reasoning
Figure 17. Comparison of reward curves for PPO architecture abla-
tions. Using both gradient stopping and a larger value head works
best.
directly approximate V∗stepwise ORMs or SORMs .
G. PPO Architecture Ablations
H. CommonsenseQA Benc","approach for aligning LLM outputs with human preferences. Inspired by the success of RLHF, we study the per- formance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Op- timization ( PPO ), Return-Conditioned RL) on improving LLM reasoning capabilities. We in- vestigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model. We additionally start from mul- tiple initializations with and without supervised fine-tuning ( SFT) data. Overall, we find models fine-tuned with Expert Iteration to consistently achieve the highest task accuracy with PPO and RCRL close behind. Surprisingly, the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of 106 samples to converge from a pretrained checkpoint. We investigate why this is the case, concluding that during RL training models fail to explore sig- nificantly beyond solutions already produced by SFT models. Additionally, we discuss a trade off between maj@1 and pass@96 metric perfor- mance during SFT training and how conversely RL training improves both simultaneously. We then conclude by discussing the implications of our findings for RLHF and the future role of RL in LLM fine-tuning. 1. Introduction The reasoning abilities of large language models ( LLMs ) are rapidly improving as measured by their performance on numerous math, science and code benchmarks (Cobbe et al., 2021; Hendrycks et al., 2021b; Sawada et al., 2023; Liang *Equal contribution1Meta2Georgia Tech3StabilityAI 4Anthropic. Correspondence to: Alex Havrilla <ahavrilla3@gatech.edu >. The first AI for MATH Workshop at the 41st International Confer- ence on Machine Learning, Vienna, Austria. Copyright 2024 by the author(s)et al., 2022; Srivastava et al., 2022; Rein et al., 2023; Mialon et al., 2023; Chollet, 2019; Mishra et al., 2022; Hendrycks et al., 2021a; Austin et al., 2021; Patel et al., 2021; Gao et al., 2021). Simultaneously, Reinforc","future work. Figure 16. Scores of synthetically backwards generated (Q, A ) pairs. Note: the score refers to the percentage of times the forward student model MQ Arecovers the intended final answer. Question A school of 100 musicians goes on a skiing trip. 40% are beginners, 30% are interme- diate, and 50% are advanced. How many people went on the skiing trip? Answer There are 100 * 0.4 = 40 beginner skiiers. There are 100 * 0.3 = 30 intermediate ski- iers. There are 100 * 0.5 = 50 advanced skiiers. Therefore there are 40 + 30 + 50 = 120 skiiers total. F. RCRL Step-label Generating Process Another natural candidate which could be used to iden- tify mistakes at each step is a Process Based Reward Model (PRM) (Lightman et al., 2023). A PRM es- timates the probability of correctness of a step Si, p(Sicorrect |Q, S 1, S2, ..., S i)independently of its im- pact on the final answer. However, this would be expen- sive, requiring collecting human annotated samples. Instead, we propose to approximate the optimal value function V of the reasoning task. V corresponds to the value func- tion of the optimal policy which is able to successfully solve the reasoning task from any logically valid interme- diate state Sj. Such an optimal value function would have V (Q, S 1, ..., S i) = 1 for a solution prefix with no mis- takes, and V (Q, S 1, ..., S i) = 0 if the prefix already con- tains a mistake which will result in an incorrect final an- swer. Note however, V does not exactly correspond to a PRM. This is because a partial solution S1, ..., S iwith a mistake at step j =iand valid terminal step Siwill haveV (Q, S 1, ..., S i) = 0 andPRM (Q, S 1, ..., S i) = 1 . To make this distinction clear, we call models we train to 18 RL for LLM Reasoning Figure 17. Comparison of reward curves for PPO architecture abla- tions. Using both gradient stopping and a larger value head works best. directly approximate V stepwise ORMs or SORMs . G. PPO Architecture Ablations H. CommonsenseQA Benc","Teaching Large Language Models to Reason with Reinforcement Learning We compare the performance of multiple RL algorithms across multiple setups for improving LLM reasoning Reinforcement Learning from Human Feedback (\textbf{RLHF}) has emerged as a dominant approach for aligning LLM outputs with human preferences. Inspired by the success of RLHF, we study the performance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization (\textbf{PPO}), Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model. We additionally start from multiple initializations with and without supervised fine-tuning (\textbf{SFT}) data. Overall, we find models fine-tuned with Expert Iteration to consistently achieve the highest task accuracy with PPO and RCRL close behind. Surprisingly, the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of samples to converge from a pretrained checkpoint. We investigate why this is the case, concluding that during RL training models fail to explore significantly beyond solutions already produced by SFT models. Additionally, we discuss a trade off between maj@1 and pass@96 metric performance during SFT training and how conversely RL training improves both simultaneously. We then conclude by discussing the implications of our findings for RLHF and the future role of RL in LLM fine-tuning.","Teaching Large Language Models to Reason with Reinforcement Learning We compare the performance of multiple RL algorithms across multiple setups for improving LLM reasoning Reinforcement Learning from Human Feedback (\textbf{RLHF}) has emerged as a dominant approach for aligning LLM outputs with human preferences. Inspired by the success of RLHF, we study the performance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization (\textbf{PPO}), Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model. We additionally start from multiple initializations with and without supervised fine-tuning (\textbf{SFT}) data. Overall, we find models fine-tuned with Expert Iteration to consistently achieve the highest task accuracy with PPO and RCRL close behind. Surprisingly, the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of samples to converge from a pretrained checkpoint. We investigate why this is the case, concluding that during RL training models fail to explore significantly beyond solutions already produced by SFT models. Additionally, we discuss a trade off between maj@1 and pass@96 metric performance during SFT training and how conversely RL training improves both simultaneously. We then conclude by discussing the implications of our findings for RLHF and the future role of RL in LLM fine-tuning. future work. Figure 16. Scores of synthetically backwards generated (Q, A ) pairs. Note: the score refers to the percentage of times the forward student model MQ Arecovers the intended final answer. Question A school of 100 musicians goes on a skiing trip. 40% are beginners, 30% are interme- diate, and 50% are advanced. How many people went on the skiing trip? Answer There are 100 * 0.4 = 40 beginner skiiers. There are 100 * 0.3 = 30 intermediate ski- iers. There are 100 * 0.5 = 50 advanced skiiers. Therefore there are 40 + 30 + 50 = 120 skiiers total. F. RCRL Step-label Generating Process Another natural candidate which could be used to iden- tify mistakes at each step is a Process Based Reward Model (PRM) (Lightman et al., 2023). A PRM es- timates the probability of correctness of a step Si, p(Sicorrect |Q, S 1, S2, ..., S i)independently of its im- pact on the final answer. However, this would be expen- sive, requiring collecting human annotated samples. Instead, we propose to approximate the optimal value function V of the reasoning task. V corresponds to the value func- tion of the optimal policy which is able to successfully solve the reasoning task from any logically valid interme- diate state Sj. Such an optimal value function would have V (Q, S 1, ..., S i) = 1 for a solution prefix with no mis- takes, and V (Q, S 1, ..., S i) = 0 if the prefix already con- tains a mistake which will result in an incorrect final an- swer. Note however, V does not exactly correspond to a PRM. This is because a partial solution S1, ..., S iwith a mistake at step j =iand valid terminal step Siwill haveV (Q, S 1, ..., S i) = 0 andPRM (Q, S 1, ..., S i) = 1 . To make this distinction clear, we call models we train to 18 RL for LLM Reasoning Figure 17. Comparison of reward curves for PPO architecture abla- tions. Using both gradient stopping and a larger value head works best. directly approximate V stepwise ORMs or SORMs . G. PPO Architecture Ablations H. CommonsenseQA Benc approach for aligning LLM outputs with human preferences. Inspired by the success of RLHF, we study the per- formance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Op- timization ( PPO ), Return-Conditioned RL) on improving LLM reasoning capabilities. We in- vestigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model. We additionally start from mul- tiple initializations with and without supervised fine-tuning ( SFT) data. Overall, we find models fine-tuned with Expert Iteration to consistently achieve the highest task accuracy with PPO and RCRL close behind. Surprisingly, the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of 106 samples to converge from a pretrained checkpoint. We investigate why this is the case, concluding that during RL training models fail to explore sig- nificantly beyond solutions already produced by SFT models. Additionally, we discuss a trade off between maj@1 and pass@96 metric perfor- mance during SFT training and how conversely RL training improves both simultaneously. We then conclude by discussing the implications of our findings for RLHF and the future role of RL in LLM fine-tuning. 1. Introduction The reasoning abilities of large language models ( LLMs ) are rapidly improving as measured by their performance on numerous math, science and code benchmarks (Cobbe et al., 2021; Hendrycks et al., 2021b; Sawada et al., 2023; Liang *Equal contribution1Meta2Georgia Tech3StabilityAI 4Anthropic. Correspondence to: Alex Havrilla <ahavrilla3@gatech.edu >. The first AI for MATH Workshop at the 41st International Confer- ence on Machine Learning, Vienna, Austria. Copyright 2024 by the author(s)et al., 2022; Srivastava et al., 2022; Rein et al., 2023; Mialon et al., 2023; Chollet, 2019; Mishra et al., 2022; Hendrycks et al., 2021a; Austin et al., 2021; Patel et al., 2021; Gao et al., 2021). Simultaneously, Reinforc",0
1843f03c96256409a143dfdb23c8318c0813895d,AI for an inverse problem: Physical model solving quantum gravity,"['Koji Hashimoto', 'Koshiro Matsuo', 'Masaki Murata', 'Gakuto Ogiwara', 'Daichi Takeda']",https://openreview.net/pdf/1843f03c96256409a143dfdb23c8318c0813895d.pdf,"AI for an inverse problem: Physical model solving quantum gravity Mathematical inverse problems of determining a governing differential equation for given solution data remain a fundamental challenge. To find a working example of AI for math, we provide a concrete example using a physical setup of a quantum gravity problem. We present a novel sparse Neural Network (NN) model which is interpretable, to solve the inverse problem: the AdS/CFT correspondence. According to the conjectured correspondence, a special condensed matter system on a ring is equivalent to a gravity system on a bulk disk. The inverse problem is to reconstruct the higher-dimensional gravity metric from the data of the condensed matter system. We use the response functions of a condensed matter system as our data, and by supervised machine learning, we successfully train the neural network which is equivalent to a scalar field equation on an emergent geometry of the bulk spacetime. The developed method may work as a ground for generic bulk reconstruction, i.e. a solution to the inverse problem of the AdS/CFT correspondence. From a technical perspective, to achieve better numerical control, our neural network model incorporates a novel layer that implements the Runge-Kutta method.",1843f03c96256409a143dfdb23c8318c0813895d.pdf,"approach this question, we set up a con-
crete physical model concerning the long-standing problem
of quantum gravity. The most promising formulation of
quantum gravity is to use the AdS/CFT correspondence
(Maldacena, 1999), which is a conjecture that two physical
quantum theories, conformal field theory (CFT) and gravity
theory, are equivalent to each other. Unfortunately, there
has been no proof of the conjecture, but there are a lot of
working examples. The principal concern against this con-
jecture is that for a given CFT data, there is no constructive
way to find a background geometry in the dual equivalent
gravity theory. This is precisely the inverse problem which
we stated above: CFT provides a boundary data, and the
issue is to find a differential equation in a curved geometry
(which is unknown) whose solution is consistent with the
boundary data.
A possible solution to this inverse problem (widely known
as “bulk reconstruction program” in the field of theoretical
particle physics) can be a ground-breaking path to a proof
of the conjecture, and furthermore, a working example for
AI for math of inverse problems.
The best and the simplest setup for solving this inverse prob-
lem of the AdS/CFT correspondence is with a low-enough
spacetime dimensions and the simplest matter content in
the bulk spacetime, thus we follow the situation provided in
1
AI for an inverse problem: Physical model solving quantum gravity
(Hashimoto et al., 2023) where the CFT (material quantum
theory) lives on a one-dimensional circle while the corre-
sponding gravity background is a disk which is rotationally
symmetric. We follow the method developed in (Hashimoto
et al., 2018a) which replaces the bulk differential equation
with a sparse neural network securing the interpretability
by regarding the weights as a metric on the curved space-
time.1With this setup, if we find what kind of differential
equations are emergent and what are not, it would be a great
step for proving the AdS/CF","discussions. The work of K. H., K. M., M. M. and
G. O. was supported in part by JSPS KAKENHI Grant
Nos. JP22H01217, JP22H05111, and JP22H05115. The
work of D. T. was supported in part by Grant-in-Aid for
JSPS Fellows No. 22KJ1944.
References
Akutagawa, T., Hashimoto, K., and Sumimoto, T. Deep
learning and ads/qcd. Physical Review D , 102(2):026020,
2020.
Cai, T., Merz, G. W., Charton, F., Nolte, N., Wilhelm,
M., Cranmer, K., and Dixon, L. J. Transforming the
bootstrap: Using transformers to compute scattering am-
plitudes in planar n= 4 super yang-mills theory. arXiv
preprint arXiv:2405.06107 , 2024.
Chakraborty, S. Boundary Terms of the Einstein–Hilbert
Action , pp. 43–59. Springer International Publishing,
Cham, 2017.
Charton, F. Linear algebra with transformers. arXiv preprint
arXiv:2112.01898 , 2021.
Chen, R. T., Rubanova, Y ., Bettencourt, J., and Duvenaud,
D. K. Neural ordinary differential equations. Advances
in neural information processing systems , 31, 2018.
Cybenko, G. Approximation by superpositions of a sig-
moidal function. Mathematics of control, signals and
systems , 2(4):303–314, 1989.
Hashimoto, K. Ads/cft correspondence as a deep boltzmann
machine. Physical Review D , 99(10):106017, 2019.
Hashimoto, K., Sugishita, S., Tanaka, A., and Tomiya, A.
Deep learning and the ads/cft correspondence. Physical
Review D , 98(4):046019, 2018a.
Hashimoto, K., Sugishita, S., Tanaka, A., and Tomiya, A.
Deep learning and holographic qcd. Physical Review D ,
98(10):106014, 2018b.
Hashimoto, K., Hu, H.-Y ., and You, Y .-Z. Neural ordinary
differential equation and holographic quantum chromo-
dynamics. Machine Learning: Science and Technology ,
2(3):035011, 2021.
9
AI for an inverse problem: Physical model solving quantum gravity
Hashimoto, K., Takeda, D., Tanaka, K., and Yonezawa,
S. Spacetime-emergent ring toward tabletop quantum
gravity experiments. Physical Review Research , 5(2):
023168, 2023.
Hashimoto, K., Hirono, Y ., and Sannai, A. Unification of
symmetries i","approach this question, we set up a con- crete physical model concerning the long-standing problem of quantum gravity. The most promising formulation of quantum gravity is to use the AdS/CFT correspondence (Maldacena, 1999), which is a conjecture that two physical quantum theories, conformal field theory (CFT) and gravity theory, are equivalent to each other. Unfortunately, there has been no proof of the conjecture, but there are a lot of working examples. The principal concern against this con- jecture is that for a given CFT data, there is no constructive way to find a background geometry in the dual equivalent gravity theory. This is precisely the inverse problem which we stated above: CFT provides a boundary data, and the issue is to find a differential equation in a curved geometry (which is unknown) whose solution is consistent with the boundary data. A possible solution to this inverse problem (widely known as bulk reconstruction program in the field of theoretical particle physics) can be a ground-breaking path to a proof of the conjecture, and furthermore, a working example for AI for math of inverse problems. The best and the simplest setup for solving this inverse prob- lem of the AdS/CFT correspondence is with a low-enough spacetime dimensions and the simplest matter content in the bulk spacetime, thus we follow the situation provided in 1 AI for an inverse problem: Physical model solving quantum gravity (Hashimoto et al., 2023) where the CFT (material quantum theory) lives on a one-dimensional circle while the corre- sponding gravity background is a disk which is rotationally symmetric. We follow the method developed in (Hashimoto et al., 2018a) which replaces the bulk differential equation with a sparse neural network securing the interpretability by regarding the weights as a metric on the curved space- time.1With this setup, if we find what kind of differential equations are emergent and what are not, it would be a great step for proving the AdS/CF","discussions. The work of K. H., K. M., M. M. and G. O. was supported in part by JSPS KAKENHI Grant Nos. JP22H01217, JP22H05111, and JP22H05115. The work of D. T. was supported in part by Grant-in-Aid for JSPS Fellows No. 22KJ1944.","AI for an inverse problem: Physical model solving quantum gravity Mathematical inverse problems of determining a governing differential equation for given solution data remain a fundamental challenge. To find a working example of AI for math, we provide a concrete example using a physical setup of a quantum gravity problem. We present a novel sparse Neural Network (NN) model which is interpretable, to solve the inverse problem: the AdS/CFT correspondence. According to the conjectured correspondence, a special condensed matter system on a ring is equivalent to a gravity system on a bulk disk. The inverse problem is to reconstruct the higher-dimensional gravity metric from the data of the condensed matter system. We use the response functions of a condensed matter system as our data, and by supervised machine learning, we successfully train the neural network which is equivalent to a scalar field equation on an emergent geometry of the bulk spacetime. The developed method may work as a ground for generic bulk reconstruction, i.e. a solution to the inverse problem of the AdS/CFT correspondence. From a technical perspective, to achieve better numerical control, our neural network model incorporates a novel layer that implements the Runge-Kutta method.","AI for an inverse problem: Physical model solving quantum gravity Mathematical inverse problems of determining a governing differential equation for given solution data remain a fundamental challenge. To find a working example of AI for math, we provide a concrete example using a physical setup of a quantum gravity problem. We present a novel sparse Neural Network (NN) model which is interpretable, to solve the inverse problem: the AdS/CFT correspondence. According to the conjectured correspondence, a special condensed matter system on a ring is equivalent to a gravity system on a bulk disk. The inverse problem is to reconstruct the higher-dimensional gravity metric from the data of the condensed matter system. We use the response functions of a condensed matter system as our data, and by supervised machine learning, we successfully train the neural network which is equivalent to a scalar field equation on an emergent geometry of the bulk spacetime. The developed method may work as a ground for generic bulk reconstruction, i.e. a solution to the inverse problem of the AdS/CFT correspondence. From a technical perspective, to achieve better numerical control, our neural network model incorporates a novel layer that implements the Runge-Kutta method. discussions. The work of K. H., K. M., M. M. and G. O. was supported in part by JSPS KAKENHI Grant Nos. JP22H01217, JP22H05111, and JP22H05115. The work of D. T. was supported in part by Grant-in-Aid for JSPS Fellows No. 22KJ1944. approach this question, we set up a con- crete physical model concerning the long-standing problem of quantum gravity. The most promising formulation of quantum gravity is to use the AdS/CFT correspondence (Maldacena, 1999), which is a conjecture that two physical quantum theories, conformal field theory (CFT) and gravity theory, are equivalent to each other. Unfortunately, there has been no proof of the conjecture, but there are a lot of working examples. The principal concern against this con- jecture is that for a given CFT data, there is no constructive way to find a background geometry in the dual equivalent gravity theory. This is precisely the inverse problem which we stated above: CFT provides a boundary data, and the issue is to find a differential equation in a curved geometry (which is unknown) whose solution is consistent with the boundary data. A possible solution to this inverse problem (widely known as bulk reconstruction program in the field of theoretical particle physics) can be a ground-breaking path to a proof of the conjecture, and furthermore, a working example for AI for math of inverse problems. The best and the simplest setup for solving this inverse prob- lem of the AdS/CFT correspondence is with a low-enough spacetime dimensions and the simplest matter content in the bulk spacetime, thus we follow the situation provided in 1 AI for an inverse problem: Physical model solving quantum gravity (Hashimoto et al., 2023) where the CFT (material quantum theory) lives on a one-dimensional circle while the corre- sponding gravity background is a disk which is rotationally symmetric. We follow the method developed in (Hashimoto et al., 2018a) which replaces the bulk differential equation with a sparse neural network securing the interpretability by regarding the weights as a metric on the curved space- time.1With this setup, if we find what kind of differential equations are emergent and what are not, it would be a great step for proving the AdS/CF",0
92d4eaa2df1188934d60f18bffc337a55e590e99,Specify What? A Case-Study using GPT-4 and Formal Methods For Specification Synthesis,"['George Granberry', 'Wolfgang Ahrendt', 'Moa Johansson']",https://openreview.net/pdf/92d4eaa2df1188934d60f18bffc337a55e590e99.pdf,"Specify What? A Case-Study using GPT-4 and Formal Methods For Specification Synthesis We study how including the output of formal methods tools in LLM prompts can affect specification synthesis. Formal specifications are supposed to unambigu- ously describe the behaviour of (parts of) pro- grams and are usually provided as extra annota- tions of the program code. The intention is both to document the code and to be able to automati- cally check compliance of programs using formal methods tools. Writing good specifications can however be both difficult and time-consuming for the programmer. In this case-study, we investigate how GPT-4 can help with the task. We propose a neuro-symbolic integration, by which we aug- ment the LLM prompts with outputs from two formal methods tools in the Frama-C ecosystem (Pathcrawler and EVA), and produce C program annotations in the specifications language ACSL. We demonstrate how this impacts the quality of annotations: information about input/output ex- amples from Pathcrawler produce more context- aware annotations, while the inclusion of EVA reports yields annotations more attuned to run- time errors.",92d4eaa2df1188934d60f18bffc337a55e590e99.pdf,"methodology
does not attempt to benchmark the generated specifications
against a predefined gold standard, nor does it aim to de-
termine the optimal approach to creating specifications. In-
stead, our focus is on identifying the behaviors and patterns
that emerge from incorporating symbolic analysis outputs
into the specification generation prompts. This approach
allows us to better understand the dynamics at play and what
kinds of output to expect given a particular prompt.
We propose a primarily qualitative evaluation from two
different angles:
•Types of annotations : In addition to counting the dif-
ferent types of annotations produced per prompt, we
use a human-in-the-loop qualitative analysis to inter-
pret the specification and identify trends depending on
which prompt was used, to assess how the different
symbolic tool outputs influence the results of the LLM.
For this we use the programs in the pathcrawler set.
•Implementation vs. Intent : We specifically exam-
ine programs in the mutated setto study how errors
introduced into the program affect the resultant specifi-
cations. This analysis explores how errors, symbolic
analyses, intended program functionality, and actual
implementation interact.
5.1. Types of Annotations
To give an overview, Figure 1 displays the number anno-
tations generated for each annotation type for the three
promtps. For all three cases, the most common annota-
tions are unsurprisingly the requires andensures statements,
which are used to define pre- and post-conditions of func-
tions, followed by assigns statements and loop invariants .
5.1.1. B ASELINE PROMPT
Many of the annotations produced with the baseline prompt
were rather simplistic. While not necessarily incorrect or
completely useless, these specifications tended to focus on
surface-level details of the programs, overlooking deeper,
more substantive aspects. This can be seen in Appendix D
3
Specification Generation for C programs
Figure 1. Annotation-type counts for each p","conclusions from.
While open source models such as Llama-3 have recently
gained traction, the setup and fine tuning of such a model
was out scope for this project, and remain as further work.
We prompt GPT-4 with a C program, instructions for how
to generate ACSL annotations (in a step-by-step manner).
We also include a few examples of valid annotations in
the prompts (see Appendix A). We also experiment with
prompts which in addition contain outputs from the EV A
and Pathcrawler tools (see Appendix B and C).
2
Specification Generation for C programs
3. C-program Test Suits
For our study, we have chosen to utilize the 55 programs
from the closed-source test suite1of Pathcrawler which we
will refer to as the pathcrawler set. This suite includes a
variety of program types, balancing well-known algorithms
like Binary Search with more niche programs such as a
Soup Heater controller. It also contains small, specially
crafted programs designed to test specific capabilities of
Pathcrawler, adding another layer of diversity to our tests.
Additionally, pathcrawler tests includes files that provide
preconditions to Pathcrawler when it creates test inputs.
This was convenient as it saved us from having to provide
sensible test inputs for every program that we wanted to use
Pathcrawler with. Using a closed-source test suite also has
the advantage that at least some of the programs and their
annotations are less likley to have appeared in the training
data for GPT-4. This test suit helps us test to what extent
accurate annotations can be produced for correct programs.
To also investigate if our approach can help with buggy
programs, we created a second suite of programs titled
mutated set. This comprises 8 of ”correct” programs with
handcrafted mutations simulating typos, designed to explore
a range of programs across two key dimensions: clarity of
intent and complexity. To thoroughly study the interactions
between these dimensions, this set includes various types of
programs: s","methodology does not attempt to benchmark the generated specifications against a predefined gold standard, nor does it aim to de- termine the optimal approach to creating specifications. In- stead, our focus is on identifying the behaviors and patterns that emerge from incorporating symbolic analysis outputs into the specification generation prompts. This approach allows us to better understand the dynamics at play and what kinds of output to expect given a particular prompt. We propose a primarily qualitative evaluation from two different angles: Types of annotations : In addition to counting the dif- ferent types of annotations produced per prompt, we use a human-in-the-loop qualitative analysis to inter- pret the specification and identify trends depending on which prompt was used, to assess how the different symbolic tool outputs influence the results of the LLM. For this we use the programs in the pathcrawler set. Implementation vs. Intent : We specifically exam- ine programs in the mutated setto study how errors introduced into the program affect the resultant specifi- cations. This analysis explores how errors, symbolic analyses, intended program functionality, and actual implementation interact. 5.1. Types of Annotations To give an overview, Figure 1 displays the number anno- tations generated for each annotation type for the three promtps. For all three cases, the most common annota- tions are unsurprisingly the requires andensures statements, which are used to define pre- and post-conditions of func- tions, followed by assigns statements and loop invariants . 5.1.1. B ASELINE PROMPT Many of the annotations produced with the baseline prompt were rather simplistic. While not necessarily incorrect or completely useless, these specifications tended to focus on surface-level details of the programs, overlooking deeper, more substantive aspects. This can be seen in Appendix D 3 Specification Generation for C programs Figure 1. Annotation-type counts for each p","conclusions from. While open source models such as Llama-3 have recently gained traction, the setup and fine tuning of such a model was out scope for this project, and remain as further work. We prompt GPT-4 with a C program, instructions for how to generate ACSL annotations (in a step-by-step manner). We also include a few examples of valid annotations in the prompts (see Appendix A). We also experiment with prompts which in addition contain outputs from the EV A and Pathcrawler tools (see Appendix B and C). 2 Specification Generation for C programs 3. C-program Test Suits For our study, we have chosen to utilize the 55 programs from the closed-source test suite1of Pathcrawler which we will refer to as the pathcrawler set. This suite includes a variety of program types, balancing well-known algorithms like Binary Search with more niche programs such as a Soup Heater controller. It also contains small, specially crafted programs designed to test specific capabilities of Pathcrawler, adding another layer of diversity to our tests. Additionally, pathcrawler tests includes files that provide preconditions to Pathcrawler when it creates test inputs. This was convenient as it saved us from having to provide sensible test inputs for every program that we wanted to use Pathcrawler with. Using a closed-source test suite also has the advantage that at least some of the programs and their annotations are less likley to have appeared in the training data for GPT-4. This test suit helps us test to what extent accurate annotations can be produced for correct programs. To also investigate if our approach can help with buggy programs, we created a second suite of programs titled mutated set. This comprises 8 of correct programs with handcrafted mutations simulating typos, designed to explore a range of programs across two key dimensions: clarity of intent and complexity. To thoroughly study the interactions between these dimensions, this set includes various types of programs: s","Specify What? A Case-Study using GPT-4 and Formal Methods For Specification Synthesis We study how including the output of formal methods tools in LLM prompts can affect specification synthesis. Formal specifications are supposed to unambigu- ously describe the behaviour of (parts of) pro- grams and are usually provided as extra annota- tions of the program code. The intention is both to document the code and to be able to automati- cally check compliance of programs using formal methods tools. Writing good specifications can however be both difficult and time-consuming for the programmer. In this case-study, we investigate how GPT-4 can help with the task. We propose a neuro-symbolic integration, by which we aug- ment the LLM prompts with outputs from two formal methods tools in the Frama-C ecosystem (Pathcrawler and EVA), and produce C program annotations in the specifications language ACSL. We demonstrate how this impacts the quality of annotations: information about input/output ex- amples from Pathcrawler produce more context- aware annotations, while the inclusion of EVA reports yields annotations more attuned to run- time errors.","Specify What? A Case-Study using GPT-4 and Formal Methods For Specification Synthesis We study how including the output of formal methods tools in LLM prompts can affect specification synthesis. Formal specifications are supposed to unambigu- ously describe the behaviour of (parts of) pro- grams and are usually provided as extra annota- tions of the program code. The intention is both to document the code and to be able to automati- cally check compliance of programs using formal methods tools. Writing good specifications can however be both difficult and time-consuming for the programmer. In this case-study, we investigate how GPT-4 can help with the task. We propose a neuro-symbolic integration, by which we aug- ment the LLM prompts with outputs from two formal methods tools in the Frama-C ecosystem (Pathcrawler and EVA), and produce C program annotations in the specifications language ACSL. We demonstrate how this impacts the quality of annotations: information about input/output ex- amples from Pathcrawler produce more context- aware annotations, while the inclusion of EVA reports yields annotations more attuned to run- time errors. conclusions from. While open source models such as Llama-3 have recently gained traction, the setup and fine tuning of such a model was out scope for this project, and remain as further work. We prompt GPT-4 with a C program, instructions for how to generate ACSL annotations (in a step-by-step manner). We also include a few examples of valid annotations in the prompts (see Appendix A). We also experiment with prompts which in addition contain outputs from the EV A and Pathcrawler tools (see Appendix B and C). 2 Specification Generation for C programs 3. C-program Test Suits For our study, we have chosen to utilize the 55 programs from the closed-source test suite1of Pathcrawler which we will refer to as the pathcrawler set. This suite includes a variety of program types, balancing well-known algorithms like Binary Search with more niche programs such as a Soup Heater controller. It also contains small, specially crafted programs designed to test specific capabilities of Pathcrawler, adding another layer of diversity to our tests. Additionally, pathcrawler tests includes files that provide preconditions to Pathcrawler when it creates test inputs. This was convenient as it saved us from having to provide sensible test inputs for every program that we wanted to use Pathcrawler with. Using a closed-source test suite also has the advantage that at least some of the programs and their annotations are less likley to have appeared in the training data for GPT-4. This test suit helps us test to what extent accurate annotations can be produced for correct programs. To also investigate if our approach can help with buggy programs, we created a second suite of programs titled mutated set. This comprises 8 of correct programs with handcrafted mutations simulating typos, designed to explore a range of programs across two key dimensions: clarity of intent and complexity. To thoroughly study the interactions between these dimensions, this set includes various types of programs: s methodology does not attempt to benchmark the generated specifications against a predefined gold standard, nor does it aim to de- termine the optimal approach to creating specifications. In- stead, our focus is on identifying the behaviors and patterns that emerge from incorporating symbolic analysis outputs into the specification generation prompts. This approach allows us to better understand the dynamics at play and what kinds of output to expect given a particular prompt. We propose a primarily qualitative evaluation from two different angles: Types of annotations : In addition to counting the dif- ferent types of annotations produced per prompt, we use a human-in-the-loop qualitative analysis to inter- pret the specification and identify trends depending on which prompt was used, to assess how the different symbolic tool outputs influence the results of the LLM. For this we use the programs in the pathcrawler set. Implementation vs. Intent : We specifically exam- ine programs in the mutated setto study how errors introduced into the program affect the resultant specifi- cations. This analysis explores how errors, symbolic analyses, intended program functionality, and actual implementation interact. 5.1. Types of Annotations To give an overview, Figure 1 displays the number anno- tations generated for each annotation type for the three promtps. For all three cases, the most common annota- tions are unsurprisingly the requires andensures statements, which are used to define pre- and post-conditions of func- tions, followed by assigns statements and loop invariants . 5.1.1. B ASELINE PROMPT Many of the annotations produced with the baseline prompt were rather simplistic. While not necessarily incorrect or completely useless, these specifications tended to focus on surface-level details of the programs, overlooking deeper, more substantive aspects. This can be seen in Appendix D 3 Specification Generation for C programs Figure 1. Annotation-type counts for each p",0
46caee17a170a1a8ce500d42f3be4659ebc8f34b,DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation,"['Xueqing Wu', 'Rui Zheng', 'Jingzhen Sha', 'Te-Lin Wu', 'Hanyu Zhou', 'Tang Mohan', 'Kai-Wei Chang', 'Nanyun Peng', 'Haoran Huang']",https://openreview.net/pdf/46caee17a170a1a8ce500d42f3be4659ebc8f34b.pdf,"DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation We introduce DACO, a dataset for data analysis, containing (1) 440 databases of tabular data, (2) 2k automatically generated query-answer pairs that can serve as weak supervision for model training, and (3) a manually refined test set for evaluation. Data analysis is a crucial analytical process to generate in-depth studies and conclusive insights to comprehensively answer a given user query for tabular data. In this work, we aim to propose new resources and benchmarks to inspire future research on this crucial yet challenging and under-explored task. However, collecting data analysis annotations curated by experts can be prohibitively expensive. We propose to automatically generate high-quality answer annotations leveraging the code-generation capabilities of LLMs with a multi-turn prompting technique. We construct the **DACO dataset**, containing (1) 440 databases (of tabular data) collected from real-world scenarios, (2) ~2k query-answer pairs that can serve as weak supervision for model training, and (3) a concentrated but high-quality test set with human refined annotations that serves as our main evaluation benchmark. We train a 6B supervised fine-tuning (SFT) model on DACO dataset, and find that the SFT model learns reasonable data analysis capabilities. To further align the models with human preference, we use reinforcement learning to encourage generating analysis perceived by human as helpful, and design a set of dense rewards to propagate the sparse human preference reward to intermediate code generation steps. Our DACO-RL algorithm is evaluated by human annotators to produce more helpful answers than SFT model in 57.72% cases, validating the effectiveness of our proposed algorithm.",46caee17a170a1a8ce500d42f3be4659ebc8f34b.pdf,"approach
(Ouyang et al., 2022; Wu et al., 2023; Zheng et al., 2023b).
Given two analyses generated by two different systems, the
annotator (either human or simulated by ChatGPT) selects
the more helpful one based on our defined criteria. The
winning rate of each system is reported as helpfulness score.
To obtain a comparable set of numbers for all models, we
report the winning rate of each model against TestAand
TestHannotations. The upper bound for this score would
be 50, as a score of 50 indicates that the model generations
are perceived as helpful as annotations.
3. D ACO-RL
While DACO contains mostly algorithmic machine gen-
erated analyses, the machine generations without human
refinement cannot well align with human preferences (of
“good” analyses). Our human refinement process shows
that only 47.4% bullet points are “good” points perfectly
addressing user queries; the majority of 52.2% are evalu-
ated as “borderline” points that only partially aligns with
Table 2: Distribution of DACO queries. We display the top 15
verbs and their top 3 direct noun objectives, demonstrating the
diversity of D ACO queries.
human expectations; and the remaining 0.4% are considered
as “bad”.
We are therefore interested in investigating whether aligning
human preferences via an RLHF fashion could lead to better
machine generated analyses. We thus propose the DACO-RL
algorithm, which is illustrated in the left half of Figure 4.
Our end goal is to optimize the helpfulness of the analyzed
points, which is modelled with an answer RM Ra. In ad-
dition to this sparse reward signal, we use a heuristically
defined contribution RM Rcto reward each intermediate
step, which is further regularized with a regularization RM
Rrto prevent reward hacking. In the following sections,
we first explain the three reward models sequentially, and
eventually explain our whole RLHF pipeline.
Notations. We train a language model that interacts
with the python interpreter in a conversational man-
ner. Forma","Conclusion
In this work, we propose a novel and challenging data anal-
ysis task, which involves decomposing user query into mul-
tiple perspectives, grounding each perspective to the input
data and performing logical and mathematical reasoning.
To support this task, we build the DACO dataset containing
large-scale annotations automatically generated by GPT-4
and a small but high-quality test set with human curated
annotations. We employ LLM enhanced with code gener-
ation to this task and evaluate three models on our dataset:
zero-shot ChatGPT, zero-shot GPT-4 and a 6B SFT model.
While GPT-4 consistently performs the best, SFT achieves
reasonably good helpfulness with much less computation.
On top of the SFT model, we further proposed our DACO-
RL algorithm that significantly boosts the human evaluated
helpfulness.
References
Austin, J., Odena, A., Nye, M. I., Bosma, M., Michalewski,
H., Dohan, D., Jiang, E., Cai, C. J., Terry, M., Le, Q. V .,
and Sutton, C. Program synthesis with large language
models. CoRR , abs/2108.07732, 2021. URL https:
//arxiv.org/abs/2108.07732 .
Bai, Y ., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-
Sarma, N., Drain, D., Fort, S., Ganguli, D., Henighan,
8
DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation
T., Joseph, N., Kadavath, S., Kernion, J., Conerly, T.,
Showk, S. E., Elhage, N., Hatfield-Dodds, Z., Hernandez,
D., Hume, T., Johnston, S., Kravec, S., Lovitt, L., Nanda,
N., Olsson, C., Amodei, D., Brown, T. B., Clark, J., Mc-
Candlish, S., Olah, C., Mann, B., and Kaplan, J. Train-
ing a helpful and harmless assistant with reinforcement
learning from human feedback. CoRR , abs/2204.05862,
2022a. doi: 10.48550/ARXIV .2204.05862. URL https:
//doi.org/10.48550/arXiv.2204.05862 .
Bai, Y ., Kadavath, S., Kundu, S., Askell, A., Kernion, J.,
Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKin-
non, C., Chen, C., Olsson, C., Olah, C., Hernandez, D.,
Drain, D., Ganguli, D., Li, D., Tran-Johnson, E., Per","approach (Ouyang et al., 2022; Wu et al., 2023; Zheng et al., 2023b). Given two analyses generated by two different systems, the annotator (either human or simulated by ChatGPT) selects the more helpful one based on our defined criteria. The winning rate of each system is reported as helpfulness score. To obtain a comparable set of numbers for all models, we report the winning rate of each model against TestAand TestHannotations. The upper bound for this score would be 50, as a score of 50 indicates that the model generations are perceived as helpful as annotations. 3. D ACO-RL While DACO contains mostly algorithmic machine gen- erated analyses, the machine generations without human refinement cannot well align with human preferences (of good analyses). Our human refinement process shows that only 47.4% bullet points are good points perfectly addressing user queries; the majority of 52.2% are evalu- ated as borderline points that only partially aligns with Table 2: Distribution of DACO queries. We display the top 15 verbs and their top 3 direct noun objectives, demonstrating the diversity of D ACO queries. human expectations; and the remaining 0.4% are considered as bad . We are therefore interested in investigating whether aligning human preferences via an RLHF fashion could lead to better machine generated analyses. We thus propose the DACO-RL algorithm, which is illustrated in the left half of Figure 4. Our end goal is to optimize the helpfulness of the analyzed points, which is modelled with an answer RM Ra. In ad- dition to this sparse reward signal, we use a heuristically defined contribution RM Rcto reward each intermediate step, which is further regularized with a regularization RM Rrto prevent reward hacking. In the following sections, we first explain the three reward models sequentially, and eventually explain our whole RLHF pipeline. Notations. We train a language model that interacts with the python interpreter in a conversational man- ner. Forma","Conclusion In this work, we propose a novel and challenging data anal- ysis task, which involves decomposing user query into mul- tiple perspectives, grounding each perspective to the input data and performing logical and mathematical reasoning. To support this task, we build the DACO dataset containing large-scale annotations automatically generated by GPT-4 and a small but high-quality test set with human curated annotations. We employ LLM enhanced with code gener- ation to this task and evaluate three models on our dataset: zero-shot ChatGPT, zero-shot GPT-4 and a 6B SFT model. While GPT-4 consistently performs the best, SFT achieves reasonably good helpfulness with much less computation. On top of the SFT model, we further proposed our DACO- RL algorithm that significantly boosts the human evaluated helpfulness.","DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation We introduce DACO, a dataset for data analysis, containing (1) 440 databases of tabular data, (2) 2k automatically generated query-answer pairs that can serve as weak supervision for model training, and (3) a manually refined test set for evaluation. Data analysis is a crucial analytical process to generate in-depth studies and conclusive insights to comprehensively answer a given user query for tabular data. In this work, we aim to propose new resources and benchmarks to inspire future research on this crucial yet challenging and under-explored task. However, collecting data analysis annotations curated by experts can be prohibitively expensive. We propose to automatically generate high-quality answer annotations leveraging the code-generation capabilities of LLMs with a multi-turn prompting technique. We construct the **DACO dataset**, containing (1) 440 databases (of tabular data) collected from real-world scenarios, (2) ~2k query-answer pairs that can serve as weak supervision for model training, and (3) a concentrated but high-quality test set with human refined annotations that serves as our main evaluation benchmark. We train a 6B supervised fine-tuning (SFT) model on DACO dataset, and find that the SFT model learns reasonable data analysis capabilities. To further align the models with human preference, we use reinforcement learning to encourage generating analysis perceived by human as helpful, and design a set of dense rewards to propagate the sparse human preference reward to intermediate code generation steps. Our DACO-RL algorithm is evaluated by human annotators to produce more helpful answers than SFT model in 57.72% cases, validating the effectiveness of our proposed algorithm.","DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation We introduce DACO, a dataset for data analysis, containing (1) 440 databases of tabular data, (2) 2k automatically generated query-answer pairs that can serve as weak supervision for model training, and (3) a manually refined test set for evaluation. Data analysis is a crucial analytical process to generate in-depth studies and conclusive insights to comprehensively answer a given user query for tabular data. In this work, we aim to propose new resources and benchmarks to inspire future research on this crucial yet challenging and under-explored task. However, collecting data analysis annotations curated by experts can be prohibitively expensive. We propose to automatically generate high-quality answer annotations leveraging the code-generation capabilities of LLMs with a multi-turn prompting technique. We construct the **DACO dataset**, containing (1) 440 databases (of tabular data) collected from real-world scenarios, (2) ~2k query-answer pairs that can serve as weak supervision for model training, and (3) a concentrated but high-quality test set with human refined annotations that serves as our main evaluation benchmark. We train a 6B supervised fine-tuning (SFT) model on DACO dataset, and find that the SFT model learns reasonable data analysis capabilities. To further align the models with human preference, we use reinforcement learning to encourage generating analysis perceived by human as helpful, and design a set of dense rewards to propagate the sparse human preference reward to intermediate code generation steps. Our DACO-RL algorithm is evaluated by human annotators to produce more helpful answers than SFT model in 57.72% cases, validating the effectiveness of our proposed algorithm. Conclusion In this work, we propose a novel and challenging data anal- ysis task, which involves decomposing user query into mul- tiple perspectives, grounding each perspective to the input data and performing logical and mathematical reasoning. To support this task, we build the DACO dataset containing large-scale annotations automatically generated by GPT-4 and a small but high-quality test set with human curated annotations. We employ LLM enhanced with code gener- ation to this task and evaluate three models on our dataset: zero-shot ChatGPT, zero-shot GPT-4 and a 6B SFT model. While GPT-4 consistently performs the best, SFT achieves reasonably good helpfulness with much less computation. On top of the SFT model, we further proposed our DACO- RL algorithm that significantly boosts the human evaluated helpfulness. approach (Ouyang et al., 2022; Wu et al., 2023; Zheng et al., 2023b). Given two analyses generated by two different systems, the annotator (either human or simulated by ChatGPT) selects the more helpful one based on our defined criteria. The winning rate of each system is reported as helpfulness score. To obtain a comparable set of numbers for all models, we report the winning rate of each model against TestAand TestHannotations. The upper bound for this score would be 50, as a score of 50 indicates that the model generations are perceived as helpful as annotations. 3. D ACO-RL While DACO contains mostly algorithmic machine gen- erated analyses, the machine generations without human refinement cannot well align with human preferences (of good analyses). Our human refinement process shows that only 47.4% bullet points are good points perfectly addressing user queries; the majority of 52.2% are evalu- ated as borderline points that only partially aligns with Table 2: Distribution of DACO queries. We display the top 15 verbs and their top 3 direct noun objectives, demonstrating the diversity of D ACO queries. human expectations; and the remaining 0.4% are considered as bad . We are therefore interested in investigating whether aligning human preferences via an RLHF fashion could lead to better machine generated analyses. We thus propose the DACO-RL algorithm, which is illustrated in the left half of Figure 4. Our end goal is to optimize the helpfulness of the analyzed points, which is modelled with an answer RM Ra. In ad- dition to this sparse reward signal, we use a heuristically defined contribution RM Rcto reward each intermediate step, which is further regularized with a regularization RM Rrto prevent reward hacking. In the following sections, we first explain the three reward models sequentially, and eventually explain our whole RLHF pipeline. Notations. We train a language model that interacts with the python interpreter in a conversational man- ner. Forma",0
b5ceba0148c6dcdfa3dd15ada8c3e29bc88ec2c8,Distilling LLMs’ Decomposition Abilities into Compact Language Models,"['Denis Tarasov', 'Kumar Shridhar']",https://openreview.net/pdf/b5ceba0148c6dcdfa3dd15ada8c3e29bc88ec2c8.pdf,"Distilling LLMs Decomposition Abilities into Compact Language Models In this work we develop AI-generated benchmark for the distillation of LLMs' decomposition abilities into smaller models and provide multiple baselines Large Language Models (LLMs) have demonstrated proficiency in their reasoning abilities, yet their large size presents scalability challenges and limits any further customization. In contrast, compact models offer customized training but often fall short in solving complex reasoning tasks. This study focuses on distilling the LLMs' decomposition skills into compact models using offline reinforcement learning. We leverage the advancements in the LLM`s capabilities to provide feedback and generate a specialized task-specific dataset for training compact models. The development of an AI-generated dataset and the establishment of baselines constitute the primary contributions of our work, underscoring the potential of compact models in replicating complex problem-solving skills.",b5ceba0148c6dcdfa3dd15ada8c3e29bc88ec2c8.pdf,"methodology inspired
by Shridhar et al. (2022), we use the BLEU score (Papineni
et al., 2002) calculated between the generated questions and
those produced by ChatGPT. Given that the primary goal
of BC is to replicate the original policy’s behavior, BLEU
serves as a suitable metric, indicating the similarity between
two texts. Our empirical observation show that BLEU cor-
relates with the final performance, making it a reasonable
choice for model evaluation in the context of BC. For all
subsequent approaches, the best BC model serves as the
initialization for the LM.
Filtered Behavioral Cloning. Filtered BC (Chen et al.,
2021) introduces a modification of BC by considering only a
fraction of the best trajectories in the dataset. This approach
proves particularly effective when a substantial number of
high-quality examples are at disposal. In the context of our
task, we exclusively retain samples corresponding to sub-
question sets that result in the correct solution. The model
selection process remains consistent with the standard BC
approach.
Implicit Language Q-Learning. Implicit Language Q-
Learning (ILQL) (Snell et al., 2022) represents an adaptation
of the offline RL approach known as IQL (Kostrikov et al.,
2021) to NLP tasks. The core idea behind ILQL involves
training additional Value (V) and Q-function heads with IQLobjectives. These additional functions are then employed to
reweight the original LM outputs using the advantage value,
which is the difference between V and Q values.
The selection of ILQL is motivated by the effectiveness of
IQL as one of the strongest offline RL approaches in diverse
domains (Tarasov et al., 2022). Given the limited adaptation
of offline RL approaches to NLP problems, ILQL emerges
as the state-of-the-art choice. Given that IQL optimizes for
rewards, which may not inherently correlate with the dataset
policy, selecting the best model becomes challenging. In
the absence of a clear best model selection criterion, we
have tried to","Future Work.
Our work serves as a foundational exploration, opening
avenues for various future directions.
Development of Offline RL Approaches: A pivotal area
for future exploration involves advancing offline RL or other
suitable methodologies for distilling reasoning abilities from
static datasets. This extension could contribute to more
effective utilization of language models in reasoning tasks.
Creation of a Larger Benchmark: Expanding our method-
ology, future work could focus on generating a more ex-
tensive benchmark as it requires only the access to ground
truth-answers in the datset which usualy holds. This bench-
mark might incorporate a diverse set of reasoning datasets,
such as MATH (Hendrycks et al., 2021) or AQuA (Ling
et al., 2017), providing a broader assessment of reasoning
capabilities.
Concentration on Sub-Question Answering: Delving
deeper into the sub-question answering aspect of the rea-
soning process presents a promising direction. While our
dataset includes ChatGPT responses for sub-questions, their
scoring and utilization remain unexplored. Future studies
could investigate this component to enhance understanding
and performance.
Utilization of Open-Source Models: Exploring the appli-
cation of open-source models, such as LLaMA, for sub-
question generation emerges as a cost-effective alterna-
tive. Accessible without financial constraints, these models
present an opportunity for researchers to delve into sub-
question generation without monetary limitations. We were
not able to run such kind of experiments ourselves due to
the computational limitations.
7. Conclusion
This work introduces a novel AI-generated benchmark tai-
lored for evaluating sub-questioning in reasoning tasks. We
employ diverse offline learning approaches, varying model
sizes for baselines, and assess the performance using dif-
ferent LLMs. Our experiments aim to shed light on the
challenges and potential avenues for enhancing reasoning
capabilities.
The outcomes reveal","methodology inspired by Shridhar et al. (2022), we use the BLEU score (Papineni et al., 2002) calculated between the generated questions and those produced by ChatGPT. Given that the primary goal of BC is to replicate the original policy s behavior, BLEU serves as a suitable metric, indicating the similarity between two texts. Our empirical observation show that BLEU cor- relates with the final performance, making it a reasonable choice for model evaluation in the context of BC. For all subsequent approaches, the best BC model serves as the initialization for the LM. Filtered Behavioral Cloning. Filtered BC (Chen et al., 2021) introduces a modification of BC by considering only a fraction of the best trajectories in the dataset. This approach proves particularly effective when a substantial number of high-quality examples are at disposal. In the context of our task, we exclusively retain samples corresponding to sub- question sets that result in the correct solution. The model selection process remains consistent with the standard BC approach. Implicit Language Q-Learning. Implicit Language Q- Learning (ILQL) (Snell et al., 2022) represents an adaptation of the offline RL approach known as IQL (Kostrikov et al., 2021) to NLP tasks. The core idea behind ILQL involves training additional Value (V) and Q-function heads with IQLobjectives. These additional functions are then employed to reweight the original LM outputs using the advantage value, which is the difference between V and Q values. The selection of ILQL is motivated by the effectiveness of IQL as one of the strongest offline RL approaches in diverse domains (Tarasov et al., 2022). Given the limited adaptation of offline RL approaches to NLP problems, ILQL emerges as the state-of-the-art choice. Given that IQL optimizes for rewards, which may not inherently correlate with the dataset policy, selecting the best model becomes challenging. In the absence of a clear best model selection criterion, we have tried to","Future Work. Our work serves as a foundational exploration, opening avenues for various future directions. Development of Offline RL Approaches: A pivotal area for future exploration involves advancing offline RL or other suitable methodologies for distilling reasoning abilities from static datasets. This extension could contribute to more effective utilization of language models in reasoning tasks. Creation of a Larger Benchmark: Expanding our method- ology, future work could focus on generating a more ex- tensive benchmark as it requires only the access to ground truth-answers in the datset which usualy holds. This bench- mark might incorporate a diverse set of reasoning datasets, such as MATH (Hendrycks et al., 2021) or AQuA (Ling et al., 2017), providing a broader assessment of reasoning capabilities. Concentration on Sub-Question Answering: Delving deeper into the sub-question answering aspect of the rea- soning process presents a promising direction. While our dataset includes ChatGPT responses for sub-questions, their scoring and utilization remain unexplored. Future studies could investigate this component to enhance understanding and performance. Utilization of Open-Source Models: Exploring the appli- cation of open-source models, such as LLaMA, for sub- question generation emerges as a cost-effective alterna- tive. Accessible without financial constraints, these models present an opportunity for researchers to delve into sub- question generation without monetary limitations. We were not able to run such kind of experiments ourselves due to the computational limitations. 7. Conclusion This work introduces a novel AI-generated benchmark tai- lored for evaluating sub-questioning in reasoning tasks. We employ diverse offline learning approaches, varying model sizes for baselines, and assess the performance using dif- ferent LLMs. Our experiments aim to shed light on the challenges and potential avenues for enhancing reasoning capabilities. The outcomes reveal","Distilling LLMs Decomposition Abilities into Compact Language Models In this work we develop AI-generated benchmark for the distillation of LLMs' decomposition abilities into smaller models and provide multiple baselines Large Language Models (LLMs) have demonstrated proficiency in their reasoning abilities, yet their large size presents scalability challenges and limits any further customization. In contrast, compact models offer customized training but often fall short in solving complex reasoning tasks. This study focuses on distilling the LLMs' decomposition skills into compact models using offline reinforcement learning. We leverage the advancements in the LLM`s capabilities to provide feedback and generate a specialized task-specific dataset for training compact models. The development of an AI-generated dataset and the establishment of baselines constitute the primary contributions of our work, underscoring the potential of compact models in replicating complex problem-solving skills.","Distilling LLMs Decomposition Abilities into Compact Language Models In this work we develop AI-generated benchmark for the distillation of LLMs' decomposition abilities into smaller models and provide multiple baselines Large Language Models (LLMs) have demonstrated proficiency in their reasoning abilities, yet their large size presents scalability challenges and limits any further customization. In contrast, compact models offer customized training but often fall short in solving complex reasoning tasks. This study focuses on distilling the LLMs' decomposition skills into compact models using offline reinforcement learning. We leverage the advancements in the LLM`s capabilities to provide feedback and generate a specialized task-specific dataset for training compact models. The development of an AI-generated dataset and the establishment of baselines constitute the primary contributions of our work, underscoring the potential of compact models in replicating complex problem-solving skills. Future Work. Our work serves as a foundational exploration, opening avenues for various future directions. Development of Offline RL Approaches: A pivotal area for future exploration involves advancing offline RL or other suitable methodologies for distilling reasoning abilities from static datasets. This extension could contribute to more effective utilization of language models in reasoning tasks. Creation of a Larger Benchmark: Expanding our method- ology, future work could focus on generating a more ex- tensive benchmark as it requires only the access to ground truth-answers in the datset which usualy holds. This bench- mark might incorporate a diverse set of reasoning datasets, such as MATH (Hendrycks et al., 2021) or AQuA (Ling et al., 2017), providing a broader assessment of reasoning capabilities. Concentration on Sub-Question Answering: Delving deeper into the sub-question answering aspect of the rea- soning process presents a promising direction. While our dataset includes ChatGPT responses for sub-questions, their scoring and utilization remain unexplored. Future studies could investigate this component to enhance understanding and performance. Utilization of Open-Source Models: Exploring the appli- cation of open-source models, such as LLaMA, for sub- question generation emerges as a cost-effective alterna- tive. Accessible without financial constraints, these models present an opportunity for researchers to delve into sub- question generation without monetary limitations. We were not able to run such kind of experiments ourselves due to the computational limitations. 7. Conclusion This work introduces a novel AI-generated benchmark tai- lored for evaluating sub-questioning in reasoning tasks. We employ diverse offline learning approaches, varying model sizes for baselines, and assess the performance using dif- ferent LLMs. Our experiments aim to shed light on the challenges and potential avenues for enhancing reasoning capabilities. The outcomes reveal methodology inspired by Shridhar et al. (2022), we use the BLEU score (Papineni et al., 2002) calculated between the generated questions and those produced by ChatGPT. Given that the primary goal of BC is to replicate the original policy s behavior, BLEU serves as a suitable metric, indicating the similarity between two texts. Our empirical observation show that BLEU cor- relates with the final performance, making it a reasonable choice for model evaluation in the context of BC. For all subsequent approaches, the best BC model serves as the initialization for the LM. Filtered Behavioral Cloning. Filtered BC (Chen et al., 2021) introduces a modification of BC by considering only a fraction of the best trajectories in the dataset. This approach proves particularly effective when a substantial number of high-quality examples are at disposal. In the context of our task, we exclusively retain samples corresponding to sub- question sets that result in the correct solution. The model selection process remains consistent with the standard BC approach. Implicit Language Q-Learning. Implicit Language Q- Learning (ILQL) (Snell et al., 2022) represents an adaptation of the offline RL approach known as IQL (Kostrikov et al., 2021) to NLP tasks. The core idea behind ILQL involves training additional Value (V) and Q-function heads with IQLobjectives. These additional functions are then employed to reweight the original LM outputs using the advantage value, which is the difference between V and Q values. The selection of ILQL is motivated by the effectiveness of IQL as one of the strongest offline RL approaches in diverse domains (Tarasov et al., 2022). Given the limited adaptation of offline RL approaches to NLP problems, ILQL emerges as the state-of-the-art choice. Given that IQL optimizes for rewards, which may not inherently correlate with the dataset policy, selecting the best model becomes challenging. In the absence of a clear best model selection criterion, we have tried to",0
e6106df113d9ae7f3ed2d023ebc8c8f9d8c3a3f5,Progressive-Hint Prompting Improves Reasoning in Large Language Models,"['Chuanyang Zheng', 'Zhengying Liu', 'Enze Xie', 'Zhenguo Li', 'Yu Li']",https://openreview.net/pdf/e6106df113d9ae7f3ed2d023ebc8c8f9d8c3a3f5.pdf,"Progressive-Hint Prompting Improves Reasoning in Large Language Models We propose a new prompting strategy, Progressive-Hint Promoting, that can be easily combined with Chain-Of-Thought and Self-Consistency to improve performance. The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that en- hance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic mul- tiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experi- ments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text- davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sam- ple paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% 91.9%), GSM8K (92% 95.5%), AQuA (76.4% 79.9%) and MATH (50.3% 53.9%).",e6106df113d9ae7f3ed2d023ebc8c8f9d8c3a3f5.pdf,"approaches
to promote intermediate reasoning steps (Wei et al., 2022;
Zhou et al., 2023; Fu et al., 2023). Other works in this area,
such as Least-to-Most (Zhou et al., 2023) and Complex
CoT (Fu et al., 2023), have also explored this direction. An-
other area of research is self-consistency-related approaches.
In comparison to CoT-related work that focuses on design-
ing better prompts, self-consistency proposes to sample mul-
tiple answers from the LLMs and arrive at the correct answer
through a majority vote (Fu et al., 2023). This approach
is further improved upon by complex-based selection (Fu
et al., 2023). CoT-related and self-consistency-related works
can be seamlessly combined without any conflict.
Prior research has not explored the potential of leveraging
the outputs of LLM to refine reasoning paths iteratively.
It stands to reason that similar to human cognition, LLM
could benefit from reevaluating and adjusting its generated
reasoning paths in order to correct errors and enhance over-
all performance. In this paper, we propose a new method
named Progressive-Hint Prompting (PHP) that involves se-
quentially interacting with LLM to approach the correct an-
swer gradually. The method operates as follows: (1) given
a question, we ask the LLM to provide a Base Answer; (2)
we combine the question and answer to re-ask the LLM and
obtain the Subsequent Answer; (3) we repeat the operation
in (2) until the answer is stable and does not change over
the last two answers. PHP follows a human-like thought
process where previous answers are leveraged as hints to
arrive at the correct answer after re-evaluating the question.
Figure 1 illustrates the proposed PHP framework. We use
the base prompt to obtain the initial base answer, and then
employ the PHP prompt for subsequent questions. If the cur-
rent answer matches the previous answer, it is more likely to
be correct, and we terminate the LLM inquiry. With Com-
plex CoT and GPT-4, after adding PHP, the performance
ach","Conclusion
This paper introduces a novel approach named Progressive-
Hint Prompting (PHP) for interacting with LLMs, which
offers multiple advantages: 1) PHP achieves substantial per-
formance improvements on math reasoning tasks, leading
to state-of-the-art results on several reasoning benchmarks;
2) with more powerful models and prompts, PHP can better
and consistently benefit the LLMs; 3) PHP can be easily
combined with CoT and self-consistency to further improve
performance.
To better enhance the progressive-hint prompting approach,
future research endeavors can focus on improving the de-
sign of handcrafted hints in the question phase and prompt
sentences in the answer part. Additionally, novel hints that
aid the LLMs to reconsider the questions can be identified
and extracted beside the answer.
8
Progressive-Hint Prompting Improves Reasoning in Large Language Models
References
Asai, A., Hashimoto, K., Hajishirzi, H., Socher, R., and
Xiong, C. Learning to retrieve reasoning paths over
wikipedia graph for question answering. In International
Conference on Learning Representations , 2020. 2
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:
1877–1901, 2020. 2, 4
Chen, J., Lin, S.-t., and Durrett, G. Multi-hop ques-
tion answering via reasoning chains. arXiv preprint
arXiv:1910.02610 , 2019. 3
Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program
of thoughts prompting: Disentangling computation from
reasoning for numerical reasoning tasks. arXiv preprint
arXiv:2211.12588 , 2022. 7
Chowdhary, K. and Chowdhary, K. Natural language pro-
cessing. Fundamentals of artificial intelligence , pp. 603–
649, 2020. 1
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,
Gehrmann, S., et al. Palm: Scaling language modeling
with pathways. arXiv pr","approaches to promote intermediate reasoning steps (Wei et al., 2022; Zhou et al., 2023; Fu et al., 2023). Other works in this area, such as Least-to-Most (Zhou et al., 2023) and Complex CoT (Fu et al., 2023), have also explored this direction. An- other area of research is self-consistency-related approaches. In comparison to CoT-related work that focuses on design- ing better prompts, self-consistency proposes to sample mul- tiple answers from the LLMs and arrive at the correct answer through a majority vote (Fu et al., 2023). This approach is further improved upon by complex-based selection (Fu et al., 2023). CoT-related and self-consistency-related works can be seamlessly combined without any conflict. Prior research has not explored the potential of leveraging the outputs of LLM to refine reasoning paths iteratively. It stands to reason that similar to human cognition, LLM could benefit from reevaluating and adjusting its generated reasoning paths in order to correct errors and enhance over- all performance. In this paper, we propose a new method named Progressive-Hint Prompting (PHP) that involves se- quentially interacting with LLM to approach the correct an- swer gradually. The method operates as follows: (1) given a question, we ask the LLM to provide a Base Answer; (2) we combine the question and answer to re-ask the LLM and obtain the Subsequent Answer; (3) we repeat the operation in (2) until the answer is stable and does not change over the last two answers. PHP follows a human-like thought process where previous answers are leveraged as hints to arrive at the correct answer after re-evaluating the question. Figure 1 illustrates the proposed PHP framework. We use the base prompt to obtain the initial base answer, and then employ the PHP prompt for subsequent questions. If the cur- rent answer matches the previous answer, it is more likely to be correct, and we terminate the LLM inquiry. With Com- plex CoT and GPT-4, after adding PHP, the performance ach","Conclusion This paper introduces a novel approach named Progressive- Hint Prompting (PHP) for interacting with LLMs, which offers multiple advantages: 1) PHP achieves substantial per- formance improvements on math reasoning tasks, leading to state-of-the-art results on several reasoning benchmarks; 2) with more powerful models and prompts, PHP can better and consistently benefit the LLMs; 3) PHP can be easily combined with CoT and self-consistency to further improve performance. To better enhance the progressive-hint prompting approach, future research endeavors can focus on improving the de- sign of handcrafted hints in the question phase and prompt sentences in the answer part. Additionally, novel hints that aid the LLMs to reconsider the questions can be identified and extracted beside the answer. 8 Progressive-Hint Prompting Improves Reasoning in Large Language Models","Progressive-Hint Prompting Improves Reasoning in Large Language Models We propose a new prompting strategy, Progressive-Hint Promoting, that can be easily combined with Chain-Of-Thought and Self-Consistency to improve performance. The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that en- hance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic mul- tiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experi- ments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text- davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sam- ple paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% 91.9%), GSM8K (92% 95.5%), AQuA (76.4% 79.9%) and MATH (50.3% 53.9%).","Progressive-Hint Prompting Improves Reasoning in Large Language Models We propose a new prompting strategy, Progressive-Hint Promoting, that can be easily combined with Chain-Of-Thought and Self-Consistency to improve performance. The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that en- hance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic mul- tiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experi- ments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text- davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sam- ple paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% 91.9%), GSM8K (92% 95.5%), AQuA (76.4% 79.9%) and MATH (50.3% 53.9%). Conclusion This paper introduces a novel approach named Progressive- Hint Prompting (PHP) for interacting with LLMs, which offers multiple advantages: 1) PHP achieves substantial per- formance improvements on math reasoning tasks, leading to state-of-the-art results on several reasoning benchmarks; 2) with more powerful models and prompts, PHP can better and consistently benefit the LLMs; 3) PHP can be easily combined with CoT and self-consistency to further improve performance. To better enhance the progressive-hint prompting approach, future research endeavors can focus on improving the de- sign of handcrafted hints in the question phase and prompt sentences in the answer part. Additionally, novel hints that aid the LLMs to reconsider the questions can be identified and extracted beside the answer. 8 Progressive-Hint Prompting Improves Reasoning in Large Language Models approaches to promote intermediate reasoning steps (Wei et al., 2022; Zhou et al., 2023; Fu et al., 2023). Other works in this area, such as Least-to-Most (Zhou et al., 2023) and Complex CoT (Fu et al., 2023), have also explored this direction. An- other area of research is self-consistency-related approaches. In comparison to CoT-related work that focuses on design- ing better prompts, self-consistency proposes to sample mul- tiple answers from the LLMs and arrive at the correct answer through a majority vote (Fu et al., 2023). This approach is further improved upon by complex-based selection (Fu et al., 2023). CoT-related and self-consistency-related works can be seamlessly combined without any conflict. Prior research has not explored the potential of leveraging the outputs of LLM to refine reasoning paths iteratively. It stands to reason that similar to human cognition, LLM could benefit from reevaluating and adjusting its generated reasoning paths in order to correct errors and enhance over- all performance. In this paper, we propose a new method named Progressive-Hint Prompting (PHP) that involves se- quentially interacting with LLM to approach the correct an- swer gradually. The method operates as follows: (1) given a question, we ask the LLM to provide a Base Answer; (2) we combine the question and answer to re-ask the LLM and obtain the Subsequent Answer; (3) we repeat the operation in (2) until the answer is stable and does not change over the last two answers. PHP follows a human-like thought process where previous answers are leveraged as hints to arrive at the correct answer after re-evaluating the question. Figure 1 illustrates the proposed PHP framework. We use the base prompt to obtain the initial base answer, and then employ the PHP prompt for subsequent questions. If the cur- rent answer matches the previous answer, it is more likely to be correct, and we terminate the LLM inquiry. With Com- plex CoT and GPT-4, after adding PHP, the performance ach",0
2e116b08e4c0d273b422594f60a3c16662b3d034,VerityMath: Advancing Mathematical Reasoning by Self-Verification Through Unit Consistency,"['Vernon Toh Yan Han', 'Ratish Puduppully', 'Nancy F. Chen']",https://openreview.net/pdf/2e116b08e4c0d273b422594f60a3c16662b3d034.pdf,"VerityMath: Advancing Mathematical Reasoning by Self-Verification Through Unit Consistency Study evaluates 7B LLaMA, Code Llama and Mistral on math word problems, introduces ""Unit Consistency Programs"" for multi-unit challenges, and reports initial results with the enhanced ""VerityMath"" model. Large Language Models (LLMs), combined with program-based solving techniques, are increasingly demonstrating proficiency in mathematical reasoning. For example, closed-source models such as OpenAI GPT-4 and Claude show excellent results in solving math word problems. However, progress in math word problem-solving for open-source LLMs is limited, and the challenges these models face are not well-studied. In this paper, we study the performance of strong open-source LLMs, including Llama 2 (7B), Code Llama (7B), and Mistral (7B) on math word problems using program-based solving techniques. Specifically, we analyze the outputs of these models when applied to math word problems and identify a category of problems that pose a significant challenge, particularly those involving quantities spanning multiple units. To address this issue, we propose a systematic approach by defining the units for each quantity and ensuring the consistency of these units during mathematical operations. We developed Unit Consistency Programs (UCPs), an annotated dataset of math word problems, each paired with programs containing unit specifications and unit verification routines. We fine-tuned Llama 2 (7B), Code Llama (7B), and Mistral (7B) models with UCPs to produce their VerityMath variants. Our findings indicate that our approach, which incorporates unit consistency, currently slightly underperforms compared to an approach that does not. To understand the reasons behind this, we conducted an in-depth error analysis and suggested options for future improvements.",2e116b08e4c0d273b422594f60a3c16662b3d034.pdf,"Methodology
3.1. Unit Consistency Programs
Unit consistency checks are essential safeguards, helping
to identify and prevent errors from inconsistent units in
mathematical equations. In contrast to PAL/PoT approaches
that directly generate programs to solve math word prob-
lems, our method enhances these programs by integrating
specialized Counter objects. These objects are respon-
sible for tracking variable units and ensuring the correct
handling of operations with differing units. Additionally,
we incorporate assert statements after each equation, as
illustrated in Figure 1 (bottom). These assert statements
verify unit consistency within equations, triggering an error
if unit mismatches are detected.
Consider the example in Figure 1 (bottom), illustrating a
multiplication operation between shirts count (mea-
sured in ‘shirts’) and cost pershirt (measured in ‘dol-
lars per shirt’). In this operation, the units of ‘shirts’ fromPositive Predicted Negative Predicted
Actual Positive 37 16
Actual Negative 9 38
Precision Recall Accuracy
80.4% 69.8% 75.0%
Table 3: Small human evaluation compared on GPT-3.5
Turbo classification on 100 randomly sampled test examples
from GSM8K. Human annotations were done by the first
author.
shirts count and ‘per shirt’ from cost pershirt
naturally cancel each other out, resulting in a unit of ‘dol-
lars’. An assert statement is used to verify this expected
cancellation of units. In our notation, the exponent of a unit
in the numerator is represented as +1, and in the denominator
as -1. Therefore, in this multiplication, the positive exponent
of ‘shirts’ in shirts count cancels with the negative ex-
ponent of ‘per shirt’ in cost pershirt , aligning the
product’s right-hand side (RHS) with the expected left-hand
side (LHS) unit of total cost before discount ,
confirming it is in ‘dollars’. The example also illustrates
a unitless quantity, specifically a percentage. In this case,
there won’t be any units specified in the Counter initial-
iz","Future Work
In this study, we analyzed open-source Large Language
Models (LLMs) and pinpointed their struggle with math
problems involving multiple units, highlighting a key im-
provement area. We introduced Unit Consistency Programs(UCPs) as a novel method to address LLMs’ reasoning and
verification abilities, especially in complex math problems.
We identified some limitations in our current approach. Fu-
ture work will focus on advancing unit check methodologies
in UCPs to address these limitations.
Limitations
Recent creations of synthetic datasets for math problem-
solving often rely on prompting large language models
(LLMs), such as GPT-4. However, this approach can be
costly, and on a large scale, the expenses escalate. Our
dataset creation incurred a total cost of approximately $350
USD. Due to budget constraints, we couldn’t sample multi-
ple reasoning paths per question, as presented in Wang et al.
(2023), limiting the potential for increased annotations.
Impact Statement
This paper presents work whose goal is to advance the field
of math problem-solving using LLMs. However, it is also
crucial to be aware of the potential risks associated with
VerityMath. Due to the current challenges of VerityMath,
the units initialized by Counter andassert statements
may not always be accurate. Consequently, it is strongly rec-
ommended to exercise caution when relying on VerityMath
outputs for any use.
Acknowledgements
This research is supported by the Ministry of Education,
Singapore, under its Science of Learning Grant (award ID:
MOESOL2021-0006). Any opinions, findings and con-
clusions or recommendations expressed in this material
are those of the author(s) and do not reflect the views of
the Ministry of Education, Singapore. The computational
work for this article was partially performed on resources
of the National Supercomputing Centre (NSCC), Singapore
(https://www.nscc.sg ).
References
Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D.,
Passos, A., Shake","Methodology 3.1. Unit Consistency Programs Unit consistency checks are essential safeguards, helping to identify and prevent errors from inconsistent units in mathematical equations. In contrast to PAL/PoT approaches that directly generate programs to solve math word prob- lems, our method enhances these programs by integrating specialized Counter objects. These objects are respon- sible for tracking variable units and ensuring the correct handling of operations with differing units. Additionally, we incorporate assert statements after each equation, as illustrated in Figure 1 (bottom). These assert statements verify unit consistency within equations, triggering an error if unit mismatches are detected. Consider the example in Figure 1 (bottom), illustrating a multiplication operation between shirts count (mea- sured in shirts ) and cost pershirt (measured in dol- lars per shirt ). In this operation, the units of shirts fromPositive Predicted Negative Predicted Actual Positive 37 16 Actual Negative 9 38 Precision Recall Accuracy 80.4% 69.8% 75.0% Table 3: Small human evaluation compared on GPT-3.5 Turbo classification on 100 randomly sampled test examples from GSM8K. Human annotations were done by the first author. shirts count and per shirt from cost pershirt naturally cancel each other out, resulting in a unit of dol- lars . An assert statement is used to verify this expected cancellation of units. In our notation, the exponent of a unit in the numerator is represented as +1, and in the denominator as -1. Therefore, in this multiplication, the positive exponent of shirts in shirts count cancels with the negative ex- ponent of per shirt in cost pershirt , aligning the product s right-hand side (RHS) with the expected left-hand side (LHS) unit of total cost before discount , confirming it is in dollars . The example also illustrates a unitless quantity, specifically a percentage. In this case, there won t be any units specified in the Counter initial- iz","Future Work In this study, we analyzed open-source Large Language Models (LLMs) and pinpointed their struggle with math problems involving multiple units, highlighting a key im- provement area. We introduced Unit Consistency Programs(UCPs) as a novel method to address LLMs reasoning and verification abilities, especially in complex math problems. We identified some limitations in our current approach. Fu- ture work will focus on advancing unit check methodologies in UCPs to address these limitations. Limitations Recent creations of synthetic datasets for math problem- solving often rely on prompting large language models (LLMs), such as GPT-4. However, this approach can be costly, and on a large scale, the expenses escalate. Our dataset creation incurred a total cost of approximately $350 USD. Due to budget constraints, we couldn t sample multi- ple reasoning paths per question, as presented in Wang et al. (2023), limiting the potential for increased annotations. Impact Statement This paper presents work whose goal is to advance the field of math problem-solving using LLMs. However, it is also crucial to be aware of the potential risks associated with VerityMath. Due to the current challenges of VerityMath, the units initialized by Counter andassert statements may not always be accurate. Consequently, it is strongly rec- ommended to exercise caution when relying on VerityMath outputs for any use.","VerityMath: Advancing Mathematical Reasoning by Self-Verification Through Unit Consistency Study evaluates 7B LLaMA, Code Llama and Mistral on math word problems, introduces ""Unit Consistency Programs"" for multi-unit challenges, and reports initial results with the enhanced ""VerityMath"" model. Large Language Models (LLMs), combined with program-based solving techniques, are increasingly demonstrating proficiency in mathematical reasoning. For example, closed-source models such as OpenAI GPT-4 and Claude show excellent results in solving math word problems. However, progress in math word problem-solving for open-source LLMs is limited, and the challenges these models face are not well-studied. In this paper, we study the performance of strong open-source LLMs, including Llama 2 (7B), Code Llama (7B), and Mistral (7B) on math word problems using program-based solving techniques. Specifically, we analyze the outputs of these models when applied to math word problems and identify a category of problems that pose a significant challenge, particularly those involving quantities spanning multiple units. To address this issue, we propose a systematic approach by defining the units for each quantity and ensuring the consistency of these units during mathematical operations. We developed Unit Consistency Programs (UCPs), an annotated dataset of math word problems, each paired with programs containing unit specifications and unit verification routines. We fine-tuned Llama 2 (7B), Code Llama (7B), and Mistral (7B) models with UCPs to produce their VerityMath variants. Our findings indicate that our approach, which incorporates unit consistency, currently slightly underperforms compared to an approach that does not. To understand the reasons behind this, we conducted an in-depth error analysis and suggested options for future improvements.","VerityMath: Advancing Mathematical Reasoning by Self-Verification Through Unit Consistency Study evaluates 7B LLaMA, Code Llama and Mistral on math word problems, introduces ""Unit Consistency Programs"" for multi-unit challenges, and reports initial results with the enhanced ""VerityMath"" model. Large Language Models (LLMs), combined with program-based solving techniques, are increasingly demonstrating proficiency in mathematical reasoning. For example, closed-source models such as OpenAI GPT-4 and Claude show excellent results in solving math word problems. However, progress in math word problem-solving for open-source LLMs is limited, and the challenges these models face are not well-studied. In this paper, we study the performance of strong open-source LLMs, including Llama 2 (7B), Code Llama (7B), and Mistral (7B) on math word problems using program-based solving techniques. Specifically, we analyze the outputs of these models when applied to math word problems and identify a category of problems that pose a significant challenge, particularly those involving quantities spanning multiple units. To address this issue, we propose a systematic approach by defining the units for each quantity and ensuring the consistency of these units during mathematical operations. We developed Unit Consistency Programs (UCPs), an annotated dataset of math word problems, each paired with programs containing unit specifications and unit verification routines. We fine-tuned Llama 2 (7B), Code Llama (7B), and Mistral (7B) models with UCPs to produce their VerityMath variants. Our findings indicate that our approach, which incorporates unit consistency, currently slightly underperforms compared to an approach that does not. To understand the reasons behind this, we conducted an in-depth error analysis and suggested options for future improvements. Future Work In this study, we analyzed open-source Large Language Models (LLMs) and pinpointed their struggle with math problems involving multiple units, highlighting a key im- provement area. We introduced Unit Consistency Programs(UCPs) as a novel method to address LLMs reasoning and verification abilities, especially in complex math problems. We identified some limitations in our current approach. Fu- ture work will focus on advancing unit check methodologies in UCPs to address these limitations. Limitations Recent creations of synthetic datasets for math problem- solving often rely on prompting large language models (LLMs), such as GPT-4. However, this approach can be costly, and on a large scale, the expenses escalate. Our dataset creation incurred a total cost of approximately $350 USD. Due to budget constraints, we couldn t sample multi- ple reasoning paths per question, as presented in Wang et al. (2023), limiting the potential for increased annotations. Impact Statement This paper presents work whose goal is to advance the field of math problem-solving using LLMs. However, it is also crucial to be aware of the potential risks associated with VerityMath. Due to the current challenges of VerityMath, the units initialized by Counter andassert statements may not always be accurate. Consequently, it is strongly rec- ommended to exercise caution when relying on VerityMath outputs for any use. Methodology 3.1. Unit Consistency Programs Unit consistency checks are essential safeguards, helping to identify and prevent errors from inconsistent units in mathematical equations. In contrast to PAL/PoT approaches that directly generate programs to solve math word prob- lems, our method enhances these programs by integrating specialized Counter objects. These objects are respon- sible for tracking variable units and ensuring the correct handling of operations with differing units. Additionally, we incorporate assert statements after each equation, as illustrated in Figure 1 (bottom). These assert statements verify unit consistency within equations, triggering an error if unit mismatches are detected. Consider the example in Figure 1 (bottom), illustrating a multiplication operation between shirts count (mea- sured in shirts ) and cost pershirt (measured in dol- lars per shirt ). In this operation, the units of shirts fromPositive Predicted Negative Predicted Actual Positive 37 16 Actual Negative 9 38 Precision Recall Accuracy 80.4% 69.8% 75.0% Table 3: Small human evaluation compared on GPT-3.5 Turbo classification on 100 randomly sampled test examples from GSM8K. Human annotations were done by the first author. shirts count and per shirt from cost pershirt naturally cancel each other out, resulting in a unit of dol- lars . An assert statement is used to verify this expected cancellation of units. In our notation, the exponent of a unit in the numerator is represented as +1, and in the denominator as -1. Therefore, in this multiplication, the positive exponent of shirts in shirts count cancels with the negative ex- ponent of per shirt in cost pershirt , aligning the product s right-hand side (RHS) with the expected left-hand side (LHS) unit of total cost before discount , confirming it is in dollars . The example also illustrates a unitless quantity, specifically a percentage. In this case, there won t be any units specified in the Counter initial- iz",0
e71382c2b2bdd9fa5eaeae740459879c5613a036,Smart Vision-Language Reasoners,"['Denisa Roberts', 'Lucas Roberts']",https://openreview.net/pdf/e71382c2b2bdd9fa5eaeae740459879c5613a036.pdf,"Smart Vision-Language Reasoners Deep learning innovations led to improvement in reasoning ability of vision-language models. In this article, we investigate vision-language models (VLM) as reasoners. The ability to form abstractions underlies mathematical reasoning, problem-solving, and other Math AI tasks. Several formalisms have been given to these underlying abstractions and skills utilized by humans and intelligent systems for reasoning. Furthermore, human reasoning is inherently multimodal, and as such, we focus our investigations on multimodal AI. In this article, we employ the abstractions given in the SMART task (Simple Multimodal Algorithmic Reasoning Task) introduced in \cite{cherian2022deep} as meta-reasoning and problem-solving skills along eight axes: math, counting, path, measure, logic, spatial, and pattern. We investigate the ability of vision-language models to reason along these axes and seek avenues of improvement. Including composite representations with vision-language cross-attention enabled learning multimodal representations adaptively from fused frozen pretrained backbones for better visual grounding. Furthermore, proper hyperparameter and other training choices led to strong improvements (up to gain in accuracy) on the SMART task, further underscoring the power of deep multimodal learning. The smartest VLM, which includes a novel QF multimodal layer, improves upon the best previous baselines in every one of the eight fundamental reasoning skills. End-to-end code is available at \href{https://github.com/D-Roberts/smarter}{github.com/D-Roberts/smarter}.",e71382c2b2bdd9fa5eaeae740459879c5613a036.pdf,"Methodology
We formalize the problem as supervised learning with classi-
fication loss. For each image-question instance, we predict
the probability of one of five answer options. When the
4
Smart Vision-Language Reasoners
Table 3. QF Ablations. Validation Accuracy per Skill Class (counting, math, logic, path) per Architectural, Optimization and Hyperparam-
eter Choices. The fused vision encoder is DinoV2+SigLIP and the text encoder is SigLIP. From CometML multimodalAI.
CHOICES COUNTING MATH LOGIC PATH
SMARTEST VLM 33.8 8.5 26.2 20.1
1 MHA HEADS 29.7 8.2 23.1 19.4
3 MHA HEADS 34.2 8.6 25.8 19.9
4 MHA HEADS 32.9 8.7 25.6 20.1
8 MHA HEADS 33.1 8.4 26.9 20.3
QF I NTERMEDIATE SIZE 128 33.1 8.6 25.6 19.8
QF I NTERMEDIATE SIZE 512 33.4 8.1 27.3 19.9
QF I NTERMEDIATE SIZE 768 33.4 8.7 25.1 19.3
QF I NTERMEDIATE RELU 32.8 8.7 26.7 19.8
QF I NTERMEDIATE SILU 33.2 8.7 26.5 19.6
COMPOSITE :NOQF LAYER 32.8 8.5 23.8 19.7
COMPOSITE : QF ONLY 32.2 8.0 26.3 18.8
COMPOSITE : QF AND VISION ONLY 33.7 8.7 24.8 20.4
COMPOSITE : QF AND LANGUAGE ONLY 33.6 8.9 24.9 19.4
NO RESIDUAL CONNECTION IN QF INTERMEDIATE 32 8.4 26.4 19
DROPOUT 0INQF LAYER 33 8.2 26.9 19.6
DROPOUT 0.1 INQF LAYER 30.3 8.1 25.5 19
Table 4. QF Layer Ablations. Validation Accuracy per Skill Class (algebra, measure, spatial, pattern) per Architectural, Optimization and
Hyperparameter Choices. The fused vision encoder is DinoV2+SigLIP and the text encoder is SigLIP. From CometML multimodalAI.
CHOICES ALGEBRA MEASURE SPATIAL PATTERN
SMARTEST VLM 11.2 10.4 26.8 27
1 MHA HEADS 10.5 10.8 23.2 22.7
3 MHA HEADS 11.1 11.3 26.8 27.0
4 MHA HEADS 11.2 10.6 27.8 26.6
8 MHA HEADS 11.6 10.4 27.9 25.8
QF I NTERMEDIATE SIZE 128 11.1 10.2 26.9 27.1
QF I NTERMEDIATE SIZE 512 11.3 10.4 27.8 26.4
QF I NTERMEDIATE SIZE 768 11.3 11.5 27 26.7
QF I NTERMEDIATE RELU 10.8 9.9 28.1 25.5
QF I NTERMEDIATE SILU 10.9 9.5 27.4 26
COMPOSITE :NOQF 11.5 10.3 25.6 25.4
COMPOSITE : QF ONLY 11.3 10.7 27.4 25.7
COMPOSITE : QF AND VISION ONLY 11.3 11.5 27.3 27.","future works. In a related vein, (Wu &
Xie, 2023) and (Tong et al., 2024), proffer the reasoning ca-pabilities of multimodal large language models and explores
their visual representation learning abilities.
2.1. Benchmark, Dataset, and Challenges
So how can we help (deep) artificial neural networks rea-
son better? In (Cherian et al., 2022) experiments show
that the visual signal is very important in solving complex
multi-reasoning skill puzzles and, despite being very large,
language-only models lag behind visual language models in
terms of performance. Conversely, in (Zhang et al., 2024b)
the conclusion appears to be that large multimodal models
cannot truly understand the visual diagrams for mathemati-
cal reasoning, along the line of weak visual grounding and
poor attention to visual detail in (Tong et al., 2024) and
(Wu & Xie, 2023) for large multimodal models for math,
question answering, and other reasoning tasks. The Simple
Multimodal Algorithmic Reasoning Task (SMART) intro-
duced in (Cherian et al., 2022) contains puzzles that measure
intelligence across eight different reasoning skill classes:
counting, math, logic, path, measure, logic, and pattern.
Problems include an image and a text question and are for-
mulated as multiple choice. We can see a few examples of
problems in Figure 2. Baseline models trained in (Cherian
et al., 2022) struggle to solve this task, especially when
employing transformers. In the past, specialized neural net-
works such as (Mikuła et al., 2023) have been developed to
solve specific reasoning tasks, specifically premise selection
in automated theorem proving. In this article, we investigate
how we can craft and train deep neural networks which em-
ploy several types of deep learning blocks and multimodal
inputs from deep frozen transformers to reason better across
the eight meta reasoning axes in the SMART task.
The SMART reasoning task and baselines . A set of vision-
language models are trained as benchmarks in (Cherian
et","Methodology We formalize the problem as supervised learning with classi- fication loss. For each image-question instance, we predict the probability of one of five answer options. When the 4 Smart Vision-Language Reasoners Table 3. QF Ablations. Validation Accuracy per Skill Class (counting, math, logic, path) per Architectural, Optimization and Hyperparam- eter Choices. The fused vision encoder is DinoV2+SigLIP and the text encoder is SigLIP. From CometML multimodalAI. CHOICES COUNTING MATH LOGIC PATH SMARTEST VLM 33.8 8.5 26.2 20.1 1 MHA HEADS 29.7 8.2 23.1 19.4 3 MHA HEADS 34.2 8.6 25.8 19.9 4 MHA HEADS 32.9 8.7 25.6 20.1 8 MHA HEADS 33.1 8.4 26.9 20.3 QF I NTERMEDIATE SIZE 128 33.1 8.6 25.6 19.8 QF I NTERMEDIATE SIZE 512 33.4 8.1 27.3 19.9 QF I NTERMEDIATE SIZE 768 33.4 8.7 25.1 19.3 QF I NTERMEDIATE RELU 32.8 8.7 26.7 19.8 QF I NTERMEDIATE SILU 33.2 8.7 26.5 19.6 COMPOSITE :NOQF LAYER 32.8 8.5 23.8 19.7 COMPOSITE : QF ONLY 32.2 8.0 26.3 18.8 COMPOSITE : QF AND VISION ONLY 33.7 8.7 24.8 20.4 COMPOSITE : QF AND LANGUAGE ONLY 33.6 8.9 24.9 19.4 NO RESIDUAL CONNECTION IN QF INTERMEDIATE 32 8.4 26.4 19 DROPOUT 0INQF LAYER 33 8.2 26.9 19.6 DROPOUT 0.1 INQF LAYER 30.3 8.1 25.5 19 Table 4. QF Layer Ablations. Validation Accuracy per Skill Class (algebra, measure, spatial, pattern) per Architectural, Optimization and Hyperparameter Choices. The fused vision encoder is DinoV2+SigLIP and the text encoder is SigLIP. From CometML multimodalAI. CHOICES ALGEBRA MEASURE SPATIAL PATTERN SMARTEST VLM 11.2 10.4 26.8 27 1 MHA HEADS 10.5 10.8 23.2 22.7 3 MHA HEADS 11.1 11.3 26.8 27.0 4 MHA HEADS 11.2 10.6 27.8 26.6 8 MHA HEADS 11.6 10.4 27.9 25.8 QF I NTERMEDIATE SIZE 128 11.1 10.2 26.9 27.1 QF I NTERMEDIATE SIZE 512 11.3 10.4 27.8 26.4 QF I NTERMEDIATE SIZE 768 11.3 11.5 27 26.7 QF I NTERMEDIATE RELU 10.8 9.9 28.1 25.5 QF I NTERMEDIATE SILU 10.9 9.5 27.4 26 COMPOSITE :NOQF 11.5 10.3 25.6 25.4 COMPOSITE : QF ONLY 11.3 10.7 27.4 25.7 COMPOSITE : QF AND VISION ONLY 11.3 11.5 27.3 27.","future works. In a related vein, (Wu & Xie, 2023) and (Tong et al., 2024), proffer the reasoning ca-pabilities of multimodal large language models and explores their visual representation learning abilities. 2.1. Benchmark, Dataset, and Challenges So how can we help (deep) artificial neural networks rea- son better? In (Cherian et al., 2022) experiments show that the visual signal is very important in solving complex multi-reasoning skill puzzles and, despite being very large, language-only models lag behind visual language models in terms of performance. Conversely, in (Zhang et al., 2024b) the conclusion appears to be that large multimodal models cannot truly understand the visual diagrams for mathemati- cal reasoning, along the line of weak visual grounding and poor attention to visual detail in (Tong et al., 2024) and (Wu & Xie, 2023) for large multimodal models for math, question answering, and other reasoning tasks. The Simple Multimodal Algorithmic Reasoning Task (SMART) intro- duced in (Cherian et al., 2022) contains puzzles that measure intelligence across eight different reasoning skill classes: counting, math, logic, path, measure, logic, and pattern. Problems include an image and a text question and are for- mulated as multiple choice. We can see a few examples of problems in Figure 2. Baseline models trained in (Cherian et al., 2022) struggle to solve this task, especially when employing transformers. In the past, specialized neural net- works such as (Miku a et al., 2023) have been developed to solve specific reasoning tasks, specifically premise selection in automated theorem proving. In this article, we investigate how we can craft and train deep neural networks which em- ploy several types of deep learning blocks and multimodal inputs from deep frozen transformers to reason better across the eight meta reasoning axes in the SMART task. The SMART reasoning task and baselines . A set of vision- language models are trained as benchmarks in (Cherian et","Smart Vision-Language Reasoners Deep learning innovations led to improvement in reasoning ability of vision-language models. In this article, we investigate vision-language models (VLM) as reasoners. The ability to form abstractions underlies mathematical reasoning, problem-solving, and other Math AI tasks. Several formalisms have been given to these underlying abstractions and skills utilized by humans and intelligent systems for reasoning. Furthermore, human reasoning is inherently multimodal, and as such, we focus our investigations on multimodal AI. In this article, we employ the abstractions given in the SMART task (Simple Multimodal Algorithmic Reasoning Task) introduced in \cite{cherian2022deep} as meta-reasoning and problem-solving skills along eight axes: math, counting, path, measure, logic, spatial, and pattern. We investigate the ability of vision-language models to reason along these axes and seek avenues of improvement. Including composite representations with vision-language cross-attention enabled learning multimodal representations adaptively from fused frozen pretrained backbones for better visual grounding. Furthermore, proper hyperparameter and other training choices led to strong improvements (up to gain in accuracy) on the SMART task, further underscoring the power of deep multimodal learning. The smartest VLM, which includes a novel QF multimodal layer, improves upon the best previous baselines in every one of the eight fundamental reasoning skills. End-to-end code is available at \href{https://github.com/D-Roberts/smarter}{github.com/D-Roberts/smarter}.","Smart Vision-Language Reasoners Deep learning innovations led to improvement in reasoning ability of vision-language models. In this article, we investigate vision-language models (VLM) as reasoners. The ability to form abstractions underlies mathematical reasoning, problem-solving, and other Math AI tasks. Several formalisms have been given to these underlying abstractions and skills utilized by humans and intelligent systems for reasoning. Furthermore, human reasoning is inherently multimodal, and as such, we focus our investigations on multimodal AI. In this article, we employ the abstractions given in the SMART task (Simple Multimodal Algorithmic Reasoning Task) introduced in \cite{cherian2022deep} as meta-reasoning and problem-solving skills along eight axes: math, counting, path, measure, logic, spatial, and pattern. We investigate the ability of vision-language models to reason along these axes and seek avenues of improvement. Including composite representations with vision-language cross-attention enabled learning multimodal representations adaptively from fused frozen pretrained backbones for better visual grounding. Furthermore, proper hyperparameter and other training choices led to strong improvements (up to gain in accuracy) on the SMART task, further underscoring the power of deep multimodal learning. The smartest VLM, which includes a novel QF multimodal layer, improves upon the best previous baselines in every one of the eight fundamental reasoning skills. End-to-end code is available at \href{https://github.com/D-Roberts/smarter}{github.com/D-Roberts/smarter}. future works. In a related vein, (Wu & Xie, 2023) and (Tong et al., 2024), proffer the reasoning ca-pabilities of multimodal large language models and explores their visual representation learning abilities. 2.1. Benchmark, Dataset, and Challenges So how can we help (deep) artificial neural networks rea- son better? In (Cherian et al., 2022) experiments show that the visual signal is very important in solving complex multi-reasoning skill puzzles and, despite being very large, language-only models lag behind visual language models in terms of performance. Conversely, in (Zhang et al., 2024b) the conclusion appears to be that large multimodal models cannot truly understand the visual diagrams for mathemati- cal reasoning, along the line of weak visual grounding and poor attention to visual detail in (Tong et al., 2024) and (Wu & Xie, 2023) for large multimodal models for math, question answering, and other reasoning tasks. The Simple Multimodal Algorithmic Reasoning Task (SMART) intro- duced in (Cherian et al., 2022) contains puzzles that measure intelligence across eight different reasoning skill classes: counting, math, logic, path, measure, logic, and pattern. Problems include an image and a text question and are for- mulated as multiple choice. We can see a few examples of problems in Figure 2. Baseline models trained in (Cherian et al., 2022) struggle to solve this task, especially when employing transformers. In the past, specialized neural net- works such as (Miku a et al., 2023) have been developed to solve specific reasoning tasks, specifically premise selection in automated theorem proving. In this article, we investigate how we can craft and train deep neural networks which em- ploy several types of deep learning blocks and multimodal inputs from deep frozen transformers to reason better across the eight meta reasoning axes in the SMART task. The SMART reasoning task and baselines . A set of vision- language models are trained as benchmarks in (Cherian et Methodology We formalize the problem as supervised learning with classi- fication loss. For each image-question instance, we predict the probability of one of five answer options. When the 4 Smart Vision-Language Reasoners Table 3. QF Ablations. Validation Accuracy per Skill Class (counting, math, logic, path) per Architectural, Optimization and Hyperparam- eter Choices. The fused vision encoder is DinoV2+SigLIP and the text encoder is SigLIP. From CometML multimodalAI. CHOICES COUNTING MATH LOGIC PATH SMARTEST VLM 33.8 8.5 26.2 20.1 1 MHA HEADS 29.7 8.2 23.1 19.4 3 MHA HEADS 34.2 8.6 25.8 19.9 4 MHA HEADS 32.9 8.7 25.6 20.1 8 MHA HEADS 33.1 8.4 26.9 20.3 QF I NTERMEDIATE SIZE 128 33.1 8.6 25.6 19.8 QF I NTERMEDIATE SIZE 512 33.4 8.1 27.3 19.9 QF I NTERMEDIATE SIZE 768 33.4 8.7 25.1 19.3 QF I NTERMEDIATE RELU 32.8 8.7 26.7 19.8 QF I NTERMEDIATE SILU 33.2 8.7 26.5 19.6 COMPOSITE :NOQF LAYER 32.8 8.5 23.8 19.7 COMPOSITE : QF ONLY 32.2 8.0 26.3 18.8 COMPOSITE : QF AND VISION ONLY 33.7 8.7 24.8 20.4 COMPOSITE : QF AND LANGUAGE ONLY 33.6 8.9 24.9 19.4 NO RESIDUAL CONNECTION IN QF INTERMEDIATE 32 8.4 26.4 19 DROPOUT 0INQF LAYER 33 8.2 26.9 19.6 DROPOUT 0.1 INQF LAYER 30.3 8.1 25.5 19 Table 4. QF Layer Ablations. Validation Accuracy per Skill Class (algebra, measure, spatial, pattern) per Architectural, Optimization and Hyperparameter Choices. The fused vision encoder is DinoV2+SigLIP and the text encoder is SigLIP. From CometML multimodalAI. CHOICES ALGEBRA MEASURE SPATIAL PATTERN SMARTEST VLM 11.2 10.4 26.8 27 1 MHA HEADS 10.5 10.8 23.2 22.7 3 MHA HEADS 11.1 11.3 26.8 27.0 4 MHA HEADS 11.2 10.6 27.8 26.6 8 MHA HEADS 11.6 10.4 27.9 25.8 QF I NTERMEDIATE SIZE 128 11.1 10.2 26.9 27.1 QF I NTERMEDIATE SIZE 512 11.3 10.4 27.8 26.4 QF I NTERMEDIATE SIZE 768 11.3 11.5 27 26.7 QF I NTERMEDIATE RELU 10.8 9.9 28.1 25.5 QF I NTERMEDIATE SILU 10.9 9.5 27.4 26 COMPOSITE :NOQF 11.5 10.3 25.6 25.4 COMPOSITE : QF ONLY 11.3 10.7 27.4 25.7 COMPOSITE : QF AND VISION ONLY 11.3 11.5 27.3 27.",0
2fb1e8e4e7c044ab018b903eb62e5ffdb089546a,Progress or Regress? Self-Improvement Reversal in Post-training,"['Ting Wu', 'Xuefeng Li', 'Pengfei Liu']",https://openreview.net/pdf/2fb1e8e4e7c044ab018b903eb62e5ffdb089546a.pdf,"Progress or Regress? Self-Improvement Reversal in Post-training A comprehensive evaluative framework to scrutinize the underlying mechanisms and outcomes of post-training self-improvement. Self-improvement through post-training methods such as iterative preference learning has been acclaimed for enhancing the problem-solving capabilities~(e.g., mathematical reasoning) of Large Language Models~(LLMs) without human intervention. However, as exploration deepens, it becomes crucial to assess whether these improvements genuinely signify progress in solving more challenging problems or if they could lead to unintended regressions. To address this, we propose a comprehensive evaluative framework that goes beyond the superficial pass@1 metric to scrutinize the underlying enhancements of post-training paradigms for self-improvement. Through rigorous experimentation and analysis across diverse problem-solving tasks, the empirical results point out the phenomenon of \emph{self-improvement reversal}, where models showing improved performance across benchmarks will paradoxically exhibit declines in broader, essential capabilities, like output diversity and out-of-distribution~(OOD) generalization. These findings indicate that current self-improvement practices through post-training are inadequate for equipping models to tackle more complex problems. Furthermore, they underscore the necessity of our critical evaluation metrics in discerning the \emph{progress or regress} dichotomy for self-improving LLMs.",2fb1e8e4e7c044ab018b903eb62e5ffdb089546a.pdf,"methodology to CSQA, GSM8K,
MATH and MBPP datasets with three post-training meth-
ods. Generation sampling Nvaries from 21to26with the
temperature set as 0.75.
Reversal Observation As depicted in Figure 3, contrary
to prior assumptions, the rapid increase in pass@N accu-
racy with increasing Nchallenges the notion of progres-
sively harder problem-solving. Specifically, as Ngrows,
M1achieves near-perfect pass@N accuracy on IS(t) across
all evaluated datasets, suggesting its inherent capacity to
tackle the deemed improvement problems .
Selection Optimization for Answer Alignment The em-
pirical findings depicted in Figure 3 offer a critical insight:
iterative self-improvement hardly entails the acquisition of
new problem-solving abilities, but rather the enhancement
of the model’s correct answer selection within its generation
space.
7
Progress or Regress? Self-Improvement Reversal in Post-training
5.2. Solutions Diversity
While pass@1 accuracy measures the correctness of the
final answer, it does not capture the diversity of solutions
a model can generate. We posit that a model’s capacity
to produce diverse solutions is indicative of its robustness
and flexibility in problem-solving. To thoroughly under-
stand the evolution of answer diversity during the process
of iterative self-improvement, we employ a combination of
Distinct N-grams (Li et al., 2016) and Sentence-BERT
embedding cosine similarity (Reimers & Gurevych, 2019)
to measure mod diversity. These metrics have been shown
to correlate well with human assessments of diversity (Tevet
& Berant, 2021). Additionally, for mathematical reasoning,
we introduce Distinct Equations to measure the diversity of
mathematical answers by analyzing the variety of equations
in the generated solutions.
Each diversity metric Div takes a set of Nmodel outputs,
and produces a scalar score representing how diverse the
set is. Distinct N-grams measures syntactic diversity by
counting the number of unique n-grams (averaged over
n= 1","Conclusion
In this paper, we foster a comprehensive understanding
of the current landscape of post-training practices in self-
improvement. Our evaluation, beyond simple pass@1 ac-
curacy, utilizing multifaceted metrics such as improvementproblems, solutions diversity and OOD generalization, un-
derscores the necessity for a critical examination of both the
progressive and regressive effects in current self-improving
post-training methods. By broadening the scope of our
analysis, we provide deeper insights into the true nature of
iterative self-improvement with post-training, paving the
way for more robust and genuinely self-improving LLMs.
Impact Statement
The current landscape of post-training practices for self-
improvement in large language models (LLMs) necessitates
a thorough understanding to address both their progressive
and regressive effects. In this work, we conduct an in-depth
evaluation beyond simple pass@1 accuracy, utilizing mul-
tifaceted metrics such as improvement problems, solutions
diversity, and OOD generalization. Our findings under-
score the critical need for a comprehensive examination
of these methods. By expanding our analytical scope, we
provide deeper insights into the true nature of iterative self-
improvement through post-training, paving the way for more
robust and genuinely self-improving LLMs. This research
aims to empower scholars and engineers to develop more
reliable and effective post-training strategies, ultimately ad-
vancing the field of LLMs and pushing the boundaries of
model capabilities to tackle more complex challenges.
References
AI@Meta. Introducing meta llama 3: The most capable
openly available llm to date. 2024. URL https://ai.
meta.com/blog/meta-llama-3/ .
Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski,
H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.
Program synthesis with large language models. arXiv
preprint arXiv:2108.07732 , 2021.
Chen, Z., Deng, Y ., Yuan, H., Ji, K., and Gu, Q. Self-
play","methodology to CSQA, GSM8K, MATH and MBPP datasets with three post-training meth- ods. Generation sampling Nvaries from 21to26with the temperature set as 0.75. Reversal Observation As depicted in Figure 3, contrary to prior assumptions, the rapid increase in pass@N accu- racy with increasing Nchallenges the notion of progres- sively harder problem-solving. Specifically, as Ngrows, M1achieves near-perfect pass@N accuracy on IS(t) across all evaluated datasets, suggesting its inherent capacity to tackle the deemed improvement problems . Selection Optimization for Answer Alignment The em- pirical findings depicted in Figure 3 offer a critical insight: iterative self-improvement hardly entails the acquisition of new problem-solving abilities, but rather the enhancement of the model s correct answer selection within its generation space. 7 Progress or Regress? Self-Improvement Reversal in Post-training 5.2. Solutions Diversity While pass@1 accuracy measures the correctness of the final answer, it does not capture the diversity of solutions a model can generate. We posit that a model s capacity to produce diverse solutions is indicative of its robustness and flexibility in problem-solving. To thoroughly under- stand the evolution of answer diversity during the process of iterative self-improvement, we employ a combination of Distinct N-grams (Li et al., 2016) and Sentence-BERT embedding cosine similarity (Reimers & Gurevych, 2019) to measure mod diversity. These metrics have been shown to correlate well with human assessments of diversity (Tevet & Berant, 2021). Additionally, for mathematical reasoning, we introduce Distinct Equations to measure the diversity of mathematical answers by analyzing the variety of equations in the generated solutions. Each diversity metric Div takes a set of Nmodel outputs, and produces a scalar score representing how diverse the set is. Distinct N-grams measures syntactic diversity by counting the number of unique n-grams (averaged over n= 1","Conclusion In this paper, we foster a comprehensive understanding of the current landscape of post-training practices in self- improvement. Our evaluation, beyond simple pass@1 ac- curacy, utilizing multifaceted metrics such as improvementproblems, solutions diversity and OOD generalization, un- derscores the necessity for a critical examination of both the progressive and regressive effects in current self-improving post-training methods. By broadening the scope of our analysis, we provide deeper insights into the true nature of iterative self-improvement with post-training, paving the way for more robust and genuinely self-improving LLMs. Impact Statement The current landscape of post-training practices for self- improvement in large language models (LLMs) necessitates a thorough understanding to address both their progressive and regressive effects. In this work, we conduct an in-depth evaluation beyond simple pass@1 accuracy, utilizing mul- tifaceted metrics such as improvement problems, solutions diversity, and OOD generalization. Our findings under- score the critical need for a comprehensive examination of these methods. By expanding our analytical scope, we provide deeper insights into the true nature of iterative self- improvement through post-training, paving the way for more robust and genuinely self-improving LLMs. This research aims to empower scholars and engineers to develop more reliable and effective post-training strategies, ultimately ad- vancing the field of LLMs and pushing the boundaries of model capabilities to tackle more complex challenges.","Progress or Regress? Self-Improvement Reversal in Post-training A comprehensive evaluative framework to scrutinize the underlying mechanisms and outcomes of post-training self-improvement. Self-improvement through post-training methods such as iterative preference learning has been acclaimed for enhancing the problem-solving capabilities~(e.g., mathematical reasoning) of Large Language Models~(LLMs) without human intervention. However, as exploration deepens, it becomes crucial to assess whether these improvements genuinely signify progress in solving more challenging problems or if they could lead to unintended regressions. To address this, we propose a comprehensive evaluative framework that goes beyond the superficial pass@1 metric to scrutinize the underlying enhancements of post-training paradigms for self-improvement. Through rigorous experimentation and analysis across diverse problem-solving tasks, the empirical results point out the phenomenon of \emph{self-improvement reversal}, where models showing improved performance across benchmarks will paradoxically exhibit declines in broader, essential capabilities, like output diversity and out-of-distribution~(OOD) generalization. These findings indicate that current self-improvement practices through post-training are inadequate for equipping models to tackle more complex problems. Furthermore, they underscore the necessity of our critical evaluation metrics in discerning the \emph{progress or regress} dichotomy for self-improving LLMs.","Progress or Regress? Self-Improvement Reversal in Post-training A comprehensive evaluative framework to scrutinize the underlying mechanisms and outcomes of post-training self-improvement. Self-improvement through post-training methods such as iterative preference learning has been acclaimed for enhancing the problem-solving capabilities~(e.g., mathematical reasoning) of Large Language Models~(LLMs) without human intervention. However, as exploration deepens, it becomes crucial to assess whether these improvements genuinely signify progress in solving more challenging problems or if they could lead to unintended regressions. To address this, we propose a comprehensive evaluative framework that goes beyond the superficial pass@1 metric to scrutinize the underlying enhancements of post-training paradigms for self-improvement. Through rigorous experimentation and analysis across diverse problem-solving tasks, the empirical results point out the phenomenon of \emph{self-improvement reversal}, where models showing improved performance across benchmarks will paradoxically exhibit declines in broader, essential capabilities, like output diversity and out-of-distribution~(OOD) generalization. These findings indicate that current self-improvement practices through post-training are inadequate for equipping models to tackle more complex problems. Furthermore, they underscore the necessity of our critical evaluation metrics in discerning the \emph{progress or regress} dichotomy for self-improving LLMs. Conclusion In this paper, we foster a comprehensive understanding of the current landscape of post-training practices in self- improvement. Our evaluation, beyond simple pass@1 ac- curacy, utilizing multifaceted metrics such as improvementproblems, solutions diversity and OOD generalization, un- derscores the necessity for a critical examination of both the progressive and regressive effects in current self-improving post-training methods. By broadening the scope of our analysis, we provide deeper insights into the true nature of iterative self-improvement with post-training, paving the way for more robust and genuinely self-improving LLMs. Impact Statement The current landscape of post-training practices for self- improvement in large language models (LLMs) necessitates a thorough understanding to address both their progressive and regressive effects. In this work, we conduct an in-depth evaluation beyond simple pass@1 accuracy, utilizing mul- tifaceted metrics such as improvement problems, solutions diversity, and OOD generalization. Our findings under- score the critical need for a comprehensive examination of these methods. By expanding our analytical scope, we provide deeper insights into the true nature of iterative self- improvement through post-training, paving the way for more robust and genuinely self-improving LLMs. This research aims to empower scholars and engineers to develop more reliable and effective post-training strategies, ultimately ad- vancing the field of LLMs and pushing the boundaries of model capabilities to tackle more complex challenges. methodology to CSQA, GSM8K, MATH and MBPP datasets with three post-training meth- ods. Generation sampling Nvaries from 21to26with the temperature set as 0.75. Reversal Observation As depicted in Figure 3, contrary to prior assumptions, the rapid increase in pass@N accu- racy with increasing Nchallenges the notion of progres- sively harder problem-solving. Specifically, as Ngrows, M1achieves near-perfect pass@N accuracy on IS(t) across all evaluated datasets, suggesting its inherent capacity to tackle the deemed improvement problems . Selection Optimization for Answer Alignment The em- pirical findings depicted in Figure 3 offer a critical insight: iterative self-improvement hardly entails the acquisition of new problem-solving abilities, but rather the enhancement of the model s correct answer selection within its generation space. 7 Progress or Regress? Self-Improvement Reversal in Post-training 5.2. Solutions Diversity While pass@1 accuracy measures the correctness of the final answer, it does not capture the diversity of solutions a model can generate. We posit that a model s capacity to produce diverse solutions is indicative of its robustness and flexibility in problem-solving. To thoroughly under- stand the evolution of answer diversity during the process of iterative self-improvement, we employ a combination of Distinct N-grams (Li et al., 2016) and Sentence-BERT embedding cosine similarity (Reimers & Gurevych, 2019) to measure mod diversity. These metrics have been shown to correlate well with human assessments of diversity (Tevet & Berant, 2021). Additionally, for mathematical reasoning, we introduce Distinct Equations to measure the diversity of mathematical answers by analyzing the variety of equations in the generated solutions. Each diversity metric Div takes a set of Nmodel outputs, and produces a scalar score representing how diverse the set is. Distinct N-grams measures syntactic diversity by counting the number of unique n-grams (averaged over n= 1",0
4c53285eb3ec1484b1b90f4da18df757f653be53,Pre-Calc: Learning to Use the Calculator Improves Numeracy in Language Models,"['Vishruth Veerendranath', 'Vishwa Shah', 'Kshitish Ghate']",https://openreview.net/pdf/4c53285eb3ec1484b1b90f4da18df757f653be53.pdf,"Pre-Calc: Learning to Use the Calculator Improves Numeracy in Language Models We propose Pre-Calc, our method to teach smaller language models how to use calculators. By pre-finetuning BERT, RoBERTa, and Flan-T5 on calculator use tasks, we improved these models' performance on tasks requiring numerical understanding. Quantitative and numerical comprehension in language is an important task in many fields like education and finance, but still remains a challenging task for language models. While tool and calculator usage has shown to be helpful to improve mathematical reasoning in large pretrained decoder-only language models, this remains unexplored for smaller language models with encoders. In this paper, we propose Pre-Calc, a simple pre-finetuning objective of learning to use the calculator for both encoder-only and encoder-decoder architectures, formulated as a discriminative and generative task respectively. We pre-train BERT and RoBERTa for discriminative calculator use and Flan-T5 for generative calculator use on the MAWPS, SVAMP, and AsDiv-A datasets, which improves performance on downstream tasks that require numerical understanding.",4c53285eb3ec1484b1b90f4da18df757f653be53.pdf,"methodology.
3. Pre-Calc Methodology
We posit that learning to use a calculator requires under-
standing of numbers and ways in which numbers can be
combined. This is used to formulate the Pre-Calc objec-
tives described below.
3.1. Encoder-Only
3.1.1. D ATA PREPROCESSING
We preprocess Calc-MAWPS, Calc-SV AMP and Calc-
AsDiv-A (from the Calc-X collection) (Kadl ˇc´ık et al., 2023)and add 2 new features required for Pre-Calc. First is the
operand tag sequence , which is a sequence of binary tags
that is 1 if the original token it corresponds to is an operand
and 0 if it isn’t. Secondly we extract the Operation , which
is the operation among {+ (add), - (subtract), * (multiply),
/ (divide) }that is required for the question. We extract the
operation either directly from the equation or the reasoning
chain in Calc-X and generate the operand tag sequence, by
first extracting the operands and then tagging the occurances
of the operands in the binary sequence with a 1. As part
of this process we also filter out instances where there are
more than one distinct operations as part of the equation.
3.1.2. P RE-CALC METHOD
An illustration of the Pre-Calc method for Encoder-only
model can be seen in Fig 1. This is decomposed into two
tasks as a dual-objective.
Firstly, we use the pretrained Encoder-only language model
for the task of Operand Identification , which is a token-level
classification task. The tags possible for each token are 1
and 0.
Secondly, we perform the task of Operation Classficiation
by adding a special [OP] token at the end of each sequence
and using this [OP] token’s final layer representation to
classify the operation required in this sequence (+, -, *, /).
Hence, this is essentially a sequence-level classification task
similar to classifying from the representation of a [CLS]
token. However, we do not use the [CLS] token at the
start of the sequence, to enable this objective even in non-
bidirectional models with an autoregressive attention mask
(like de","Future Work
In this work, we improve the numeracy in language models
on the QNLI and QQA tasks which involve textual and com-
putational quantitative reasoning. We do so by proposing
calculator usage as a pre-finetuning task in a discrimina-
tive and generative fashion for encoder-only and encoder-
decoder models respectively. This improves encoder-only
models across various downstream tasks and improves
encoder-decoder models on tasks that require explicit com-
putation.
Future work can address the balance between textual under-
standing and numerical reasoning, by refining regularization
strategies to maintain the language model’s core strengths
while enhancing its computational abilities. Tool-use in
encoder-only models could also be extended to more com-
plex tools similar to decoder-only models.
Acknowledgments
We thank Robert Lo for the helpful discussions.References
Aghajanyan, A., Gupta, A., Shrivastava, A., Chen, X.,
Zettlemoyer, L., and Gupta, S. Muppet: Massive multi-
task representations with pre-finetuning. arXiv preprint
arXiv:2101.11038 , 2021.
Chen, C.-C., Huang, H.-H., Takamura, H., and Chen, H.-
H. Numeracy-600k: Learning numeracy for detecting
exaggerated information in market comments. In Pro-
ceedings of the 57th Annual Meeting of the Association
for Computational Linguistics , pp. 6307–6313, 2019.
Chen, C.-C., Huang, H.-H., and Chen, H.-H. Nquad:
70,000+ questions for machine comprehension of the
numerals in text. In Proceedings of the 30th ACM Inter-
national Conference on Information & Knowledge Man-
agement , pp. 2925–2929, 2021.
Chen, C.-C., Takamura, H., Kobayashi, I., and Miyao,
Y . Improving numeracy by input reframing and quan-
titative pre-finetuning task. In Findings of the Associ-
ation for Computational Linguistics: EACL 2023 , pp.
69–77, Dubrovnik, Croatia, May 2023. Association for
Computational Linguistics. doi: 10.18653/v1/2023.
findings-eacl.4. URL https://aclanthology.
org/2023.findings-eacl.4 .
Devlin, J., Chang, M.-W., Lee,","methodology. 3. Pre-Calc Methodology We posit that learning to use a calculator requires under- standing of numbers and ways in which numbers can be combined. This is used to formulate the Pre-Calc objec- tives described below. 3.1. Encoder-Only 3.1.1. D ATA PREPROCESSING We preprocess Calc-MAWPS, Calc-SV AMP and Calc- AsDiv-A (from the Calc-X collection) (Kadl c k et al., 2023)and add 2 new features required for Pre-Calc. First is the operand tag sequence , which is a sequence of binary tags that is 1 if the original token it corresponds to is an operand and 0 if it isn t. Secondly we extract the Operation , which is the operation among {+ (add), - (subtract), * (multiply), / (divide) }that is required for the question. We extract the operation either directly from the equation or the reasoning chain in Calc-X and generate the operand tag sequence, by first extracting the operands and then tagging the occurances of the operands in the binary sequence with a 1. As part of this process we also filter out instances where there are more than one distinct operations as part of the equation. 3.1.2. P RE-CALC METHOD An illustration of the Pre-Calc method for Encoder-only model can be seen in Fig 1. This is decomposed into two tasks as a dual-objective. Firstly, we use the pretrained Encoder-only language model for the task of Operand Identification , which is a token-level classification task. The tags possible for each token are 1 and 0. Secondly, we perform the task of Operation Classficiation by adding a special [OP] token at the end of each sequence and using this [OP] token s final layer representation to classify the operation required in this sequence (+, -, *, /). Hence, this is essentially a sequence-level classification task similar to classifying from the representation of a [CLS] token. However, we do not use the [CLS] token at the start of the sequence, to enable this objective even in non- bidirectional models with an autoregressive attention mask (like de","Future Work In this work, we improve the numeracy in language models on the QNLI and QQA tasks which involve textual and com- putational quantitative reasoning. We do so by proposing calculator usage as a pre-finetuning task in a discrimina- tive and generative fashion for encoder-only and encoder- decoder models respectively. This improves encoder-only models across various downstream tasks and improves encoder-decoder models on tasks that require explicit com- putation. Future work can address the balance between textual under- standing and numerical reasoning, by refining regularization strategies to maintain the language model s core strengths while enhancing its computational abilities. Tool-use in encoder-only models could also be extended to more com- plex tools similar to decoder-only models. Acknowledgments We thank Robert Lo for the helpful discussions.","Pre-Calc: Learning to Use the Calculator Improves Numeracy in Language Models We propose Pre-Calc, our method to teach smaller language models how to use calculators. By pre-finetuning BERT, RoBERTa, and Flan-T5 on calculator use tasks, we improved these models' performance on tasks requiring numerical understanding. Quantitative and numerical comprehension in language is an important task in many fields like education and finance, but still remains a challenging task for language models. While tool and calculator usage has shown to be helpful to improve mathematical reasoning in large pretrained decoder-only language models, this remains unexplored for smaller language models with encoders. In this paper, we propose Pre-Calc, a simple pre-finetuning objective of learning to use the calculator for both encoder-only and encoder-decoder architectures, formulated as a discriminative and generative task respectively. We pre-train BERT and RoBERTa for discriminative calculator use and Flan-T5 for generative calculator use on the MAWPS, SVAMP, and AsDiv-A datasets, which improves performance on downstream tasks that require numerical understanding.","Pre-Calc: Learning to Use the Calculator Improves Numeracy in Language Models We propose Pre-Calc, our method to teach smaller language models how to use calculators. By pre-finetuning BERT, RoBERTa, and Flan-T5 on calculator use tasks, we improved these models' performance on tasks requiring numerical understanding. Quantitative and numerical comprehension in language is an important task in many fields like education and finance, but still remains a challenging task for language models. While tool and calculator usage has shown to be helpful to improve mathematical reasoning in large pretrained decoder-only language models, this remains unexplored for smaller language models with encoders. In this paper, we propose Pre-Calc, a simple pre-finetuning objective of learning to use the calculator for both encoder-only and encoder-decoder architectures, formulated as a discriminative and generative task respectively. We pre-train BERT and RoBERTa for discriminative calculator use and Flan-T5 for generative calculator use on the MAWPS, SVAMP, and AsDiv-A datasets, which improves performance on downstream tasks that require numerical understanding. Future Work In this work, we improve the numeracy in language models on the QNLI and QQA tasks which involve textual and com- putational quantitative reasoning. We do so by proposing calculator usage as a pre-finetuning task in a discrimina- tive and generative fashion for encoder-only and encoder- decoder models respectively. This improves encoder-only models across various downstream tasks and improves encoder-decoder models on tasks that require explicit com- putation. Future work can address the balance between textual under- standing and numerical reasoning, by refining regularization strategies to maintain the language model s core strengths while enhancing its computational abilities. Tool-use in encoder-only models could also be extended to more com- plex tools similar to decoder-only models. Acknowledgments We thank Robert Lo for the helpful discussions. methodology. 3. Pre-Calc Methodology We posit that learning to use a calculator requires under- standing of numbers and ways in which numbers can be combined. This is used to formulate the Pre-Calc objec- tives described below. 3.1. Encoder-Only 3.1.1. D ATA PREPROCESSING We preprocess Calc-MAWPS, Calc-SV AMP and Calc- AsDiv-A (from the Calc-X collection) (Kadl c k et al., 2023)and add 2 new features required for Pre-Calc. First is the operand tag sequence , which is a sequence of binary tags that is 1 if the original token it corresponds to is an operand and 0 if it isn t. Secondly we extract the Operation , which is the operation among {+ (add), - (subtract), * (multiply), / (divide) }that is required for the question. We extract the operation either directly from the equation or the reasoning chain in Calc-X and generate the operand tag sequence, by first extracting the operands and then tagging the occurances of the operands in the binary sequence with a 1. As part of this process we also filter out instances where there are more than one distinct operations as part of the equation. 3.1.2. P RE-CALC METHOD An illustration of the Pre-Calc method for Encoder-only model can be seen in Fig 1. This is decomposed into two tasks as a dual-objective. Firstly, we use the pretrained Encoder-only language model for the task of Operand Identification , which is a token-level classification task. The tags possible for each token are 1 and 0. Secondly, we perform the task of Operation Classficiation by adding a special [OP] token at the end of each sequence and using this [OP] token s final layer representation to classify the operation required in this sequence (+, -, *, /). Hence, this is essentially a sequence-level classification task similar to classifying from the representation of a [CLS] token. However, we do not use the [CLS] token at the start of the sequence, to enable this objective even in non- bidirectional models with an autoregressive attention mask (like de",0
017a4ef5896e1abf13238bf6c96df91a351c84ea,Learning Efficient Recursive Numeral Systems via Reinforcement Learning,"['Jonathan David Thomas', 'Andrea Silvi', 'Devdatt Dubhashi', 'Emil Carlsson', 'Moa Johansson']",https://openreview.net/pdf/017a4ef5896e1abf13238bf6c96df91a351c84ea.pdf,"Learning Efficient Recursive Numeral Systems via Reinforcement Learning How did recursive numeral systems emerge? We take steps towards a mechanistic explanation via a Reinforcement Learning approach which optimizes a lexicon under a given meta-grammar. The emergence of mathematical concepts, such as number systems, is an understudied area in AI for mathematics and reasoning. It has previously been shown (Carlsson et al., 2021) that by using reinforcement learning (RL), agents can derive simple approximate and exact-restricted numeral systems. However, it is a major challenge to show how more complex recursive numeral systems, similar to the one utilised in English, could arise via a simple learning mechanism such as RL. Here, we introduce an approach towards deriving a mechanistic explanation of the emergence of recursive number systems where we consider an RL agent which directly optimizes a lexicon under a given meta-grammar. Utilising a slightly modified version of the seminal meta-grammar of (Hurford, 1975), we demonstrate that our RL agent can effectively modify the lexicon towards Pareto-optimal configurations which are comparable to those observed within human numeral systems.",017a4ef5896e1abf13238bf6c96df91a351c84ea.pdf,"approach towards deriv-
ing a mechanistic explanation of the emergence
of recursive number systems where we consider
an RL agent which directly optimizes a lexicon
under a given meta-grammar. Utilising a slightly
modified version of the seminal meta-grammar
of (Hurford, 1975), we demonstrate that our RL
agent can effectively modify the lexicon towards
Pareto-optimal configurations which are compa-
rable to those observed within human numeral
systems.
1. Introduction
While there is evidence to suggest that animals, young in-
fants and adult humans possess a biologically determined,
domain-specific representation of numbers and elementary
arithmetic operations, only humans have a capacity for gen-
erating an infinite set of natural numbers, while all other
species seem to lack such a capacity (Hauser et al., 2002;
Chomsky, 1982;9; Dehaene & Changeux, 1993; Dehaene,
1997). This unique capacity is central to many aspects of
human cognition, including, of course, the development of
sophisticated mathematics. The fundamental mechanism
underlying this is the use of a finite symbolic system to
represent arbitrarily large discrete numerical magnitudes
i.e. the positive integers. However, the work within AI on
developing and changing representations of mathematical
1Chalmers University of Technology, Gothenburg,
Sweden. Correspondence to: Jonathan D. Thomas
<jonathan.thomas@chalmers.se >.concepts, such as number systems, is limited, and primarily
concerns revising representations of logical theories (Bundy
& Li, 2023).
In cognitive science, a recent influential body of work sug-
gests language is shaped by a pressure for efficient communi-
cation which involves an information-theoretic trade-off be-
tween cognitive load and informativeness (Kemp & Regier,
2012; Gibson et al., 2017; Zaslavsky et al., 2019). This
means that language is under pressure to be simultaneously
informative, in order to support effective communication,
while also being simple, to minimize the cognitive","future work.
We note that human languages do not tend to cluster to a
single point of the Pareto frontier like the lexicons our agent
finds, but instead present a sizable variety in terms of lex-
icons size and even morphemes that are lexicalized. We
hypothesize that this might be due to the current limitations
of our RL-based approach which does not capture the dif-
ferent influences that may shape the evolution of language.
Furthermore, our current approach does not allow for the
emergence of some classes of numeral systems e.g. Type
2-Russian (as can be found in (Deni ´c & Szymanik, 2024)).
We are unable to achieve this representation as it includes
numerals within DandMwhich are non-sequential which
is unachievable with our current action space.
6. Conclusions and Future Work
Our work serves as an exploration of RL as a method for
the direct optimization of a lexicon within the context of
numeral systems. We have demonstrated that this is possible
and that an RL agent can learn to manipulate its lexicalisa-
tions in order to find an optimal trade-off between average
morphosyntactic complexity and lexicon size. The resultant
lexicons are comparable to those which we observe within
human numeral systems. A key enabler of this was a mi-
nor modification to a well-established meta-grammar for
expressing numeral systems. Optimization of the existing
meta-grammar produces systems which do not possess reg-
ular recursive structures or forms. Our modification avoids
the aforementioned issues and enables an RL agent to opti-
mize several lexicons towards the Pareto frontier found via
this new meta-grammar. An interesting direction we intend
4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5
Lexicon Size1.01.21.41.61.82.02.22.4Average Morphosyntactic ComplexityPareto Frontier
Path 1
Path 2
Path 3
Final ConfigurationFigure 6. Myopic RL-based Agent optimising configurations over
a fixed length episode (8-steps). Each path is the median of the
trajectories that the agent follows when starting","approach towards deriv- ing a mechanistic explanation of the emergence of recursive number systems where we consider an RL agent which directly optimizes a lexicon under a given meta-grammar. Utilising a slightly modified version of the seminal meta-grammar of (Hurford, 1975), we demonstrate that our RL agent can effectively modify the lexicon towards Pareto-optimal configurations which are compa- rable to those observed within human numeral systems. 1. Introduction While there is evidence to suggest that animals, young in- fants and adult humans possess a biologically determined, domain-specific representation of numbers and elementary arithmetic operations, only humans have a capacity for gen- erating an infinite set of natural numbers, while all other species seem to lack such a capacity (Hauser et al., 2002; Chomsky, 1982;9; Dehaene & Changeux, 1993; Dehaene, 1997). This unique capacity is central to many aspects of human cognition, including, of course, the development of sophisticated mathematics. The fundamental mechanism underlying this is the use of a finite symbolic system to represent arbitrarily large discrete numerical magnitudes i.e. the positive integers. However, the work within AI on developing and changing representations of mathematical 1Chalmers University of Technology, Gothenburg, Sweden. Correspondence to: Jonathan D. Thomas <jonathan.thomas@chalmers.se >.concepts, such as number systems, is limited, and primarily concerns revising representations of logical theories (Bundy & Li, 2023). In cognitive science, a recent influential body of work sug- gests language is shaped by a pressure for efficient communi- cation which involves an information-theoretic trade-off be- tween cognitive load and informativeness (Kemp & Regier, 2012; Gibson et al., 2017; Zaslavsky et al., 2019). This means that language is under pressure to be simultaneously informative, in order to support effective communication, while also being simple, to minimize the cognitive","future work. We note that human languages do not tend to cluster to a single point of the Pareto frontier like the lexicons our agent finds, but instead present a sizable variety in terms of lex- icons size and even morphemes that are lexicalized. We hypothesize that this might be due to the current limitations of our RL-based approach which does not capture the dif- ferent influences that may shape the evolution of language. Furthermore, our current approach does not allow for the emergence of some classes of numeral systems e.g. Type 2-Russian (as can be found in (Deni c & Szymanik, 2024)). We are unable to achieve this representation as it includes numerals within DandMwhich are non-sequential which is unachievable with our current action space. 6. Conclusions and Future Work Our work serves as an exploration of RL as a method for the direct optimization of a lexicon within the context of numeral systems. We have demonstrated that this is possible and that an RL agent can learn to manipulate its lexicalisa- tions in order to find an optimal trade-off between average morphosyntactic complexity and lexicon size. The resultant lexicons are comparable to those which we observe within human numeral systems. A key enabler of this was a mi- nor modification to a well-established meta-grammar for expressing numeral systems. Optimization of the existing meta-grammar produces systems which do not possess reg- ular recursive structures or forms. Our modification avoids the aforementioned issues and enables an RL agent to opti- mize several lexicons towards the Pareto frontier found via this new meta-grammar. An interesting direction we intend 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 Lexicon Size1.01.21.41.61.82.02.22.4Average Morphosyntactic ComplexityPareto Frontier Path 1 Path 2 Path 3 Final ConfigurationFigure 6. Myopic RL-based Agent optimising configurations over a fixed length episode (8-steps). Each path is the median of the trajectories that the agent follows when starting","Learning Efficient Recursive Numeral Systems via Reinforcement Learning How did recursive numeral systems emerge? We take steps towards a mechanistic explanation via a Reinforcement Learning approach which optimizes a lexicon under a given meta-grammar. The emergence of mathematical concepts, such as number systems, is an understudied area in AI for mathematics and reasoning. It has previously been shown (Carlsson et al., 2021) that by using reinforcement learning (RL), agents can derive simple approximate and exact-restricted numeral systems. However, it is a major challenge to show how more complex recursive numeral systems, similar to the one utilised in English, could arise via a simple learning mechanism such as RL. Here, we introduce an approach towards deriving a mechanistic explanation of the emergence of recursive number systems where we consider an RL agent which directly optimizes a lexicon under a given meta-grammar. Utilising a slightly modified version of the seminal meta-grammar of (Hurford, 1975), we demonstrate that our RL agent can effectively modify the lexicon towards Pareto-optimal configurations which are comparable to those observed within human numeral systems.","Learning Efficient Recursive Numeral Systems via Reinforcement Learning How did recursive numeral systems emerge? We take steps towards a mechanistic explanation via a Reinforcement Learning approach which optimizes a lexicon under a given meta-grammar. The emergence of mathematical concepts, such as number systems, is an understudied area in AI for mathematics and reasoning. It has previously been shown (Carlsson et al., 2021) that by using reinforcement learning (RL), agents can derive simple approximate and exact-restricted numeral systems. However, it is a major challenge to show how more complex recursive numeral systems, similar to the one utilised in English, could arise via a simple learning mechanism such as RL. Here, we introduce an approach towards deriving a mechanistic explanation of the emergence of recursive number systems where we consider an RL agent which directly optimizes a lexicon under a given meta-grammar. Utilising a slightly modified version of the seminal meta-grammar of (Hurford, 1975), we demonstrate that our RL agent can effectively modify the lexicon towards Pareto-optimal configurations which are comparable to those observed within human numeral systems. future work. We note that human languages do not tend to cluster to a single point of the Pareto frontier like the lexicons our agent finds, but instead present a sizable variety in terms of lex- icons size and even morphemes that are lexicalized. We hypothesize that this might be due to the current limitations of our RL-based approach which does not capture the dif- ferent influences that may shape the evolution of language. Furthermore, our current approach does not allow for the emergence of some classes of numeral systems e.g. Type 2-Russian (as can be found in (Deni c & Szymanik, 2024)). We are unable to achieve this representation as it includes numerals within DandMwhich are non-sequential which is unachievable with our current action space. 6. Conclusions and Future Work Our work serves as an exploration of RL as a method for the direct optimization of a lexicon within the context of numeral systems. We have demonstrated that this is possible and that an RL agent can learn to manipulate its lexicalisa- tions in order to find an optimal trade-off between average morphosyntactic complexity and lexicon size. The resultant lexicons are comparable to those which we observe within human numeral systems. A key enabler of this was a mi- nor modification to a well-established meta-grammar for expressing numeral systems. Optimization of the existing meta-grammar produces systems which do not possess reg- ular recursive structures or forms. Our modification avoids the aforementioned issues and enables an RL agent to opti- mize several lexicons towards the Pareto frontier found via this new meta-grammar. An interesting direction we intend 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 Lexicon Size1.01.21.41.61.82.02.22.4Average Morphosyntactic ComplexityPareto Frontier Path 1 Path 2 Path 3 Final ConfigurationFigure 6. Myopic RL-based Agent optimising configurations over a fixed length episode (8-steps). Each path is the median of the trajectories that the agent follows when starting approach towards deriv- ing a mechanistic explanation of the emergence of recursive number systems where we consider an RL agent which directly optimizes a lexicon under a given meta-grammar. Utilising a slightly modified version of the seminal meta-grammar of (Hurford, 1975), we demonstrate that our RL agent can effectively modify the lexicon towards Pareto-optimal configurations which are compa- rable to those observed within human numeral systems. 1. Introduction While there is evidence to suggest that animals, young in- fants and adult humans possess a biologically determined, domain-specific representation of numbers and elementary arithmetic operations, only humans have a capacity for gen- erating an infinite set of natural numbers, while all other species seem to lack such a capacity (Hauser et al., 2002; Chomsky, 1982;9; Dehaene & Changeux, 1993; Dehaene, 1997). This unique capacity is central to many aspects of human cognition, including, of course, the development of sophisticated mathematics. The fundamental mechanism underlying this is the use of a finite symbolic system to represent arbitrarily large discrete numerical magnitudes i.e. the positive integers. However, the work within AI on developing and changing representations of mathematical 1Chalmers University of Technology, Gothenburg, Sweden. Correspondence to: Jonathan D. Thomas <jonathan.thomas@chalmers.se >.concepts, such as number systems, is limited, and primarily concerns revising representations of logical theories (Bundy & Li, 2023). In cognitive science, a recent influential body of work sug- gests language is shaped by a pressure for efficient communi- cation which involves an information-theoretic trade-off be- tween cognitive load and informativeness (Kemp & Regier, 2012; Gibson et al., 2017; Zaslavsky et al., 2019). This means that language is under pressure to be simultaneously informative, in order to support effective communication, while also being simple, to minimize the cognitive",0
648d1ef62a6f6ea84faa78689bbf3a7ceec0fc9a,"More Details, Please: Improving Autoformalization with More Detailed Proofs","['Guillem Tarrach', 'Albert Q. Jiang', 'Daniel Raggi', 'Wenda Li', 'Mateja Jamnik']",https://openreview.net/pdf/648d1ef62a6f6ea84faa78689bbf3a7ceec0fc9a.pdf,"More Details, Please: Improving Autoformalization with More Detailed Proofs We propose SPADeR, an approach to autoformalization that uses language models to infer and explicitly incorporate implicit details from informal proofs The formalization of mathematical theorems and their proofs is a time-consuming and tedious process which, despite recent advances in the reasoning capabilities of AI systems, remains a challenging task for computers. Existing attempts to automate the process with language models struggle with the difference in level of detail between formal and informal proofs. Successful autoformalization requires models to understand and be able to explain the nuances of logical arguments, a critical aspect of reasoning that is often overlooked in existing research. In this work, we introduce Sketch, Prove, Add Detail & Repeat (SPADeR), an approach that enhances proof autoformalizers by using language models to infer and explicitly incorporate implicit details from informal proofs. With the same number of autoformalization attempts, our method increases the percentage of successfully formalized problems in the miniF2F test dataset from 34.8% to 38.1%.",648d1ef62a6f6ea84faa78689bbf3a7ceec0fc9a.pdf,"approach that enhances proof aut-
oformalizers by using language models to infer
and explicitly incorporate implicit details from
informal proofs. With the same number of auto-
formalization attempts, our method increases the
percentage of successfully formalized problems
in the miniF2F test dataset from 34.8% to 38.1%.
1. Introduction
A significant body of recent work has investigated the rea-
soning capabilities of Large Language Models (LLMs), par-
ticularly in the context of solving mathematical problems.
One frequently studied task is Automated Theorem Prov-
ing (ATP), which involves automatically generating formal
proofs of mathematical theorems. However, few studies
have investigated the ability of LLMs to understand and
explain mathematical arguments. In this work, we introduce
an approach that leverages this capability to construct more
detailed informal mathematical proofs, thereby improving
the process of autoformalization – the translation of infor-
mal proofs into formally verifiable formal proofs. Informal
proofs lack many details that are necessary to verify their
correctness. While formal proofs do not suffer from this
1University of Cambridge2University of Edinburgh. Corre-
spondence to: Guillem Tarrach <guillem.tarrach@gmail.com >.
AI for MATH Workshop at ICML 2024 , Vienna, Austria. Copyright
2024 by the author(s).issue, in practice the focus on low-level details makes for-
mal automated theorem provers less successful at high-level
planning. As a result, autoformalization systems struggle
with the discrepancy in the level of detail in formal and in-
formal proofs (Jiang et al., 2023, Section 5.2 and Appendix
C). Our approach uses LLMs to explain informal proofs by
inferring and incorporating implicit details, thereby bridging
the gap between informal and formal proofs.
To plan ahead and focus on the overall proof strategy, math-
ematicians usually write proofs in a non-linear, hierarchical
manner: They start by writing a high-level proof draft an","conclusion, we make the following contributions:
•We propose a method for using LLMs to construct
more detailed proofs by inferring implicit details in
informal mathematical proofs.
•We demonstrate the usefulness of the presented method
for autoformalization.
Our work shows that LLMs can provide detailed mathemat-
ical proofs by inferring and explaining implicit reasoning
steps. This ability helps bridge the gap between informal
and formal mathematical proofs and enables LLM-based
autoformalization systems to verify more theorems.2. Background and Related Work
2.1. Mathematical Reasoning with Language Models
With recent advances in language models, particularly the
introduction of LLMs, there has been an increase in research
into their reasoning capabilities, particularly in the context
of mathematical problem-solving (Hendrycks et al., 2021;
Drori et al., 2022; Welleck et al., 2021). While alterna-
tive prompting methods (Wei et al., 2022; Yao et al., 2023;
Zheng et al., 2023a) help improve the accuracy of reasoning
arguments, language models still frequently make mistakes.
These challenges highlight the need for robust verification
methods to complement informal reasoning.
Furthermore, the ability of LLMs to understand and explain
existing arguments remains largely unexplored. In this
work, we investigate these abilities and their evaluation
2
More Details, Please: Improving Autoformalization with More Detailed Proofs
through autoformalization and formal verification.
2.2. Autoformalization
To address the limitations of reasoning with language mod-
els, recent work has explored the combination of informal
reasoning with formal verification through autoformaliza-
tion. While early approaches to autoformalization with deep
learning took inspiration from Neural Machine Translation
(Wang et al., 2018), it has been observed (Wu et al., 2022)
that LLMs are better suited for this task because of their in-
context few-shot learning capabilities (Brown et al., 2020)
and th","approach that enhances proof aut- oformalizers by using language models to infer and explicitly incorporate implicit details from informal proofs. With the same number of auto- formalization attempts, our method increases the percentage of successfully formalized problems in the miniF2F test dataset from 34.8% to 38.1%. 1. Introduction A significant body of recent work has investigated the rea- soning capabilities of Large Language Models (LLMs), par- ticularly in the context of solving mathematical problems. One frequently studied task is Automated Theorem Prov- ing (ATP), which involves automatically generating formal proofs of mathematical theorems. However, few studies have investigated the ability of LLMs to understand and explain mathematical arguments. In this work, we introduce an approach that leverages this capability to construct more detailed informal mathematical proofs, thereby improving the process of autoformalization the translation of infor- mal proofs into formally verifiable formal proofs. Informal proofs lack many details that are necessary to verify their correctness. While formal proofs do not suffer from this 1University of Cambridge2University of Edinburgh. Corre- spondence to: Guillem Tarrach <guillem.tarrach@gmail.com >. AI for MATH Workshop at ICML 2024 , Vienna, Austria. Copyright 2024 by the author(s).issue, in practice the focus on low-level details makes for- mal automated theorem provers less successful at high-level planning. As a result, autoformalization systems struggle with the discrepancy in the level of detail in formal and in- formal proofs (Jiang et al., 2023, Section 5.2 and Appendix C). Our approach uses LLMs to explain informal proofs by inferring and incorporating implicit details, thereby bridging the gap between informal and formal proofs. To plan ahead and focus on the overall proof strategy, math- ematicians usually write proofs in a non-linear, hierarchical manner: They start by writing a high-level proof draft an","conclusion, we make the following contributions: We propose a method for using LLMs to construct more detailed proofs by inferring implicit details in informal mathematical proofs. We demonstrate the usefulness of the presented method for autoformalization. Our work shows that LLMs can provide detailed mathemat- ical proofs by inferring and explaining implicit reasoning steps. This ability helps bridge the gap between informal and formal mathematical proofs and enables LLM-based autoformalization systems to verify more theorems.2. Background and Related Work 2.1. Mathematical Reasoning with Language Models With recent advances in language models, particularly the introduction of LLMs, there has been an increase in research into their reasoning capabilities, particularly in the context of mathematical problem-solving (Hendrycks et al., 2021; Drori et al., 2022; Welleck et al., 2021). While alterna- tive prompting methods (Wei et al., 2022; Yao et al., 2023; Zheng et al., 2023a) help improve the accuracy of reasoning arguments, language models still frequently make mistakes. These challenges highlight the need for robust verification methods to complement informal reasoning. Furthermore, the ability of LLMs to understand and explain existing arguments remains largely unexplored. In this work, we investigate these abilities and their evaluation 2 More Details, Please: Improving Autoformalization with More Detailed Proofs through autoformalization and formal verification. 2.2. Autoformalization To address the limitations of reasoning with language mod- els, recent work has explored the combination of informal reasoning with formal verification through autoformaliza- tion. While early approaches to autoformalization with deep learning took inspiration from Neural Machine Translation (Wang et al., 2018), it has been observed (Wu et al., 2022) that LLMs are better suited for this task because of their in- context few-shot learning capabilities (Brown et al., 2020) and th","More Details, Please: Improving Autoformalization with More Detailed Proofs We propose SPADeR, an approach to autoformalization that uses language models to infer and explicitly incorporate implicit details from informal proofs The formalization of mathematical theorems and their proofs is a time-consuming and tedious process which, despite recent advances in the reasoning capabilities of AI systems, remains a challenging task for computers. Existing attempts to automate the process with language models struggle with the difference in level of detail between formal and informal proofs. Successful autoformalization requires models to understand and be able to explain the nuances of logical arguments, a critical aspect of reasoning that is often overlooked in existing research. In this work, we introduce Sketch, Prove, Add Detail & Repeat (SPADeR), an approach that enhances proof autoformalizers by using language models to infer and explicitly incorporate implicit details from informal proofs. With the same number of autoformalization attempts, our method increases the percentage of successfully formalized problems in the miniF2F test dataset from 34.8% to 38.1%.","More Details, Please: Improving Autoformalization with More Detailed Proofs We propose SPADeR, an approach to autoformalization that uses language models to infer and explicitly incorporate implicit details from informal proofs The formalization of mathematical theorems and their proofs is a time-consuming and tedious process which, despite recent advances in the reasoning capabilities of AI systems, remains a challenging task for computers. Existing attempts to automate the process with language models struggle with the difference in level of detail between formal and informal proofs. Successful autoformalization requires models to understand and be able to explain the nuances of logical arguments, a critical aspect of reasoning that is often overlooked in existing research. In this work, we introduce Sketch, Prove, Add Detail & Repeat (SPADeR), an approach that enhances proof autoformalizers by using language models to infer and explicitly incorporate implicit details from informal proofs. With the same number of autoformalization attempts, our method increases the percentage of successfully formalized problems in the miniF2F test dataset from 34.8% to 38.1%. conclusion, we make the following contributions: We propose a method for using LLMs to construct more detailed proofs by inferring implicit details in informal mathematical proofs. We demonstrate the usefulness of the presented method for autoformalization. Our work shows that LLMs can provide detailed mathemat- ical proofs by inferring and explaining implicit reasoning steps. This ability helps bridge the gap between informal and formal mathematical proofs and enables LLM-based autoformalization systems to verify more theorems.2. Background and Related Work 2.1. Mathematical Reasoning with Language Models With recent advances in language models, particularly the introduction of LLMs, there has been an increase in research into their reasoning capabilities, particularly in the context of mathematical problem-solving (Hendrycks et al., 2021; Drori et al., 2022; Welleck et al., 2021). While alterna- tive prompting methods (Wei et al., 2022; Yao et al., 2023; Zheng et al., 2023a) help improve the accuracy of reasoning arguments, language models still frequently make mistakes. These challenges highlight the need for robust verification methods to complement informal reasoning. Furthermore, the ability of LLMs to understand and explain existing arguments remains largely unexplored. In this work, we investigate these abilities and their evaluation 2 More Details, Please: Improving Autoformalization with More Detailed Proofs through autoformalization and formal verification. 2.2. Autoformalization To address the limitations of reasoning with language mod- els, recent work has explored the combination of informal reasoning with formal verification through autoformaliza- tion. While early approaches to autoformalization with deep learning took inspiration from Neural Machine Translation (Wang et al., 2018), it has been observed (Wu et al., 2022) that LLMs are better suited for this task because of their in- context few-shot learning capabilities (Brown et al., 2020) and th approach that enhances proof aut- oformalizers by using language models to infer and explicitly incorporate implicit details from informal proofs. With the same number of auto- formalization attempts, our method increases the percentage of successfully formalized problems in the miniF2F test dataset from 34.8% to 38.1%. 1. Introduction A significant body of recent work has investigated the rea- soning capabilities of Large Language Models (LLMs), par- ticularly in the context of solving mathematical problems. One frequently studied task is Automated Theorem Prov- ing (ATP), which involves automatically generating formal proofs of mathematical theorems. However, few studies have investigated the ability of LLMs to understand and explain mathematical arguments. In this work, we introduce an approach that leverages this capability to construct more detailed informal mathematical proofs, thereby improving the process of autoformalization the translation of infor- mal proofs into formally verifiable formal proofs. Informal proofs lack many details that are necessary to verify their correctness. While formal proofs do not suffer from this 1University of Cambridge2University of Edinburgh. Corre- spondence to: Guillem Tarrach <guillem.tarrach@gmail.com >. AI for MATH Workshop at ICML 2024 , Vienna, Austria. Copyright 2024 by the author(s).issue, in practice the focus on low-level details makes for- mal automated theorem provers less successful at high-level planning. As a result, autoformalization systems struggle with the discrepancy in the level of detail in formal and in- formal proofs (Jiang et al., 2023, Section 5.2 and Appendix C). Our approach uses LLMs to explain informal proofs by inferring and incorporating implicit details, thereby bridging the gap between informal and formal proofs. To plan ahead and focus on the overall proof strategy, math- ematicians usually write proofs in a non-linear, hierarchical manner: They start by writing a high-level proof draft an",0
0587d424a1a041845ea791237cb2754d46157cc3,Technical Report for ICML 2024 Automated Math Reasoning Challenge: Solving Optimization Problems with Open Source Large Language Model,"['Duc M. Nguyen', 'Sungahn Ko']",https://openreview.net/pdf/0587d424a1a041845ea791237cb2754d46157cc3.pdf,"Technical Report for ICML 2024 Automated Math Reasoning Challenge: Solving Optimization Problems with Open Source Large Language Model We investigated open-source Large Language Model's capabilities to solve optimization problem This technical report presents an approach utilizing open-source Large Language Models for Automated Optimization Problem-solving With Code Challenge at the ICML 2024 AI4Math Workshop. This challenge emphasizes the ability of Large Language Models (LLMs) to handle complex mathematical reasoning from formulating to solving the problem at hand. By exploring different prompting techniques, such as few-shot, self-consistency, chain-of-thought, and tree-of-thought, we aim to explore the current state-of-the-art LLMs' mathematical reasoning abilities.",0587d424a1a041845ea791237cb2754d46157cc3.pdf,"approach uti-
lizing open-source Large Language Models for
Automated Optimization Problem-solving With
Code Challenge at the ICML 2024 AI4Math
Workshop. This challenge emphasizes the abil-
ity of Large Language Models (LLMs) to handle
complex mathematical reasoning from formulat-
ing to solving the problem at hand. By exploring
different prompting techniques, such as few-shot,
self-consistency, chain-of-thought, and tree-of-
thought, we aim to explore the current state-of-
the-art LLMs’ mathematical reasoning abilities.
1. Introduction
Recent research has highlighted the remarkable potential
of state-of-the-art Large Language Models like GPT-4
(Achiam et al., 2023) showcasing their promising abilities in
reasoning across diverse fields, encompassing tasks such as
solving mathematical word problems and proving theorems
(Huang et al., 2024). Automated mathematical reasoning,
which requires sophisticated multi-step planning and rea-
soning, has attracted active research to evaluate and develop
intelligent agents capable of obtaining advanced forms of
human intelligence such as mathematical reasoning.
In this technical report, we investigate the ability to for-
mulate and solve optimization problems, which is critical
across various domains, ranging from operations research
and engineering to finance and machine learning, by Open
Source Large Language Models. Traditionally, solving opti-
mization problems has required human expertise in math-
ematical modeling and algorithm design. However, the
rise of LLMs presents an opportunity to automate this pro-
cess, enabling machines to understand, interpret, and solve
optimization problems expressed in natural language. To
1Department of Computer Science and Engineering, Ulsan
National Institute of Science and Technology, Ulsan, Republic of
Korea. Correspondence to: Sungahn Ko <sako@unist.ac.kr >.
The first AI for Math Workshop at the 41stInternational Confer-
ence on Machine Learning , Vienna, Austria. Copyright 2024 by
the a","Future Work
Even though open-source LLMs demonstrate good exper-
imental results in the challenge, there are important areas
needing further investigation.
•Improvement of Evaluation Mechanisms . The un-
derperformance of ToT prompting suggests a need for
better evaluation mechanisms within LLMs. Future
work can focus on improving the self-evaluation capa-
bility of LLMs by training with synthetic data similar
to Chen et al. (2024) or utilizing RAG-based mecha-
nism (Wei et al., 2022b).
•Fine-tuning on the training data set . The perfor-
mance can be further enhanced by fine-tuning an LLM
or a small model on the competition training data set,
which we have omitted in this technical report due to
the lack of resources.
•Incorporate more difficult questions in the data set .
Most of the questions in the data set are Linear Pro-
gramming problems, lacking diversity in difficulties.
Future work could focus on building a more diverse
data set consisting of other types of problems in opti-
mization such as Convex Optimization, Dynamic Pro-
gramming, or Stochastic Optimal Control.
6. Conclusion
This technical report has investigated the capabilities of
open-source Large Language Models (LLMs) in formu-
lating and solving optimization problems through various
prompting techniques. The exploration of methods such as
few-shot prompting, self-consistency prompting, chain-of-
thought (CoT) prompting, and tree-of-thought (ToT) prompt-
ing has provided valuable insights into how LLMs can be
effectively utilized for complex mathematical reasoning
tasks.
6
Solving Optimization Problems with Open Source Large Language Model
References
Introducing meta llama 3: The most capable openly avail-
able llm to date. URL https://ai.meta.com/
blog/meta-llama-3/ .
Achiam, O. J., Adler, S., Agarwal, S., Ahmad, L., Akkaya,
I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman,
S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Bal-
com, V ., Baltescu, P., Bao, H., Bavarian, M., Belgum,
J","approach uti- lizing open-source Large Language Models for Automated Optimization Problem-solving With Code Challenge at the ICML 2024 AI4Math Workshop. This challenge emphasizes the abil- ity of Large Language Models (LLMs) to handle complex mathematical reasoning from formulat- ing to solving the problem at hand. By exploring different prompting techniques, such as few-shot, self-consistency, chain-of-thought, and tree-of- thought, we aim to explore the current state-of- the-art LLMs mathematical reasoning abilities. 1. Introduction Recent research has highlighted the remarkable potential of state-of-the-art Large Language Models like GPT-4 (Achiam et al., 2023) showcasing their promising abilities in reasoning across diverse fields, encompassing tasks such as solving mathematical word problems and proving theorems (Huang et al., 2024). Automated mathematical reasoning, which requires sophisticated multi-step planning and rea- soning, has attracted active research to evaluate and develop intelligent agents capable of obtaining advanced forms of human intelligence such as mathematical reasoning. In this technical report, we investigate the ability to for- mulate and solve optimization problems, which is critical across various domains, ranging from operations research and engineering to finance and machine learning, by Open Source Large Language Models. Traditionally, solving opti- mization problems has required human expertise in math- ematical modeling and algorithm design. However, the rise of LLMs presents an opportunity to automate this pro- cess, enabling machines to understand, interpret, and solve optimization problems expressed in natural language. To 1Department of Computer Science and Engineering, Ulsan National Institute of Science and Technology, Ulsan, Republic of Korea. Correspondence to: Sungahn Ko <sako@unist.ac.kr >. The first AI for Math Workshop at the 41stInternational Confer- ence on Machine Learning , Vienna, Austria. Copyright 2024 by the a","Future Work Even though open-source LLMs demonstrate good exper- imental results in the challenge, there are important areas needing further investigation. Improvement of Evaluation Mechanisms . The un- derperformance of ToT prompting suggests a need for better evaluation mechanisms within LLMs. Future work can focus on improving the self-evaluation capa- bility of LLMs by training with synthetic data similar to Chen et al. (2024) or utilizing RAG-based mecha- nism (Wei et al., 2022b). Fine-tuning on the training data set . The perfor- mance can be further enhanced by fine-tuning an LLM or a small model on the competition training data set, which we have omitted in this technical report due to the lack of resources. Incorporate more difficult questions in the data set . Most of the questions in the data set are Linear Pro- gramming problems, lacking diversity in difficulties. Future work could focus on building a more diverse data set consisting of other types of problems in opti- mization such as Convex Optimization, Dynamic Pro- gramming, or Stochastic Optimal Control. 6. Conclusion This technical report has investigated the capabilities of open-source Large Language Models (LLMs) in formu- lating and solving optimization problems through various prompting techniques. The exploration of methods such as few-shot prompting, self-consistency prompting, chain-of- thought (CoT) prompting, and tree-of-thought (ToT) prompt- ing has provided valuable insights into how LLMs can be effectively utilized for complex mathematical reasoning tasks. 6 Solving Optimization Problems with Open Source Large Language Model","Technical Report for ICML 2024 Automated Math Reasoning Challenge: Solving Optimization Problems with Open Source Large Language Model We investigated open-source Large Language Model's capabilities to solve optimization problem This technical report presents an approach utilizing open-source Large Language Models for Automated Optimization Problem-solving With Code Challenge at the ICML 2024 AI4Math Workshop. This challenge emphasizes the ability of Large Language Models (LLMs) to handle complex mathematical reasoning from formulating to solving the problem at hand. By exploring different prompting techniques, such as few-shot, self-consistency, chain-of-thought, and tree-of-thought, we aim to explore the current state-of-the-art LLMs' mathematical reasoning abilities.","Technical Report for ICML 2024 Automated Math Reasoning Challenge: Solving Optimization Problems with Open Source Large Language Model We investigated open-source Large Language Model's capabilities to solve optimization problem This technical report presents an approach utilizing open-source Large Language Models for Automated Optimization Problem-solving With Code Challenge at the ICML 2024 AI4Math Workshop. This challenge emphasizes the ability of Large Language Models (LLMs) to handle complex mathematical reasoning from formulating to solving the problem at hand. By exploring different prompting techniques, such as few-shot, self-consistency, chain-of-thought, and tree-of-thought, we aim to explore the current state-of-the-art LLMs' mathematical reasoning abilities. Future Work Even though open-source LLMs demonstrate good exper- imental results in the challenge, there are important areas needing further investigation. Improvement of Evaluation Mechanisms . The un- derperformance of ToT prompting suggests a need for better evaluation mechanisms within LLMs. Future work can focus on improving the self-evaluation capa- bility of LLMs by training with synthetic data similar to Chen et al. (2024) or utilizing RAG-based mecha- nism (Wei et al., 2022b). Fine-tuning on the training data set . The perfor- mance can be further enhanced by fine-tuning an LLM or a small model on the competition training data set, which we have omitted in this technical report due to the lack of resources. Incorporate more difficult questions in the data set . Most of the questions in the data set are Linear Pro- gramming problems, lacking diversity in difficulties. Future work could focus on building a more diverse data set consisting of other types of problems in opti- mization such as Convex Optimization, Dynamic Pro- gramming, or Stochastic Optimal Control. 6. Conclusion This technical report has investigated the capabilities of open-source Large Language Models (LLMs) in formu- lating and solving optimization problems through various prompting techniques. The exploration of methods such as few-shot prompting, self-consistency prompting, chain-of- thought (CoT) prompting, and tree-of-thought (ToT) prompt- ing has provided valuable insights into how LLMs can be effectively utilized for complex mathematical reasoning tasks. 6 Solving Optimization Problems with Open Source Large Language Model approach uti- lizing open-source Large Language Models for Automated Optimization Problem-solving With Code Challenge at the ICML 2024 AI4Math Workshop. This challenge emphasizes the abil- ity of Large Language Models (LLMs) to handle complex mathematical reasoning from formulat- ing to solving the problem at hand. By exploring different prompting techniques, such as few-shot, self-consistency, chain-of-thought, and tree-of- thought, we aim to explore the current state-of- the-art LLMs mathematical reasoning abilities. 1. Introduction Recent research has highlighted the remarkable potential of state-of-the-art Large Language Models like GPT-4 (Achiam et al., 2023) showcasing their promising abilities in reasoning across diverse fields, encompassing tasks such as solving mathematical word problems and proving theorems (Huang et al., 2024). Automated mathematical reasoning, which requires sophisticated multi-step planning and rea- soning, has attracted active research to evaluate and develop intelligent agents capable of obtaining advanced forms of human intelligence such as mathematical reasoning. In this technical report, we investigate the ability to for- mulate and solve optimization problems, which is critical across various domains, ranging from operations research and engineering to finance and machine learning, by Open Source Large Language Models. Traditionally, solving opti- mization problems has required human expertise in math- ematical modeling and algorithm design. However, the rise of LLMs presents an opportunity to automate this pro- cess, enabling machines to understand, interpret, and solve optimization problems expressed in natural language. To 1Department of Computer Science and Engineering, Ulsan National Institute of Science and Technology, Ulsan, Republic of Korea. Correspondence to: Sungahn Ko <sako@unist.ac.kr >. The first AI for Math Workshop at the 41stInternational Confer- ence on Machine Learning , Vienna, Austria. Copyright 2024 by the a",0
cdf640fc47f06c403bd674317fc554bcacd8d5e9,Advancing LLM Reasoning Generalists with Preference Trees,"['Lifan Yuan', 'Ganqu Cui', 'Hanbin Wang', 'Ning Ding', 'Xingyao Wang', 'Jia Deng', 'Boji Shan', 'Huimin Chen', 'Ruobing Xie', 'Yankai Lin', 'Zhenghao Liu', 'Bowen Zhou', 'Hao Peng', 'Zhiyuan Liu', 'Maosong Sun']",https://openreview.net/pdf/cdf640fc47f06c403bd674317fc554bcacd8d5e9.pdf,"Advancing LLM Reasoning Generalists with Preference Trees We present Eurus, state-of-the-art open LLM reasoning generalists and its recipe. We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of EURUS can be primarily attributed to ULTRAINTERACT, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. ULTRAINTERACT can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting of (1) reasoning chains with diverse planning strategies in a unified format, (2) multi-turn interaction trajectories with the environment and the critique, and (3) pairwise data to facilitate preference learning. ULTRAINTERACT allows us to conduct an in-depth exploration of preference learning for reasoning tasks. Our investigation reveals that some well-established preference learning algorithms may be less suitable for reasoning tasks compared to their effectiveness in general conversations. Inspired by this, we derive a novel reward modeling objective which, together with ULTRAINTERACT, leads to a strong reward model. All artifacts produced during this research will be made public.",cdf640fc47f06c403bd674317fc554bcacd8d5e9.pdf,"approach in the open-source community (Tun-
stall et al., 2023; Bai et al., 2023) with the proposal of
DPO (Rafailov et al., 2023) and high-quality preference
datasets (Cui et al., 2023; Zhu et al., 2023). Different from
open-domain chatbots, preference learning is largely under-
explored in complex reasoning. Recent research showed
performance degradation when applying DPO on reasoning
tasks, but some newly proposed algorithms demonstrated a
positive effect (Ethayarajh et al., 2024; Chen et al., 2024a;
Mitra et al., 2024; Shao et al., 2024). However, a deep un-
derstanding of preference learning, specifically its efficacy
on complex reasoning, is not yet established.
8. Conclusion
We strive to narrow the huge gap between open-source mod-
els and proprietary models from the perspective of align-
ment. Our work pushes the boundaries of open-source rea-
soning generalists by (1) releasing a high-quality multi-turn
reasoning dataset ULTRA INTERACT with preference trees,
(2) introducing EURUS -series LLMs which achieve new
SOTA on challenging reasoning benchmarks and (3) provid-
ing insights on preference learning for reasoning through
analysis, leading to new reward modeling objectives as well
as a powerful reward model for reasoning.
8
Advancing LLM Reasoning Generalists with Preference Trees
References
Amini, A., Gabriel, S., Lin, S., Koncel-Kedziorski, R., Choi,
Y ., and Hajishirzi, H. MathQA: Towards interpretable
math word problem solving with operation-based for-
malisms. In Proc. ofNAACL-HLT, 2019.
Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski,
H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.
Program synthesis with large language models. ArXiv
preprint, abs/2108.07732, 2021.
Bai, J., Bai, S., Chu, Y ., Cui, Z., Dang, K., Deng, X., Fan,
Y ., Ge, W., Han, Y ., Huang, F., Hui, B., Ji, L., Li, M.,
Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., Ma, J.,
Men, R., Ren, X., Ren, X., Tan, C., Tan, S., Tu, J., Wang,
P., Wang, S., Wang, W., Wu, S.,","conclusions of concurrent work (Chen et al.,
2024b; Wang et al., 2024). Training only on open-source
data without U LTRA INTERACT greatly hurts the reasoning
performance, confirming the effectiveness of ULTRA IN-
TERACT . Meanwhile, training only on ULTRA INTERACT
suffers a performance drop except for BBH, especially in
instruction following. We attribute the performance drop
to a worse instruction-following ability. This suggests the
necessity of mixing ULTRA INTERACT with other alignment
data for better all-around supervised fine-tuning.
7. Related Work
Open LLMs in Reasoning. Open-source LLMs have
shown remarkable progress in building specialists that ex-
cel in mathematics reasoning (Luo et al., 2023a; Yue et al.,
2023; Toshniwal et al., 2024) or coding abilities (Roziere
et al., 2023; Wei et al., 2023; Guo et al., 2024a; Zheng et al.,
2024). On the contrary, mastering general reasoning capabil-
ities still challenges open models, while the most advanced
ones (DeepSeek-AI, 2024; Bai et al., 2023; Touvron et al.,
2023; Jiang et al., 2024) are well behind proprietary mod-
els. More, these cutting-edge open general-purpose models
maintain their alignment recipes confidential, which fur-
ther hinders the replication and development of open-sourcereasoning models.
Preference Learning for Reasoning. Aligning language
models from human or AI preferences has emerged as a
prevalent approach in the open-source community (Tun-
stall et al., 2023; Bai et al., 2023) with the proposal of
DPO (Rafailov et al., 2023) and high-quality preference
datasets (Cui et al., 2023; Zhu et al., 2023). Different from
open-domain chatbots, preference learning is largely under-
explored in complex reasoning. Recent research showed
performance degradation when applying DPO on reasoning
tasks, but some newly proposed algorithms demonstrated a
positive effect (Ethayarajh et al., 2024; Chen et al., 2024a;
Mitra et al., 2024; Shao et al., 2024). However, a deep un-
derstanding of preference learn","approach in the open-source community (Tun- stall et al., 2023; Bai et al., 2023) with the proposal of DPO (Rafailov et al., 2023) and high-quality preference datasets (Cui et al., 2023; Zhu et al., 2023). Different from open-domain chatbots, preference learning is largely under- explored in complex reasoning. Recent research showed performance degradation when applying DPO on reasoning tasks, but some newly proposed algorithms demonstrated a positive effect (Ethayarajh et al., 2024; Chen et al., 2024a; Mitra et al., 2024; Shao et al., 2024). However, a deep un- derstanding of preference learning, specifically its efficacy on complex reasoning, is not yet established. 8. Conclusion We strive to narrow the huge gap between open-source mod- els and proprietary models from the perspective of align- ment. Our work pushes the boundaries of open-source rea- soning generalists by (1) releasing a high-quality multi-turn reasoning dataset ULTRA INTERACT with preference trees, (2) introducing EURUS -series LLMs which achieve new SOTA on challenging reasoning benchmarks and (3) provid- ing insights on preference learning for reasoning through analysis, leading to new reward modeling objectives as well as a powerful reward model for reasoning. 8 Advancing LLM Reasoning Generalists with Preference Trees","conclusions of concurrent work (Chen et al., 2024b; Wang et al., 2024). Training only on open-source data without U LTRA INTERACT greatly hurts the reasoning performance, confirming the effectiveness of ULTRA IN- TERACT . Meanwhile, training only on ULTRA INTERACT suffers a performance drop except for BBH, especially in instruction following. We attribute the performance drop to a worse instruction-following ability. This suggests the necessity of mixing ULTRA INTERACT with other alignment data for better all-around supervised fine-tuning. 7. Related Work Open LLMs in Reasoning. Open-source LLMs have shown remarkable progress in building specialists that ex- cel in mathematics reasoning (Luo et al., 2023a; Yue et al., 2023; Toshniwal et al., 2024) or coding abilities (Roziere et al., 2023; Wei et al., 2023; Guo et al., 2024a; Zheng et al., 2024). On the contrary, mastering general reasoning capabil- ities still challenges open models, while the most advanced ones (DeepSeek-AI, 2024; Bai et al., 2023; Touvron et al., 2023; Jiang et al., 2024) are well behind proprietary mod- els. More, these cutting-edge open general-purpose models maintain their alignment recipes confidential, which fur- ther hinders the replication and development of open-sourcereasoning models. Preference Learning for Reasoning. Aligning language models from human or AI preferences has emerged as a prevalent approach in the open-source community (Tun- stall et al., 2023; Bai et al., 2023) with the proposal of DPO (Rafailov et al., 2023) and high-quality preference datasets (Cui et al., 2023; Zhu et al., 2023). Different from open-domain chatbots, preference learning is largely under- explored in complex reasoning. Recent research showed performance degradation when applying DPO on reasoning tasks, but some newly proposed algorithms demonstrated a positive effect (Ethayarajh et al., 2024; Chen et al., 2024a; Mitra et al., 2024; Shao et al., 2024). However, a deep un- derstanding of preference learn","Advancing LLM Reasoning Generalists with Preference Trees We present Eurus, state-of-the-art open LLM reasoning generalists and its recipe. We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of EURUS can be primarily attributed to ULTRAINTERACT, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. ULTRAINTERACT can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting of (1) reasoning chains with diverse planning strategies in a unified format, (2) multi-turn interaction trajectories with the environment and the critique, and (3) pairwise data to facilitate preference learning. ULTRAINTERACT allows us to conduct an in-depth exploration of preference learning for reasoning tasks. Our investigation reveals that some well-established preference learning algorithms may be less suitable for reasoning tasks compared to their effectiveness in general conversations. Inspired by this, we derive a novel reward modeling objective which, together with ULTRAINTERACT, leads to a strong reward model. All artifacts produced during this research will be made public.","Advancing LLM Reasoning Generalists with Preference Trees We present Eurus, state-of-the-art open LLM reasoning generalists and its recipe. We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of EURUS can be primarily attributed to ULTRAINTERACT, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. ULTRAINTERACT can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting of (1) reasoning chains with diverse planning strategies in a unified format, (2) multi-turn interaction trajectories with the environment and the critique, and (3) pairwise data to facilitate preference learning. ULTRAINTERACT allows us to conduct an in-depth exploration of preference learning for reasoning tasks. Our investigation reveals that some well-established preference learning algorithms may be less suitable for reasoning tasks compared to their effectiveness in general conversations. Inspired by this, we derive a novel reward modeling objective which, together with ULTRAINTERACT, leads to a strong reward model. All artifacts produced during this research will be made public. conclusions of concurrent work (Chen et al., 2024b; Wang et al., 2024). Training only on open-source data without U LTRA INTERACT greatly hurts the reasoning performance, confirming the effectiveness of ULTRA IN- TERACT . Meanwhile, training only on ULTRA INTERACT suffers a performance drop except for BBH, especially in instruction following. We attribute the performance drop to a worse instruction-following ability. This suggests the necessity of mixing ULTRA INTERACT with other alignment data for better all-around supervised fine-tuning. 7. Related Work Open LLMs in Reasoning. Open-source LLMs have shown remarkable progress in building specialists that ex- cel in mathematics reasoning (Luo et al., 2023a; Yue et al., 2023; Toshniwal et al., 2024) or coding abilities (Roziere et al., 2023; Wei et al., 2023; Guo et al., 2024a; Zheng et al., 2024). On the contrary, mastering general reasoning capabil- ities still challenges open models, while the most advanced ones (DeepSeek-AI, 2024; Bai et al., 2023; Touvron et al., 2023; Jiang et al., 2024) are well behind proprietary mod- els. More, these cutting-edge open general-purpose models maintain their alignment recipes confidential, which fur- ther hinders the replication and development of open-sourcereasoning models. Preference Learning for Reasoning. Aligning language models from human or AI preferences has emerged as a prevalent approach in the open-source community (Tun- stall et al., 2023; Bai et al., 2023) with the proposal of DPO (Rafailov et al., 2023) and high-quality preference datasets (Cui et al., 2023; Zhu et al., 2023). Different from open-domain chatbots, preference learning is largely under- explored in complex reasoning. Recent research showed performance degradation when applying DPO on reasoning tasks, but some newly proposed algorithms demonstrated a positive effect (Ethayarajh et al., 2024; Chen et al., 2024a; Mitra et al., 2024; Shao et al., 2024). However, a deep un- derstanding of preference learn approach in the open-source community (Tun- stall et al., 2023; Bai et al., 2023) with the proposal of DPO (Rafailov et al., 2023) and high-quality preference datasets (Cui et al., 2023; Zhu et al., 2023). Different from open-domain chatbots, preference learning is largely under- explored in complex reasoning. Recent research showed performance degradation when applying DPO on reasoning tasks, but some newly proposed algorithms demonstrated a positive effect (Ethayarajh et al., 2024; Chen et al., 2024a; Mitra et al., 2024; Shao et al., 2024). However, a deep un- derstanding of preference learning, specifically its efficacy on complex reasoning, is not yet established. 8. Conclusion We strive to narrow the huge gap between open-source mod- els and proprietary models from the perspective of align- ment. Our work pushes the boundaries of open-source rea- soning generalists by (1) releasing a high-quality multi-turn reasoning dataset ULTRA INTERACT with preference trees, (2) introducing EURUS -series LLMs which achieve new SOTA on challenging reasoning benchmarks and (3) provid- ing insights on preference learning for reasoning through analysis, leading to new reward modeling objectives as well as a powerful reward model for reasoning. 8 Advancing LLM Reasoning Generalists with Preference Trees",0
3ba5283059bb755e01651618340073d09b23f233,GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning,"['Mehran Kazemi', 'Hamidreza Alvari', 'Ankit Anand', 'Jialin Wu', 'Xi Chen', 'Radu Soricut']",https://openreview.net/pdf/3ba5283059bb755e01651618340073d09b23f233.pdf,"GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning We created a dataset of geometry problems and conducted a systematic evaluation of large models. Large language models have shown impressive results for multi-hop mathematical reasoning when the input question is only textual. Many mathematical reasoning problems, however, contain both text and image. With the ever-increasing adoption of vision language models (VLMs), understanding their reasoning abilities for such problems is crucial. In this paper, we evaluate the reasoning capabilities of VLMs along various axes through the lens of geometry problems. We procedurally create a synthetic dataset of geometry questions with controllable difficulty levels along multiple axes, thus enabling a systematic evaluation. The empirical results obtained using our benchmark for state-of-the-art VLMs indicate that these models are not as capable in subjects like geometry (and, by generalization, other topics requiring similar reasoning) as suggested by previous benchmarks. This is made especially clear by the construction of our benchmark at various depth levels, since solving higher-depth problems requires long chains of reasoning rather than additional memorized knowledge.",3ba5283059bb755e01651618340073d09b23f233.pdf,"Approaches: Some of the ap-
proaches for improving the multi-hop reasoning of LLMs
and VLMs range from pre-training on relevant data
(Hendrycks et al., 2021; Lewkowycz et al., 2022), fine-
tuning with (Nye et al., 2022; Dalvi et al., 2021; Zelikman
et al., 2022; Kazemi et al., 2023a) and without (Clark et al.,
2021; Betz et al., 2021) explicitly generating the solution,
in-context learning with solutions (Wei et al., 2022), decom-
posing the problem into smaller pieces and solving them
separately (Zhou et al., 2023; Khot et al., 2023) and using
2
A Systematic Evaluation of Large Models for Geometric Reasoning
Dataset →
Feature ↓
ProofWriter
(Tafjord et al., 2021)
BoardgameQA
(Kazemi et al., 2023a)
AR-LSAT
(Zhong et al., 2021)
AQUA
(Ling et al., 2017)
GSM8k
(Cobbe et al., 2021)
CLEVR-Math
(Lindstr ¨om & Abraham, 2022)
ChartQA
(Masry et al., 2022)
GeoS
(Seo et al., 2015)
GeoQA
(Chen et al., 2021)
Geometry3k
(Lu et al., 2021)
UniGeo
(Chen et al., 2022a)
PGPS9K
(Zhang et al., 2023)
GeomVerse
Textual
Understanding✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
Visual
Understanding✗ ✗ ✗ ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
Mathematical
Reasoning✗ ∼ ✗ ✓ ✓ ∼ ∼ ✓ ✓ ✓ ✓ ✓ ✓
Automatic
Difficulty
Control✓ ✓ ✗ ✗ ✗ ∼ ∼ ✗ ✗ ✗ ✗ ✗ ✓
Table 1: A comparison of GeomVerse with some of the recent and/or widely-used multi-hop (logical or mathematical)
reasoning datasets. We use ∼when a dataset contains a property to a limited extent.
LLMs/VLMs as tools within classical algorithms (Kazemi
et al., 2023b; Creswell et al., 2023). In the realm of reason-
ing about geometry problems, existing work typically de-
velops specialized models or tools (e.g, (Trinh et al., 2024))
or resorts to distillation strategies (e.g., (Gao et al., 2023));
measuring the reasoning ability of general-purpose VLMs
is less studied.
3. The GeomVerse Dataset
We start with some preliminaries and terminologies. Then,
we explain how GeomVerse is created. The dataset will be
publicly available upon the acceptance of the paper.
3.1. Multi-Hop Logical Reasoning
A","Future work can verify the merit of finetuning
models on synthetic geometry problems for improving their
performance on real datasets. In an initial experiment, we
measured the performance of the PaLI 5B model on Ge-
ometry3k with and without finetuning on GeomVerse and
observed modest improvements (from almost 0 to almost 2
percent accuracy). We believe this is due to the difference
in the visual and textual features of the Geomety3k and
GeomVerse , as well as the poor generalization of PaLI to
geometry problems beyond its training distribution. Bet-
ter aligning the textual and visual features and using more
powerful models can yield more gains.
References
Abdelghani, R., Wang, Y .-H., Yuan, X., Wang, T., Lu-
cas, P., Sauz ´eon, H., and Oudeyer, P.-Y . Gpt-3-driven
pedagogical agents to train children’s curious question-
asking skills. International Journal of Artificial Intelli-
gence in Education , June 2023. ISSN 1560-4306. doi:
10.1007/s40593-023-00340-7. URL http://dx.doi.
org/10.1007/s40593-023-00340-7 .
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,
Hasson, Y ., Lenc, K., Mensch, A., Millican, K., Reynolds,
M., et al. Flamingo: a visual language model for few-shot
learning. Advances in Neural Information Processing
Systems , 35:23716–23736, 2022.
Allaway, E., Hwang, J. D., Bhagavatula, C., McKeown, K.,
Downey, D., and Choi, Y . Penguins don’t fly: Reason-
ing about generics through instantiations and exceptions.
arXiv preprint arXiv:2205.11658 , 2022.
8
A Systematic Evaluation of Large Models for Geometric Reasoning
Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin,
D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen,
Z., et al. Palm 2 technical report. arXiv preprint
arXiv:2305.10403 , 2023.
Betz, G., V oigt, C., and Richardson, K. Critical think-
ing for language models. In Proceedings of the 14th
International Conference on Computational Semantics
(IWCS) , pp. 63–75, Groningen, The Netherlands (on-
line), June 2021. Association for","Approaches: Some of the ap- proaches for improving the multi-hop reasoning of LLMs and VLMs range from pre-training on relevant data (Hendrycks et al., 2021; Lewkowycz et al., 2022), fine- tuning with (Nye et al., 2022; Dalvi et al., 2021; Zelikman et al., 2022; Kazemi et al., 2023a) and without (Clark et al., 2021; Betz et al., 2021) explicitly generating the solution, in-context learning with solutions (Wei et al., 2022), decom- posing the problem into smaller pieces and solving them separately (Zhou et al., 2023; Khot et al., 2023) and using 2 A Systematic Evaluation of Large Models for Geometric Reasoning Dataset Feature ProofWriter (Tafjord et al., 2021) BoardgameQA (Kazemi et al., 2023a) AR-LSAT (Zhong et al., 2021) AQUA (Ling et al., 2017) GSM8k (Cobbe et al., 2021) CLEVR-Math (Lindstr om & Abraham, 2022) ChartQA (Masry et al., 2022) GeoS (Seo et al., 2015) GeoQA (Chen et al., 2021) Geometry3k (Lu et al., 2021) UniGeo (Chen et al., 2022a) PGPS9K (Zhang et al., 2023) GeomVerse Textual Understanding Visual Understanding Mathematical Reasoning Automatic Difficulty Control Table 1: A comparison of GeomVerse with some of the recent and/or widely-used multi-hop (logical or mathematical) reasoning datasets. We use when a dataset contains a property to a limited extent. LLMs/VLMs as tools within classical algorithms (Kazemi et al., 2023b; Creswell et al., 2023). In the realm of reason- ing about geometry problems, existing work typically de- velops specialized models or tools (e.g, (Trinh et al., 2024)) or resorts to distillation strategies (e.g., (Gao et al., 2023)); measuring the reasoning ability of general-purpose VLMs is less studied. 3. The GeomVerse Dataset We start with some preliminaries and terminologies. Then, we explain how GeomVerse is created. The dataset will be publicly available upon the acceptance of the paper. 3.1. Multi-Hop Logical Reasoning A","Future work can verify the merit of finetuning models on synthetic geometry problems for improving their performance on real datasets. In an initial experiment, we measured the performance of the PaLI 5B model on Ge- ometry3k with and without finetuning on GeomVerse and observed modest improvements (from almost 0 to almost 2 percent accuracy). We believe this is due to the difference in the visual and textual features of the Geomety3k and GeomVerse , as well as the poor generalization of PaLI to geometry problems beyond its training distribution. Bet- ter aligning the textual and visual features and using more powerful models can yield more gains.","GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning We created a dataset of geometry problems and conducted a systematic evaluation of large models. Large language models have shown impressive results for multi-hop mathematical reasoning when the input question is only textual. Many mathematical reasoning problems, however, contain both text and image. With the ever-increasing adoption of vision language models (VLMs), understanding their reasoning abilities for such problems is crucial. In this paper, we evaluate the reasoning capabilities of VLMs along various axes through the lens of geometry problems. We procedurally create a synthetic dataset of geometry questions with controllable difficulty levels along multiple axes, thus enabling a systematic evaluation. The empirical results obtained using our benchmark for state-of-the-art VLMs indicate that these models are not as capable in subjects like geometry (and, by generalization, other topics requiring similar reasoning) as suggested by previous benchmarks. This is made especially clear by the construction of our benchmark at various depth levels, since solving higher-depth problems requires long chains of reasoning rather than additional memorized knowledge.","GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning We created a dataset of geometry problems and conducted a systematic evaluation of large models. Large language models have shown impressive results for multi-hop mathematical reasoning when the input question is only textual. Many mathematical reasoning problems, however, contain both text and image. With the ever-increasing adoption of vision language models (VLMs), understanding their reasoning abilities for such problems is crucial. In this paper, we evaluate the reasoning capabilities of VLMs along various axes through the lens of geometry problems. We procedurally create a synthetic dataset of geometry questions with controllable difficulty levels along multiple axes, thus enabling a systematic evaluation. The empirical results obtained using our benchmark for state-of-the-art VLMs indicate that these models are not as capable in subjects like geometry (and, by generalization, other topics requiring similar reasoning) as suggested by previous benchmarks. This is made especially clear by the construction of our benchmark at various depth levels, since solving higher-depth problems requires long chains of reasoning rather than additional memorized knowledge. Future work can verify the merit of finetuning models on synthetic geometry problems for improving their performance on real datasets. In an initial experiment, we measured the performance of the PaLI 5B model on Ge- ometry3k with and without finetuning on GeomVerse and observed modest improvements (from almost 0 to almost 2 percent accuracy). We believe this is due to the difference in the visual and textual features of the Geomety3k and GeomVerse , as well as the poor generalization of PaLI to geometry problems beyond its training distribution. Bet- ter aligning the textual and visual features and using more powerful models can yield more gains. Approaches: Some of the ap- proaches for improving the multi-hop reasoning of LLMs and VLMs range from pre-training on relevant data (Hendrycks et al., 2021; Lewkowycz et al., 2022), fine- tuning with (Nye et al., 2022; Dalvi et al., 2021; Zelikman et al., 2022; Kazemi et al., 2023a) and without (Clark et al., 2021; Betz et al., 2021) explicitly generating the solution, in-context learning with solutions (Wei et al., 2022), decom- posing the problem into smaller pieces and solving them separately (Zhou et al., 2023; Khot et al., 2023) and using 2 A Systematic Evaluation of Large Models for Geometric Reasoning Dataset Feature ProofWriter (Tafjord et al., 2021) BoardgameQA (Kazemi et al., 2023a) AR-LSAT (Zhong et al., 2021) AQUA (Ling et al., 2017) GSM8k (Cobbe et al., 2021) CLEVR-Math (Lindstr om & Abraham, 2022) ChartQA (Masry et al., 2022) GeoS (Seo et al., 2015) GeoQA (Chen et al., 2021) Geometry3k (Lu et al., 2021) UniGeo (Chen et al., 2022a) PGPS9K (Zhang et al., 2023) GeomVerse Textual Understanding Visual Understanding Mathematical Reasoning Automatic Difficulty Control Table 1: A comparison of GeomVerse with some of the recent and/or widely-used multi-hop (logical or mathematical) reasoning datasets. We use when a dataset contains a property to a limited extent. LLMs/VLMs as tools within classical algorithms (Kazemi et al., 2023b; Creswell et al., 2023). In the realm of reason- ing about geometry problems, existing work typically de- velops specialized models or tools (e.g, (Trinh et al., 2024)) or resorts to distillation strategies (e.g., (Gao et al., 2023)); measuring the reasoning ability of general-purpose VLMs is less studied. 3. The GeomVerse Dataset We start with some preliminaries and terminologies. Then, we explain how GeomVerse is created. The dataset will be publicly available upon the acceptance of the paper. 3.1. Multi-Hop Logical Reasoning A",0
b658ad52e686b34e585fbe860bd4a1bbf08341ab,Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving,"['Aniket Rajiv Didolkar', 'Anirudh Goyal', 'Nan Rosemary Ke', 'Siyuan Guo', 'Michal Valko', 'Timothy P Lillicrap', 'Danilo Jimenez Rezende', 'Yoshua Bengio', 'Michael Curtis Mozer', 'Sanjeev Arora']",https://openreview.net/pdf/b658ad52e686b34e585fbe860bd4a1bbf08341ab.pdf,"Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving Extracting metacognitive knowledge from LLMs and using it to improve math reasoning. \emph{Metacognitive knowledge} refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans. To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH. (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label. This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic, even though this article applies it to math problems.",b658ad52e686b34e585fbe860bd4a1bbf08341ab.pdf,"methodology presented is domain-agnostic,
even though this article applies it to math prob-
lems.
*Equal contribution1Mila, University of Mon-
treal2Google Deepmind3The University of Cambridge
4Princeton University. Correspondence to: Aniket Didolkar
<adidolkar123@gmail.com >, Anirudh Goyal <anirud-
hgoyal9119@gmail.com >.
The first AI for MATH Workshop at the 41st International Confer-
ence on Machine Learning, Vienna, Austria. Copyright 2024 by
the author(s).1. Introduction
Large language models (LLMs) have demonstrated remark-
able advancements in recent years at natural language in-
ference tasks ( 1–7), as well as scientific and mathematical
problems ( 8–11), although their limitations on mathematical
problems are also well-documented (12–17).
A core concept in human pedagogy is Metacognition (18),
sometimes described as thinking about thinking . It refers
to ability to reason about one’s own cognitive processes as
well as about learning-relevant properties of information
or data. Metacognitive Knowledge refers to the learner’s
accumulated knowledge of this type. Pedagogy research
shows that improving learners’ metacognitive knowledge
can improve their capabilities, for example on math ( 19;20).
The current paper raises the question “Do LLMs also have
metacognitive knowledge?” And if yes, Can we bootstrap
such knowledge to further improve LLM capabilities?
At first glance, this quest seems difficult. Deciphering
LLMs’ inner working from their huge set of parameters
–all results of non-linear optimization— is notoriously hard.
Furthermore, scientists lack parameter access to most lead-
ing AI models. But there are still reasons to hope we can
understand metacognition by interacting with LLMs. They
display some human tics, such as ability to improve their
math reasoning via Chain of Thought (CoT) (21) and also
the “Let’s think step by step” prompt ( 22). These were gen-
erally perceived as convenient tricks to get around the limi-
tations imposed by the LLM’s aut","future work.
Paper organization and main results: Section 3 describes
the method and Section 4 describes experiments. Using
a strong LLM - GPT-4 - to identify skills, we validate
the usefulness of these skills by demonstrates a significant
11.6% enhancement over CoT on the MATH Dataset us-
ing the method described in Section 3. Furthermore, the
identified skills also improve the generation of code-based
solutions for the problems within the MATH dataset giv-
ing a 7.52% improvement over the baseline PAL approach
(24), which also instructs the model to generate code. Sec-
tion 4.3 shows that the the skill exemplar repository created
for MATH noticeably improved in-context performance for
weaker LLMs on the same dataset and that the repository for
GSM8K helped improve in-context performance for other
math datasets. This shows that a powerful LLM can be used
for deeper understanding of skills that translates across other
LLMs and related datasets.
2. Related Works
For human learning, statistical methods can infer latent skills
from data and use the inferred skills to more accurately
forecast student learning ( 25;26). In machine learning,
works that study learning via skill induction include ( 27–30).
These start with some definition of skills in terms of model
parameters, whereas we use a powerful LLM in a black
box way to identify and consolidate skills. A discussion
of various prompting strategies is covered in Section 4 and
Appendix Section 8.
3. Automated Skill Discovery
We describe an automated process for categorizing mathe-
matical questions according to specific skills needed to solve
them. See Figure 1. Recent works relating skills and LLMs
(31;32) were an inspiration. Conceptually, the strategy in-
volves the creation of a detailed skill exemplar repository,
which contains a compilation of skill names alongside re-
spective illustrative examples (comprising both questions
2
Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving
Da","methodology presented is domain-agnostic, even though this article applies it to math prob- lems. *Equal contribution1Mila, University of Mon- treal2Google Deepmind3The University of Cambridge 4Princeton University. Correspondence to: Aniket Didolkar <adidolkar123@gmail.com >, Anirudh Goyal <anirud- hgoyal9119@gmail.com >. The first AI for MATH Workshop at the 41st International Confer- ence on Machine Learning, Vienna, Austria. Copyright 2024 by the author(s).1. Introduction Large language models (LLMs) have demonstrated remark- able advancements in recent years at natural language in- ference tasks ( 1 7), as well as scientific and mathematical problems ( 8 11), although their limitations on mathematical problems are also well-documented (12 17). A core concept in human pedagogy is Metacognition (18), sometimes described as thinking about thinking . It refers to ability to reason about one s own cognitive processes as well as about learning-relevant properties of information or data. Metacognitive Knowledge refers to the learner s accumulated knowledge of this type. Pedagogy research shows that improving learners metacognitive knowledge can improve their capabilities, for example on math ( 19;20). The current paper raises the question Do LLMs also have metacognitive knowledge? And if yes, Can we bootstrap such knowledge to further improve LLM capabilities? At first glance, this quest seems difficult. Deciphering LLMs inner working from their huge set of parameters all results of non-linear optimization is notoriously hard. Furthermore, scientists lack parameter access to most lead- ing AI models. But there are still reasons to hope we can understand metacognition by interacting with LLMs. They display some human tics, such as ability to improve their math reasoning via Chain of Thought (CoT) (21) and also the Let s think step by step prompt ( 22). These were gen- erally perceived as convenient tricks to get around the limi- tations imposed by the LLM s aut","future work. Paper organization and main results: Section 3 describes the method and Section 4 describes experiments. Using a strong LLM - GPT-4 - to identify skills, we validate the usefulness of these skills by demonstrates a significant 11.6% enhancement over CoT on the MATH Dataset us- ing the method described in Section 3. Furthermore, the identified skills also improve the generation of code-based solutions for the problems within the MATH dataset giv- ing a 7.52% improvement over the baseline PAL approach (24), which also instructs the model to generate code. Sec- tion 4.3 shows that the the skill exemplar repository created for MATH noticeably improved in-context performance for weaker LLMs on the same dataset and that the repository for GSM8K helped improve in-context performance for other math datasets. This shows that a powerful LLM can be used for deeper understanding of skills that translates across other LLMs and related datasets. 2. Related Works For human learning, statistical methods can infer latent skills from data and use the inferred skills to more accurately forecast student learning ( 25;26). In machine learning, works that study learning via skill induction include ( 27 30). These start with some definition of skills in terms of model parameters, whereas we use a powerful LLM in a black box way to identify and consolidate skills. A discussion of various prompting strategies is covered in Section 4 and Appendix Section 8. 3. Automated Skill Discovery We describe an automated process for categorizing mathe- matical questions according to specific skills needed to solve them. See Figure 1. Recent works relating skills and LLMs (31;32) were an inspiration. Conceptually, the strategy in- volves the creation of a detailed skill exemplar repository, which contains a compilation of skill names alongside re- spective illustrative examples (comprising both questions 2 Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving Da","Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving Extracting metacognitive knowledge from LLMs and using it to improve math reasoning. \emph{Metacognitive knowledge} refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans. To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH. (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label. This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic, even though this article applies it to math problems.","Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving Extracting metacognitive knowledge from LLMs and using it to improve math reasoning. \emph{Metacognitive knowledge} refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans. To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH. (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label. This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic, even though this article applies it to math problems. future work. Paper organization and main results: Section 3 describes the method and Section 4 describes experiments. Using a strong LLM - GPT-4 - to identify skills, we validate the usefulness of these skills by demonstrates a significant 11.6% enhancement over CoT on the MATH Dataset us- ing the method described in Section 3. Furthermore, the identified skills also improve the generation of code-based solutions for the problems within the MATH dataset giv- ing a 7.52% improvement over the baseline PAL approach (24), which also instructs the model to generate code. Sec- tion 4.3 shows that the the skill exemplar repository created for MATH noticeably improved in-context performance for weaker LLMs on the same dataset and that the repository for GSM8K helped improve in-context performance for other math datasets. This shows that a powerful LLM can be used for deeper understanding of skills that translates across other LLMs and related datasets. 2. Related Works For human learning, statistical methods can infer latent skills from data and use the inferred skills to more accurately forecast student learning ( 25;26). In machine learning, works that study learning via skill induction include ( 27 30). These start with some definition of skills in terms of model parameters, whereas we use a powerful LLM in a black box way to identify and consolidate skills. A discussion of various prompting strategies is covered in Section 4 and Appendix Section 8. 3. Automated Skill Discovery We describe an automated process for categorizing mathe- matical questions according to specific skills needed to solve them. See Figure 1. Recent works relating skills and LLMs (31;32) were an inspiration. Conceptually, the strategy in- volves the creation of a detailed skill exemplar repository, which contains a compilation of skill names alongside re- spective illustrative examples (comprising both questions 2 Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving Da methodology presented is domain-agnostic, even though this article applies it to math prob- lems. *Equal contribution1Mila, University of Mon- treal2Google Deepmind3The University of Cambridge 4Princeton University. Correspondence to: Aniket Didolkar <adidolkar123@gmail.com >, Anirudh Goyal <anirud- hgoyal9119@gmail.com >. The first AI for MATH Workshop at the 41st International Confer- ence on Machine Learning, Vienna, Austria. Copyright 2024 by the author(s).1. Introduction Large language models (LLMs) have demonstrated remark- able advancements in recent years at natural language in- ference tasks ( 1 7), as well as scientific and mathematical problems ( 8 11), although their limitations on mathematical problems are also well-documented (12 17). A core concept in human pedagogy is Metacognition (18), sometimes described as thinking about thinking . It refers to ability to reason about one s own cognitive processes as well as about learning-relevant properties of information or data. Metacognitive Knowledge refers to the learner s accumulated knowledge of this type. Pedagogy research shows that improving learners metacognitive knowledge can improve their capabilities, for example on math ( 19;20). The current paper raises the question Do LLMs also have metacognitive knowledge? And if yes, Can we bootstrap such knowledge to further improve LLM capabilities? At first glance, this quest seems difficult. Deciphering LLMs inner working from their huge set of parameters all results of non-linear optimization is notoriously hard. Furthermore, scientists lack parameter access to most lead- ing AI models. But there are still reasons to hope we can understand metacognition by interacting with LLMs. They display some human tics, such as ability to improve their math reasoning via Chain of Thought (CoT) (21) and also the Let s think step by step prompt ( 22). These were gen- erally perceived as convenient tricks to get around the limi- tations imposed by the LLM s aut",0
65288dfe6291289a7a0dcf5c03978ad5a57b5d76,Equivariant Flow Matching for Molecular Conformer Generation,"['Majdi Hassan', 'Nikhil Shenoy', 'Jungyoon Lee', 'Hannes Stark', 'Stephan Thaler', 'Dominique Beaini']",https://openreview.net/pdf/65288dfe6291289a7a0dcf5c03978ad5a57b5d76.pdf,"Equivariant Flow Matching for Molecular Conformer Generation We leverage flow matching and equivariant graph networks to state-of-the-art performance on generating conformers. Predicting low-energy molecular conformations given a molecular graph is an important but challenging task in computational drug discovery. Existing state-of-the-art approaches either resort to large scale transformer-based models that diffuse over conformer fields, or use computationally expensive methods to generate initial structures and diffuse over torsion angles. In this work, we introduce Equivariant Transformer Flow (ET-Flow). We showcase that a well-designed flow matching approach with equivariance and harmonic prior alleviates the need for complex internal geometry calculations and large architectures, contrary to the prevailing methods in the field. Our approach results in a straightforward and scalable method that directly operates on all-atom coordinates with minimal assumptions. ET-Flow outperforms or matches the previous state-of-the-art in molecular conformer generation benchmarks with significantly fewer parameters, no dependence on internal geometry, and fast inference.",65288dfe6291289a7a0dcf5c03978ad5a57b5d76.pdf,"methodology is similar to that
of (Jing et al., 2022). First, we look at RMSD based met-
rics like Coverage and Average Minimum RMSD (AMR)
between generated and ground truth conformer ensembles.
For this, we generate 2Kconformers for a molecule with K
Submission and Formatting Instructions for ML4LMS @ ICML 2024
ground truth conformers. Second, we look at chemical simi-
larity using properties like Energy ( E), dipole moment ( µ),
HOMO-LUMO gap ( ∆ϵ) and the minimum energy ( Emin)
calculated using xTB (Bannwarth et al., 2019).
Baselines : We benchmark ET-Flow against leading ap-
proaches outlined in Section A. Specifically, we assess the
performance of GeoMol (Ganea et al., 2021), GeoDiff (Xu
et al., 2022), Torsional Diff (Jing et al., 2022), and MCF
(Wang et al., 2024). Notably, the most recent among these,
MCF, has demonstrated superior performance across eval-
uation metrics compared to its predecessors. It is worth
mentioning that GeoDiff initially utilized a limited subset
of the DRUGS dataset; thus, for a fair comparison, we con-
sider its re-evaluated performance as presented in (Jing et al.,
2022).
3.2. Ensemble RMSD
As shown in Table 1 and Section C.2, ET-Flow outperforms
all preceding methods and demonstrates competitive perfor-
mance with the previous state-of-the-art, MCF (Wang et al.,
2024). Despite being significantly smaller with only 8.3M
parameters, ET-Flow shows a substantial improvement in
the quality of generated conformers, as evidenced by supe-
rior Precision metrics across all MCF models, including the
largest MCF-L. When compared to MCF-S, which is closer
in size, ET-Flow achieves better Precision while the impact
on Recall is less significant and limited to Recall Coverage.
Notably, our Recall AMR remains competitive with much
bigger MCF-B, underscoring the inherent advantage of our
method in accurately predicting overall structures.
3.3. Ensemble Properties
RMSD provides a geometric measure for assessing ensem-
ble quality, but it is also e","Future Works : While ET-Flow demonstrates competitive
performance in molecular conformer generation, there are
areas where it can be improved. We propose three future
directions. First, we observe that a well-designed sampling
process incorporating stochasticity can enhance the quality
and diversity of generated samples. An extension of our
method could involve using Stochastic Differential Equa-
tions (SDEs), which utilize both vector field and score in
inference, potentially improving the diversity of samples
(Ma et al., 2024). Second, we propose to scale the num-
ber of parameters of ET-Flow, which has been shown to be
useful in molecular conformer generation for MCF (Wang
et al., 2024), especially for out-of-distribution generation
in GEOM-XL. Lastly, we aim to alleviate the need for an
additional chirality correction step via architectural changes.
Submission and Formatting Instructions for ML4LMS @ ICML 2024
References
Albergo, M. S. and Vanden-Eijnden, E. Building normaliz-
ing flows with stochastic interpolants, 2023.
Albergo, M. S., Boffi, N. M., and Vanden-Eijnden, E.
Stochastic interpolants: A unifying framework for flows
and diffusions. arXiv preprint arXiv:2303.08797 , 2023.
Anderson, B., Hy, T. S., and Kondor, R. Cormorant: Co-
variant molecular neural networks. Advances in neural
information processing systems , 32, 2019.
Axelrod, S. and Gomez-Bombarelli, R. Geom, energy-
annotated molecular conformations for property predic-
tion and molecular generation. Scientific Data , 9(1):185,
2022.
Axelrod, S. and Gomez-Bombarelli, R. Molecular machine
learning with conformer ensembles. Machine Learning:
Science and Technology , 4(3):035025, 2023.
Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.
arXiv preprint arXiv:1607.06450 , 2016.
Ballard, A. J., Martiniani, S., Stevenson, J. D., Somani, S.,
and Wales, D. J. Exploiting the potential energy landscape
to sample free energy. Wiley Interdisciplinary Reviews:
Computational Molecular Science , 5(3","methodology is similar to that of (Jing et al., 2022). First, we look at RMSD based met- rics like Coverage and Average Minimum RMSD (AMR) between generated and ground truth conformer ensembles. For this, we generate 2Kconformers for a molecule with K Submission and Formatting Instructions for ML4LMS @ ICML 2024 ground truth conformers. Second, we look at chemical simi- larity using properties like Energy ( E), dipole moment ( ), HOMO-LUMO gap ( ) and the minimum energy ( Emin) calculated using xTB (Bannwarth et al., 2019). Baselines : We benchmark ET-Flow against leading ap- proaches outlined in Section A. Specifically, we assess the performance of GeoMol (Ganea et al., 2021), GeoDiff (Xu et al., 2022), Torsional Diff (Jing et al., 2022), and MCF (Wang et al., 2024). Notably, the most recent among these, MCF, has demonstrated superior performance across eval- uation metrics compared to its predecessors. It is worth mentioning that GeoDiff initially utilized a limited subset of the DRUGS dataset; thus, for a fair comparison, we con- sider its re-evaluated performance as presented in (Jing et al., 2022). 3.2. Ensemble RMSD As shown in Table 1 and Section C.2, ET-Flow outperforms all preceding methods and demonstrates competitive perfor- mance with the previous state-of-the-art, MCF (Wang et al., 2024). Despite being significantly smaller with only 8.3M parameters, ET-Flow shows a substantial improvement in the quality of generated conformers, as evidenced by supe- rior Precision metrics across all MCF models, including the largest MCF-L. When compared to MCF-S, which is closer in size, ET-Flow achieves better Precision while the impact on Recall is less significant and limited to Recall Coverage. Notably, our Recall AMR remains competitive with much bigger MCF-B, underscoring the inherent advantage of our method in accurately predicting overall structures. 3.3. Ensemble Properties RMSD provides a geometric measure for assessing ensem- ble quality, but it is also e","Future Works : While ET-Flow demonstrates competitive performance in molecular conformer generation, there are areas where it can be improved. We propose three future directions. First, we observe that a well-designed sampling process incorporating stochasticity can enhance the quality and diversity of generated samples. An extension of our method could involve using Stochastic Differential Equa- tions (SDEs), which utilize both vector field and score in inference, potentially improving the diversity of samples (Ma et al., 2024). Second, we propose to scale the num- ber of parameters of ET-Flow, which has been shown to be useful in molecular conformer generation for MCF (Wang et al., 2024), especially for out-of-distribution generation in GEOM-XL. Lastly, we aim to alleviate the need for an additional chirality correction step via architectural changes. Submission and Formatting Instructions for ML4LMS @ ICML 2024","Equivariant Flow Matching for Molecular Conformer Generation We leverage flow matching and equivariant graph networks to state-of-the-art performance on generating conformers. Predicting low-energy molecular conformations given a molecular graph is an important but challenging task in computational drug discovery. Existing state-of-the-art approaches either resort to large scale transformer-based models that diffuse over conformer fields, or use computationally expensive methods to generate initial structures and diffuse over torsion angles. In this work, we introduce Equivariant Transformer Flow (ET-Flow). We showcase that a well-designed flow matching approach with equivariance and harmonic prior alleviates the need for complex internal geometry calculations and large architectures, contrary to the prevailing methods in the field. Our approach results in a straightforward and scalable method that directly operates on all-atom coordinates with minimal assumptions. ET-Flow outperforms or matches the previous state-of-the-art in molecular conformer generation benchmarks with significantly fewer parameters, no dependence on internal geometry, and fast inference.","Equivariant Flow Matching for Molecular Conformer Generation We leverage flow matching and equivariant graph networks to state-of-the-art performance on generating conformers. Predicting low-energy molecular conformations given a molecular graph is an important but challenging task in computational drug discovery. Existing state-of-the-art approaches either resort to large scale transformer-based models that diffuse over conformer fields, or use computationally expensive methods to generate initial structures and diffuse over torsion angles. In this work, we introduce Equivariant Transformer Flow (ET-Flow). We showcase that a well-designed flow matching approach with equivariance and harmonic prior alleviates the need for complex internal geometry calculations and large architectures, contrary to the prevailing methods in the field. Our approach results in a straightforward and scalable method that directly operates on all-atom coordinates with minimal assumptions. ET-Flow outperforms or matches the previous state-of-the-art in molecular conformer generation benchmarks with significantly fewer parameters, no dependence on internal geometry, and fast inference. Future Works : While ET-Flow demonstrates competitive performance in molecular conformer generation, there are areas where it can be improved. We propose three future directions. First, we observe that a well-designed sampling process incorporating stochasticity can enhance the quality and diversity of generated samples. An extension of our method could involve using Stochastic Differential Equa- tions (SDEs), which utilize both vector field and score in inference, potentially improving the diversity of samples (Ma et al., 2024). Second, we propose to scale the num- ber of parameters of ET-Flow, which has been shown to be useful in molecular conformer generation for MCF (Wang et al., 2024), especially for out-of-distribution generation in GEOM-XL. Lastly, we aim to alleviate the need for an additional chirality correction step via architectural changes. Submission and Formatting Instructions for ML4LMS @ ICML 2024 methodology is similar to that of (Jing et al., 2022). First, we look at RMSD based met- rics like Coverage and Average Minimum RMSD (AMR) between generated and ground truth conformer ensembles. For this, we generate 2Kconformers for a molecule with K Submission and Formatting Instructions for ML4LMS @ ICML 2024 ground truth conformers. Second, we look at chemical simi- larity using properties like Energy ( E), dipole moment ( ), HOMO-LUMO gap ( ) and the minimum energy ( Emin) calculated using xTB (Bannwarth et al., 2019). Baselines : We benchmark ET-Flow against leading ap- proaches outlined in Section A. Specifically, we assess the performance of GeoMol (Ganea et al., 2021), GeoDiff (Xu et al., 2022), Torsional Diff (Jing et al., 2022), and MCF (Wang et al., 2024). Notably, the most recent among these, MCF, has demonstrated superior performance across eval- uation metrics compared to its predecessors. It is worth mentioning that GeoDiff initially utilized a limited subset of the DRUGS dataset; thus, for a fair comparison, we con- sider its re-evaluated performance as presented in (Jing et al., 2022). 3.2. Ensemble RMSD As shown in Table 1 and Section C.2, ET-Flow outperforms all preceding methods and demonstrates competitive perfor- mance with the previous state-of-the-art, MCF (Wang et al., 2024). Despite being significantly smaller with only 8.3M parameters, ET-Flow shows a substantial improvement in the quality of generated conformers, as evidenced by supe- rior Precision metrics across all MCF models, including the largest MCF-L. When compared to MCF-S, which is closer in size, ET-Flow achieves better Precision while the impact on Recall is less significant and limited to Recall Coverage. Notably, our Recall AMR remains competitive with much bigger MCF-B, underscoring the inherent advantage of our method in accurately predicting overall structures. 3.3. Ensemble Properties RMSD provides a geometric measure for assessing ensem- ble quality, but it is also e",0
40d97758e200a762a600d7d1a2233ee30cb179e7,A generative foundation model for antibody sequence understanding,[],https://openreview.net/pdf/40d97758e200a762a600d7d1a2233ee30cb179e7.pdf,"A generative foundation model for antibody sequence understanding Here we introduce FAbCon, a generative antibody-specific language model comprising 2.4 billion parameters. A commonly accepted wisdom in developing large language models is that increasing model scale will translate to higher performance on downstream tasks. Starting from a 144-million parameter setup, we show that progressively larger models achieve greater accuracy in predicting antigen binding and can also be used to design new antibodies with good predicted developability potential.",40d97758e200a762a600d7d1a2233ee30cb179e7.pdf,"Methods). After pre-
training, FAbCon develops a rich representation of antibody
sequences that can be leveraged for an extensive range of
tasks; here, we focus on antigen binding prediction and
antibody design. FAbCon-small (144 million parameters),
FAbCon-medium (297 million parameters) and FAbCon-
large (2.4 billion parameters) are available in our Hugging
Face repository. Each variant can accept heavy-chain only,
light-chain only, or paired chain inputs.
2. Methods
2.1. Datasets
FAbCon was pre-trained using CLM on an antibody-specific
corpus, comprising both paired (i.e. heavy and light chains
A generative foundation model for antibody sequence understanding
Figure 1. FAbCon is a generative, antibody-specific LLM, capable of sequence understanding. A. FAbCon is pre-trained by causal
language modelling (CLM). FAbCon learns to predict the next amino acid using only the preceding N-terminal residues. By default,
FAbCon generates paired antibody sequences. B. FAbCon is a foundation model that can be fine-tuned for library generation or few-shot
antibody-antigen binding prediction. After fine-tuning for antibody-antigen binding, the model can act as an ‘oracle’ to screen libraries
generated in vivo, in vitro, or in silico by a generative FAbCon model. Screened variants can then supplement binding datasets to ‘close
the loop’ for antibody engineering. C. Area under the precision recall (AUPR) score of LLMs on HER2 and SARS-CoV-2 binding
prediction versus the log of the number of model parameters. For IgT5, only the encoder module is publicly available, and we plot
the number of IgT5’s encoder parameters here. Antibody-specific LLMs are marked in pink. Error bars are standard deviations across
five random seeds. Dotted line represents a line of best fit between the log of the number of parameters versus AUPR. D. HER2 and
SARS-CoV-2 binding prediction AUPR as a function of training set sample size. Shaded areas represent standard deviations from five
different seeds for","future work, we envisage
controlled generation to create better lead molecules.
Acknowledgements
The authors would like to thank Bruno Trentini, Has-
san Sirelkhatim, Christian Dallago, Maxine Kennedy, Lee
Carter, and the wider NVIDIA team for their support. The
authors would also like to thank Will Shaw and Camillo
Anania from AWS for their support.
Contributions
J.B. and J.L. conceived and designed the experiments. J.B.
and J.L. performed the experiments. J.B., A.G. and J.L.
processed the datasets for the study. J.H.R.F. and H.D.
helped set up cloud and security infrastructure. D.A.Y .,
D.H.M., C.P., S.V .T. prepared the samples for single-cell
sequencing of Alchemab’s proprietary data. D.A.Y . and
S.V .T. performed the single-cell sequencing. J.D., F.N.,
O.S., S.V .T. prepared and sequenced the samples for bulk
sequencing of Alchemab’s proprietary data. J.B., J.L., J.D.G.
analysed the results. J.B., J.L., J.D.G. wrote the manuscript.
All authors reviewed the manuscript.
A generative foundation model for antibody sequence understanding
References
10x Genomics. Human PBMC from a Healthy Donor, 10k
cells (v2). https://tinyurl.com/y2e9zmcu ,
2020a.
10x Genomics. Splenocytes from C57BL/6 mice, 1k cells
(v2). https://tinyurl.com/mr23zpv6 , 2020b.
Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A.,
Cojocaru, R., Debbah, M., ´Etienne Goffinet, Hesslow,
D., Launay, J., Malartic, Q., Mazzotta, D., Noune, B.,
Pannier, B., and Penedo, G. The Falcon Series of Open
Language Models. arXiv , 2023.
Bachas, S., Rakocevic, G., Spencer, D., Sastry, A. V ., Haile,
R., Sutton, J. M., Kasun, G., Stachyra, A., Gutierrez,
J. M., Yassine, E., Medjo, B., Blay, V ., Kohnert, C.,
Stanton, J. T., Brown, A., Tijanic, N., McCloskey, C.,
Viazzo, R., Consbruck, R., Carter, H., Levine, S., Abdul-
haqq, S., Shaul, J., Ventura, A. B., Olson, R. S., Yapici,
E., Meier, J., McClain, S., Weinstock, M., Hannum, G.,
Schwartz, A., Gander, M., and Spreafico, R. Antibody
optimization enabled by artifi","Methods). After pre- training, FAbCon develops a rich representation of antibody sequences that can be leveraged for an extensive range of tasks; here, we focus on antigen binding prediction and antibody design. FAbCon-small (144 million parameters), FAbCon-medium (297 million parameters) and FAbCon- large (2.4 billion parameters) are available in our Hugging Face repository. Each variant can accept heavy-chain only, light-chain only, or paired chain inputs. 2. Methods 2.1. Datasets FAbCon was pre-trained using CLM on an antibody-specific corpus, comprising both paired (i.e. heavy and light chains A generative foundation model for antibody sequence understanding Figure 1. FAbCon is a generative, antibody-specific LLM, capable of sequence understanding. A. FAbCon is pre-trained by causal language modelling (CLM). FAbCon learns to predict the next amino acid using only the preceding N-terminal residues. By default, FAbCon generates paired antibody sequences. B. FAbCon is a foundation model that can be fine-tuned for library generation or few-shot antibody-antigen binding prediction. After fine-tuning for antibody-antigen binding, the model can act as an oracle to screen libraries generated in vivo, in vitro, or in silico by a generative FAbCon model. Screened variants can then supplement binding datasets to close the loop for antibody engineering. C. Area under the precision recall (AUPR) score of LLMs on HER2 and SARS-CoV-2 binding prediction versus the log of the number of model parameters. For IgT5, only the encoder module is publicly available, and we plot the number of IgT5 s encoder parameters here. Antibody-specific LLMs are marked in pink. Error bars are standard deviations across five random seeds. Dotted line represents a line of best fit between the log of the number of parameters versus AUPR. D. HER2 and SARS-CoV-2 binding prediction AUPR as a function of training set sample size. Shaded areas represent standard deviations from five different seeds for","future work, we envisage controlled generation to create better lead molecules.","A generative foundation model for antibody sequence understanding Here we introduce FAbCon, a generative antibody-specific language model comprising 2.4 billion parameters. A commonly accepted wisdom in developing large language models is that increasing model scale will translate to higher performance on downstream tasks. Starting from a 144-million parameter setup, we show that progressively larger models achieve greater accuracy in predicting antigen binding and can also be used to design new antibodies with good predicted developability potential.","A generative foundation model for antibody sequence understanding Here we introduce FAbCon, a generative antibody-specific language model comprising 2.4 billion parameters. A commonly accepted wisdom in developing large language models is that increasing model scale will translate to higher performance on downstream tasks. Starting from a 144-million parameter setup, we show that progressively larger models achieve greater accuracy in predicting antigen binding and can also be used to design new antibodies with good predicted developability potential. future work, we envisage controlled generation to create better lead molecules. Methods). After pre- training, FAbCon develops a rich representation of antibody sequences that can be leveraged for an extensive range of tasks; here, we focus on antigen binding prediction and antibody design. FAbCon-small (144 million parameters), FAbCon-medium (297 million parameters) and FAbCon- large (2.4 billion parameters) are available in our Hugging Face repository. Each variant can accept heavy-chain only, light-chain only, or paired chain inputs. 2. Methods 2.1. Datasets FAbCon was pre-trained using CLM on an antibody-specific corpus, comprising both paired (i.e. heavy and light chains A generative foundation model for antibody sequence understanding Figure 1. FAbCon is a generative, antibody-specific LLM, capable of sequence understanding. A. FAbCon is pre-trained by causal language modelling (CLM). FAbCon learns to predict the next amino acid using only the preceding N-terminal residues. By default, FAbCon generates paired antibody sequences. B. FAbCon is a foundation model that can be fine-tuned for library generation or few-shot antibody-antigen binding prediction. After fine-tuning for antibody-antigen binding, the model can act as an oracle to screen libraries generated in vivo, in vitro, or in silico by a generative FAbCon model. Screened variants can then supplement binding datasets to close the loop for antibody engineering. C. Area under the precision recall (AUPR) score of LLMs on HER2 and SARS-CoV-2 binding prediction versus the log of the number of model parameters. For IgT5, only the encoder module is publicly available, and we plot the number of IgT5 s encoder parameters here. Antibody-specific LLMs are marked in pink. Error bars are standard deviations across five random seeds. Dotted line represents a line of best fit between the log of the number of parameters versus AUPR. D. HER2 and SARS-CoV-2 binding prediction AUPR as a function of training set sample size. Shaded areas represent standard deviations from five different seeds for",0
f8bc0ace232c735192259521a6074bf07bad0c27,Improving Fragment-Based Deep Molecular Generative Models,"['Panukorn Taleongpong', 'Brooks Paige']",https://openreview.net/pdf/f8bc0ace232c735192259521a6074bf07bad0c27.pdf,"Improving Fragment-Based Deep Molecular Generative Models Novel Molecule fragmentation algorithm; works great for use in VAEs Deep molecular generative models have shown promising results and paved a new way for drug discovery. Their ability to explore the molecular space, estimated to be 1060, is significantly greater than traditional methods used for the virtual screening of existing databases. We introduce a novel fragmentation algorithm particularly suitable for use in deep generative models. In contrast to existing fragmentation algorithms, our procedure sequentially breaks a molecule along BRIC bonds in such a manner that the linearization of fragments is directly invertible, guaranteed to be able to reconstruct the original molecule from the fragment sequence. This makes it appropriate for use in deep generative models trained with sequential models as likelihoods. We compare with previous fragment-based SMILES VAE methods and observe that our approach significantly enhances coverage of the molecular space and outperforms on distribution learning benchmarks.",f8bc0ace232c735192259521a6074bf07bad0c27.pdf,"Methodology
We improve upon the methodology developed by Podda
et al. (2020) by introducing a more efficient fragmentation
algorithm and using richer molecular embeddings. We refer
to the model and methods by Podda et al. (2020) as the
Improving Fragment-Based Deep Molecular Generative Models
Figure 1: Example of fragments derived from a molecule
using the BRICS algorithm. The asterisks represent dummy
atoms, and the red dotted lines denote the ‘cuts’ in the
molecule.
benchmark , with all experiments in this research undertaken
on the ZINC dataset (Irwin et al., 2012).
3.1. Fragmentation
Our fragmentation process uses the same procedure
as that performed by Podda et al. (2020) to identify
breakable bonds: the Breaking of Retrosynthetically
Interesting Chemical Substructure (BRICS) algorithm
(Degen et al., 2008). The BRICS algorithm iden-
tifies strategic bonds based on medicinal chemistry
concepts and performs retrosynthetic cuts simultane-
ously. Figure 1 shows an example of a molecule given
by its SMILES representation ‘CCCN(CCc1cccc(-
c2ccccc2)c1)C(=O)C1OC(C(=O)O)=CC(N)C1NC(C)=O’
broken down into fragments using the BRICS algorithm.
These fragments consist of the following ‘*c1cccc(*)c1’,
‘*CCC’, ‘*C(C)=O’, ‘*C1OC(C(=O)O)=CC(N)C1*’,
‘*CC*’, ‘*N(*)*’, ‘*N*’, ‘*C(*)=O’ and ‘*c1ccccc1’.
For our research, and based on the work of (Bowman
et al., 2016), we view fragments as words that make up the
molecule, which is the sentence. Hence, the fragmentation
process must be more constrained to ensure that the
fragments can be sequentially represented and that the
original molecule can be reconstructed. Algorithm 1
details the pseudocode for our breadth-first fragmentation
algorithm. Equivalent to the benchmark algorithm, our
algorithm scans the original molecule in the order imposed
by the canonicalised SMILES representation. When a
BRIC bond is identified, the molecule is cut at that point
with ‘dummy’ atoms attached to the end of the cleavage
sites (Podda et al., 2020).","Conclusion
Our results demonstrate advancements in SMILES-based
fragment-based drug discovery models for distribution learn-
ing in the following ways:
1.We propose a novel fragmentation and fragment re-
construction algorithm that produces fragments much
more efficiently whilst guaranteeing the reconstruction
of the original molecule.
2.Our model demonstrates superior performance met-
rics of the generated molecules by simply applying a
factor to the sampling variance rather than using low-
frequency masking.
3.Utilisation of fingerprint-based Mol2Vec fragment en-
coding improves distribution learning results and paves
the way for SMILES-based fragment-based property
prediction and molecular optimisation.
The next steps to extend this model include extending from
a distribution-learning model to a goal-directed model, and
investigating different methods to prevent posterior collapse,
such as average and max pooling (Long et al., 2020).
Improving Fragment-Based Deep Molecular Generative Models
Software and Data
For reproducibility, we publicly release
the code repository for this project at
https://github.com/panukorn17/DEFRAGMO.
References
Awale, M. and Reymond, J.-L. Polypharmacology Browser
PPB2: Target Prediction Combining Nearest Neighbors
with Machine Learning. Journal of Chemical Information
and Modeling , 59(1):10–17, January 2019. ISSN 1549-
9596. doi: 10.1021/acs.jcim.8b00524. URL https://
doi.org/10.1021/acs.jcim.8b00524 . Pub-
lisher: American Chemical Society.
Bohacek, R. S., McMartin, C., and Guida, W. C. The art and
practice of structure-based drug design: a molecular mod-
eling perspective. Medicinal Research Reviews , 16(1):3–
50, January 1996. ISSN 0198-6325. doi: 10.1002/(SICI)
1098-1128(199601)16:1 ⟨3::AID-MED1 ⟩3.0.CO;2-6.
Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A., Jozefowicz,
R., and Bengio, S. Generating Sentences from a Continu-
ous Space. In Proceedings of the 20th SIGNLL Confer-
ence on Computational Natural Language Learning , pp.
10","Methodology We improve upon the methodology developed by Podda et al. (2020) by introducing a more efficient fragmentation algorithm and using richer molecular embeddings. We refer to the model and methods by Podda et al. (2020) as the Improving Fragment-Based Deep Molecular Generative Models Figure 1: Example of fragments derived from a molecule using the BRICS algorithm. The asterisks represent dummy atoms, and the red dotted lines denote the cuts in the molecule. benchmark , with all experiments in this research undertaken on the ZINC dataset (Irwin et al., 2012). 3.1. Fragmentation Our fragmentation process uses the same procedure as that performed by Podda et al. (2020) to identify breakable bonds: the Breaking of Retrosynthetically Interesting Chemical Substructure (BRICS) algorithm (Degen et al., 2008). The BRICS algorithm iden- tifies strategic bonds based on medicinal chemistry concepts and performs retrosynthetic cuts simultane- ously. Figure 1 shows an example of a molecule given by its SMILES representation CCCN(CCc1cccc(- c2ccccc2)c1)C(=O)C1OC(C(=O)O)=CC(N)C1NC(C)=O broken down into fragments using the BRICS algorithm. These fragments consist of the following *c1cccc(*)c1 , *CCC , *C(C)=O , *C1OC(C(=O)O)=CC(N)C1* , *CC* , *N(*)* , *N* , *C(*)=O and *c1ccccc1 . For our research, and based on the work of (Bowman et al., 2016), we view fragments as words that make up the molecule, which is the sentence. Hence, the fragmentation process must be more constrained to ensure that the fragments can be sequentially represented and that the original molecule can be reconstructed. Algorithm 1 details the pseudocode for our breadth-first fragmentation algorithm. Equivalent to the benchmark algorithm, our algorithm scans the original molecule in the order imposed by the canonicalised SMILES representation. When a BRIC bond is identified, the molecule is cut at that point with dummy atoms attached to the end of the cleavage sites (Podda et al., 2020).","Conclusion Our results demonstrate advancements in SMILES-based fragment-based drug discovery models for distribution learn- ing in the following ways: 1.We propose a novel fragmentation and fragment re- construction algorithm that produces fragments much more efficiently whilst guaranteeing the reconstruction of the original molecule. 2.Our model demonstrates superior performance met- rics of the generated molecules by simply applying a factor to the sampling variance rather than using low- frequency masking. 3.Utilisation of fingerprint-based Mol2Vec fragment en- coding improves distribution learning results and paves the way for SMILES-based fragment-based property prediction and molecular optimisation. The next steps to extend this model include extending from a distribution-learning model to a goal-directed model, and investigating different methods to prevent posterior collapse, such as average and max pooling (Long et al., 2020). Improving Fragment-Based Deep Molecular Generative Models Software and Data For reproducibility, we publicly release the code repository for this project at https://github.com/panukorn17/DEFRAGMO.","Improving Fragment-Based Deep Molecular Generative Models Novel Molecule fragmentation algorithm; works great for use in VAEs Deep molecular generative models have shown promising results and paved a new way for drug discovery. Their ability to explore the molecular space, estimated to be 1060, is significantly greater than traditional methods used for the virtual screening of existing databases. We introduce a novel fragmentation algorithm particularly suitable for use in deep generative models. In contrast to existing fragmentation algorithms, our procedure sequentially breaks a molecule along BRIC bonds in such a manner that the linearization of fragments is directly invertible, guaranteed to be able to reconstruct the original molecule from the fragment sequence. This makes it appropriate for use in deep generative models trained with sequential models as likelihoods. We compare with previous fragment-based SMILES VAE methods and observe that our approach significantly enhances coverage of the molecular space and outperforms on distribution learning benchmarks.","Improving Fragment-Based Deep Molecular Generative Models Novel Molecule fragmentation algorithm; works great for use in VAEs Deep molecular generative models have shown promising results and paved a new way for drug discovery. Their ability to explore the molecular space, estimated to be 1060, is significantly greater than traditional methods used for the virtual screening of existing databases. We introduce a novel fragmentation algorithm particularly suitable for use in deep generative models. In contrast to existing fragmentation algorithms, our procedure sequentially breaks a molecule along BRIC bonds in such a manner that the linearization of fragments is directly invertible, guaranteed to be able to reconstruct the original molecule from the fragment sequence. This makes it appropriate for use in deep generative models trained with sequential models as likelihoods. We compare with previous fragment-based SMILES VAE methods and observe that our approach significantly enhances coverage of the molecular space and outperforms on distribution learning benchmarks. Conclusion Our results demonstrate advancements in SMILES-based fragment-based drug discovery models for distribution learn- ing in the following ways: 1.We propose a novel fragmentation and fragment re- construction algorithm that produces fragments much more efficiently whilst guaranteeing the reconstruction of the original molecule. 2.Our model demonstrates superior performance met- rics of the generated molecules by simply applying a factor to the sampling variance rather than using low- frequency masking. 3.Utilisation of fingerprint-based Mol2Vec fragment en- coding improves distribution learning results and paves the way for SMILES-based fragment-based property prediction and molecular optimisation. The next steps to extend this model include extending from a distribution-learning model to a goal-directed model, and investigating different methods to prevent posterior collapse, such as average and max pooling (Long et al., 2020). Improving Fragment-Based Deep Molecular Generative Models Software and Data For reproducibility, we publicly release the code repository for this project at https://github.com/panukorn17/DEFRAGMO. Methodology We improve upon the methodology developed by Podda et al. (2020) by introducing a more efficient fragmentation algorithm and using richer molecular embeddings. We refer to the model and methods by Podda et al. (2020) as the Improving Fragment-Based Deep Molecular Generative Models Figure 1: Example of fragments derived from a molecule using the BRICS algorithm. The asterisks represent dummy atoms, and the red dotted lines denote the cuts in the molecule. benchmark , with all experiments in this research undertaken on the ZINC dataset (Irwin et al., 2012). 3.1. Fragmentation Our fragmentation process uses the same procedure as that performed by Podda et al. (2020) to identify breakable bonds: the Breaking of Retrosynthetically Interesting Chemical Substructure (BRICS) algorithm (Degen et al., 2008). The BRICS algorithm iden- tifies strategic bonds based on medicinal chemistry concepts and performs retrosynthetic cuts simultane- ously. Figure 1 shows an example of a molecule given by its SMILES representation CCCN(CCc1cccc(- c2ccccc2)c1)C(=O)C1OC(C(=O)O)=CC(N)C1NC(C)=O broken down into fragments using the BRICS algorithm. These fragments consist of the following *c1cccc(*)c1 , *CCC , *C(C)=O , *C1OC(C(=O)O)=CC(N)C1* , *CC* , *N(*)* , *N* , *C(*)=O and *c1ccccc1 . For our research, and based on the work of (Bowman et al., 2016), we view fragments as words that make up the molecule, which is the sentence. Hence, the fragmentation process must be more constrained to ensure that the fragments can be sequentially represented and that the original molecule can be reconstructed. Algorithm 1 details the pseudocode for our breadth-first fragmentation algorithm. Equivalent to the benchmark algorithm, our algorithm scans the original molecule in the order imposed by the canonicalised SMILES representation. When a BRIC bond is identified, the molecule is cut at that point with dummy atoms attached to the end of the cleavage sites (Podda et al., 2020).",0
132106c8f143e55bf5334c1f9ac7fe16f07ab109,Chemical Language Modeling with Structured State Spaces,"['Rıza Özçelik', 'Sarah de Ruiter', 'Emanuele Criscuolo', 'Francesca Grisoni']",https://openreview.net/pdf/132106c8f143e55bf5334c1f9ac7fe16f07ab109.pdf,"Chemical Language Modeling with Structured State Spaces First application of structured state space sequence models (S4s) to de novo design. Generative deep learning is reshaping drug design. Chemical language models (CLMs) which generate molecules in the form of molecular strings bear particular promise for this endeavor. Here, we introduce a recent deep learning architecture, termed Structured State-Space Sequence (S4) model, into de novo drug design. In addition to its unprecedented performance in various fields, S4 has shown remarkable capabilities to learn the global properties of sequences. This aspect is intriguing in chemical language modeling, where complex molecular properties like bioactivity can 'emerge' from separated portions in the molecular string. This observation gives rise to the following question: Can S4 advance chemical language modeling for de novo design? To provide an answer, we systematically benchmark S4 with state-of-the-art CLMs on an array of drug discovery tasks, such as the identification of bioactive compounds, and the design of drug-like molecules and natural products. S4 showed a superior capacity to learn complex molecular properties, while at the same time exploring diverse scaffolds. Finally, when applied prospectively to kinase inhibition, S4 designed eight of out ten molecules that were predicted as highly active by molecular dynamics simulations. Taken together, these findings advocate for the introduction of S4 into chemical language modeling uncovering its untapped potential in the molecular sciences.",132106c8f143e55bf5334c1f9ac7fe16f07ab109.pdf,"methodology and encoding
rules. Journal of chemical information and computer
sciences , 28(1):31–36, 1988.
Weiss, K., Khoshgoftaar, T. M., and Wang, D. A survey of
transfer learning. Journal of Big data , 3(1):1–40, 2016.
Weng, G., Zhao, H., Nie, D., Zhang, H., Liu, L., Hou,
T., and Kang, Y . Rediscmol: Benchmarking molecular
generation models in biological properties. Journal of
Medicinal Chemistry , 2024.
Wildman, S. A. and Crippen, G. M. Prediction of physic-
ochemical parameters by atomic contributions. Journal
of chemical information and computer sciences , 39(5):
868–873, 1999.
Woolson, R. F. Wilcoxon signed-rank test. Wiley encyclope-
dia of clinical trials , pp. 1–3, 2007.
Yang, L., Yang, G., Bing, Z., Tian, Y ., Niu, Y ., Huang,
L., and Yang, L. Transformer-based generative model
accelerating the development of novel braf inhibitors.
ACS omega , 6(49):33864–33873, 2021.
Yuan, W., Jiang, D., Nambiar, D. K., Liew, L. P., Hay, M. P.,
Bloomstein, J., Lu, P., Turner, B., Le, Q.-T., Tibshirani,
R., et al. Chemical space mimicry for drug discovery.
Journal of chemical information and modeling , 57(4):
875–882, 2017.","Conclusions
This study pioneered the introduction of structured state-
space sequence models (S4s) into chemical language mod-
eling. The unique dual nature of S4s, involving convolution
during training and recurrent generation, makes them partic-
ularly intriguing for de novo design with SMILES strings.
Our systematic analysis against GPT and LSTM on a variety
of drug discovery tasks revealed S4’s remarkable strengths:
while recurrent generation (LSTM and S4) is superior in
learning the chemical syntax and exploring diverse scaffolds,
learning holistically on the entire SMILES sequence (GPT
and S4) excels in capturing certain complex properties, like
bioactivity. S4 with its dual nature, makes ‘ the best of both
worlds ’: it demonstrated comparable or better performance
than LSTM in designing valid and diverse molecules, and
systematically outperformed both benchmarks in capturing
complex molecular properties. The application of S4 to
MAPK1 inhibition, validated by MD simulations, further
showcases its potential to design potent bioactive molecules.
Several aspects of S4 await to be explored in the molec-
ular sciences, such as its potential with longer sequences
(e.g., macrocyclic peptides and protein sequences) and on
additional molecular tasks ( e.g., organic reaction planning).
We envision the relevance of S4 for molecule discovery
to increase in the future, and to potentially replace widely
established chemical language models like LSTM and GPT.
Chemical Language Modeling with Structured State Spaces
Acknowledgements
The Python code and data to replicate and extend our
study are available on GitHub at the following URL:
https://github.com/molML/s4-for-de-novo-drug-design.
This research was co-funded by the European Union (ERC,
ReMINDER, 101077879). Views and opinions expressed
are however those of the author(s) only and do not neces-
sarily reflect those of the European Union or the European
Research Council. Neither the European Union nor the
granting authori","methodology and encoding rules. Journal of chemical information and computer sciences , 28(1):31 36, 1988. Weiss, K., Khoshgoftaar, T. M., and Wang, D. A survey of transfer learning. Journal of Big data , 3(1):1 40, 2016. Weng, G., Zhao, H., Nie, D., Zhang, H., Liu, L., Hou, T., and Kang, Y . Rediscmol: Benchmarking molecular generation models in biological properties. Journal of Medicinal Chemistry , 2024. Wildman, S. A. and Crippen, G. M. Prediction of physic- ochemical parameters by atomic contributions. Journal of chemical information and computer sciences , 39(5): 868 873, 1999. Woolson, R. F. Wilcoxon signed-rank test. Wiley encyclope- dia of clinical trials , pp. 1 3, 2007. Yang, L., Yang, G., Bing, Z., Tian, Y ., Niu, Y ., Huang, L., and Yang, L. Transformer-based generative model accelerating the development of novel braf inhibitors. ACS omega , 6(49):33864 33873, 2021. Yuan, W., Jiang, D., Nambiar, D. K., Liew, L. P., Hay, M. P., Bloomstein, J., Lu, P., Turner, B., Le, Q.-T., Tibshirani, R., et al. Chemical space mimicry for drug discovery. Journal of chemical information and modeling , 57(4): 875 882, 2017.","Conclusions This study pioneered the introduction of structured state- space sequence models (S4s) into chemical language mod- eling. The unique dual nature of S4s, involving convolution during training and recurrent generation, makes them partic- ularly intriguing for de novo design with SMILES strings. Our systematic analysis against GPT and LSTM on a variety of drug discovery tasks revealed S4 s remarkable strengths: while recurrent generation (LSTM and S4) is superior in learning the chemical syntax and exploring diverse scaffolds, learning holistically on the entire SMILES sequence (GPT and S4) excels in capturing certain complex properties, like bioactivity. S4 with its dual nature, makes the best of both worlds : it demonstrated comparable or better performance than LSTM in designing valid and diverse molecules, and systematically outperformed both benchmarks in capturing complex molecular properties. The application of S4 to MAPK1 inhibition, validated by MD simulations, further showcases its potential to design potent bioactive molecules. Several aspects of S4 await to be explored in the molec- ular sciences, such as its potential with longer sequences (e.g., macrocyclic peptides and protein sequences) and on additional molecular tasks ( e.g., organic reaction planning). We envision the relevance of S4 for molecule discovery to increase in the future, and to potentially replace widely established chemical language models like LSTM and GPT. Chemical Language Modeling with Structured State Spaces","Chemical Language Modeling with Structured State Spaces First application of structured state space sequence models (S4s) to de novo design. Generative deep learning is reshaping drug design. Chemical language models (CLMs) which generate molecules in the form of molecular strings bear particular promise for this endeavor. Here, we introduce a recent deep learning architecture, termed Structured State-Space Sequence (S4) model, into de novo drug design. In addition to its unprecedented performance in various fields, S4 has shown remarkable capabilities to learn the global properties of sequences. This aspect is intriguing in chemical language modeling, where complex molecular properties like bioactivity can 'emerge' from separated portions in the molecular string. This observation gives rise to the following question: Can S4 advance chemical language modeling for de novo design? To provide an answer, we systematically benchmark S4 with state-of-the-art CLMs on an array of drug discovery tasks, such as the identification of bioactive compounds, and the design of drug-like molecules and natural products. S4 showed a superior capacity to learn complex molecular properties, while at the same time exploring diverse scaffolds. Finally, when applied prospectively to kinase inhibition, S4 designed eight of out ten molecules that were predicted as highly active by molecular dynamics simulations. Taken together, these findings advocate for the introduction of S4 into chemical language modeling uncovering its untapped potential in the molecular sciences.","Chemical Language Modeling with Structured State Spaces First application of structured state space sequence models (S4s) to de novo design. Generative deep learning is reshaping drug design. Chemical language models (CLMs) which generate molecules in the form of molecular strings bear particular promise for this endeavor. Here, we introduce a recent deep learning architecture, termed Structured State-Space Sequence (S4) model, into de novo drug design. In addition to its unprecedented performance in various fields, S4 has shown remarkable capabilities to learn the global properties of sequences. This aspect is intriguing in chemical language modeling, where complex molecular properties like bioactivity can 'emerge' from separated portions in the molecular string. This observation gives rise to the following question: Can S4 advance chemical language modeling for de novo design? To provide an answer, we systematically benchmark S4 with state-of-the-art CLMs on an array of drug discovery tasks, such as the identification of bioactive compounds, and the design of drug-like molecules and natural products. S4 showed a superior capacity to learn complex molecular properties, while at the same time exploring diverse scaffolds. Finally, when applied prospectively to kinase inhibition, S4 designed eight of out ten molecules that were predicted as highly active by molecular dynamics simulations. Taken together, these findings advocate for the introduction of S4 into chemical language modeling uncovering its untapped potential in the molecular sciences. Conclusions This study pioneered the introduction of structured state- space sequence models (S4s) into chemical language mod- eling. The unique dual nature of S4s, involving convolution during training and recurrent generation, makes them partic- ularly intriguing for de novo design with SMILES strings. Our systematic analysis against GPT and LSTM on a variety of drug discovery tasks revealed S4 s remarkable strengths: while recurrent generation (LSTM and S4) is superior in learning the chemical syntax and exploring diverse scaffolds, learning holistically on the entire SMILES sequence (GPT and S4) excels in capturing certain complex properties, like bioactivity. S4 with its dual nature, makes the best of both worlds : it demonstrated comparable or better performance than LSTM in designing valid and diverse molecules, and systematically outperformed both benchmarks in capturing complex molecular properties. The application of S4 to MAPK1 inhibition, validated by MD simulations, further showcases its potential to design potent bioactive molecules. Several aspects of S4 await to be explored in the molec- ular sciences, such as its potential with longer sequences (e.g., macrocyclic peptides and protein sequences) and on additional molecular tasks ( e.g., organic reaction planning). We envision the relevance of S4 for molecule discovery to increase in the future, and to potentially replace widely established chemical language models like LSTM and GPT. Chemical Language Modeling with Structured State Spaces methodology and encoding rules. Journal of chemical information and computer sciences , 28(1):31 36, 1988. Weiss, K., Khoshgoftaar, T. M., and Wang, D. A survey of transfer learning. Journal of Big data , 3(1):1 40, 2016. Weng, G., Zhao, H., Nie, D., Zhang, H., Liu, L., Hou, T., and Kang, Y . Rediscmol: Benchmarking molecular generation models in biological properties. Journal of Medicinal Chemistry , 2024. Wildman, S. A. and Crippen, G. M. Prediction of physic- ochemical parameters by atomic contributions. Journal of chemical information and computer sciences , 39(5): 868 873, 1999. Woolson, R. F. Wilcoxon signed-rank test. Wiley encyclope- dia of clinical trials , pp. 1 3, 2007. Yang, L., Yang, G., Bing, Z., Tian, Y ., Niu, Y ., Huang, L., and Yang, L. Transformer-based generative model accelerating the development of novel braf inhibitors. ACS omega , 6(49):33864 33873, 2021. Yuan, W., Jiang, D., Nambiar, D. K., Liew, L. P., Hay, M. P., Bloomstein, J., Lu, P., Turner, B., Le, Q.-T., Tibshirani, R., et al. Chemical space mimicry for drug discovery. Journal of chemical information and modeling , 57(4): 875 882, 2017.",0
2fdd2a925ef8a06cae0a2d977c6c95f7f093492f,Closed-Form Test Functions for Biophysical Sequence Optimization Algorithms,"['Samuel Don Stanton', 'Robert G Alberstein', 'Nathan C. Frey', 'Andrew Martin Watkins', 'Kyunghyun Cho']",https://openreview.net/pdf/2fdd2a925ef8a06cae0a2d977c6c95f7f093492f.pdf,"Closed-Form Test Functions for Biophysical Sequence Optimization Algorithms We propose a new closed-form test function for black-box sequence optimization with geometric similarities to real protein design problems There is a growing body of work seeking to replicate the success of machine learning (ML) on domains like computer vision (CV) and natural language processing (NLP) to applications involving biophysical data. One of the key ingredients of prior successes in CV and NLP was the broad acceptance of difficult benchmarks that distilled key subproblems into approachable tasks that any junior researcher could investigate, but good benchmarks for biophysical domains are rare. This scarcity is partially due to a narrow focus on benchmarks which simulate biophysical data; we propose instead to carefully abstract biophysical problems into simpler ones with key geometric similarities. In particular we propose a new class of closed-form test functions for biophysical sequence optimization, which we call Ehrlich functions. We provide empirical results demonstrating these functions are interesting objects of study and can be non-trivial to solve with a standard genetic optimization baseline.",2fdd2a925ef8a06cae0a2d977c6c95f7f093492f.pdf,"approachable tasks
that any junior researcher could investigate, but
good benchmarks for biophysical domains are
rare. This scarcity is partially due to a narrow
focus on benchmarks which simulate biophysi-
cal data; we propose instead to carefully abstract
biophysical problems into simpler ones with key
geometric similarities. In particular we propose
a new class of closed-form test functions for bio-
physical sequence optimization, which we call
Ehrlich functions . We provide empirical results
demonstrating these functions are interesting ob-
jects of study and can be non-trivial to solve with
a standard genetic optimization baseline.
1. Introduction
Rigorous benchmarking is an essential element of good
practice in science and engineering. Good benchmarks al-
low developers to evaluate new ideas rapidly in a low-stakes
environment and thoroughly understand the strengths and
weaknesses of their methods before applying them in costly,
consequential settings. To see the benefit of a good bench-
mark, we need look no further than the Critical Assessment
of Structure Prediction (CASP) competition (Bourne, 2003),
which motivated AlphaFold (Jumper et al., 2021), or the
many benchmarks in CV and NLP that shaped the develop-
ment of modern deep learning (Russakovsky et al., 2015;
Bojar et al., 2016; Hendrycks et al., 2020). While there
has been a surge of investment into ML algorithms for ap-
1Prescient Design, Genentech, New York City, USA2Prescient
Design, Genentech, San Francisco, USA. Correspondence to:
Samuel Stanton <stanton.samuel@gene.com >.
Accepted at the 1st Machine Learning for Life and Material Sci-
ences Workshop at ICML 2024 . Copyright 2024 by the author(s).
−40−30−20−10010203040x2
−40−30−20−10010203040
x10510152025
f(x)Fig. 1: The Ackley function is widely used to evaluate
black-box optimization algorithms such as Bayesian opti-
mization that have been successfully applied to many real-
world problems. The relevance of the Ackley function is
not its semant","future work we intend to use this benchmark to thor-
oughly evaluate generative ML algorithms end to end for
sequence optimization. Intuitively we can expect that this
benchmark will help illuminate one of the key benefits of
generative search, namely the ability to search deeper into
sequence space in each iteration without sacrificing feasi-
bility, a key advantage over uniform random search. We
also hope our contribution will encourage other researchers
to consider how they might distill their application into
simple abstracted problems that can be easily studied by
the broader research community, building more common
ground of rigorous, easily reproducible empirical results.
Closed-Form Test Functions for Biophysical Sequence Optimization Algorithms
Acknowledgements
The authors thank Matthieu Kirchmeyer, Sidney Lisanza,
and Clara Wong-Fannjiang for helpful feedback and discus-
sion.
References
Angermueller, C., Belanger, D., Gane, A., Mariet, Z.,
Dohan, D., Murphy, K., Colwell, L., and Sculley, D.
Population-based black-box optimization for biological
sequence design. In International conference on machine
learning , pp. 324–334. PMLR, 2020.
Back, T. Evolutionary algorithms in theory and practice:
evolution strategies, evolutionary programming, genetic
algorithms . Oxford university press, 1996.
Balandat, M., Karrer, B., Jiang, D., Daulton, S., Letham, B.,
Wilson, A. G., and Bakshy, E. Botorch: A framework for
efficient monte-carlo bayesian optimization. Advances in
neural information processing systems , 33:21524–21538,
2020.
Barlow, K. A., O Conchuir, S., Thompson, S., Suresh, P.,
Lucas, J. E., Heinonen, M., and Kortemme, T. Flex ddg:
Rosetta ensemble-based estimation of changes in protein–
protein binding affinity upon mutation. The Journal of
Physical Chemistry B , 122(21):5389–5399, 2018.
Barrera, L. A., Vedenko, A., Kurland, J. V ., Rogers, J. M.,
Gisselbrecht, S. S., Rossin, E. J., Woodard, J., Mariani,
L., Kock, K. H., Inukai, S., et al. Survey of variat","approachable tasks that any junior researcher could investigate, but good benchmarks for biophysical domains are rare. This scarcity is partially due to a narrow focus on benchmarks which simulate biophysi- cal data; we propose instead to carefully abstract biophysical problems into simpler ones with key geometric similarities. In particular we propose a new class of closed-form test functions for bio- physical sequence optimization, which we call Ehrlich functions . We provide empirical results demonstrating these functions are interesting ob- jects of study and can be non-trivial to solve with a standard genetic optimization baseline. 1. Introduction Rigorous benchmarking is an essential element of good practice in science and engineering. Good benchmarks al- low developers to evaluate new ideas rapidly in a low-stakes environment and thoroughly understand the strengths and weaknesses of their methods before applying them in costly, consequential settings. To see the benefit of a good bench- mark, we need look no further than the Critical Assessment of Structure Prediction (CASP) competition (Bourne, 2003), which motivated AlphaFold (Jumper et al., 2021), or the many benchmarks in CV and NLP that shaped the develop- ment of modern deep learning (Russakovsky et al., 2015; Bojar et al., 2016; Hendrycks et al., 2020). While there has been a surge of investment into ML algorithms for ap- 1Prescient Design, Genentech, New York City, USA2Prescient Design, Genentech, San Francisco, USA. Correspondence to: Samuel Stanton <stanton.samuel@gene.com >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s). 40 30 20 10010203040x2 40 30 20 10010203040 x10510152025 f(x)Fig. 1: The Ackley function is widely used to evaluate black-box optimization algorithms such as Bayesian opti- mization that have been successfully applied to many real- world problems. The relevance of the Ackley function is not its semant","future work we intend to use this benchmark to thor- oughly evaluate generative ML algorithms end to end for sequence optimization. Intuitively we can expect that this benchmark will help illuminate one of the key benefits of generative search, namely the ability to search deeper into sequence space in each iteration without sacrificing feasi- bility, a key advantage over uniform random search. We also hope our contribution will encourage other researchers to consider how they might distill their application into simple abstracted problems that can be easily studied by the broader research community, building more common ground of rigorous, easily reproducible empirical results. Closed-Form Test Functions for Biophysical Sequence Optimization Algorithms","Closed-Form Test Functions for Biophysical Sequence Optimization Algorithms We propose a new closed-form test function for black-box sequence optimization with geometric similarities to real protein design problems There is a growing body of work seeking to replicate the success of machine learning (ML) on domains like computer vision (CV) and natural language processing (NLP) to applications involving biophysical data. One of the key ingredients of prior successes in CV and NLP was the broad acceptance of difficult benchmarks that distilled key subproblems into approachable tasks that any junior researcher could investigate, but good benchmarks for biophysical domains are rare. This scarcity is partially due to a narrow focus on benchmarks which simulate biophysical data; we propose instead to carefully abstract biophysical problems into simpler ones with key geometric similarities. In particular we propose a new class of closed-form test functions for biophysical sequence optimization, which we call Ehrlich functions. We provide empirical results demonstrating these functions are interesting objects of study and can be non-trivial to solve with a standard genetic optimization baseline.","Closed-Form Test Functions for Biophysical Sequence Optimization Algorithms We propose a new closed-form test function for black-box sequence optimization with geometric similarities to real protein design problems There is a growing body of work seeking to replicate the success of machine learning (ML) on domains like computer vision (CV) and natural language processing (NLP) to applications involving biophysical data. One of the key ingredients of prior successes in CV and NLP was the broad acceptance of difficult benchmarks that distilled key subproblems into approachable tasks that any junior researcher could investigate, but good benchmarks for biophysical domains are rare. This scarcity is partially due to a narrow focus on benchmarks which simulate biophysical data; we propose instead to carefully abstract biophysical problems into simpler ones with key geometric similarities. In particular we propose a new class of closed-form test functions for biophysical sequence optimization, which we call Ehrlich functions. We provide empirical results demonstrating these functions are interesting objects of study and can be non-trivial to solve with a standard genetic optimization baseline. future work we intend to use this benchmark to thor- oughly evaluate generative ML algorithms end to end for sequence optimization. Intuitively we can expect that this benchmark will help illuminate one of the key benefits of generative search, namely the ability to search deeper into sequence space in each iteration without sacrificing feasi- bility, a key advantage over uniform random search. We also hope our contribution will encourage other researchers to consider how they might distill their application into simple abstracted problems that can be easily studied by the broader research community, building more common ground of rigorous, easily reproducible empirical results. Closed-Form Test Functions for Biophysical Sequence Optimization Algorithms approachable tasks that any junior researcher could investigate, but good benchmarks for biophysical domains are rare. This scarcity is partially due to a narrow focus on benchmarks which simulate biophysi- cal data; we propose instead to carefully abstract biophysical problems into simpler ones with key geometric similarities. In particular we propose a new class of closed-form test functions for bio- physical sequence optimization, which we call Ehrlich functions . We provide empirical results demonstrating these functions are interesting ob- jects of study and can be non-trivial to solve with a standard genetic optimization baseline. 1. Introduction Rigorous benchmarking is an essential element of good practice in science and engineering. Good benchmarks al- low developers to evaluate new ideas rapidly in a low-stakes environment and thoroughly understand the strengths and weaknesses of their methods before applying them in costly, consequential settings. To see the benefit of a good bench- mark, we need look no further than the Critical Assessment of Structure Prediction (CASP) competition (Bourne, 2003), which motivated AlphaFold (Jumper et al., 2021), or the many benchmarks in CV and NLP that shaped the develop- ment of modern deep learning (Russakovsky et al., 2015; Bojar et al., 2016; Hendrycks et al., 2020). While there has been a surge of investment into ML algorithms for ap- 1Prescient Design, Genentech, New York City, USA2Prescient Design, Genentech, San Francisco, USA. Correspondence to: Samuel Stanton <stanton.samuel@gene.com >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s). 40 30 20 10010203040x2 40 30 20 10010203040 x10510152025 f(x)Fig. 1: The Ackley function is widely used to evaluate black-box optimization algorithms such as Bayesian opti- mization that have been successfully applied to many real- world problems. The relevance of the Ackley function is not its semant",0
3cb4aa8d6633f76486002b32ff3c2fb113d95457,Machine learning nominal max oxygen consumption from wearable reflective pulse oximetry with density functional theory,"['Saleem Abdul Fattah Ahmed Al Dajani', 'Frédéric Laquai']",https://openreview.net/pdf/3cb4aa8d6633f76486002b32ff3c2fb113d95457.pdf,"Machine learning nominal max oxygen consumption from wearable reflective pulse oximetry with density functional theory We present a novel method for AI-driven precision medicine to infer max oxygen consumption rate based on random forest regression of wearable sensor physiological measurements using inputs from human hemoglobin density functional theory models. Wearable sensors have revolutionized modern health towards precision medicine. Many measurements taken by wearables are not currently used for medical purposes, and their physical roots are not understood. Here we show the origin of inferring oxygen saturation from measuring estimated oxygen variation while preparing for a marathon via spectrophotometric pulsatile blood flow transcutaneous reflectance oximetry, building six years of data tracking the stages of running a marathon. Based on inputs from quantum mechanical simulations into classical wave and electromagnetic theory models, the imaginary part of the complex dielectric function of the active site of human hemoglobin is altered upon oxygen binding to the heme group, changing the extinction coefficient and selectively absorbing wavelengths of light in a way that enables its detection from the ratio of red to infrared absorbance. A fundamental nominal max oxygen consumption is inferred from quantum mechanical absorbance spectra with and without oxygen binding by training a machine learning model on 1.5 years of daily wearable reflective pulse oximetry data with an of 0.84. Reflectance oximetry is strongly correlated ( of 0.96) to standard pulse oximetry, such as through earlobe or fingertip, enabling accurate, rapid, regular, and automated monitoring of vital signs with wearable sensors on smart watches and fitness trackers, and supplying artificial intelligence inferences of function-symptom relationships for precision medicine.",3cb4aa8d6633f76486002b32ff3c2fb113d95457.pdf,"approach to density functional theory
(DFT) involves solving a set of non-interacting particles
that mimic the behavior of real, interacting particles. The
total energy functional in Kohn-Sham DFT is given by the
application of the Kohn-Sham Hamiltonian to the ground
state electron density (Schwingenschl ¨ogl, 2004):
E[ρ0] =Ts[ρ0] +Z
Vext(r)ρ0(r)dr (3)
+1
2Z Zρ0(r)ρ0(r′)
|r−r′|drdr′+Exc[ρ]where Ts[ρ]is the kinetic energy of the non-interacting ref-
erence system, Vext(r)is the external potential energy, the
third term represents the classical Coulomb interaction of
the electron density, Exc[ρ]is the exchange-correlation en-
ergy functional. To minimize the energy functional with
respect to the density ρ, under the constraint of normal-
ized wavefunctions, we introduce the Kohn-Sham orbitals
ψi(r)where ρ(r) =PN
i=1|ψi(r)|2. The variation of the
energy functional with respect to ψ∗
i(r)leads to the Kohn-
Sham equations by Euler-Lagrange formulation (Schwin-
genschl ¨ogl, 2004):

−¯h2
2m∇2+Veff(r)
ψi(r) =ϵiψi(r) (4)
where ϵiis the Lagrange parameter approximation of the
energy eigenvalue associated with the state ψi, andVeff(r)is
the effective potential given by (Schwingenschl ¨ogl, 2004):
Veff(r) =Vext(r) +Zρ(r′)
|r−r′|dr′+Vxc(r) (5)
andVxc(r)is the functional derivative of the exchange-
correlation energy Exc[ρ]with respect to the density,δExc
δρ.
The ground state electron density ρ0(r)is calculated by
summing the square of the absolute value of each occupied
Kohn-Sham orbital, where Nis the number of electrons (or
occupied states) (Schwingenschl ¨ogl, 2004):
ρ0(r) =NX
i=1|ψi(r)|2(6)
Figure 2. Time-series data for (top) reflective pulse oximetry and
(bottom) max oxygen consumption rate for 1.5 years while prepar-
ing for marathon in March 2022 for nominal conditions prior.
Machine learning nominal max oxygen consumption from wearable reflective pulse oximetry with density functional theory
Figure 3. Scatter of raw data of absorbance ratio versus max oxy-
gen consum","Future work could extend the approach to other biological
systems and signals, such as glucose and insulin measure-
ments. Similar simplified DFT models for active sites of
biomolecules could be constructed, and wearable sensor
physiological measurements could be calculated and used
for inferences of nominal quantities of the physiological
function of interest.
Machine learning nominal max oxygen consumption from wearable reflective pulse oximetry with density functional theory
References
Angelopoulos, Anastasios N et al. Prediction-Powered In-
ference. Science , 382(6671):669–674, 2023.
Breiman, L. Random Forests. Machine Learning , 45:5–32,
2001.
Dresselhaus, M. Optical Properties of Solids. Solid State
Physics, Part 2. Lecture Notes from MIT Course, 6.732
Solid State Physics , 2001.
Fitbit Help Center. How do i track my estimated oxygen
variation in the fitbit app? Fitbit Help Center, 2024.
URLhttps://support.google.com/fitbit/
answer/14237927 . Accessed: 2024-05-15.
Fox, M. Optical Properties of Solids , volume 3. Oxford
Master Series in Condensed Matter Physics, Oxford Uni-
versity Press, 2010.
Hoffman, H. Pulse Oximetry Basic Principles and Interpre-
tation. University of Iowa Department of Otolaryngology ,
2017.
Hung, Nguyen Tuan, Nugraha, Ahmad R. T., and Saito,
Riichiro. Quantum ESPRESSO Course for Solid-State
Physics . CRC Press, 2022.
Iurii Timrov, Oscar Baseggip. Hands-on Time-Dependent
Density Functional Perturbation Theory: Calculation of
Absorption Spectra of Molecules. Quantum ESPRESSO
Summer School. 2019.
Johnson, Kevin B et al. Precision Medicine, AI, and the
Future of Personalized Health Care. Clinical and Trans-
lational Science , 14(1):86–93, 2021.
Li, K. Wireless Reflectance Pulse Oximeter Design and
Photoplethysmographic Signal Processing . PhD thesis,
Kansas State University, 2010.
Mendelson, Y ., Cheung, P. W., Neuman, M. R., Fleming,
D. G., and Cahn, S. D. Spectrophotometric Investigation
of Pulsatile Blood Flow for Transcutaneous Reflectan","approach to density functional theory (DFT) involves solving a set of non-interacting particles that mimic the behavior of real, interacting particles. The total energy functional in Kohn-Sham DFT is given by the application of the Kohn-Sham Hamiltonian to the ground state electron density (Schwingenschl ogl, 2004): E[ 0] =Ts[ 0] +Z Vext(r) 0(r)dr (3) +1 2Z Z 0(r) 0(r ) |r r |drdr +Exc[ ]where Ts[ ]is the kinetic energy of the non-interacting ref- erence system, Vext(r)is the external potential energy, the third term represents the classical Coulomb interaction of the electron density, Exc[ ]is the exchange-correlation en- ergy functional. To minimize the energy functional with respect to the density , under the constraint of normal- ized wavefunctions, we introduce the Kohn-Sham orbitals i(r)where (r) =PN i=1| i(r)|2. The variation of the energy functional with respect to i(r)leads to the Kohn- Sham equations by Euler-Lagrange formulation (Schwin- genschl ogl, 2004):  h2 2m 2+Veff(r) i(r) = i i(r) (4) where iis the Lagrange parameter approximation of the energy eigenvalue associated with the state i, andVeff(r)is the effective potential given by (Schwingenschl ogl, 2004): Veff(r) =Vext(r) +Z (r ) |r r |dr +Vxc(r) (5) andVxc(r)is the functional derivative of the exchange- correlation energy Exc[ ]with respect to the density, Exc . The ground state electron density 0(r)is calculated by summing the square of the absolute value of each occupied Kohn-Sham orbital, where Nis the number of electrons (or occupied states) (Schwingenschl ogl, 2004): 0(r) =NX i=1| i(r)|2(6) Figure 2. Time-series data for (top) reflective pulse oximetry and (bottom) max oxygen consumption rate for 1.5 years while prepar- ing for marathon in March 2022 for nominal conditions prior. Machine learning nominal max oxygen consumption from wearable reflective pulse oximetry with density functional theory Figure 3. Scatter of raw data of absorbance ratio versus max oxy- gen consum","Future work could extend the approach to other biological systems and signals, such as glucose and insulin measure- ments. Similar simplified DFT models for active sites of biomolecules could be constructed, and wearable sensor physiological measurements could be calculated and used for inferences of nominal quantities of the physiological function of interest. Machine learning nominal max oxygen consumption from wearable reflective pulse oximetry with density functional theory","Machine learning nominal max oxygen consumption from wearable reflective pulse oximetry with density functional theory We present a novel method for AI-driven precision medicine to infer max oxygen consumption rate based on random forest regression of wearable sensor physiological measurements using inputs from human hemoglobin density functional theory models. Wearable sensors have revolutionized modern health towards precision medicine. Many measurements taken by wearables are not currently used for medical purposes, and their physical roots are not understood. Here we show the origin of inferring oxygen saturation from measuring estimated oxygen variation while preparing for a marathon via spectrophotometric pulsatile blood flow transcutaneous reflectance oximetry, building six years of data tracking the stages of running a marathon. Based on inputs from quantum mechanical simulations into classical wave and electromagnetic theory models, the imaginary part of the complex dielectric function of the active site of human hemoglobin is altered upon oxygen binding to the heme group, changing the extinction coefficient and selectively absorbing wavelengths of light in a way that enables its detection from the ratio of red to infrared absorbance. A fundamental nominal max oxygen consumption is inferred from quantum mechanical absorbance spectra with and without oxygen binding by training a machine learning model on 1.5 years of daily wearable reflective pulse oximetry data with an of 0.84. Reflectance oximetry is strongly correlated ( of 0.96) to standard pulse oximetry, such as through earlobe or fingertip, enabling accurate, rapid, regular, and automated monitoring of vital signs with wearable sensors on smart watches and fitness trackers, and supplying artificial intelligence inferences of function-symptom relationships for precision medicine.","Machine learning nominal max oxygen consumption from wearable reflective pulse oximetry with density functional theory We present a novel method for AI-driven precision medicine to infer max oxygen consumption rate based on random forest regression of wearable sensor physiological measurements using inputs from human hemoglobin density functional theory models. Wearable sensors have revolutionized modern health towards precision medicine. Many measurements taken by wearables are not currently used for medical purposes, and their physical roots are not understood. Here we show the origin of inferring oxygen saturation from measuring estimated oxygen variation while preparing for a marathon via spectrophotometric pulsatile blood flow transcutaneous reflectance oximetry, building six years of data tracking the stages of running a marathon. Based on inputs from quantum mechanical simulations into classical wave and electromagnetic theory models, the imaginary part of the complex dielectric function of the active site of human hemoglobin is altered upon oxygen binding to the heme group, changing the extinction coefficient and selectively absorbing wavelengths of light in a way that enables its detection from the ratio of red to infrared absorbance. A fundamental nominal max oxygen consumption is inferred from quantum mechanical absorbance spectra with and without oxygen binding by training a machine learning model on 1.5 years of daily wearable reflective pulse oximetry data with an of 0.84. Reflectance oximetry is strongly correlated ( of 0.96) to standard pulse oximetry, such as through earlobe or fingertip, enabling accurate, rapid, regular, and automated monitoring of vital signs with wearable sensors on smart watches and fitness trackers, and supplying artificial intelligence inferences of function-symptom relationships for precision medicine. Future work could extend the approach to other biological systems and signals, such as glucose and insulin measure- ments. Similar simplified DFT models for active sites of biomolecules could be constructed, and wearable sensor physiological measurements could be calculated and used for inferences of nominal quantities of the physiological function of interest. Machine learning nominal max oxygen consumption from wearable reflective pulse oximetry with density functional theory approach to density functional theory (DFT) involves solving a set of non-interacting particles that mimic the behavior of real, interacting particles. The total energy functional in Kohn-Sham DFT is given by the application of the Kohn-Sham Hamiltonian to the ground state electron density (Schwingenschl ogl, 2004): E[ 0] =Ts[ 0] +Z Vext(r) 0(r)dr (3) +1 2Z Z 0(r) 0(r ) |r r |drdr +Exc[ ]where Ts[ ]is the kinetic energy of the non-interacting ref- erence system, Vext(r)is the external potential energy, the third term represents the classical Coulomb interaction of the electron density, Exc[ ]is the exchange-correlation en- ergy functional. To minimize the energy functional with respect to the density , under the constraint of normal- ized wavefunctions, we introduce the Kohn-Sham orbitals i(r)where (r) =PN i=1| i(r)|2. The variation of the energy functional with respect to i(r)leads to the Kohn- Sham equations by Euler-Lagrange formulation (Schwin- genschl ogl, 2004):  h2 2m 2+Veff(r) i(r) = i i(r) (4) where iis the Lagrange parameter approximation of the energy eigenvalue associated with the state i, andVeff(r)is the effective potential given by (Schwingenschl ogl, 2004): Veff(r) =Vext(r) +Z (r ) |r r |dr +Vxc(r) (5) andVxc(r)is the functional derivative of the exchange- correlation energy Exc[ ]with respect to the density, Exc . The ground state electron density 0(r)is calculated by summing the square of the absolute value of each occupied Kohn-Sham orbital, where Nis the number of electrons (or occupied states) (Schwingenschl ogl, 2004): 0(r) =NX i=1| i(r)|2(6) Figure 2. Time-series data for (top) reflective pulse oximetry and (bottom) max oxygen consumption rate for 1.5 years while prepar- ing for marathon in March 2022 for nominal conditions prior. Machine learning nominal max oxygen consumption from wearable reflective pulse oximetry with density functional theory Figure 3. Scatter of raw data of absorbance ratio versus max oxy- gen consum",1
ce1a97b65d6b64209d253d03f99032c1589715fc,A Recipe for Charge Density Prediction,"['Xiang Fu', 'Andrew Scott Rosen', 'Kyle Bystrom', 'Rui Wang', 'Albert Musaelian', 'Boris Kozinsky', 'Tess Smidt', 'Tommi Jaakkola']",https://openreview.net/pdf/ce1a97b65d6b64209d253d03f99032c1589715fc.pdf,"A Recipe for Charge Density Prediction A method that predicts charge density from atomic coordinates that outperforms state-of-the-art while being more than an order of magnitude faster. In density functional theory, charge density is the core attribute of atomic systems from which all chemical properties can be derived. Machine learning methods are promising in significantly accelerating charge density prediction, yet existing approaches either lack accuracy or scalability. We propose a recipe that can achieve both. In particular, we identify three key ingredients: (1) representing the charge density with atomic and virtual orbitals (spherical fields centered at atom/virtual coordinates); (2) using expressive and learnable orbital basis sets (basis function for the spherical fields); and (3) using high-capacity equivariant neural network architecture. Our method achieves state-of-the-art accuracy while being more than an order of magnitude faster than existing methods. Furthermore, our method enables flexible efficiency-accuracy trade-offs by adjusting the model/basis sizes.",ce1a97b65d6b64209d253d03f99032c1589715fc.pdf,"methodology
for controlling the basis set size is to use an even-tempered
Gaussian basis set (Bardo & Ruedenberg, 1974). Based on
a reference atomic orbital basis set (e.g., def2-QZVPPD ),
the even-tempered basis set constructs a series of GTOs with
a set of angular momentum quantum numbers ldetermined
by the atomic number and exponents αgiven by:
αk=α·βkfork= 0,1,2, . . . , N l. (4)
For each spherical harmonics degree l,αandNlare cho-
sen such that the exponents in the reference atomic orbital
basis set are well-covered1.βcontrols the number of basis
functions — a smaller βcreates a more expressive basis
set with denser exponents. The use of an even-tempered
basis set allows us to smoothly control the number of basis
functions Ni
beffectively. Figure 4 (b) shows how the num-
ber of orbital basis functions for elements H, C, N, O, and
F grows with a smaller βfor the even-tempered Gaussian
basis derived from the def2-QZVPPD basis set.
Scaling factors for orbital exponents. In existing orbital-
based models (Fabrizio et al., 2019; Qiao et al., 2022; Rack-
ers et al., 2023; Cheng & Peng, 2023; del Rio et al., 2023),
while the coefficients for the basis functions are predicted by
the ML model, the exponents are fixed for each atom type
and not trainable. However, atoms in different local atomic
environments can exhibit significantly different charge den-
1We adopt the implementation of PySCF (Sun et al., 2020) and
refer interested readers to the original paper/code for more details
on the construction of even-tempered Gaussian basis.
A Recipe for Charge Density Prediction
sity patterns around them, especially for the virtual orbitals
that aim at capturing interatomic interactions. To further
improve the expressivity of the basis set, we make the expo-
nents trainable by learning a positive scaling factor s >0,
such that Equation (2) becomes:
Φα,l,m, ri(r, s) =zα,l,sexp(−s·αr2)rlYl,mr−ri
r
.
(5)
The charge density is now represented with coefficients
ci,j,m and scaling","future work: (1) Despite substantial improve-
ments in efficiency, the computational cost for training the
current model is still significant: our best-performing model
was pretrained for six days and fine-tuned for six days over
four NVIDIA A100 GPUs for the QM9 charge density pre-
diction task. The scaling factor fine-tuning stage requires a
small learning rate, which prolongs training. The prediction
model can benefit from resolving the training instability is-
sues with the scaling factors as well as further improvement
on the model architecture (Liao et al., 2023). (2) While our
approach achieves state-of-the-art performance on the QM9
charge density prediction benchmark, its effectiveness in
crystalline materials (Jain et al., 2013; Shen et al., 2022) has
yet to be validated. The GTOs and the equivariant network
can be applied to materials without modification. The bond-
midpoint-based virtual node assignment for molecules can
be generalized to crystals through a crystal graph construc-
tion algorithm, such as CrystalNN (Zimmermann & Jain,
2020). Alternatively, virtual nodes can be iteratively added
to occupy void space inside the unit cell of the material
using an algorithm based on the V oronoi diagram (Alexa
et al., 2003). The virtual nodes are expected to play an
even more important role in prediction accuracy — this is
because the diverse atomic species in materials and their
complex interactions induce even more complex charge den-
sity patterns. (3) To better validate the practical utility of
the predicted charge density, evaluation on the reduction
of self-consistent field calculations, or on recovering phys-
ical observables, such as energy and forces (Jørgensen &
Bhowmik, 2022; Sunshine et al., 2023; Koker et al., 2023),
will be highly valuable.
Acknowledgements
We thank Teddy Koker, Chaoran Cheng, and Aria Mansouri
Tehrani for their helpful discussions and insights. This work
was supported by the GIST-MIT Research Collaboration
grant funded by GIST","methodology for controlling the basis set size is to use an even-tempered Gaussian basis set (Bardo & Ruedenberg, 1974). Based on a reference atomic orbital basis set (e.g., def2-QZVPPD ), the even-tempered basis set constructs a series of GTOs with a set of angular momentum quantum numbers ldetermined by the atomic number and exponents given by: k= kfork= 0,1,2, . . . , N l. (4) For each spherical harmonics degree l, andNlare cho- sen such that the exponents in the reference atomic orbital basis set are well-covered1. controls the number of basis functions a smaller creates a more expressive basis set with denser exponents. The use of an even-tempered basis set allows us to smoothly control the number of basis functions Ni beffectively. Figure 4 (b) shows how the num- ber of orbital basis functions for elements H, C, N, O, and F grows with a smaller for the even-tempered Gaussian basis derived from the def2-QZVPPD basis set. Scaling factors for orbital exponents. In existing orbital- based models (Fabrizio et al., 2019; Qiao et al., 2022; Rack- ers et al., 2023; Cheng & Peng, 2023; del Rio et al., 2023), while the coefficients for the basis functions are predicted by the ML model, the exponents are fixed for each atom type and not trainable. However, atoms in different local atomic environments can exhibit significantly different charge den- 1We adopt the implementation of PySCF (Sun et al., 2020) and refer interested readers to the original paper/code for more details on the construction of even-tempered Gaussian basis. A Recipe for Charge Density Prediction sity patterns around them, especially for the virtual orbitals that aim at capturing interatomic interactions. To further improve the expressivity of the basis set, we make the expo- nents trainable by learning a positive scaling factor s >0, such that Equation (2) becomes: ,l,m, ri(r, s) =z ,l,sexp( s r2)rlYl,mr ri r . (5) The charge density is now represented with coefficients ci,j,m and scaling","future work: (1) Despite substantial improve- ments in efficiency, the computational cost for training the current model is still significant: our best-performing model was pretrained for six days and fine-tuned for six days over four NVIDIA A100 GPUs for the QM9 charge density pre- diction task. The scaling factor fine-tuning stage requires a small learning rate, which prolongs training. The prediction model can benefit from resolving the training instability is- sues with the scaling factors as well as further improvement on the model architecture (Liao et al., 2023). (2) While our approach achieves state-of-the-art performance on the QM9 charge density prediction benchmark, its effectiveness in crystalline materials (Jain et al., 2013; Shen et al., 2022) has yet to be validated. The GTOs and the equivariant network can be applied to materials without modification. The bond- midpoint-based virtual node assignment for molecules can be generalized to crystals through a crystal graph construc- tion algorithm, such as CrystalNN (Zimmermann & Jain, 2020). Alternatively, virtual nodes can be iteratively added to occupy void space inside the unit cell of the material using an algorithm based on the V oronoi diagram (Alexa et al., 2003). The virtual nodes are expected to play an even more important role in prediction accuracy this is because the diverse atomic species in materials and their complex interactions induce even more complex charge den- sity patterns. (3) To better validate the practical utility of the predicted charge density, evaluation on the reduction of self-consistent field calculations, or on recovering phys- ical observables, such as energy and forces (J rgensen & Bhowmik, 2022; Sunshine et al., 2023; Koker et al., 2023), will be highly valuable.","A Recipe for Charge Density Prediction A method that predicts charge density from atomic coordinates that outperforms state-of-the-art while being more than an order of magnitude faster. In density functional theory, charge density is the core attribute of atomic systems from which all chemical properties can be derived. Machine learning methods are promising in significantly accelerating charge density prediction, yet existing approaches either lack accuracy or scalability. We propose a recipe that can achieve both. In particular, we identify three key ingredients: (1) representing the charge density with atomic and virtual orbitals (spherical fields centered at atom/virtual coordinates); (2) using expressive and learnable orbital basis sets (basis function for the spherical fields); and (3) using high-capacity equivariant neural network architecture. Our method achieves state-of-the-art accuracy while being more than an order of magnitude faster than existing methods. Furthermore, our method enables flexible efficiency-accuracy trade-offs by adjusting the model/basis sizes.","A Recipe for Charge Density Prediction A method that predicts charge density from atomic coordinates that outperforms state-of-the-art while being more than an order of magnitude faster. In density functional theory, charge density is the core attribute of atomic systems from which all chemical properties can be derived. Machine learning methods are promising in significantly accelerating charge density prediction, yet existing approaches either lack accuracy or scalability. We propose a recipe that can achieve both. In particular, we identify three key ingredients: (1) representing the charge density with atomic and virtual orbitals (spherical fields centered at atom/virtual coordinates); (2) using expressive and learnable orbital basis sets (basis function for the spherical fields); and (3) using high-capacity equivariant neural network architecture. Our method achieves state-of-the-art accuracy while being more than an order of magnitude faster than existing methods. Furthermore, our method enables flexible efficiency-accuracy trade-offs by adjusting the model/basis sizes. future work: (1) Despite substantial improve- ments in efficiency, the computational cost for training the current model is still significant: our best-performing model was pretrained for six days and fine-tuned for six days over four NVIDIA A100 GPUs for the QM9 charge density pre- diction task. The scaling factor fine-tuning stage requires a small learning rate, which prolongs training. The prediction model can benefit from resolving the training instability is- sues with the scaling factors as well as further improvement on the model architecture (Liao et al., 2023). (2) While our approach achieves state-of-the-art performance on the QM9 charge density prediction benchmark, its effectiveness in crystalline materials (Jain et al., 2013; Shen et al., 2022) has yet to be validated. The GTOs and the equivariant network can be applied to materials without modification. The bond- midpoint-based virtual node assignment for molecules can be generalized to crystals through a crystal graph construc- tion algorithm, such as CrystalNN (Zimmermann & Jain, 2020). Alternatively, virtual nodes can be iteratively added to occupy void space inside the unit cell of the material using an algorithm based on the V oronoi diagram (Alexa et al., 2003). The virtual nodes are expected to play an even more important role in prediction accuracy this is because the diverse atomic species in materials and their complex interactions induce even more complex charge den- sity patterns. (3) To better validate the practical utility of the predicted charge density, evaluation on the reduction of self-consistent field calculations, or on recovering phys- ical observables, such as energy and forces (J rgensen & Bhowmik, 2022; Sunshine et al., 2023; Koker et al., 2023), will be highly valuable. methodology for controlling the basis set size is to use an even-tempered Gaussian basis set (Bardo & Ruedenberg, 1974). Based on a reference atomic orbital basis set (e.g., def2-QZVPPD ), the even-tempered basis set constructs a series of GTOs with a set of angular momentum quantum numbers ldetermined by the atomic number and exponents given by: k= kfork= 0,1,2, . . . , N l. (4) For each spherical harmonics degree l, andNlare cho- sen such that the exponents in the reference atomic orbital basis set are well-covered1. controls the number of basis functions a smaller creates a more expressive basis set with denser exponents. The use of an even-tempered basis set allows us to smoothly control the number of basis functions Ni beffectively. Figure 4 (b) shows how the num- ber of orbital basis functions for elements H, C, N, O, and F grows with a smaller for the even-tempered Gaussian basis derived from the def2-QZVPPD basis set. Scaling factors for orbital exponents. In existing orbital- based models (Fabrizio et al., 2019; Qiao et al., 2022; Rack- ers et al., 2023; Cheng & Peng, 2023; del Rio et al., 2023), while the coefficients for the basis functions are predicted by the ML model, the exponents are fixed for each atom type and not trainable. However, atoms in different local atomic environments can exhibit significantly different charge den- 1We adopt the implementation of PySCF (Sun et al., 2020) and refer interested readers to the original paper/code for more details on the construction of even-tempered Gaussian basis. A Recipe for Charge Density Prediction sity patterns around them, especially for the virtual orbitals that aim at capturing interatomic interactions. To further improve the expressivity of the basis set, we make the expo- nents trainable by learning a positive scaling factor s >0, such that Equation (2) becomes: ,l,m, ri(r, s) =z ,l,sexp( s r2)rlYl,mr ri r . (5) The charge density is now represented with coefficients ci,j,m and scaling",0
7752d2747bf1cf1a34717bfc6acdd769e8bd1a42,Finding Structure-Property Relationships for Molecular Property Predictions with Globally Explainable AI,"['Jonas Teufel', 'Pascal Friederich']",https://openreview.net/pdf/7752d2747bf1cf1a34717bfc6acdd769e8bd1a42.pdf,"Finding Structure-Property Relationships for Molecular Property Predictions with Globally Explainable AI We use global concept explanability to extract general structure property relationships from graph neural networks. AI models have advanced various branches of society, industry, and science. In the domains of chemistry and material science, for example, graph neural networks have proven a useful tool for molecular and material property predictions. Despite their superior predictive performance, the inner workings of most complex AI models remain elusive to human observers. Methods of explainable AI (xAI) can be used to increase the transparency of these predictions to gain a better understanding not only of the model's behavior but also of the underlying task itself. We introduce a method to extract structure-property relationships for molecular property predictions from global concept explanations. By clustering in a latent space of subgraph embeddings, we discover molecules with similar subgraph motifs. For each cluster of similar substructures we can compute an average contribution towards the model's target prediction - therefore reconstructing the general structure-property relationships on which the model's decisions are based on. Finally, a language model can be prompted with all information about the observed structural motifs to provide a hypothesis for a causal explanation of the method. We validate our method on various synthetic and real-world graph property prediction tasks and find that it is able to reproduce known chemical rules of thumb.",7752d2747bf1cf1a34717bfc6acdd769e8bd1a42.pdf,,,,,"Finding Structure-Property Relationships for Molecular Property Predictions with Globally Explainable AI We use global concept explanability to extract general structure property relationships from graph neural networks. AI models have advanced various branches of society, industry, and science. In the domains of chemistry and material science, for example, graph neural networks have proven a useful tool for molecular and material property predictions. Despite their superior predictive performance, the inner workings of most complex AI models remain elusive to human observers. Methods of explainable AI (xAI) can be used to increase the transparency of these predictions to gain a better understanding not only of the model's behavior but also of the underlying task itself. We introduce a method to extract structure-property relationships for molecular property predictions from global concept explanations. By clustering in a latent space of subgraph embeddings, we discover molecules with similar subgraph motifs. For each cluster of similar substructures we can compute an average contribution towards the model's target prediction - therefore reconstructing the general structure-property relationships on which the model's decisions are based on. Finally, a language model can be prompted with all information about the observed structural motifs to provide a hypothesis for a causal explanation of the method. We validate our method on various synthetic and real-world graph property prediction tasks and find that it is able to reproduce known chemical rules of thumb.","Finding Structure-Property Relationships for Molecular Property Predictions with Globally Explainable AI We use global concept explanability to extract general structure property relationships from graph neural networks. AI models have advanced various branches of society, industry, and science. In the domains of chemistry and material science, for example, graph neural networks have proven a useful tool for molecular and material property predictions. Despite their superior predictive performance, the inner workings of most complex AI models remain elusive to human observers. Methods of explainable AI (xAI) can be used to increase the transparency of these predictions to gain a better understanding not only of the model's behavior but also of the underlying task itself. We introduce a method to extract structure-property relationships for molecular property predictions from global concept explanations. By clustering in a latent space of subgraph embeddings, we discover molecules with similar subgraph motifs. For each cluster of similar substructures we can compute an average contribution towards the model's target prediction - therefore reconstructing the general structure-property relationships on which the model's decisions are based on. Finally, a language model can be prompted with all information about the observed structural motifs to provide a hypothesis for a causal explanation of the method. We validate our method on various synthetic and real-world graph property prediction tasks and find that it is able to reproduce known chemical rules of thumb.  ",0
11b2bf2ea710bb34cc6d5631e1aca78d4dc7aeaa,TAGMol: Target-Aware Gradient-guided Molecule Generation,"['Vineeth Dorna', 'D. Subhalingam', 'Keshav Kolluru', 'Shreshth Tuli', 'Mrityunjay Singh', 'Saurabh Singal', 'N M Anoop Krishnan', 'Sayan Ranu']",https://openreview.net/pdf/11b2bf2ea710bb34cc6d5631e1aca78d4dc7aeaa.pdf,"TAGMol: Target-Aware Gradient-guided Molecule Generation Introducing TAGMol: A novel approach to structure-based drug design that generates target-specific, drug-like molecules with optimal properties (QED, SA) through gradient-guided, diffusion-based molecule generation. 3D generative models have shown significant promise in *structure-based drug design (SBDD)*, particularly in discovering ligands tailored to specific target binding sites. Existing algorithms often focus primarily on ligand-target binding, characterized by binding affinity. Moreover, models trained solely on target-ligand distribution may fall short in addressing the broader objectives of drug discovery, such as the development of novel ligands with desired properties like drug-likeness, and synthesizability, underscoring the multifaceted nature of the drug design process. To overcome these challenges, we decouple the problem into molecular generation and property prediction. The latter synergistically *guides* the diffusion sampling process, facilitating guided diffusion and resulting in the creation of meaningful molecules with the desired properties. We call this guided molecular generation process as TAGMol. Through experiments on benchmark datasets, TAGMol demonstrates superior performance compared to state-of-the-art baselines, achieving a 22% improvement in average Vina Score and yielding favorable outcomes in essential auxiliary properties. This establishes TAGMol as a comprehensive framework for drug generation. The code is available at https://github.com/moleculeai/TAGMol.",11b2bf2ea710bb34cc6d5631e1aca78d4dc7aeaa.pdf,"methodology, which omits protein atoms into consideration.
Thekvalue was specifically tailored to each property: set at 6 for both QED and SA and increased to 32 for the binding
affinity property predictor to optimize the model’s performance in different contexts. For training our binding affinity guide ,
we used Autodock Vina scores from the CrossDocked2020 dataset (Eberhardt et al., 2021; Francoeur et al., 2020), and for
QED and SA, we calculated scores using the RDKit package (Landrum, 2013). To effectively guide the denoising phase, we
train our guide using the same noise as our backbone model, TargetDiff, which applies Gaussian noise to coordinates and
uniform noise to categories.
The models are trained with a batch size of 4 to minimize the Root Mean Square Error (RMSE) loss, utilizing a gradient
descent algorithm. The initial learning rates were set at 0.001 for the binding affinity andQED guides , and at 5e-4 for the SA
guide . Additionally, to maintain the stability of the training process, the gradient norms were clipped at a maximum value
of 8. The learning rate underwent exponential decay by a factor of 0.6, with a lower bound set at 1e-5. This decay was
triggered in the absence of any improvement in the validation loss across 10 consecutive evaluations. Evaluations were
conducted every 2000 training steps, and convergence of all models was achieved within 200,000 steps. All experiments
were conducted using the NVIDIA A100 GPU.
TAGM OL: Target-Aware Gradient-guided Molecule Generation
E.2. Evaluation
The performance of each guide model was evaluated for their respective property predictions using the same test dataset. As
outlined in §4, we employed the same split to evaluate our binding affinity guide model. While the train-test split ensures the
uniqueness of the protein-ligand complexes between the sets, 34 ligands are common to both, leading to potential train-test
leakage. Consequently, for QED andSAguides that depend solely on ligands, we have excl","future work.
Table 8. Evaluation of poses of generated molecules on biophysical benchmarks, PoseCheck. ( ↑) / (↓) indicates whether a larger or smaller
value is preferable. The first-place results are emphasized in bold.
Methods Clashes ( ↓)Strain Energy ( ↓)
TargetDiff 9.19 1258.02
DecompDiff 12.53 539.89
TAGM OL 6.05 2143.03
Table 9. Evaluation of poses of generated molecules on biophysical benchmarks, PoseCheck, after redocking. ( ↑) / (↓) indicates whether a
larger or smaller value is preferable. The first-place results are emphasized in bold.
Methods Clashes ( ↓)Strain Energy ( ↓)
Reference 3.5 118.17
TargetDiff 5.9 602.00
DecompDiff 5.75 441.61
TAGM OL 5.36 709.35
F.7. Challenges in SA optimization
The SA score serves as a comprehensive metric for assessing synthetic feasibility, considering various non-standard structural
features such as large rings, unconventional ring fusions, stereocomplexity, and overall molecule size (Ertl & Schuffenhauer,
2009). However, diffusion-based generative models face a limitation in accurately positioning atoms, leading to unrealistic
molecular topologies, especially the formation of large rings, which can negatively impact SA scores (Peng et al., 2023).
As evident from Table 10, the diffusion-based models exhibit a subpar distribution of ring sizes, notably characterized by a
prevalence of larger rings. In TAGM OL, the provision of multiple guidance signals to coordinates increases the likelihood
TAGM OL: Target-Aware Gradient-guided Molecule Generation
of forming large rings. Consequently, this propensity towards larger ring formations contributes to either a drop in SA values
or minimal improvement despite the guidance provided.
Table 10. Distribution of ring sizes in reference and generated molecules, expressed as percentages
Ring Ref Pocket2 Target Decomp TAGM OL
Size Mol Diff Diff
3 1.7% 0.1% 0.0% 2.9% 0.0%
4 0.0% 0.0% 2.8% 3.7% 2.4%
5 30.2% 16.4% 30.8% 30.8% 26.9%
6 67.4% 80.4% 50.7% 45.6% 48.6%
7 0.7% 2.6% 12.1% 11.6%","methodology, which omits protein atoms into consideration. Thekvalue was specifically tailored to each property: set at 6 for both QED and SA and increased to 32 for the binding affinity property predictor to optimize the model s performance in different contexts. For training our binding affinity guide , we used Autodock Vina scores from the CrossDocked2020 dataset (Eberhardt et al., 2021; Francoeur et al., 2020), and for QED and SA, we calculated scores using the RDKit package (Landrum, 2013). To effectively guide the denoising phase, we train our guide using the same noise as our backbone model, TargetDiff, which applies Gaussian noise to coordinates and uniform noise to categories. The models are trained with a batch size of 4 to minimize the Root Mean Square Error (RMSE) loss, utilizing a gradient descent algorithm. The initial learning rates were set at 0.001 for the binding affinity andQED guides , and at 5e-4 for the SA guide . Additionally, to maintain the stability of the training process, the gradient norms were clipped at a maximum value of 8. The learning rate underwent exponential decay by a factor of 0.6, with a lower bound set at 1e-5. This decay was triggered in the absence of any improvement in the validation loss across 10 consecutive evaluations. Evaluations were conducted every 2000 training steps, and convergence of all models was achieved within 200,000 steps. All experiments were conducted using the NVIDIA A100 GPU. TAGM OL: Target-Aware Gradient-guided Molecule Generation E.2. Evaluation The performance of each guide model was evaluated for their respective property predictions using the same test dataset. As outlined in 4, we employed the same split to evaluate our binding affinity guide model. While the train-test split ensures the uniqueness of the protein-ligand complexes between the sets, 34 ligands are common to both, leading to potential train-test leakage. Consequently, for QED andSAguides that depend solely on ligands, we have excl","future work. Table 8. Evaluation of poses of generated molecules on biophysical benchmarks, PoseCheck. ( ) / ( ) indicates whether a larger or smaller value is preferable. The first-place results are emphasized in bold. Methods Clashes ( )Strain Energy ( ) TargetDiff 9.19 1258.02 DecompDiff 12.53 539.89 TAGM OL 6.05 2143.03 Table 9. Evaluation of poses of generated molecules on biophysical benchmarks, PoseCheck, after redocking. ( ) / ( ) indicates whether a larger or smaller value is preferable. The first-place results are emphasized in bold. Methods Clashes ( )Strain Energy ( ) Reference 3.5 118.17 TargetDiff 5.9 602.00 DecompDiff 5.75 441.61 TAGM OL 5.36 709.35 F.7. Challenges in SA optimization The SA score serves as a comprehensive metric for assessing synthetic feasibility, considering various non-standard structural features such as large rings, unconventional ring fusions, stereocomplexity, and overall molecule size (Ertl & Schuffenhauer, 2009). However, diffusion-based generative models face a limitation in accurately positioning atoms, leading to unrealistic molecular topologies, especially the formation of large rings, which can negatively impact SA scores (Peng et al., 2023). As evident from Table 10, the diffusion-based models exhibit a subpar distribution of ring sizes, notably characterized by a prevalence of larger rings. In TAGM OL, the provision of multiple guidance signals to coordinates increases the likelihood TAGM OL: Target-Aware Gradient-guided Molecule Generation of forming large rings. Consequently, this propensity towards larger ring formations contributes to either a drop in SA values or minimal improvement despite the guidance provided. Table 10. Distribution of ring sizes in reference and generated molecules, expressed as percentages Ring Ref Pocket2 Target Decomp TAGM OL Size Mol Diff Diff 3 1.7% 0.1% 0.0% 2.9% 0.0% 4 0.0% 0.0% 2.8% 3.7% 2.4% 5 30.2% 16.4% 30.8% 30.8% 26.9% 6 67.4% 80.4% 50.7% 45.6% 48.6% 7 0.7% 2.6% 12.1% 11.6%","TAGMol: Target-Aware Gradient-guided Molecule Generation Introducing TAGMol: A novel approach to structure-based drug design that generates target-specific, drug-like molecules with optimal properties (QED, SA) through gradient-guided, diffusion-based molecule generation. 3D generative models have shown significant promise in *structure-based drug design (SBDD)*, particularly in discovering ligands tailored to specific target binding sites. Existing algorithms often focus primarily on ligand-target binding, characterized by binding affinity. Moreover, models trained solely on target-ligand distribution may fall short in addressing the broader objectives of drug discovery, such as the development of novel ligands with desired properties like drug-likeness, and synthesizability, underscoring the multifaceted nature of the drug design process. To overcome these challenges, we decouple the problem into molecular generation and property prediction. The latter synergistically *guides* the diffusion sampling process, facilitating guided diffusion and resulting in the creation of meaningful molecules with the desired properties. We call this guided molecular generation process as TAGMol. Through experiments on benchmark datasets, TAGMol demonstrates superior performance compared to state-of-the-art baselines, achieving a 22% improvement in average Vina Score and yielding favorable outcomes in essential auxiliary properties. This establishes TAGMol as a comprehensive framework for drug generation. The code is available at https://github.com/moleculeai/TAGMol.","TAGMol: Target-Aware Gradient-guided Molecule Generation Introducing TAGMol: A novel approach to structure-based drug design that generates target-specific, drug-like molecules with optimal properties (QED, SA) through gradient-guided, diffusion-based molecule generation. 3D generative models have shown significant promise in *structure-based drug design (SBDD)*, particularly in discovering ligands tailored to specific target binding sites. Existing algorithms often focus primarily on ligand-target binding, characterized by binding affinity. Moreover, models trained solely on target-ligand distribution may fall short in addressing the broader objectives of drug discovery, such as the development of novel ligands with desired properties like drug-likeness, and synthesizability, underscoring the multifaceted nature of the drug design process. To overcome these challenges, we decouple the problem into molecular generation and property prediction. The latter synergistically *guides* the diffusion sampling process, facilitating guided diffusion and resulting in the creation of meaningful molecules with the desired properties. We call this guided molecular generation process as TAGMol. Through experiments on benchmark datasets, TAGMol demonstrates superior performance compared to state-of-the-art baselines, achieving a 22% improvement in average Vina Score and yielding favorable outcomes in essential auxiliary properties. This establishes TAGMol as a comprehensive framework for drug generation. The code is available at https://github.com/moleculeai/TAGMol. future work. Table 8. Evaluation of poses of generated molecules on biophysical benchmarks, PoseCheck. ( ) / ( ) indicates whether a larger or smaller value is preferable. The first-place results are emphasized in bold. Methods Clashes ( )Strain Energy ( ) TargetDiff 9.19 1258.02 DecompDiff 12.53 539.89 TAGM OL 6.05 2143.03 Table 9. Evaluation of poses of generated molecules on biophysical benchmarks, PoseCheck, after redocking. ( ) / ( ) indicates whether a larger or smaller value is preferable. The first-place results are emphasized in bold. Methods Clashes ( )Strain Energy ( ) Reference 3.5 118.17 TargetDiff 5.9 602.00 DecompDiff 5.75 441.61 TAGM OL 5.36 709.35 F.7. Challenges in SA optimization The SA score serves as a comprehensive metric for assessing synthetic feasibility, considering various non-standard structural features such as large rings, unconventional ring fusions, stereocomplexity, and overall molecule size (Ertl & Schuffenhauer, 2009). However, diffusion-based generative models face a limitation in accurately positioning atoms, leading to unrealistic molecular topologies, especially the formation of large rings, which can negatively impact SA scores (Peng et al., 2023). As evident from Table 10, the diffusion-based models exhibit a subpar distribution of ring sizes, notably characterized by a prevalence of larger rings. In TAGM OL, the provision of multiple guidance signals to coordinates increases the likelihood TAGM OL: Target-Aware Gradient-guided Molecule Generation of forming large rings. Consequently, this propensity towards larger ring formations contributes to either a drop in SA values or minimal improvement despite the guidance provided. Table 10. Distribution of ring sizes in reference and generated molecules, expressed as percentages Ring Ref Pocket2 Target Decomp TAGM OL Size Mol Diff Diff 3 1.7% 0.1% 0.0% 2.9% 0.0% 4 0.0% 0.0% 2.8% 3.7% 2.4% 5 30.2% 16.4% 30.8% 30.8% 26.9% 6 67.4% 80.4% 50.7% 45.6% 48.6% 7 0.7% 2.6% 12.1% 11.6% methodology, which omits protein atoms into consideration. Thekvalue was specifically tailored to each property: set at 6 for both QED and SA and increased to 32 for the binding affinity property predictor to optimize the model s performance in different contexts. For training our binding affinity guide , we used Autodock Vina scores from the CrossDocked2020 dataset (Eberhardt et al., 2021; Francoeur et al., 2020), and for QED and SA, we calculated scores using the RDKit package (Landrum, 2013). To effectively guide the denoising phase, we train our guide using the same noise as our backbone model, TargetDiff, which applies Gaussian noise to coordinates and uniform noise to categories. The models are trained with a batch size of 4 to minimize the Root Mean Square Error (RMSE) loss, utilizing a gradient descent algorithm. The initial learning rates were set at 0.001 for the binding affinity andQED guides , and at 5e-4 for the SA guide . Additionally, to maintain the stability of the training process, the gradient norms were clipped at a maximum value of 8. The learning rate underwent exponential decay by a factor of 0.6, with a lower bound set at 1e-5. This decay was triggered in the absence of any improvement in the validation loss across 10 consecutive evaluations. Evaluations were conducted every 2000 training steps, and convergence of all models was achieved within 200,000 steps. All experiments were conducted using the NVIDIA A100 GPU. TAGM OL: Target-Aware Gradient-guided Molecule Generation E.2. Evaluation The performance of each guide model was evaluated for their respective property predictions using the same test dataset. As outlined in 4, we employed the same split to evaluate our binding affinity guide model. While the train-test split ensures the uniqueness of the protein-ligand complexes between the sets, 34 ligands are common to both, leading to potential train-test leakage. Consequently, for QED andSAguides that depend solely on ligands, we have excl",0
e35754f20a409293df6bbfe47f01c1fbb9c2e12b,Future-proof vaccine design with a generative model of antibody cross-reactivity,"['Noor Youssef', 'Sarah Gurev', 'Hannah Rivka Pierce-Hoffman', 'Alexander A Cohen', 'Luis F Caldera', 'Pamela J Bjorkman', 'Debora Susan Marks']",https://openreview.net/pdf/e35754f20a409293df6bbfe47f01c1fbb9c2e12b.pdf,"Future-proof vaccine design with a generative model of antibody cross-reactivity We develop a future-proof vaccine design method using a generative model of antibody cross-reactivity to focus ellicited antibody immune responses to conserved, neutralizing regions unlikely to mutate. Mosaic nanoparticle vaccines incorporating naturally diverse sarbecovirus receptor binding domains (RBDs) represent a promising approach for pan-coronavirus vaccines. Mosaic nanoparticles elicit broad, cross-reactive immune responses, likely because elicited antibodies utilize avidity effects to preferentially bind conserved regions across neighboring RBDs. However, the diversity in natural RBDs is limited, leading to off-target antibodies that bind to conserved regions across the selected RBDs but which are likely to mutate in the future. We therefore develop a novel future-proof vaccine design method, building upon a probabilistic generative model of antibody escape, to computationally design RBDs with further diversity. This approach aims to focus antibody responses to regions that are (1) neutralizing, (2) accessible and (3) unlikely to mutate during future viral evolution. The designs will be assessed by immunizing mice and testing the breadth of neutralizability of the sera compared to a nanoparticle composed of naturally diverse strains.",e35754f20a409293df6bbfe47f01c1fbb9c2e12b.pdf,"methodology
Here we address a novel sequence design problem: given a
set of naturally diverse antigens in a viral family, identify
Future-proof vaccine design with a generative model of antibody cross-reactivity
a region to target antibody binding and generate mutations
elsewhere to minimize off-target antibodies, while retaining
antigen functionality. The mutations should minimize the
pairwise antibody cross-reactivity (where antibodies can
bind with avidity) at any possible off-target epitope. The
goal is to design a set of RBDs that, when immunizing as
a mosaic nanoparticle, generates sera that is more broadly
neutralizing against diverse existing and potential future
species in a viral family, compared to a mosaic nanopar-
ticle composed of the original, natural strains. We base
our designs on the natural mosaic sarbecoviruses used pre-
viously: SARS-CoV-2 Beta, RaTG13, Rs4081, SHC014,
Pang17, RmYN02, Rf1, and WIV1 (Cohen et al., 2021a;
2022; 2024). Critically, the model is unsupervised and does
not rely on prior knowledge of specific epitopes or experi-
mental data.
We develop a multi-step design process (Fig 1C)–rather than
a joint optimization that directly results in a final group of
designed antigens–due to the complexity of the design ob-
jectives. Moreover, with diverse antigens as the key goal, it
is important to down-sample from many potential designs to
avoid prematurely constraining design choices based on ini-
tially selected sequences. This design process builds upon
EVEscape, which models antibody escape as the product
of three components: (1) mutation maintains fitness, (2)
mutation is accessible to antibodies, (3) mutation is suffi-
ciently dissimilar to disrupt antibody binding (Thadani et al.,
2023). By virtue of EVEscape’s flexible, modular frame-
work, we repurpose each of its components, separately and
in combination, in the relevant steps of our approach.
2.1. Identify target avidity binding region
We first identify an optimal antibody bind","Discussion
We present a novel future-proof vaccine design method us-
ing a deep generative model of antibody escape, EVEscape.
Generative models are often divorced from real world ap-
plications. By wrapping our generative model into a larger,
multi-step optimization process, which we then use to de-
sign a mosaic nanoparticle vaccine for experimental testing,
we bridge this gap. Given EVEscape’s modular components,
we are able to build our design process on its foundation.
We will perform experimental testing by immunizing mice
with our designed mosaic nanoparticle, evaluating our vac-
cine based on its neutralization breadth, against both diverse
current and future strains. While we focus on the sarbe-
covirus sub-genus here, we will later explore our ability to
achieve broader protection–for instance of the betacoran-
virus genus or indeed of the entire coronavirus family. We
will evaluate the breadth of protection relative to a nanopar-
ticle composed of natural strains. To facilitate this com-
parison, we began our optimization from the same set of
natural strains as in the vaccine developed by Cohen et al.
(2021a; 2022). Once the success of our approach has been
affirmed, an additional step in the design process could be
Future-proof vaccine design with a generative model of antibody cross-reactivity
incorporated to select an initial set of diverse viral species
as the base of the designs. Moreover, given our initial ex-
perimental results, we plan to iterate on our entire design
process.
Thus far, our focus has been only on protection by neutraliz-
ing antibodies (both in our design objective and evaluation
criteria), rather than protection from other sources, such as
non-neutralizing antibody (e.g., ADCC) or T cell responses.
Current vaccine practices take this same narrow focus–likely
to their detriment. This may be beginning to change, espe-
cially for the pursuit of universal vaccines. By virtue of our
multi-step process, it is simple for us to likewise i","methodology Here we address a novel sequence design problem: given a set of naturally diverse antigens in a viral family, identify Future-proof vaccine design with a generative model of antibody cross-reactivity a region to target antibody binding and generate mutations elsewhere to minimize off-target antibodies, while retaining antigen functionality. The mutations should minimize the pairwise antibody cross-reactivity (where antibodies can bind with avidity) at any possible off-target epitope. The goal is to design a set of RBDs that, when immunizing as a mosaic nanoparticle, generates sera that is more broadly neutralizing against diverse existing and potential future species in a viral family, compared to a mosaic nanopar- ticle composed of the original, natural strains. We base our designs on the natural mosaic sarbecoviruses used pre- viously: SARS-CoV-2 Beta, RaTG13, Rs4081, SHC014, Pang17, RmYN02, Rf1, and WIV1 (Cohen et al., 2021a; 2022; 2024). Critically, the model is unsupervised and does not rely on prior knowledge of specific epitopes or experi- mental data. We develop a multi-step design process (Fig 1C) rather than a joint optimization that directly results in a final group of designed antigens due to the complexity of the design ob- jectives. Moreover, with diverse antigens as the key goal, it is important to down-sample from many potential designs to avoid prematurely constraining design choices based on ini- tially selected sequences. This design process builds upon EVEscape, which models antibody escape as the product of three components: (1) mutation maintains fitness, (2) mutation is accessible to antibodies, (3) mutation is suffi- ciently dissimilar to disrupt antibody binding (Thadani et al., 2023). By virtue of EVEscape s flexible, modular frame- work, we repurpose each of its components, separately and in combination, in the relevant steps of our approach. 2.1. Identify target avidity binding region We first identify an optimal antibody bind","Discussion We present a novel future-proof vaccine design method us- ing a deep generative model of antibody escape, EVEscape. Generative models are often divorced from real world ap- plications. By wrapping our generative model into a larger, multi-step optimization process, which we then use to de- sign a mosaic nanoparticle vaccine for experimental testing, we bridge this gap. Given EVEscape s modular components, we are able to build our design process on its foundation. We will perform experimental testing by immunizing mice with our designed mosaic nanoparticle, evaluating our vac- cine based on its neutralization breadth, against both diverse current and future strains. While we focus on the sarbe- covirus sub-genus here, we will later explore our ability to achieve broader protection for instance of the betacoran- virus genus or indeed of the entire coronavirus family. We will evaluate the breadth of protection relative to a nanopar- ticle composed of natural strains. To facilitate this com- parison, we began our optimization from the same set of natural strains as in the vaccine developed by Cohen et al. (2021a; 2022). Once the success of our approach has been affirmed, an additional step in the design process could be Future-proof vaccine design with a generative model of antibody cross-reactivity incorporated to select an initial set of diverse viral species as the base of the designs. Moreover, given our initial ex- perimental results, we plan to iterate on our entire design process. Thus far, our focus has been only on protection by neutraliz- ing antibodies (both in our design objective and evaluation criteria), rather than protection from other sources, such as non-neutralizing antibody (e.g., ADCC) or T cell responses. Current vaccine practices take this same narrow focus likely to their detriment. This may be beginning to change, espe- cially for the pursuit of universal vaccines. By virtue of our multi-step process, it is simple for us to likewise i","Future-proof vaccine design with a generative model of antibody cross-reactivity We develop a future-proof vaccine design method using a generative model of antibody cross-reactivity to focus ellicited antibody immune responses to conserved, neutralizing regions unlikely to mutate. Mosaic nanoparticle vaccines incorporating naturally diverse sarbecovirus receptor binding domains (RBDs) represent a promising approach for pan-coronavirus vaccines. Mosaic nanoparticles elicit broad, cross-reactive immune responses, likely because elicited antibodies utilize avidity effects to preferentially bind conserved regions across neighboring RBDs. However, the diversity in natural RBDs is limited, leading to off-target antibodies that bind to conserved regions across the selected RBDs but which are likely to mutate in the future. We therefore develop a novel future-proof vaccine design method, building upon a probabilistic generative model of antibody escape, to computationally design RBDs with further diversity. This approach aims to focus antibody responses to regions that are (1) neutralizing, (2) accessible and (3) unlikely to mutate during future viral evolution. The designs will be assessed by immunizing mice and testing the breadth of neutralizability of the sera compared to a nanoparticle composed of naturally diverse strains.","Future-proof vaccine design with a generative model of antibody cross-reactivity We develop a future-proof vaccine design method using a generative model of antibody cross-reactivity to focus ellicited antibody immune responses to conserved, neutralizing regions unlikely to mutate. Mosaic nanoparticle vaccines incorporating naturally diverse sarbecovirus receptor binding domains (RBDs) represent a promising approach for pan-coronavirus vaccines. Mosaic nanoparticles elicit broad, cross-reactive immune responses, likely because elicited antibodies utilize avidity effects to preferentially bind conserved regions across neighboring RBDs. However, the diversity in natural RBDs is limited, leading to off-target antibodies that bind to conserved regions across the selected RBDs but which are likely to mutate in the future. We therefore develop a novel future-proof vaccine design method, building upon a probabilistic generative model of antibody escape, to computationally design RBDs with further diversity. This approach aims to focus antibody responses to regions that are (1) neutralizing, (2) accessible and (3) unlikely to mutate during future viral evolution. The designs will be assessed by immunizing mice and testing the breadth of neutralizability of the sera compared to a nanoparticle composed of naturally diverse strains. Discussion We present a novel future-proof vaccine design method us- ing a deep generative model of antibody escape, EVEscape. Generative models are often divorced from real world ap- plications. By wrapping our generative model into a larger, multi-step optimization process, which we then use to de- sign a mosaic nanoparticle vaccine for experimental testing, we bridge this gap. Given EVEscape s modular components, we are able to build our design process on its foundation. We will perform experimental testing by immunizing mice with our designed mosaic nanoparticle, evaluating our vac- cine based on its neutralization breadth, against both diverse current and future strains. While we focus on the sarbe- covirus sub-genus here, we will later explore our ability to achieve broader protection for instance of the betacoran- virus genus or indeed of the entire coronavirus family. We will evaluate the breadth of protection relative to a nanopar- ticle composed of natural strains. To facilitate this com- parison, we began our optimization from the same set of natural strains as in the vaccine developed by Cohen et al. (2021a; 2022). Once the success of our approach has been affirmed, an additional step in the design process could be Future-proof vaccine design with a generative model of antibody cross-reactivity incorporated to select an initial set of diverse viral species as the base of the designs. Moreover, given our initial ex- perimental results, we plan to iterate on our entire design process. Thus far, our focus has been only on protection by neutraliz- ing antibodies (both in our design objective and evaluation criteria), rather than protection from other sources, such as non-neutralizing antibody (e.g., ADCC) or T cell responses. Current vaccine practices take this same narrow focus likely to their detriment. This may be beginning to change, espe- cially for the pursuit of universal vaccines. By virtue of our multi-step process, it is simple for us to likewise i methodology Here we address a novel sequence design problem: given a set of naturally diverse antigens in a viral family, identify Future-proof vaccine design with a generative model of antibody cross-reactivity a region to target antibody binding and generate mutations elsewhere to minimize off-target antibodies, while retaining antigen functionality. The mutations should minimize the pairwise antibody cross-reactivity (where antibodies can bind with avidity) at any possible off-target epitope. The goal is to design a set of RBDs that, when immunizing as a mosaic nanoparticle, generates sera that is more broadly neutralizing against diverse existing and potential future species in a viral family, compared to a mosaic nanopar- ticle composed of the original, natural strains. We base our designs on the natural mosaic sarbecoviruses used pre- viously: SARS-CoV-2 Beta, RaTG13, Rs4081, SHC014, Pang17, RmYN02, Rf1, and WIV1 (Cohen et al., 2021a; 2022; 2024). Critically, the model is unsupervised and does not rely on prior knowledge of specific epitopes or experi- mental data. We develop a multi-step design process (Fig 1C) rather than a joint optimization that directly results in a final group of designed antigens due to the complexity of the design ob- jectives. Moreover, with diverse antigens as the key goal, it is important to down-sample from many potential designs to avoid prematurely constraining design choices based on ini- tially selected sequences. This design process builds upon EVEscape, which models antibody escape as the product of three components: (1) mutation maintains fitness, (2) mutation is accessible to antibodies, (3) mutation is suffi- ciently dissimilar to disrupt antibody binding (Thadani et al., 2023). By virtue of EVEscape s flexible, modular frame- work, we repurpose each of its components, separately and in combination, in the relevant steps of our approach. 2.1. Identify target avidity binding region We first identify an optimal antibody bind",0
356c6dd9812b78992aa24f0a9c1aa6d3247e3248,Augmenting Evolutionary Models with Structure-based Retrieval,"['Yining Huang', 'Zuobai Zhang', 'Jian Tang', 'Debora Susan Marks', 'Pascal Notin']",https://openreview.net/pdf/356c6dd9812b78992aa24f0a9c1aa6d3247e3248.pdf,"Augmenting Evolutionary Models with Structure-based Retrieval Multiple Sequence Alignments (MSAs) are crucial in protein sequence analysis for identifying homologous proteins sharing a common evolutionary origin. However, traditional MSA search tools struggle to recover distantly related sequences that, despite low sequence similarity, exhibit high structural and functional resemblance often missing in the so-called midnight zone of protein similarity. To overcome these limitations, we propose the integration of structure similarity search tools to enhance the identification of homologous proteins. This approach utilizes Foldseek to search the AlphaFold database, aligning structurally similar proteins to construct Multiple Structure Alignments (MStructAs) alongside traditional MSAs. By combining these alignments, we develop family-specific generative models for protein fitness prediction, using diverse assays from the ProteinGym benchmarks. Our findings reveal that incorporating structure-based retrieval into MSAs significantly improves the performance of alignment-based methods, suggesting a robust hybrid retrieval strategy that harnesses both sequence and structure similarities.",356c6dd9812b78992aa24f0a9c1aa6d3247e3248.pdf,"approaches typically fail in recov-
ering distantly related sequences with high structure and
functional similarity but low sequence similarity, referred
to as the ‘midnight zone’ of protein similarity (Heinzinger
et al., 2021). To address the limitations of traditional Multi-
ple Sequence Alignments (MSAs), we explore the usage of
structure similarity search tools to efficiently identify homol-
ogous proteins with similar structures from large databases.
The proteins identified are then aligned with the target pro-
tein to create Multiple Structure Alignments (MStructAs),
which can uncover insights into the “blind spots”of standard
sequence-based MSAs. To demonstrate the effectiveness
of our methods, we use 197 Deep Mutational Scanning
(DMS) assays from ProteinGym (Notin et al., 2023) as ex-
amples. For each target protein, we employ Foldseek to
search for structurally similar proteins in the AlphaFold
UniProt database and align them to construct MStructAs.
We then combine the identified MSAs and MStructAs to
train family-specific generative models for protein fitness
prediction (Frazer et al., 2021). Our results indicate that the
performance of these alignment-based approaches can be
markedly improved by augmenting MSAs with sequences
recovered via structure-based retrieval, paving the way for
hybrid retrieval strategies that consider both sequence and
structure similarities.
2. Method
Inspired by recent development of fast protein structure
tools, in this section, we explore the usage of Multiple Struc-
ture Alignments (MStructAs) as a complement of Multiple
Sequence Alignments (MSAs). Specifically, we illustrate
ML4LMS Workshop @ ICML 2024
the idea on the 197 wild type proteins from the ProteinGym
benchmarks as example. We introduce the process of gener-
ating MStrcutA for a target protein (Sec. 2.1) and show the
complementary effect of MStructA on MSA (Sec. 2.2).
2.1. Multiple Structure Alignment
MStructA Searching To construct MStructA for each
wild-type prote","Future work
MStructA Quality For several protein families, the cur-
rent filtering criteria remove too many potentially-relevant
sequences, resulting in many MStructAs having a very low
depth. Future work will focus on optimizing our filtering
pipeline to balance the quality of included sequences with
the number of retrieved sequences to provide optimal struc-
tural information gain. Eventually, this pipeline should be
broadly applicable, and strike that optimal balance out-of-
the-box across all protein families and experimental assays.
Since the presence of too many gaps in the alignments
will degrade the performance of evolutionary models, our
filters remove ‘fragment’ sequences with more than 50%
ML4LMS Workshop @ ICML 2024
Table 2. Results grouped by original MSA depths for EVE trained with only MSA and EVE trained with combined MSA and MStructA.
ORIGINAL MSA D EPTH EVE (MSA) EVE (MSA+MS TRUCT A) I MPROVEMENT
LOW 0.497 0.534 0.037
MEDIUM 0.425 0.424 -0.001
HIGH 0.349 0.363 0.014
Table 3. Results grouped by function types for EVE trained with only MSA and EVE trained with combined MSA and MStructA.
FUNCTION TYPE EVE (MSA) EVE (MSA+MS TRUCT A) I MPROVEMENT
ACTIVITY 0.436 0.441 0.005
BINDING 0.444 0.510 0.066
EXPRESSION 0.450 0.462 0.012
ORGANISMAL FITNESS 0.426 0.431 0.005
gaps to ensure the robustness and reliability of the model.
However, many structurally similar sequences may have
high sequence divergence, resulting in more than 50% gaps.
To address this, we need to develop better MStructA con-
struction methods that can include such structurally sim-
ilar but sequence-dissimilar proteins. Alternatively, we
could explore models that are agnostic to gaps or can uti-
lize MStructA in formats other than sequence alignments.
This will allow us to leverage the structural information
more effectively and further improve the performance of
evolutionary models.
Benchmarking Due to limited computational resources,
we benchmarked only 30 randomly selected assays that
s","approaches typically fail in recov- ering distantly related sequences with high structure and functional similarity but low sequence similarity, referred to as the midnight zone of protein similarity (Heinzinger et al., 2021). To address the limitations of traditional Multi- ple Sequence Alignments (MSAs), we explore the usage of structure similarity search tools to efficiently identify homol- ogous proteins with similar structures from large databases. The proteins identified are then aligned with the target pro- tein to create Multiple Structure Alignments (MStructAs), which can uncover insights into the blind spots of standard sequence-based MSAs. To demonstrate the effectiveness of our methods, we use 197 Deep Mutational Scanning (DMS) assays from ProteinGym (Notin et al., 2023) as ex- amples. For each target protein, we employ Foldseek to search for structurally similar proteins in the AlphaFold UniProt database and align them to construct MStructAs. We then combine the identified MSAs and MStructAs to train family-specific generative models for protein fitness prediction (Frazer et al., 2021). Our results indicate that the performance of these alignment-based approaches can be markedly improved by augmenting MSAs with sequences recovered via structure-based retrieval, paving the way for hybrid retrieval strategies that consider both sequence and structure similarities. 2. Method Inspired by recent development of fast protein structure tools, in this section, we explore the usage of Multiple Struc- ture Alignments (MStructAs) as a complement of Multiple Sequence Alignments (MSAs). Specifically, we illustrate ML4LMS Workshop @ ICML 2024 the idea on the 197 wild type proteins from the ProteinGym benchmarks as example. We introduce the process of gener- ating MStrcutA for a target protein (Sec. 2.1) and show the complementary effect of MStructA on MSA (Sec. 2.2). 2.1. Multiple Structure Alignment MStructA Searching To construct MStructA for each wild-type prote","Future work MStructA Quality For several protein families, the cur- rent filtering criteria remove too many potentially-relevant sequences, resulting in many MStructAs having a very low depth. Future work will focus on optimizing our filtering pipeline to balance the quality of included sequences with the number of retrieved sequences to provide optimal struc- tural information gain. Eventually, this pipeline should be broadly applicable, and strike that optimal balance out-of- the-box across all protein families and experimental assays. Since the presence of too many gaps in the alignments will degrade the performance of evolutionary models, our filters remove fragment sequences with more than 50% ML4LMS Workshop @ ICML 2024 Table 2. Results grouped by original MSA depths for EVE trained with only MSA and EVE trained with combined MSA and MStructA. ORIGINAL MSA D EPTH EVE (MSA) EVE (MSA+MS TRUCT A) I MPROVEMENT LOW 0.497 0.534 0.037 MEDIUM 0.425 0.424 -0.001 HIGH 0.349 0.363 0.014 Table 3. Results grouped by function types for EVE trained with only MSA and EVE trained with combined MSA and MStructA. FUNCTION TYPE EVE (MSA) EVE (MSA+MS TRUCT A) I MPROVEMENT ACTIVITY 0.436 0.441 0.005 BINDING 0.444 0.510 0.066 EXPRESSION 0.450 0.462 0.012 ORGANISMAL FITNESS 0.426 0.431 0.005 gaps to ensure the robustness and reliability of the model. However, many structurally similar sequences may have high sequence divergence, resulting in more than 50% gaps. To address this, we need to develop better MStructA con- struction methods that can include such structurally sim- ilar but sequence-dissimilar proteins. Alternatively, we could explore models that are agnostic to gaps or can uti- lize MStructA in formats other than sequence alignments. This will allow us to leverage the structural information more effectively and further improve the performance of evolutionary models. Benchmarking Due to limited computational resources, we benchmarked only 30 randomly selected assays that s","Augmenting Evolutionary Models with Structure-based Retrieval Multiple Sequence Alignments (MSAs) are crucial in protein sequence analysis for identifying homologous proteins sharing a common evolutionary origin. However, traditional MSA search tools struggle to recover distantly related sequences that, despite low sequence similarity, exhibit high structural and functional resemblance often missing in the so-called midnight zone of protein similarity. To overcome these limitations, we propose the integration of structure similarity search tools to enhance the identification of homologous proteins. This approach utilizes Foldseek to search the AlphaFold database, aligning structurally similar proteins to construct Multiple Structure Alignments (MStructAs) alongside traditional MSAs. By combining these alignments, we develop family-specific generative models for protein fitness prediction, using diverse assays from the ProteinGym benchmarks. Our findings reveal that incorporating structure-based retrieval into MSAs significantly improves the performance of alignment-based methods, suggesting a robust hybrid retrieval strategy that harnesses both sequence and structure similarities.","Augmenting Evolutionary Models with Structure-based Retrieval Multiple Sequence Alignments (MSAs) are crucial in protein sequence analysis for identifying homologous proteins sharing a common evolutionary origin. However, traditional MSA search tools struggle to recover distantly related sequences that, despite low sequence similarity, exhibit high structural and functional resemblance often missing in the so-called midnight zone of protein similarity. To overcome these limitations, we propose the integration of structure similarity search tools to enhance the identification of homologous proteins. This approach utilizes Foldseek to search the AlphaFold database, aligning structurally similar proteins to construct Multiple Structure Alignments (MStructAs) alongside traditional MSAs. By combining these alignments, we develop family-specific generative models for protein fitness prediction, using diverse assays from the ProteinGym benchmarks. Our findings reveal that incorporating structure-based retrieval into MSAs significantly improves the performance of alignment-based methods, suggesting a robust hybrid retrieval strategy that harnesses both sequence and structure similarities. Future work MStructA Quality For several protein families, the cur- rent filtering criteria remove too many potentially-relevant sequences, resulting in many MStructAs having a very low depth. Future work will focus on optimizing our filtering pipeline to balance the quality of included sequences with the number of retrieved sequences to provide optimal struc- tural information gain. Eventually, this pipeline should be broadly applicable, and strike that optimal balance out-of- the-box across all protein families and experimental assays. Since the presence of too many gaps in the alignments will degrade the performance of evolutionary models, our filters remove fragment sequences with more than 50% ML4LMS Workshop @ ICML 2024 Table 2. Results grouped by original MSA depths for EVE trained with only MSA and EVE trained with combined MSA and MStructA. ORIGINAL MSA D EPTH EVE (MSA) EVE (MSA+MS TRUCT A) I MPROVEMENT LOW 0.497 0.534 0.037 MEDIUM 0.425 0.424 -0.001 HIGH 0.349 0.363 0.014 Table 3. Results grouped by function types for EVE trained with only MSA and EVE trained with combined MSA and MStructA. FUNCTION TYPE EVE (MSA) EVE (MSA+MS TRUCT A) I MPROVEMENT ACTIVITY 0.436 0.441 0.005 BINDING 0.444 0.510 0.066 EXPRESSION 0.450 0.462 0.012 ORGANISMAL FITNESS 0.426 0.431 0.005 gaps to ensure the robustness and reliability of the model. However, many structurally similar sequences may have high sequence divergence, resulting in more than 50% gaps. To address this, we need to develop better MStructA con- struction methods that can include such structurally sim- ilar but sequence-dissimilar proteins. Alternatively, we could explore models that are agnostic to gaps or can uti- lize MStructA in formats other than sequence alignments. This will allow us to leverage the structural information more effectively and further improve the performance of evolutionary models. Benchmarking Due to limited computational resources, we benchmarked only 30 randomly selected assays that s approaches typically fail in recov- ering distantly related sequences with high structure and functional similarity but low sequence similarity, referred to as the midnight zone of protein similarity (Heinzinger et al., 2021). To address the limitations of traditional Multi- ple Sequence Alignments (MSAs), we explore the usage of structure similarity search tools to efficiently identify homol- ogous proteins with similar structures from large databases. The proteins identified are then aligned with the target pro- tein to create Multiple Structure Alignments (MStructAs), which can uncover insights into the blind spots of standard sequence-based MSAs. To demonstrate the effectiveness of our methods, we use 197 Deep Mutational Scanning (DMS) assays from ProteinGym (Notin et al., 2023) as ex- amples. For each target protein, we employ Foldseek to search for structurally similar proteins in the AlphaFold UniProt database and align them to construct MStructAs. We then combine the identified MSAs and MStructAs to train family-specific generative models for protein fitness prediction (Frazer et al., 2021). Our results indicate that the performance of these alignment-based approaches can be markedly improved by augmenting MSAs with sequences recovered via structure-based retrieval, paving the way for hybrid retrieval strategies that consider both sequence and structure similarities. 2. Method Inspired by recent development of fast protein structure tools, in this section, we explore the usage of Multiple Struc- ture Alignments (MStructAs) as a complement of Multiple Sequence Alignments (MSAs). Specifically, we illustrate ML4LMS Workshop @ ICML 2024 the idea on the 197 wild type proteins from the ProteinGym benchmarks as example. We introduce the process of gener- ating MStrcutA for a target protein (Sec. 2.1) and show the complementary effect of MStructA on MSA (Sec. 2.2). 2.1. Multiple Structure Alignment MStructA Searching To construct MStructA for each wild-type prote",0
599513927d64a2657f889362b864f93504c03a0e,Constructing artificial life and materials scientists with accelerated AI using Deep AndersoNN,"['Saleem Abdul Fattah Ahmed Al Dajani', 'David Keyes']",https://openreview.net/pdf/599513927d64a2657f889362b864f93504c03a0e.pdf,"Constructing artificial life and materials scientists with accelerated AI using Deep AndersoNN We present a novel method for constructing artificial life and material scientists to perform high-throughput density functional theory classification based on Anderson-accelerated training and inferences using deep equilibrium networks. Deep AndersoNN is a framework for accelerating AI by taking the continuum limit as the number of explicit layers in a neural network approaches infinity, and can be taken as a single implicit layer, known as a deep equilibrium model. Solving for parameters of a deep equilibrium model reduces to a nonlinear fixed point iteration problem, enabling use of vector-to-vector iterative solvers and windowing techniques, such as Anderson extrapolation, for accelerating convergence to the fixed point deep equilibrium. Here we show that Deep AndersoNN achieves up to an order of magnitude of speed-up in training and inference. The method is demonstrated on density functional theory results for industrial applications by constructing artificial life and materials `scientists' capable of classifying biomolecules, drugs, and compounds as strongly or weakly polar, sorting metal-organic frameworks by pore size, and classifying crystalline materials as metals, semiconductors, and insulators, using graph images of node-neighbor representations transformed from atom-bond networks. Results exhibit accuracy up to 98\% and showcase synergy between Deep AndersoNN and machine learning capabilities of modern computing architectures, e.g. GPUs, for accelerated computational life and materials science by quickly identifying structure-property relationships. This paves the way for saving up to 90\% of compute required for AI, reducing its carbon footprint by up to 60 gigatons per year by 2030, and scaling above memory limitations of explicit neural networks in life and materials science, and beyond.",599513927d64a2657f889362b864f93504c03a0e.pdf,"approaches
infinity, and can be taken as a single implicit layer,
known as a deep equilibrium model. Solving for
parameters of a deep equilibrium model reduces
to a nonlinear fixed point iteration problem, en-
abling use of vector-to-vector iterative solvers and
windowing techniques, such as Anderson extrap-
olation, for accelerating convergence to the fixed
point deep equilibrium. Here we show that Deep
AndersoNN achieves up to an order of magnitude
of speed-up in training and inference. The method
is demonstrated on density functional theory re-
sults for industrial applications by constructing
artificial life and materials ‘scientists’ capable of
classifying biomolecules, drugs, and compounds
as strongly or weakly polar, sorting metal-organic
frameworks by pore size, and classifying crys-
talline materials as metals, semiconductors, and
insulators, using graph images of node-neighbor
representations transformed from atom-bond net-
works. Results exhibit accuracy up to 98% and
showcase synergy between Deep AndersoNN and
machine learning capabilities of modern comput-
ing architectures, e.g. GPUs, for accelerated com-
putational life and materials science by quickly
identifying structure-property relationships. This
paves the way for saving up to 90% of compute
required for AI, reducing its carbon footprint by
up to 60 gigatons per year by 2030, and scaling
above memory limitations of explicit neural net-
works in life and materials science, and beyond.
1Applied Physics (AP) Program and Extreme Computing Re-
search Center (ECRC), Physical Science and Engineering (PSE)
and Computer, Electrical, Mathematical Sciences and Engineering
(CEMSE) Divisions, King Abdullah University of Science and
Technology (KAUST), Thuwal, Makkah Province, Kingdom of
Saudi Arabia (KSA) 23955-6900. Correspondence to: Saleem A.
Al Dajani <saleem.abdulfattah.aldajani@gmail.com >.
Accepted at the 1st Machine Learning for Life and Material Sci-
ences Workshop at ICML 2024 . Copyright 2024 by t","Future work will incorporate larger datasets to build multi-
objective optimized models, LLMs, NLSOMs, and foun-
dation models capable of making these types of inferences
for drug discovery and biocatalysis at scale, integrating life
and materials science in a novel, unprecedented way. With
these methods, training models the size of LLMs could be
democratized for academic environments without the need
to resort to industrial scale computing resources.
References
Anderson, D. G. Iterative Procedures for Nonlinear Integral
Equations. Journal of the Association for Computing
Machinery (JACM) , 12(4):547–560, 1965.
Anderson, D. G. Comments on “Anderson Acceleration,
Mixing and Extrapolation”. Numerical Algorithms , 80:
135–234, 2019.
Andrae, Anders S.G. and Edler, Tomas. On Global Elec-
tricity Usage of Communication Technology: Trends to
2030. Challenges , 6(1):117–157, 2015.
Constructing artificial life and materials scientists with accelerated AI using Deep AndersoNN
Babuji, Y ., Blaiszik, B., Brettin, T., Chard, K., Chard, R.,
Clyde, A., Foster, I., Hong, Z., Jha, S., Li, Z., et al.
Targeting SARS-CoV-2 with AI-and HPC-Enabled Lead
Generation: A First Data Release. arXiv:2006.02431 ,
2020.
Bai, S. Equilibrium Approaches to Modern Deep Learning .
PhD thesis, Carnegie Mellon University, 2022.
Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen. Deep
Equilibrium Models. Advances in Neural Information
Processing Systems , 32, 2019.
Bai, Shaojie and Koltun, Vladlen and Kolter, J Zico. Mul-
tiscale Deep Equilibrium Models. Advances in Neural
Information Processing Systems .
Bai, Shaojie and Koltun, Vladlen and Kolter, J Zico. Sta-
bilizing Equilibrium Models by Jacobian Regularization.
arXiv:2106.14342 , 2021.
Bhati, A. P., Wan, S., Alf `e, D., Clyde, A. R., Bode, M.,
Tan, L., Titov, M., Merzky, A., Turilli, M., Jha, S., et al.
Pandemic Drugs at Pandemic Speed: Infrastructure for
Accelerating COVID-19 Drug Discovery with Hybrid
Machine Learning- and Physics-Based Simulati","approaches infinity, and can be taken as a single implicit layer, known as a deep equilibrium model. Solving for parameters of a deep equilibrium model reduces to a nonlinear fixed point iteration problem, en- abling use of vector-to-vector iterative solvers and windowing techniques, such as Anderson extrap- olation, for accelerating convergence to the fixed point deep equilibrium. Here we show that Deep AndersoNN achieves up to an order of magnitude of speed-up in training and inference. The method is demonstrated on density functional theory re- sults for industrial applications by constructing artificial life and materials scientists capable of classifying biomolecules, drugs, and compounds as strongly or weakly polar, sorting metal-organic frameworks by pore size, and classifying crys- talline materials as metals, semiconductors, and insulators, using graph images of node-neighbor representations transformed from atom-bond net- works. Results exhibit accuracy up to 98% and showcase synergy between Deep AndersoNN and machine learning capabilities of modern comput- ing architectures, e.g. GPUs, for accelerated com- putational life and materials science by quickly identifying structure-property relationships. This paves the way for saving up to 90% of compute required for AI, reducing its carbon footprint by up to 60 gigatons per year by 2030, and scaling above memory limitations of explicit neural net- works in life and materials science, and beyond. 1Applied Physics (AP) Program and Extreme Computing Re- search Center (ECRC), Physical Science and Engineering (PSE) and Computer, Electrical, Mathematical Sciences and Engineering (CEMSE) Divisions, King Abdullah University of Science and Technology (KAUST), Thuwal, Makkah Province, Kingdom of Saudi Arabia (KSA) 23955-6900. Correspondence to: Saleem A. Al Dajani <saleem.abdulfattah.aldajani@gmail.com >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by t","Future work will incorporate larger datasets to build multi- objective optimized models, LLMs, NLSOMs, and foun- dation models capable of making these types of inferences for drug discovery and biocatalysis at scale, integrating life and materials science in a novel, unprecedented way. With these methods, training models the size of LLMs could be democratized for academic environments without the need to resort to industrial scale computing resources.","Constructing artificial life and materials scientists with accelerated AI using Deep AndersoNN We present a novel method for constructing artificial life and material scientists to perform high-throughput density functional theory classification based on Anderson-accelerated training and inferences using deep equilibrium networks. Deep AndersoNN is a framework for accelerating AI by taking the continuum limit as the number of explicit layers in a neural network approaches infinity, and can be taken as a single implicit layer, known as a deep equilibrium model. Solving for parameters of a deep equilibrium model reduces to a nonlinear fixed point iteration problem, enabling use of vector-to-vector iterative solvers and windowing techniques, such as Anderson extrapolation, for accelerating convergence to the fixed point deep equilibrium. Here we show that Deep AndersoNN achieves up to an order of magnitude of speed-up in training and inference. The method is demonstrated on density functional theory results for industrial applications by constructing artificial life and materials `scientists' capable of classifying biomolecules, drugs, and compounds as strongly or weakly polar, sorting metal-organic frameworks by pore size, and classifying crystalline materials as metals, semiconductors, and insulators, using graph images of node-neighbor representations transformed from atom-bond networks. Results exhibit accuracy up to 98\% and showcase synergy between Deep AndersoNN and machine learning capabilities of modern computing architectures, e.g. GPUs, for accelerated computational life and materials science by quickly identifying structure-property relationships. This paves the way for saving up to 90\% of compute required for AI, reducing its carbon footprint by up to 60 gigatons per year by 2030, and scaling above memory limitations of explicit neural networks in life and materials science, and beyond.","Constructing artificial life and materials scientists with accelerated AI using Deep AndersoNN We present a novel method for constructing artificial life and material scientists to perform high-throughput density functional theory classification based on Anderson-accelerated training and inferences using deep equilibrium networks. Deep AndersoNN is a framework for accelerating AI by taking the continuum limit as the number of explicit layers in a neural network approaches infinity, and can be taken as a single implicit layer, known as a deep equilibrium model. Solving for parameters of a deep equilibrium model reduces to a nonlinear fixed point iteration problem, enabling use of vector-to-vector iterative solvers and windowing techniques, such as Anderson extrapolation, for accelerating convergence to the fixed point deep equilibrium. Here we show that Deep AndersoNN achieves up to an order of magnitude of speed-up in training and inference. The method is demonstrated on density functional theory results for industrial applications by constructing artificial life and materials `scientists' capable of classifying biomolecules, drugs, and compounds as strongly or weakly polar, sorting metal-organic frameworks by pore size, and classifying crystalline materials as metals, semiconductors, and insulators, using graph images of node-neighbor representations transformed from atom-bond networks. Results exhibit accuracy up to 98\% and showcase synergy between Deep AndersoNN and machine learning capabilities of modern computing architectures, e.g. GPUs, for accelerated computational life and materials science by quickly identifying structure-property relationships. This paves the way for saving up to 90\% of compute required for AI, reducing its carbon footprint by up to 60 gigatons per year by 2030, and scaling above memory limitations of explicit neural networks in life and materials science, and beyond. Future work will incorporate larger datasets to build multi- objective optimized models, LLMs, NLSOMs, and foun- dation models capable of making these types of inferences for drug discovery and biocatalysis at scale, integrating life and materials science in a novel, unprecedented way. With these methods, training models the size of LLMs could be democratized for academic environments without the need to resort to industrial scale computing resources. approaches infinity, and can be taken as a single implicit layer, known as a deep equilibrium model. Solving for parameters of a deep equilibrium model reduces to a nonlinear fixed point iteration problem, en- abling use of vector-to-vector iterative solvers and windowing techniques, such as Anderson extrap- olation, for accelerating convergence to the fixed point deep equilibrium. Here we show that Deep AndersoNN achieves up to an order of magnitude of speed-up in training and inference. The method is demonstrated on density functional theory re- sults for industrial applications by constructing artificial life and materials scientists capable of classifying biomolecules, drugs, and compounds as strongly or weakly polar, sorting metal-organic frameworks by pore size, and classifying crys- talline materials as metals, semiconductors, and insulators, using graph images of node-neighbor representations transformed from atom-bond net- works. Results exhibit accuracy up to 98% and showcase synergy between Deep AndersoNN and machine learning capabilities of modern comput- ing architectures, e.g. GPUs, for accelerated com- putational life and materials science by quickly identifying structure-property relationships. This paves the way for saving up to 90% of compute required for AI, reducing its carbon footprint by up to 60 gigatons per year by 2030, and scaling above memory limitations of explicit neural net- works in life and materials science, and beyond. 1Applied Physics (AP) Program and Extreme Computing Re- search Center (ECRC), Physical Science and Engineering (PSE) and Computer, Electrical, Mathematical Sciences and Engineering (CEMSE) Divisions, King Abdullah University of Science and Technology (KAUST), Thuwal, Makkah Province, Kingdom of Saudi Arabia (KSA) 23955-6900. Correspondence to: Saleem A. Al Dajani <saleem.abdulfattah.aldajani@gmail.com >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by t",0
f9fdc8d3f9f30a57632294fc68dabff99b7a8d3d,Gene-centric evaluation of causal variant prediction for DNA models,"['Chantriolnt-Andreas Kapourani', 'Alice Del Vecchio', 'Agnieszka Dobrowolska', 'Andrew Anighoro', 'Edith M. Hessel', 'Lindsay Edwards', 'Cristian Regep']",https://openreview.net/pdf/f9fdc8d3f9f30a57632294fc68dabff99b7a8d3d.pdf,"Gene-centric evaluation of causal variant prediction for DNA models DNA models hold significant potential for linking genetic variation to transcriptional regulation, which is crucial for understanding disease mechanisms at the genetic and molecular level and developing targeted therapies. Supervised approaches, such as Enformer and Basenji, have shown promising results in predicting causal variants. Recently, self-supervised models like Nucleotide Transformer and HyenaDNA have made remarkable advancements, with variant-centric benchmarks suggesting competitive performance on the variant effect prediction task. In this study, we propose to evaluate models also on gene-centric benchmarks, which often are of higher relevance to the genetics community for mapping causal variants to affected genes.",f9fdc8d3f9f30a57632294fc68dabff99b7a8d3d.pdf,"methodology aligns with the
approaches used in Avsec et al. (2021) and Kelley (2020).
Establishing the link between variants and their respective
affected genes presents significant challenges. One major
complexity arises from Genome-Wide Association Studies
(GWAS), which often report numerous non-causal single
nucleotide polymorphisms (SNPs) due to linkage disequi-
librium, resulting in many false positives (Uffelmann et al.,
2021). Additionally, tissue expression data is often collected
separately from genetic data, necessitating the use of exter-
nal datasets, such as those from GTEx (Consortium, 2020),
for accurate association between a variant and its transcrip-
tional effect. Developing ab initio methods, or techniques
that function with minimal data, would be highly beneficial
in this context.
Gene-centric evaluation of causal variant prediction for DNA models
Figure 1. a.The challenge with variant-centric evaluation is that using the entire sequence embedding does not provide the ability to
differentiate between specific genes. b.In contrast, gene-centric evaluation utilizes only the embedding of the TSS, enabling the prediction
of individual downstream genes affected by the variant.
Despite these challenges, advancements in genomic lan-
guage models hold considerable promise for enhancing our
ability to link genetic variations to their functional conse-
quences. The integration of these models with biobank data
could lead to significant breakthroughs in understanding
the genetic basis of disease and the development of more
effective therapies.
2. Gene-centric causal variant prediction
Kao et al. (2024) released a benchmark study of current
state-of-the-art models using an established ground-truth
dataset (see Section 4.1). Their findings suggest that gLMs
exhibit similar performance on causal variant prediction
as supervised models, with a 192kbp receptive field Nu-
cleotide Transformer (Dalla-Torre et al., 2023) achieving
performance comparable to Enformer","limitations we were not able to run the Nucleotide
Transformer. To provide a second benchmark we used a new
recent model called Caduceus (Schiff et al., 2024) which is
a bi-directional equivariant version of Mamba (Gu & Dao,
2023).
3. Results
3.1. Variant-centered evaluation
Utilizing the dataset described in 4.1, we generated 1500
bp DNA windows centered on the variant’s location. The
DNA sequences for both the reference and alternate alleles
were one-hot encoded and input into a CNN architecture as
detailed in 4.2. Subsequently, the encoded DNA sequences
were concatenated and processed through an MLP. The
results of this analysis are presented in Table 1, along with
the results from HyenaDNA, Nucleotide Transformer and
Enformer from the (Kao et al., 2024).
3.2. Gene-centered evaluation
To assess the model’s performance in a gene-centric context,
the evaluation was focused on a window centred on the
gene’s TSS. While maintaining a large receptive field atinference, an embedding narrowly focused around the TSS
region was extracted. This embedding was then utilized to
train a binary classifier model to predict whether the input
variant is causal to the gene. The outcomes of this evaluation
are summarized in Table 2.
4. Materials and Methods
4.1. Ground truth dataset
For this work we used the ground truth dataset from Avsec
et al. (2021), which was also used by Kao et al. (2024).
Briefly, this dataset contains causal variants from GTEx that
have been fine-mapped using the statistical fine-mapping
method SuSiE (Wang et al., 2020). Variants with PIP score
>0.9are labelled as positives. In the download made
available by Kao et al. (2024), the affected gene has been
removed. We re-added it using the original data from (Avsec
et al., 2021). Sometimes this results in multiple gene asso-
ciations, as a variant-tissue pair can be causal for multiple
genes. The Gencode (Frankish et al., 2021) v44 annotation
was used to define the genomic location of the TSS for each
gene.
4.2.","methodology aligns with the approaches used in Avsec et al. (2021) and Kelley (2020). Establishing the link between variants and their respective affected genes presents significant challenges. One major complexity arises from Genome-Wide Association Studies (GWAS), which often report numerous non-causal single nucleotide polymorphisms (SNPs) due to linkage disequi- librium, resulting in many false positives (Uffelmann et al., 2021). Additionally, tissue expression data is often collected separately from genetic data, necessitating the use of exter- nal datasets, such as those from GTEx (Consortium, 2020), for accurate association between a variant and its transcrip- tional effect. Developing ab initio methods, or techniques that function with minimal data, would be highly beneficial in this context. Gene-centric evaluation of causal variant prediction for DNA models Figure 1. a.The challenge with variant-centric evaluation is that using the entire sequence embedding does not provide the ability to differentiate between specific genes. b.In contrast, gene-centric evaluation utilizes only the embedding of the TSS, enabling the prediction of individual downstream genes affected by the variant. Despite these challenges, advancements in genomic lan- guage models hold considerable promise for enhancing our ability to link genetic variations to their functional conse- quences. The integration of these models with biobank data could lead to significant breakthroughs in understanding the genetic basis of disease and the development of more effective therapies. 2. Gene-centric causal variant prediction Kao et al. (2024) released a benchmark study of current state-of-the-art models using an established ground-truth dataset (see Section 4.1). Their findings suggest that gLMs exhibit similar performance on causal variant prediction as supervised models, with a 192kbp receptive field Nu- cleotide Transformer (Dalla-Torre et al., 2023) achieving performance comparable to Enformer","limitations we were not able to run the Nucleotide Transformer. To provide a second benchmark we used a new recent model called Caduceus (Schiff et al., 2024) which is a bi-directional equivariant version of Mamba (Gu & Dao, 2023). 3. Results 3.1. Variant-centered evaluation Utilizing the dataset described in 4.1, we generated 1500 bp DNA windows centered on the variant s location. The DNA sequences for both the reference and alternate alleles were one-hot encoded and input into a CNN architecture as detailed in 4.2. Subsequently, the encoded DNA sequences were concatenated and processed through an MLP. The results of this analysis are presented in Table 1, along with the results from HyenaDNA, Nucleotide Transformer and Enformer from the (Kao et al., 2024). 3.2. Gene-centered evaluation To assess the model s performance in a gene-centric context, the evaluation was focused on a window centred on the gene s TSS. While maintaining a large receptive field atinference, an embedding narrowly focused around the TSS region was extracted. This embedding was then utilized to train a binary classifier model to predict whether the input variant is causal to the gene. The outcomes of this evaluation are summarized in Table 2. 4. Materials and Methods 4.1. Ground truth dataset For this work we used the ground truth dataset from Avsec et al. (2021), which was also used by Kao et al. (2024). Briefly, this dataset contains causal variants from GTEx that have been fine-mapped using the statistical fine-mapping method SuSiE (Wang et al., 2020). Variants with PIP score >0.9are labelled as positives. In the download made available by Kao et al. (2024), the affected gene has been removed. We re-added it using the original data from (Avsec et al., 2021). Sometimes this results in multiple gene asso- ciations, as a variant-tissue pair can be causal for multiple genes. The Gencode (Frankish et al., 2021) v44 annotation was used to define the genomic location of the TSS for each gene. 4.2.","Gene-centric evaluation of causal variant prediction for DNA models DNA models hold significant potential for linking genetic variation to transcriptional regulation, which is crucial for understanding disease mechanisms at the genetic and molecular level and developing targeted therapies. Supervised approaches, such as Enformer and Basenji, have shown promising results in predicting causal variants. Recently, self-supervised models like Nucleotide Transformer and HyenaDNA have made remarkable advancements, with variant-centric benchmarks suggesting competitive performance on the variant effect prediction task. In this study, we propose to evaluate models also on gene-centric benchmarks, which often are of higher relevance to the genetics community for mapping causal variants to affected genes.","Gene-centric evaluation of causal variant prediction for DNA models DNA models hold significant potential for linking genetic variation to transcriptional regulation, which is crucial for understanding disease mechanisms at the genetic and molecular level and developing targeted therapies. Supervised approaches, such as Enformer and Basenji, have shown promising results in predicting causal variants. Recently, self-supervised models like Nucleotide Transformer and HyenaDNA have made remarkable advancements, with variant-centric benchmarks suggesting competitive performance on the variant effect prediction task. In this study, we propose to evaluate models also on gene-centric benchmarks, which often are of higher relevance to the genetics community for mapping causal variants to affected genes. limitations we were not able to run the Nucleotide Transformer. To provide a second benchmark we used a new recent model called Caduceus (Schiff et al., 2024) which is a bi-directional equivariant version of Mamba (Gu & Dao, 2023). 3. Results 3.1. Variant-centered evaluation Utilizing the dataset described in 4.1, we generated 1500 bp DNA windows centered on the variant s location. The DNA sequences for both the reference and alternate alleles were one-hot encoded and input into a CNN architecture as detailed in 4.2. Subsequently, the encoded DNA sequences were concatenated and processed through an MLP. The results of this analysis are presented in Table 1, along with the results from HyenaDNA, Nucleotide Transformer and Enformer from the (Kao et al., 2024). 3.2. Gene-centered evaluation To assess the model s performance in a gene-centric context, the evaluation was focused on a window centred on the gene s TSS. While maintaining a large receptive field atinference, an embedding narrowly focused around the TSS region was extracted. This embedding was then utilized to train a binary classifier model to predict whether the input variant is causal to the gene. The outcomes of this evaluation are summarized in Table 2. 4. Materials and Methods 4.1. Ground truth dataset For this work we used the ground truth dataset from Avsec et al. (2021), which was also used by Kao et al. (2024). Briefly, this dataset contains causal variants from GTEx that have been fine-mapped using the statistical fine-mapping method SuSiE (Wang et al., 2020). Variants with PIP score >0.9are labelled as positives. In the download made available by Kao et al. (2024), the affected gene has been removed. We re-added it using the original data from (Avsec et al., 2021). Sometimes this results in multiple gene asso- ciations, as a variant-tissue pair can be causal for multiple genes. The Gencode (Frankish et al., 2021) v44 annotation was used to define the genomic location of the TSS for each gene. 4.2. methodology aligns with the approaches used in Avsec et al. (2021) and Kelley (2020). Establishing the link between variants and their respective affected genes presents significant challenges. One major complexity arises from Genome-Wide Association Studies (GWAS), which often report numerous non-causal single nucleotide polymorphisms (SNPs) due to linkage disequi- librium, resulting in many false positives (Uffelmann et al., 2021). Additionally, tissue expression data is often collected separately from genetic data, necessitating the use of exter- nal datasets, such as those from GTEx (Consortium, 2020), for accurate association between a variant and its transcrip- tional effect. Developing ab initio methods, or techniques that function with minimal data, would be highly beneficial in this context. Gene-centric evaluation of causal variant prediction for DNA models Figure 1. a.The challenge with variant-centric evaluation is that using the entire sequence embedding does not provide the ability to differentiate between specific genes. b.In contrast, gene-centric evaluation utilizes only the embedding of the TSS, enabling the prediction of individual downstream genes affected by the variant. Despite these challenges, advancements in genomic lan- guage models hold considerable promise for enhancing our ability to link genetic variations to their functional conse- quences. The integration of these models with biobank data could lead to significant breakthroughs in understanding the genetic basis of disease and the development of more effective therapies. 2. Gene-centric causal variant prediction Kao et al. (2024) released a benchmark study of current state-of-the-art models using an established ground-truth dataset (see Section 4.1). Their findings suggest that gLMs exhibit similar performance on causal variant prediction as supervised models, with a 192kbp receptive field Nu- cleotide Transformer (Dalla-Torre et al., 2023) achieving performance comparable to Enformer",0
b3cfe2153823aaf1c08c7f7291b8f3d9e87bae03,AbFlex: Predicting the conformational flexibility of antibody CDRs,"['Fabian C Spoendlin', 'Wing Ki Wong', 'Guy Georges', 'Alexander Bujotzek', 'Charlotte Deane']",https://openreview.net/pdf/b3cfe2153823aaf1c08c7f7291b8f3d9e87bae03.pdf,"AbFlex: Predicting the conformational flexibility of antibody CDRs A dataset and method to predict the conformational flexibility of antibody CDRs Proteins are highly flexible macromolecules and the ability to adapt their shape is fundamental to many functional properties. While a single, 'static' protein structure can be predicted at high accuracy, current methods are severely limited at predicting structural flexibility. A major factor limiting such predictions is the scarcity of suitable training data. Here, we focus on the functionally important antibody CDRs and related loop motifs. We implement a strategy to create a large dataset of evidence for conformational flexibility and develop AbFlex, a method able to predict CDR flexibility with high accuracy.",b3cfe2153823aaf1c08c7f7291b8f3d9e87bae03.pdf,"approach of mining the
protein data bank (PDB) (Berman et al., 2002). In this
way, we create a large dataset of the flexibility of CDRs
and related protein loops which collects the structures of
all conformations observed in crystal structures. While
some loops adopt multiple conformational states, others
are always observed in an identical conformation (Figure 1,
Panel A-B). Using the dataset, we develop AbFlex, a method
that accurately predicts the flexibility of CDRs from inputs
of either crystal structures or predicted structural models
(Figure 1, Panel C).
AbFlex: Predicting the conformational flexibility of antibody CDRs
A B
C
Rigid Flexible
Extract loop and context E(n) -EGNN Binary classification
Figure 1. Overview of the AbFlex method. A) Example of a ‘flexible’ protein loop which is observed to adopt multiple conformations.
12 crystal structures of loops identical in sequence are overlaid. B) Example of a ‘rigid’ loop which adopts a single conformation. 14
crystal structures of sequence identical loops are overlaid. C) Flowchart detailing the AbFlex method predicting the conformational
flexibility of CDR loops. The structure and sequence of a loop (cyan) and its context (grey) are extracted from a PDB file and a graph
representation is generated. A three-layer E(n)-EGNN iteratively updates the node features, followed by binary classification of the loop
as conformationally flexible or rigid.
2. Dataset
The amount of data on the conformational flexibility of
proteins is limited which makes it difficult to train or even
benchmark methods. Evidence on conformational flexibility
can be obtained from crystal structures. Although X-ray
crystallography does not directly capture molecular mo-
tions, low-energy conformations of a protein should appear
in structures solved under different conditions. Therefore,
determining structural flexibility from crystallographic data
requires multiple solved structures of a protein which re-
stricts the number of available data","Conclusions
Conformational changes give rise to functional properties
of many classes of proteins (Teilum et al., 2009). Current
machine learning tools do not capture structural flexibility
well (Riccabona et al., 2024; Jing et al., 2024). A mainfactor that has limited methods development in this space is
the absence of large datasets necessary to train and evaluate
models. Here, we focus on the conformational flexibility
of antibody CDRs, a functionally highly important protein
motif. We mine the PDB (Berman et al., 2002) and SAbDab
(Dunbar et al., 2014) for crystallographic evidence of the
conformational flexibility of CDRs and structurally related
loops across all classes of proteins. Through this approach
we create a large dataset set of more than 20,000 loop motifs
with determined flexibility.
We develop AbFlex which shows strong predictive power
for classifying if antibody CDRs are able to transition be-
tween multiple conformational states or consistently adopt
a single stable conformation. Our method substantially out-
performs AF2-based alternative which have previously been
described as predictors of protein flexibility (Jumper et al.,
2021; del Alamo et al., 2022). By training on crystal struc-
ture data, we eliminate potential artefacts originating from
methods, e.g. MD, that approximate the flexibility of pro-
teins through simulation. The conformational flexibility of
CDRs affects functional properties of the antibody including
affinity (Mikolajek et al., 2022) and specificity (Guthmiller
et al., 2020; James et al., 2003), which are key properties
that need to be optimise in therapeutic drugs. AbFlex, there-
fore, adds a valuable tool to investigate antibody function
and assist the drug discovery process.
Furthermore, this work highlights biophysical factors that
influence the conformational flexibility of protein loop mo-
tifs. While sequence affects the tendency to adopt multiple
conformations, we identify the arrangement of residues in
the surroundi","approach of mining the protein data bank (PDB) (Berman et al., 2002). In this way, we create a large dataset of the flexibility of CDRs and related protein loops which collects the structures of all conformations observed in crystal structures. While some loops adopt multiple conformational states, others are always observed in an identical conformation (Figure 1, Panel A-B). Using the dataset, we develop AbFlex, a method that accurately predicts the flexibility of CDRs from inputs of either crystal structures or predicted structural models (Figure 1, Panel C). AbFlex: Predicting the conformational flexibility of antibody CDRs A B C Rigid Flexible Extract loop and context E(n) -EGNN Binary classification Figure 1. Overview of the AbFlex method. A) Example of a flexible protein loop which is observed to adopt multiple conformations. 12 crystal structures of loops identical in sequence are overlaid. B) Example of a rigid loop which adopts a single conformation. 14 crystal structures of sequence identical loops are overlaid. C) Flowchart detailing the AbFlex method predicting the conformational flexibility of CDR loops. The structure and sequence of a loop (cyan) and its context (grey) are extracted from a PDB file and a graph representation is generated. A three-layer E(n)-EGNN iteratively updates the node features, followed by binary classification of the loop as conformationally flexible or rigid. 2. Dataset The amount of data on the conformational flexibility of proteins is limited which makes it difficult to train or even benchmark methods. Evidence on conformational flexibility can be obtained from crystal structures. Although X-ray crystallography does not directly capture molecular mo- tions, low-energy conformations of a protein should appear in structures solved under different conditions. Therefore, determining structural flexibility from crystallographic data requires multiple solved structures of a protein which re- stricts the number of available data","Conclusions Conformational changes give rise to functional properties of many classes of proteins (Teilum et al., 2009). Current machine learning tools do not capture structural flexibility well (Riccabona et al., 2024; Jing et al., 2024). A mainfactor that has limited methods development in this space is the absence of large datasets necessary to train and evaluate models. Here, we focus on the conformational flexibility of antibody CDRs, a functionally highly important protein motif. We mine the PDB (Berman et al., 2002) and SAbDab (Dunbar et al., 2014) for crystallographic evidence of the conformational flexibility of CDRs and structurally related loops across all classes of proteins. Through this approach we create a large dataset set of more than 20,000 loop motifs with determined flexibility. We develop AbFlex which shows strong predictive power for classifying if antibody CDRs are able to transition be- tween multiple conformational states or consistently adopt a single stable conformation. Our method substantially out- performs AF2-based alternative which have previously been described as predictors of protein flexibility (Jumper et al., 2021; del Alamo et al., 2022). By training on crystal struc- ture data, we eliminate potential artefacts originating from methods, e.g. MD, that approximate the flexibility of pro- teins through simulation. The conformational flexibility of CDRs affects functional properties of the antibody including affinity (Mikolajek et al., 2022) and specificity (Guthmiller et al., 2020; James et al., 2003), which are key properties that need to be optimise in therapeutic drugs. AbFlex, there- fore, adds a valuable tool to investigate antibody function and assist the drug discovery process. Furthermore, this work highlights biophysical factors that influence the conformational flexibility of protein loop mo- tifs. While sequence affects the tendency to adopt multiple conformations, we identify the arrangement of residues in the surroundi","AbFlex: Predicting the conformational flexibility of antibody CDRs A dataset and method to predict the conformational flexibility of antibody CDRs Proteins are highly flexible macromolecules and the ability to adapt their shape is fundamental to many functional properties. While a single, 'static' protein structure can be predicted at high accuracy, current methods are severely limited at predicting structural flexibility. A major factor limiting such predictions is the scarcity of suitable training data. Here, we focus on the functionally important antibody CDRs and related loop motifs. We implement a strategy to create a large dataset of evidence for conformational flexibility and develop AbFlex, a method able to predict CDR flexibility with high accuracy.","AbFlex: Predicting the conformational flexibility of antibody CDRs A dataset and method to predict the conformational flexibility of antibody CDRs Proteins are highly flexible macromolecules and the ability to adapt their shape is fundamental to many functional properties. While a single, 'static' protein structure can be predicted at high accuracy, current methods are severely limited at predicting structural flexibility. A major factor limiting such predictions is the scarcity of suitable training data. Here, we focus on the functionally important antibody CDRs and related loop motifs. We implement a strategy to create a large dataset of evidence for conformational flexibility and develop AbFlex, a method able to predict CDR flexibility with high accuracy. Conclusions Conformational changes give rise to functional properties of many classes of proteins (Teilum et al., 2009). Current machine learning tools do not capture structural flexibility well (Riccabona et al., 2024; Jing et al., 2024). A mainfactor that has limited methods development in this space is the absence of large datasets necessary to train and evaluate models. Here, we focus on the conformational flexibility of antibody CDRs, a functionally highly important protein motif. We mine the PDB (Berman et al., 2002) and SAbDab (Dunbar et al., 2014) for crystallographic evidence of the conformational flexibility of CDRs and structurally related loops across all classes of proteins. Through this approach we create a large dataset set of more than 20,000 loop motifs with determined flexibility. We develop AbFlex which shows strong predictive power for classifying if antibody CDRs are able to transition be- tween multiple conformational states or consistently adopt a single stable conformation. Our method substantially out- performs AF2-based alternative which have previously been described as predictors of protein flexibility (Jumper et al., 2021; del Alamo et al., 2022). By training on crystal struc- ture data, we eliminate potential artefacts originating from methods, e.g. MD, that approximate the flexibility of pro- teins through simulation. The conformational flexibility of CDRs affects functional properties of the antibody including affinity (Mikolajek et al., 2022) and specificity (Guthmiller et al., 2020; James et al., 2003), which are key properties that need to be optimise in therapeutic drugs. AbFlex, there- fore, adds a valuable tool to investigate antibody function and assist the drug discovery process. Furthermore, this work highlights biophysical factors that influence the conformational flexibility of protein loop mo- tifs. While sequence affects the tendency to adopt multiple conformations, we identify the arrangement of residues in the surroundi approach of mining the protein data bank (PDB) (Berman et al., 2002). In this way, we create a large dataset of the flexibility of CDRs and related protein loops which collects the structures of all conformations observed in crystal structures. While some loops adopt multiple conformational states, others are always observed in an identical conformation (Figure 1, Panel A-B). Using the dataset, we develop AbFlex, a method that accurately predicts the flexibility of CDRs from inputs of either crystal structures or predicted structural models (Figure 1, Panel C). AbFlex: Predicting the conformational flexibility of antibody CDRs A B C Rigid Flexible Extract loop and context E(n) -EGNN Binary classification Figure 1. Overview of the AbFlex method. A) Example of a flexible protein loop which is observed to adopt multiple conformations. 12 crystal structures of loops identical in sequence are overlaid. B) Example of a rigid loop which adopts a single conformation. 14 crystal structures of sequence identical loops are overlaid. C) Flowchart detailing the AbFlex method predicting the conformational flexibility of CDR loops. The structure and sequence of a loop (cyan) and its context (grey) are extracted from a PDB file and a graph representation is generated. A three-layer E(n)-EGNN iteratively updates the node features, followed by binary classification of the loop as conformationally flexible or rigid. 2. Dataset The amount of data on the conformational flexibility of proteins is limited which makes it difficult to train or even benchmark methods. Evidence on conformational flexibility can be obtained from crystal structures. Although X-ray crystallography does not directly capture molecular mo- tions, low-energy conformations of a protein should appear in structures solved under different conditions. Therefore, determining structural flexibility from crystallographic data requires multiple solved structures of a protein which re- stricts the number of available data",0
fcf73b558f85e013ecedc671ee9ed5441dc9730f,Scoreformer: A Surrogate Model For Large-Scale Prediction of Docking Scores,"['Alvaro Ciudad Serrano', 'Adrian Morales-Pastor', 'Laura Malo', 'Isaac Filella-Merce', 'VICTOR GUALLAR', 'Alexis Molina']",https://openreview.net/pdf/fcf73b558f85e013ecedc671ee9ed5441dc9730f.pdf,"Scoreformer: A Surrogate Model For Large-Scale Prediction of Docking Scores We propose a graph transformer architecture as surrogate model for fast and accurate docking score predictions and molecular hit recovery. In this study, we present ScoreFormer, a novel graph transformer model designed to accurately predict molecular docking scores, thereby optimizing high-throughput virtual screening (HTVS) in drug discovery. The architecture integrates Principal Neighborhood Aggregation (PNA) and Learnable Random Walk Positional Encodings (LRWPE), enhancing the model's ability to understand complex molecular structures and their relationship with their respective docking scores. This approach significantly surpasses traditional HTVS methods and recent Graph Neural Network (GNN) models in both recovery and efficiency due to a wider coverage of the chemical space and enhanced performance. Our results demonstrate that ScoreFormer achieves competitive performance in docking score prediction and offers a substantial 1.65-fold reduction in inference time compared to existing models. We evaluated ScoreFormer across multiple datasets under various conditions, confirming its robustness and reliability in identifying potential drug candidates rapidly.",fcf73b558f85e013ecedc671ee9ed5441dc9730f.pdf,"approach significantly surpasses traditional
HTVS methods and recent Graph Neural Net-
work (GNN) models in both recovery and effi-
ciency due to a wider coverage of the chemical
space and enhanced performance. Our results
demonstrate that ScoreFormer achieves competi-
tive performance in docking score prediction and
offers a substantial 1.65-fold reduction in infer-
ence time compared to existing models. We eval-
uated ScoreFormer across multiple datasets under
various conditions, confirming its robustness and
reliability in identifying potential drug candidates
rapidly.
1. Introduction
Virtual screening (VS) is a key computational technique in
drug discovery, utilized for evaluating the potential binding
affinity of numerous molecules with target proteins. The
main aim of VS is to identify molecules with potential inter-
action capabilities, thereby streamlining the drug discovery
process and reducing the need for extensive, costly exper-
1Department of Artificial Intelligence, Nostrum Biodiscov-
ery S.L., Barcelona, Spain2Department of Drug Discovery,
Nostrum Biodiscovery S.L., Barcelona, Spain3Electronic and
Atomic Protein Modelling Group, Barcelona Supercomputer
Center, Barcelona, Spain4Instituci ´o Catalana de Recerca i Es-
tudis Avan c ¸ats (ICREA), Barcelona, Spain. Correspondence
to:´Alvaro Ciudad <alvaro.ciudad@nostrumbiodiscovery.com >,
Alexis Molina <alexis.molina@nostrumbiodiscovery.com >.
Accepted at the 1st Machine Learning for Life and Material Sci-
ences Workshop at ICML 2024 . Copyright 2024 by the author(s).imental assays (Lavecchia & Di Giovanni, 2013). HTVS
represents an advanced and more efficient form of VS. Char-
acterized by its ability to rapidly assess vast molecular li-
braries, HTVS significantly contributes to identifying poten-
tial biological interactions.
The evolution of HTVS has been significantly influenced
by the advancement of combinatorial chemistry, enabling
the generation of extensive and diverse compound libraries.
Previousl","Conclusions
In this study, we introduce ScoreFormer and its variant
L-ScoreFormer, graph transformer models designed to over-
come limitations in current approaches for surrogate mod-
els in HTVS. ScoreFormer’s architecture, incorporating an
attention mechanism, effectively manages global and lo-
cal molecular information, leading to superior performance
in identifying high-affinity compounds. L-ScoreFormer,
with fewer parameters, shows strong performance in general
docking score prediction, reducing overfitting risks. A keyTable 3. Metrics obtained on generated molecules. Reference
configuration corresponds to constrained ScoreFormer results on
CDK2 on Table 1. Best values, excluding the reference, are high-
lighted in bold .
CONFIGURATION RES AURTC 0.01AURTC 0.001
SCORE FORMER 0.458 0.344 0.359
L-S CORE FORMER 0.449 0.358 0.336
FILM V2 0.431 0.333 0.314
REFERENCE 0.746 0.653 0.735
feature is the improved inference speed, with a 1.86-fold
increase, vital for high-throughput virtual screening. Future
work will focus on applying ScoreFormer in active learning
contexts, integrating explicability modules, and adopting un-
certainty estimation methods, aiming to further enhance its
utility in drug discovery and contribute to the advancement
of computational chemistry with more reliable, efficient,
and interpretable methods.
References
Agrawal, V ., Gentile, F., Hsing, M., Ban, F., and Cherkasov,
A. Progressive docking-deep learning based approach for
accelerated virtual screening. In International Conference
on Artificial Neural Networks , pp. 737–740. Springer,
2019.
Scoreformer: A Surrogate Model For Large-Scale Prediction of Docking Scores
Akiba, T., Sano, S., Yanase, T., Ohta, T., and Koyama, M.
Optuna: A next-generation hyperparameter optimization
framework. In The 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining , pp.
2623–2631, 2019.
Bettayeb, K., Tirado, O. M., Marionneau-Lambot, S.,
Ferandin, Y ., Lozach, O., Morris, J. C., Mateo-Lo","approach significantly surpasses traditional HTVS methods and recent Graph Neural Net- work (GNN) models in both recovery and effi- ciency due to a wider coverage of the chemical space and enhanced performance. Our results demonstrate that ScoreFormer achieves competi- tive performance in docking score prediction and offers a substantial 1.65-fold reduction in infer- ence time compared to existing models. We eval- uated ScoreFormer across multiple datasets under various conditions, confirming its robustness and reliability in identifying potential drug candidates rapidly. 1. Introduction Virtual screening (VS) is a key computational technique in drug discovery, utilized for evaluating the potential binding affinity of numerous molecules with target proteins. The main aim of VS is to identify molecules with potential inter- action capabilities, thereby streamlining the drug discovery process and reducing the need for extensive, costly exper- 1Department of Artificial Intelligence, Nostrum Biodiscov- ery S.L., Barcelona, Spain2Department of Drug Discovery, Nostrum Biodiscovery S.L., Barcelona, Spain3Electronic and Atomic Protein Modelling Group, Barcelona Supercomputer Center, Barcelona, Spain4Instituci o Catalana de Recerca i Es- tudis Avan c ats (ICREA), Barcelona, Spain. Correspondence to: Alvaro Ciudad <alvaro.ciudad@nostrumbiodiscovery.com >, Alexis Molina <alexis.molina@nostrumbiodiscovery.com >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).imental assays (Lavecchia & Di Giovanni, 2013). HTVS represents an advanced and more efficient form of VS. Char- acterized by its ability to rapidly assess vast molecular li- braries, HTVS significantly contributes to identifying poten- tial biological interactions. The evolution of HTVS has been significantly influenced by the advancement of combinatorial chemistry, enabling the generation of extensive and diverse compound libraries. Previousl","Conclusions In this study, we introduce ScoreFormer and its variant L-ScoreFormer, graph transformer models designed to over- come limitations in current approaches for surrogate mod- els in HTVS. ScoreFormer s architecture, incorporating an attention mechanism, effectively manages global and lo- cal molecular information, leading to superior performance in identifying high-affinity compounds. L-ScoreFormer, with fewer parameters, shows strong performance in general docking score prediction, reducing overfitting risks. A keyTable 3. Metrics obtained on generated molecules. Reference configuration corresponds to constrained ScoreFormer results on CDK2 on Table 1. Best values, excluding the reference, are high- lighted in bold . CONFIGURATION RES AURTC 0.01AURTC 0.001 SCORE FORMER 0.458 0.344 0.359 L-S CORE FORMER 0.449 0.358 0.336 FILM V2 0.431 0.333 0.314 REFERENCE 0.746 0.653 0.735 feature is the improved inference speed, with a 1.86-fold increase, vital for high-throughput virtual screening. Future work will focus on applying ScoreFormer in active learning contexts, integrating explicability modules, and adopting un- certainty estimation methods, aiming to further enhance its utility in drug discovery and contribute to the advancement of computational chemistry with more reliable, efficient, and interpretable methods.","Scoreformer: A Surrogate Model For Large-Scale Prediction of Docking Scores We propose a graph transformer architecture as surrogate model for fast and accurate docking score predictions and molecular hit recovery. In this study, we present ScoreFormer, a novel graph transformer model designed to accurately predict molecular docking scores, thereby optimizing high-throughput virtual screening (HTVS) in drug discovery. The architecture integrates Principal Neighborhood Aggregation (PNA) and Learnable Random Walk Positional Encodings (LRWPE), enhancing the model's ability to understand complex molecular structures and their relationship with their respective docking scores. This approach significantly surpasses traditional HTVS methods and recent Graph Neural Network (GNN) models in both recovery and efficiency due to a wider coverage of the chemical space and enhanced performance. Our results demonstrate that ScoreFormer achieves competitive performance in docking score prediction and offers a substantial 1.65-fold reduction in inference time compared to existing models. We evaluated ScoreFormer across multiple datasets under various conditions, confirming its robustness and reliability in identifying potential drug candidates rapidly.","Scoreformer: A Surrogate Model For Large-Scale Prediction of Docking Scores We propose a graph transformer architecture as surrogate model for fast and accurate docking score predictions and molecular hit recovery. In this study, we present ScoreFormer, a novel graph transformer model designed to accurately predict molecular docking scores, thereby optimizing high-throughput virtual screening (HTVS) in drug discovery. The architecture integrates Principal Neighborhood Aggregation (PNA) and Learnable Random Walk Positional Encodings (LRWPE), enhancing the model's ability to understand complex molecular structures and their relationship with their respective docking scores. This approach significantly surpasses traditional HTVS methods and recent Graph Neural Network (GNN) models in both recovery and efficiency due to a wider coverage of the chemical space and enhanced performance. Our results demonstrate that ScoreFormer achieves competitive performance in docking score prediction and offers a substantial 1.65-fold reduction in inference time compared to existing models. We evaluated ScoreFormer across multiple datasets under various conditions, confirming its robustness and reliability in identifying potential drug candidates rapidly. Conclusions In this study, we introduce ScoreFormer and its variant L-ScoreFormer, graph transformer models designed to over- come limitations in current approaches for surrogate mod- els in HTVS. ScoreFormer s architecture, incorporating an attention mechanism, effectively manages global and lo- cal molecular information, leading to superior performance in identifying high-affinity compounds. L-ScoreFormer, with fewer parameters, shows strong performance in general docking score prediction, reducing overfitting risks. A keyTable 3. Metrics obtained on generated molecules. Reference configuration corresponds to constrained ScoreFormer results on CDK2 on Table 1. Best values, excluding the reference, are high- lighted in bold . CONFIGURATION RES AURTC 0.01AURTC 0.001 SCORE FORMER 0.458 0.344 0.359 L-S CORE FORMER 0.449 0.358 0.336 FILM V2 0.431 0.333 0.314 REFERENCE 0.746 0.653 0.735 feature is the improved inference speed, with a 1.86-fold increase, vital for high-throughput virtual screening. Future work will focus on applying ScoreFormer in active learning contexts, integrating explicability modules, and adopting un- certainty estimation methods, aiming to further enhance its utility in drug discovery and contribute to the advancement of computational chemistry with more reliable, efficient, and interpretable methods. approach significantly surpasses traditional HTVS methods and recent Graph Neural Net- work (GNN) models in both recovery and effi- ciency due to a wider coverage of the chemical space and enhanced performance. Our results demonstrate that ScoreFormer achieves competi- tive performance in docking score prediction and offers a substantial 1.65-fold reduction in infer- ence time compared to existing models. We eval- uated ScoreFormer across multiple datasets under various conditions, confirming its robustness and reliability in identifying potential drug candidates rapidly. 1. Introduction Virtual screening (VS) is a key computational technique in drug discovery, utilized for evaluating the potential binding affinity of numerous molecules with target proteins. The main aim of VS is to identify molecules with potential inter- action capabilities, thereby streamlining the drug discovery process and reducing the need for extensive, costly exper- 1Department of Artificial Intelligence, Nostrum Biodiscov- ery S.L., Barcelona, Spain2Department of Drug Discovery, Nostrum Biodiscovery S.L., Barcelona, Spain3Electronic and Atomic Protein Modelling Group, Barcelona Supercomputer Center, Barcelona, Spain4Instituci o Catalana de Recerca i Es- tudis Avan c ats (ICREA), Barcelona, Spain. Correspondence to: Alvaro Ciudad <alvaro.ciudad@nostrumbiodiscovery.com >, Alexis Molina <alexis.molina@nostrumbiodiscovery.com >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).imental assays (Lavecchia & Di Giovanni, 2013). HTVS represents an advanced and more efficient form of VS. Char- acterized by its ability to rapidly assess vast molecular li- braries, HTVS significantly contributes to identifying poten- tial biological interactions. The evolution of HTVS has been significantly influenced by the advancement of combinatorial chemistry, enabling the generation of extensive and diverse compound libraries. Previousl",0
5659bdf0d9878d42859834c9054fd1cee0bfcbe3,On the Effectiveness of Quantum Chemistry Pre-training for Pharmacological Property Prediction,[],https://openreview.net/pdf/5659bdf0d9878d42859834c9054fd1cee0bfcbe3.pdf,"On the Effectiveness of Quantum Chemistry Pre-training for Pharmacological Property Prediction We have shown that quantum chemistry pre-training is effective for pharmacological property prediction. In principle, quantum chemistry allows us to quantify all electronic and geometric properties of molecules and their interactions. Thus, incorporating pre-calculated quantum mechanical properties into deep learning models could improve their ability to predict important pharmacological properties of small molecules and potential drugs. However, this opportunity has been under-exploited in the recent wave of AI-driven drug discovery. We show that by pre-training Equivariant Graph Neural Network (EGNN) models to predict atom-centered partial charges, that have been pre-calculated using quantum mechanical methods, we can obtain more accurate models to predict absorption, distribution, metabolism, excretion, and toxicological (ADMET) properties. We compared the performance of quantum chemistry pre-training against non-quantum mechanics-based pre-training and with no pre-training at all, and found quantum chemistry pre-training to produce the most accurate models for lipophilicity, blood-brain barrier penetration, metabolism by CYP2D6, and toxicity; and very similar performance to non-pre-trained models for the much more challenging task of hepatocyte clearance prediction. By using our quantum chemistry-based pre-training approach to predict both atom-level and molecule-level properties, we obtain richer representations of the molecules than without pre-training, helping our models to learn from the underlying physics and chemistry.",5659bdf0d9878d42859834c9054fd1cee0bfcbe3.pdf,"Methodology
We used the Equivariant Graph Neural Network or EGNN
(Satorras et al., 2022) to encode each molecule’s 3D ge-ometry; and the GraphSAGE (Hamilton et al., 2018) graph
neural network to encode the molecule as a 2D graph. We
used the same model architectures for both pre-training and
downstream fine-tuning to investigate the effectiveness of
quantum chemistry pre-training. We used the EGNN and
GraphSAGE models for ADMET property prediction in
three training regimes:
•No pre-training ,i.e., direct prediction of each ADMET
property;
•Non-quantum chemical pre-training to predict ‘topo-
logical’ Gasteiger partial charges (Gasteiger & Marsili,
1980) using GraphSAGE; and
•Quantum chemical pre-training to predict: (i) quantum
mechanical partial charges, namely Mulliken (Mul-
liken, 1955) or L ¨owdin (L ¨owdin, 1970) charges; and
(ii) the HOMO-LUMO gap of the molecule’s highest
occupied and lowest unoccupied molecular orbital (a
measure of its chemical reactivity).
In the last two cases, the resulting molecular embedding is
used as input to a fine-tuning phase to predict the desired
ADMET property.
The electron distribution in a molecule allows us to under-
stand the tendency for certain intermolecular interactions to
occur. One way to approximate the electronic distribution
in a molecule at an atomic level is via atom-centred partial
charges. Thus we chose to pre-train the models on partial
charges to attain node or atom-level embeddings. There
are two distinct classes of partial charges: non-quantum
mechanical (QM) and QM-based partial charges.
2.1.Control: Pre-training on Gasteiger Partial Charges
The Partial Equalization of Orbital Electronegativity (PEOE)
method by Gasteiger & Marsili (1980) assumes that the
electronegativity, χv, of an atom type, v, is a quadratic
function of the atomic partial charge:
χv=av+bv(qv) +cv(qv)2(1)
where qvis the partial charge, and av, bv, andcvare coeffi-
cients to be optimized. According to Mulliken (1934), the
electronegativity","Future Work
We have shown that quantum chemistry pre-training is effec-
tive for ADMET property prediction. Pre-training on both
molecular and atomic labels such as HOMO-LUMO gap
and partial charges, respectively, leads to better performance
on most downstream tasks. We encourage the computational
drug discovery community to start using quantum chemical
5The corresponding downstream dataset sizes for atom-types
found in QM9 and QMugs are given in Table 5.
5
On the Effectiveness of Quantum Chemistry Pre-training for Pharmacological Property Prediction
Type of pre-trainingDownstream datasets
Absorption Distribution Metabolism Excretion Toxicity
Lipophilicity, AstraZeneca
RMSE (logD units) ↓BBB
AUROC ↑CYP2D6-Substrate
AUROC ↑Clearance-Hepatocyte
Spearman correlation coefficient ↑Acute Toxicity LD 50
RMSE (log[1/(mol/kg)] units) ↓
None - GraphSAGE 0.867±0.052 0.534±0.112 0.573±0.066 0.283±0.095 0.798±0.053
None -EGNN 0.767±0.069 0.806±0.042 0.502±0.006 0.432±0.094 0.802±0.055
Gasteiger - GraphSAGE 0.881±0.042 0.524±0.108 0.489±0.116 0.353±0.144 0.771±0.077
Mulliken only - EGNN 0.715±0.041 0.743±0.176 0.778±0.079 0.417±0.130 0.705±0.067
Mulliken + Gap - EGNN 0.707±0.040 0.647±0.133 0.878±0.069 0.410±0.176 0.700±0.070
Table 1. A comparison of non-pre-trained models against QM9 pre-trained models ( ↑higher the better, ↓lower the better )
Type of pre-trainingDownstream datasets
Absorption Distribution Metabolism Excretion Toxicity
Lipophilicity, AstraZeneca
RMSE (logD units) ↓BBB
AUROC ↑CYP2D6-Substrate
AUROC ↑Clearance-Hepatocyte
Spearman correlation coefficient ↑Acute Toxicity LD 50
RMSE (log[1/(mol/kg)] units) ↓
None - EGNN 0.767±0.069 0.806±0.042 0.502±0.006 0.432±0.094 0.802±0.055
Mulliken only - EGNN 0.760±0.053 0.864±0.032 0.771±0.067 0.413±0.071 0.779±0.040
L¨owdin only - EGNN 0.724±0.047 0.863±0.030 0.766±0.059 0.418±0.061 0.779±0.039
Mulliken + Gap - EGNN 0.731±0.037 0.860±0.031 0.769±0.068 0.400±0.091 0.781±0.042
L¨owdin + Gap - EGNN 0.688±0.039 0.861±0.029 0.767","Methodology We used the Equivariant Graph Neural Network or EGNN (Satorras et al., 2022) to encode each molecule s 3D ge-ometry; and the GraphSAGE (Hamilton et al., 2018) graph neural network to encode the molecule as a 2D graph. We used the same model architectures for both pre-training and downstream fine-tuning to investigate the effectiveness of quantum chemistry pre-training. We used the EGNN and GraphSAGE models for ADMET property prediction in three training regimes: No pre-training ,i.e., direct prediction of each ADMET property; Non-quantum chemical pre-training to predict topo- logical Gasteiger partial charges (Gasteiger & Marsili, 1980) using GraphSAGE; and Quantum chemical pre-training to predict: (i) quantum mechanical partial charges, namely Mulliken (Mul- liken, 1955) or L owdin (L owdin, 1970) charges; and (ii) the HOMO-LUMO gap of the molecule s highest occupied and lowest unoccupied molecular orbital (a measure of its chemical reactivity). In the last two cases, the resulting molecular embedding is used as input to a fine-tuning phase to predict the desired ADMET property. The electron distribution in a molecule allows us to under- stand the tendency for certain intermolecular interactions to occur. One way to approximate the electronic distribution in a molecule at an atomic level is via atom-centred partial charges. Thus we chose to pre-train the models on partial charges to attain node or atom-level embeddings. There are two distinct classes of partial charges: non-quantum mechanical (QM) and QM-based partial charges. 2.1.Control: Pre-training on Gasteiger Partial Charges The Partial Equalization of Orbital Electronegativity (PEOE) method by Gasteiger & Marsili (1980) assumes that the electronegativity, v, of an atom type, v, is a quadratic function of the atomic partial charge: v=av+bv(qv) +cv(qv)2(1) where qvis the partial charge, and av, bv, andcvare coeffi- cients to be optimized. According to Mulliken (1934), the electronegativity","Future Work We have shown that quantum chemistry pre-training is effec- tive for ADMET property prediction. Pre-training on both molecular and atomic labels such as HOMO-LUMO gap and partial charges, respectively, leads to better performance on most downstream tasks. We encourage the computational drug discovery community to start using quantum chemical 5The corresponding downstream dataset sizes for atom-types found in QM9 and QMugs are given in Table 5. 5 On the Effectiveness of Quantum Chemistry Pre-training for Pharmacological Property Prediction Type of pre-trainingDownstream datasets Absorption Distribution Metabolism Excretion Toxicity Lipophilicity, AstraZeneca RMSE (logD units) BBB AUROC CYP2D6-Substrate AUROC Clearance-Hepatocyte Spearman correlation coefficient Acute Toxicity LD 50 RMSE (log[1/(mol/kg)] units) None - GraphSAGE 0.867 0.052 0.534 0.112 0.573 0.066 0.283 0.095 0.798 0.053 None -EGNN 0.767 0.069 0.806 0.042 0.502 0.006 0.432 0.094 0.802 0.055 Gasteiger - GraphSAGE 0.881 0.042 0.524 0.108 0.489 0.116 0.353 0.144 0.771 0.077 Mulliken only - EGNN 0.715 0.041 0.743 0.176 0.778 0.079 0.417 0.130 0.705 0.067 Mulliken + Gap - EGNN 0.707 0.040 0.647 0.133 0.878 0.069 0.410 0.176 0.700 0.070 Table 1. A comparison of non-pre-trained models against QM9 pre-trained models ( higher the better, lower the better ) Type of pre-trainingDownstream datasets Absorption Distribution Metabolism Excretion Toxicity Lipophilicity, AstraZeneca RMSE (logD units) BBB AUROC CYP2D6-Substrate AUROC Clearance-Hepatocyte Spearman correlation coefficient Acute Toxicity LD 50 RMSE (log[1/(mol/kg)] units) None - EGNN 0.767 0.069 0.806 0.042 0.502 0.006 0.432 0.094 0.802 0.055 Mulliken only - EGNN 0.760 0.053 0.864 0.032 0.771 0.067 0.413 0.071 0.779 0.040 L owdin only - EGNN 0.724 0.047 0.863 0.030 0.766 0.059 0.418 0.061 0.779 0.039 Mulliken + Gap - EGNN 0.731 0.037 0.860 0.031 0.769 0.068 0.400 0.091 0.781 0.042 L owdin + Gap - EGNN 0.688 0.039 0.861 0.029 0.767","On the Effectiveness of Quantum Chemistry Pre-training for Pharmacological Property Prediction We have shown that quantum chemistry pre-training is effective for pharmacological property prediction. In principle, quantum chemistry allows us to quantify all electronic and geometric properties of molecules and their interactions. Thus, incorporating pre-calculated quantum mechanical properties into deep learning models could improve their ability to predict important pharmacological properties of small molecules and potential drugs. However, this opportunity has been under-exploited in the recent wave of AI-driven drug discovery. We show that by pre-training Equivariant Graph Neural Network (EGNN) models to predict atom-centered partial charges, that have been pre-calculated using quantum mechanical methods, we can obtain more accurate models to predict absorption, distribution, metabolism, excretion, and toxicological (ADMET) properties. We compared the performance of quantum chemistry pre-training against non-quantum mechanics-based pre-training and with no pre-training at all, and found quantum chemistry pre-training to produce the most accurate models for lipophilicity, blood-brain barrier penetration, metabolism by CYP2D6, and toxicity; and very similar performance to non-pre-trained models for the much more challenging task of hepatocyte clearance prediction. By using our quantum chemistry-based pre-training approach to predict both atom-level and molecule-level properties, we obtain richer representations of the molecules than without pre-training, helping our models to learn from the underlying physics and chemistry.","On the Effectiveness of Quantum Chemistry Pre-training for Pharmacological Property Prediction We have shown that quantum chemistry pre-training is effective for pharmacological property prediction. In principle, quantum chemistry allows us to quantify all electronic and geometric properties of molecules and their interactions. Thus, incorporating pre-calculated quantum mechanical properties into deep learning models could improve their ability to predict important pharmacological properties of small molecules and potential drugs. However, this opportunity has been under-exploited in the recent wave of AI-driven drug discovery. We show that by pre-training Equivariant Graph Neural Network (EGNN) models to predict atom-centered partial charges, that have been pre-calculated using quantum mechanical methods, we can obtain more accurate models to predict absorption, distribution, metabolism, excretion, and toxicological (ADMET) properties. We compared the performance of quantum chemistry pre-training against non-quantum mechanics-based pre-training and with no pre-training at all, and found quantum chemistry pre-training to produce the most accurate models for lipophilicity, blood-brain barrier penetration, metabolism by CYP2D6, and toxicity; and very similar performance to non-pre-trained models for the much more challenging task of hepatocyte clearance prediction. By using our quantum chemistry-based pre-training approach to predict both atom-level and molecule-level properties, we obtain richer representations of the molecules than without pre-training, helping our models to learn from the underlying physics and chemistry. Future Work We have shown that quantum chemistry pre-training is effec- tive for ADMET property prediction. Pre-training on both molecular and atomic labels such as HOMO-LUMO gap and partial charges, respectively, leads to better performance on most downstream tasks. We encourage the computational drug discovery community to start using quantum chemical 5The corresponding downstream dataset sizes for atom-types found in QM9 and QMugs are given in Table 5. 5 On the Effectiveness of Quantum Chemistry Pre-training for Pharmacological Property Prediction Type of pre-trainingDownstream datasets Absorption Distribution Metabolism Excretion Toxicity Lipophilicity, AstraZeneca RMSE (logD units) BBB AUROC CYP2D6-Substrate AUROC Clearance-Hepatocyte Spearman correlation coefficient Acute Toxicity LD 50 RMSE (log[1/(mol/kg)] units) None - GraphSAGE 0.867 0.052 0.534 0.112 0.573 0.066 0.283 0.095 0.798 0.053 None -EGNN 0.767 0.069 0.806 0.042 0.502 0.006 0.432 0.094 0.802 0.055 Gasteiger - GraphSAGE 0.881 0.042 0.524 0.108 0.489 0.116 0.353 0.144 0.771 0.077 Mulliken only - EGNN 0.715 0.041 0.743 0.176 0.778 0.079 0.417 0.130 0.705 0.067 Mulliken + Gap - EGNN 0.707 0.040 0.647 0.133 0.878 0.069 0.410 0.176 0.700 0.070 Table 1. A comparison of non-pre-trained models against QM9 pre-trained models ( higher the better, lower the better ) Type of pre-trainingDownstream datasets Absorption Distribution Metabolism Excretion Toxicity Lipophilicity, AstraZeneca RMSE (logD units) BBB AUROC CYP2D6-Substrate AUROC Clearance-Hepatocyte Spearman correlation coefficient Acute Toxicity LD 50 RMSE (log[1/(mol/kg)] units) None - EGNN 0.767 0.069 0.806 0.042 0.502 0.006 0.432 0.094 0.802 0.055 Mulliken only - EGNN 0.760 0.053 0.864 0.032 0.771 0.067 0.413 0.071 0.779 0.040 L owdin only - EGNN 0.724 0.047 0.863 0.030 0.766 0.059 0.418 0.061 0.779 0.039 Mulliken + Gap - EGNN 0.731 0.037 0.860 0.031 0.769 0.068 0.400 0.091 0.781 0.042 L owdin + Gap - EGNN 0.688 0.039 0.861 0.029 0.767 Methodology We used the Equivariant Graph Neural Network or EGNN (Satorras et al., 2022) to encode each molecule s 3D ge-ometry; and the GraphSAGE (Hamilton et al., 2018) graph neural network to encode the molecule as a 2D graph. We used the same model architectures for both pre-training and downstream fine-tuning to investigate the effectiveness of quantum chemistry pre-training. We used the EGNN and GraphSAGE models for ADMET property prediction in three training regimes: No pre-training ,i.e., direct prediction of each ADMET property; Non-quantum chemical pre-training to predict topo- logical Gasteiger partial charges (Gasteiger & Marsili, 1980) using GraphSAGE; and Quantum chemical pre-training to predict: (i) quantum mechanical partial charges, namely Mulliken (Mul- liken, 1955) or L owdin (L owdin, 1970) charges; and (ii) the HOMO-LUMO gap of the molecule s highest occupied and lowest unoccupied molecular orbital (a measure of its chemical reactivity). In the last two cases, the resulting molecular embedding is used as input to a fine-tuning phase to predict the desired ADMET property. The electron distribution in a molecule allows us to under- stand the tendency for certain intermolecular interactions to occur. One way to approximate the electronic distribution in a molecule at an atomic level is via atom-centred partial charges. Thus we chose to pre-train the models on partial charges to attain node or atom-level embeddings. There are two distinct classes of partial charges: non-quantum mechanical (QM) and QM-based partial charges. 2.1.Control: Pre-training on Gasteiger Partial Charges The Partial Equalization of Orbital Electronegativity (PEOE) method by Gasteiger & Marsili (1980) assumes that the electronegativity, v, of an atom type, v, is a quadratic function of the atomic partial charge: v=av+bv(qv) +cv(qv)2(1) where qvis the partial charge, and av, bv, andcvare coeffi- cients to be optimized. According to Mulliken (1934), the electronegativity",0
a71f0cb12f12209579ede380b6511c341f9a4c2f,FlowBack: A Flow-matching Approach for Generative Backmapping of Macromolecules,"['Michael Jones', 'Smayan Khanna', 'Andrew Ferguson']",https://openreview.net/pdf/a71f0cb12f12209579ede380b6511c341f9a4c2f.pdf,"FlowBack: A Flow-matching Approach for Generative Backmapping of Macromolecules Coarse-grained models have become ubiquitous in biomolecular modeling tasks aimed at studying slow dynamical processes such as protein folding and DNA hybridization. Although these models considerably accelerate sampling, it remains challenging to recover an ensemble of all-atom structures corresponding to coarse-grained simulations. In this work, we introduce a generative approach called FlowBack that uses a flow-matching objective to map samples from a coarse-grained prior distribution to an all-atom data distribution. We construct our prior distribution to be amenable to any coarse-grained map and any type of macromolecule, and we find that generated structures are more robust and contain less steric clashes than those generated by previous approaches. We train a protein-specific model on structures from the Protein Data Bank which achieve state-of-the-art results on bond quality on clash score. Furthermore, we train a model on DNA-protein data which achieves excellent reconstruction and generative capabilities on complexes from the PDB as well as on coarse-grained simulations of DNA-protein binding.",a71f0cb12f12209579ede380b6511c341f9a4c2f.pdf,"approach for generative
backmapping of macromolecules
Michael S. Jones1Smayan Khanna1Andrew L. Ferguson1
Abstract
Coarse-grained models have become ubiquitous
in biomolecular modeling tasks aimed at studying
slow dynamical processes such as protein folding
and DNA hybridization. Although these models
considerably accelerate sampling, it remains chal-
lenging to recover an ensemble of all-atom struc-
tures corresponding to coarse-grained simulations.
In this work, we introduce a generative approach
called FlowBack that uses a flow-matching objec-
tive to map samples from a coarse-grained prior
distribution to an all-atom data distribution. We
construct our prior distribution to be amenable to
any coarse-grained map and any type of macro-
molecule, and we find that generated structures
are more robust and contain less steric clashes
than those generated by previous approaches. We
train a protein-specific model on structures from
the Protein Data Bank which achieve state-of-the-
art results on bond quality and on clash score. Fur-
thermore, we train a model on DNA-protein data
which achieves excellent reconstruction and gen-
erative capabilities on complexes from the PDB
as well as on coarse-grained simulations of DNA-
protein binding.
1. Introduction
For decades, coarse-grained (CG) force-fields have ex-
panded the time and length scales accessible to molecu-
lar dynamics simulations. By reducing the simulated de-
grees of freedom and smoothing the underlying free energy
landscape, these simulation techniques can directly pro-
vide insight into slow processes and rare events such as
protein folding and DNA hybridization (Clementi, 2008;
Noid, 2013; Saunders & V oth, 2013; Kmiecik et al., 2016;
Mohr et al., 2022; Shmilovich et al., 2020). However, the
1UChicago, Pritzker School of Molecular Engineering, Univer-
sity of Chicago, Chicago, Illinois 60637, United States. Correspon-
dence to: Andrew Ferguson <andrewferguson@uchicago.edu >.
Accepted at the 1st Machine Learni","Future work
may include the exploration of more complex priors be-
yond Gaussians such as harmonic priors (St ¨ark et al., 2023;
Jing et al., 2023) which leverage information from the bond
network to maintain close proximity between neighboring
atoms. These priors may be tailored to particular classes of
biomolecules or to other data present in the PDB such as
small molecules and ions.
References
Albergo, M. S. and Vanden-Eijnden, E. Building normal-
izing flows with stochastic interpolants. arXiv preprint
arXiv:2209.15571 , 2022.
AlQuraishi, M. Proteinnet: a standardized data set for
machine learning of protein structure. BMC Bioinf. , 20
(1):1–10, 2019.An, Y . and Deshmukh, S. A. Machine learning approach
for accurate backmapping of coarse-grained models to
all-atom models. Chem. Commun. , 56(65):9312–9315,
2020.
Badaczewska-Dawid, A. E., Kolinski, A., and Kmiecik,
S. Computational reconstruction of atomistic protein
structures from coarse-grained models. Comput. Struct.
Biotechnol. J. , 18:162–176, 2020.
Berman, H., Henrick, K., and Nakamura, H. Announcing
the worldwide protein data bank. Nat. Struct. Mol. Biol. ,
10(12):980–980, 2003.
Berman, H. M., Westbrook, J., Feng, Z., Gilliland, G., Bhat,
T. N., Weissig, H., Shindyalov, I. N., and Bourne, P. E.
The Protein Data Bank. Nucleic Acids Res. , 28(1):235–
242, 01 2000. ISSN 0305-1048. doi: 10.1093/nar/28.1.
235. URL https://doi.org/10.1093/nar/28.
1.235 .
Bose, A. J., Akhound-Sadegh, T., Fatras, K., Huguet, G.,
Rector-Brooks, J., Liu, C.-H., Nica, A. C., Korablyov,
M., Bronstein, M., and Tong, A. Se (3)-stochastic flow
matching for protein backbone generation. arXiv preprint
arXiv:2310.02391 , 2023.
Brocos, P., Mendoza-Espinosa, P., Castillo, R., Mas-Oliva,
J., and Pineiro, A. Multiscale molecular dynamics sim-
ulations of micelles: coarse-grain for self-assembly and
atomic resolution for finer details. Soft Matter , 8(34):
9005–9014, 2012.
Chen, R. T., Rubanova, Y ., Bettencourt, J., and Duvenaud,
D. K. Neural o","approach for generative backmapping of macromolecules Michael S. Jones1Smayan Khanna1Andrew L. Ferguson1 Abstract Coarse-grained models have become ubiquitous in biomolecular modeling tasks aimed at studying slow dynamical processes such as protein folding and DNA hybridization. Although these models considerably accelerate sampling, it remains chal- lenging to recover an ensemble of all-atom struc- tures corresponding to coarse-grained simulations. In this work, we introduce a generative approach called FlowBack that uses a flow-matching objec- tive to map samples from a coarse-grained prior distribution to an all-atom data distribution. We construct our prior distribution to be amenable to any coarse-grained map and any type of macro- molecule, and we find that generated structures are more robust and contain less steric clashes than those generated by previous approaches. We train a protein-specific model on structures from the Protein Data Bank which achieve state-of-the- art results on bond quality and on clash score. Fur- thermore, we train a model on DNA-protein data which achieves excellent reconstruction and gen- erative capabilities on complexes from the PDB as well as on coarse-grained simulations of DNA- protein binding. 1. Introduction For decades, coarse-grained (CG) force-fields have ex- panded the time and length scales accessible to molecu- lar dynamics simulations. By reducing the simulated de- grees of freedom and smoothing the underlying free energy landscape, these simulation techniques can directly pro- vide insight into slow processes and rare events such as protein folding and DNA hybridization (Clementi, 2008; Noid, 2013; Saunders & V oth, 2013; Kmiecik et al., 2016; Mohr et al., 2022; Shmilovich et al., 2020). However, the 1UChicago, Pritzker School of Molecular Engineering, Univer- sity of Chicago, Chicago, Illinois 60637, United States. Correspon- dence to: Andrew Ferguson <andrewferguson@uchicago.edu >. Accepted at the 1st Machine Learni","Future work may include the exploration of more complex priors be- yond Gaussians such as harmonic priors (St ark et al., 2023; Jing et al., 2023) which leverage information from the bond network to maintain close proximity between neighboring atoms. These priors may be tailored to particular classes of biomolecules or to other data present in the PDB such as small molecules and ions.","FlowBack: A Flow-matching Approach for Generative Backmapping of Macromolecules Coarse-grained models have become ubiquitous in biomolecular modeling tasks aimed at studying slow dynamical processes such as protein folding and DNA hybridization. Although these models considerably accelerate sampling, it remains challenging to recover an ensemble of all-atom structures corresponding to coarse-grained simulations. In this work, we introduce a generative approach called FlowBack that uses a flow-matching objective to map samples from a coarse-grained prior distribution to an all-atom data distribution. We construct our prior distribution to be amenable to any coarse-grained map and any type of macromolecule, and we find that generated structures are more robust and contain less steric clashes than those generated by previous approaches. We train a protein-specific model on structures from the Protein Data Bank which achieve state-of-the-art results on bond quality on clash score. Furthermore, we train a model on DNA-protein data which achieves excellent reconstruction and generative capabilities on complexes from the PDB as well as on coarse-grained simulations of DNA-protein binding.","FlowBack: A Flow-matching Approach for Generative Backmapping of Macromolecules Coarse-grained models have become ubiquitous in biomolecular modeling tasks aimed at studying slow dynamical processes such as protein folding and DNA hybridization. Although these models considerably accelerate sampling, it remains challenging to recover an ensemble of all-atom structures corresponding to coarse-grained simulations. In this work, we introduce a generative approach called FlowBack that uses a flow-matching objective to map samples from a coarse-grained prior distribution to an all-atom data distribution. We construct our prior distribution to be amenable to any coarse-grained map and any type of macromolecule, and we find that generated structures are more robust and contain less steric clashes than those generated by previous approaches. We train a protein-specific model on structures from the Protein Data Bank which achieve state-of-the-art results on bond quality on clash score. Furthermore, we train a model on DNA-protein data which achieves excellent reconstruction and generative capabilities on complexes from the PDB as well as on coarse-grained simulations of DNA-protein binding. Future work may include the exploration of more complex priors be- yond Gaussians such as harmonic priors (St ark et al., 2023; Jing et al., 2023) which leverage information from the bond network to maintain close proximity between neighboring atoms. These priors may be tailored to particular classes of biomolecules or to other data present in the PDB such as small molecules and ions. approach for generative backmapping of macromolecules Michael S. Jones1Smayan Khanna1Andrew L. Ferguson1 Abstract Coarse-grained models have become ubiquitous in biomolecular modeling tasks aimed at studying slow dynamical processes such as protein folding and DNA hybridization. Although these models considerably accelerate sampling, it remains chal- lenging to recover an ensemble of all-atom struc- tures corresponding to coarse-grained simulations. In this work, we introduce a generative approach called FlowBack that uses a flow-matching objec- tive to map samples from a coarse-grained prior distribution to an all-atom data distribution. We construct our prior distribution to be amenable to any coarse-grained map and any type of macro- molecule, and we find that generated structures are more robust and contain less steric clashes than those generated by previous approaches. We train a protein-specific model on structures from the Protein Data Bank which achieve state-of-the- art results on bond quality and on clash score. Fur- thermore, we train a model on DNA-protein data which achieves excellent reconstruction and gen- erative capabilities on complexes from the PDB as well as on coarse-grained simulations of DNA- protein binding. 1. Introduction For decades, coarse-grained (CG) force-fields have ex- panded the time and length scales accessible to molecu- lar dynamics simulations. By reducing the simulated de- grees of freedom and smoothing the underlying free energy landscape, these simulation techniques can directly pro- vide insight into slow processes and rare events such as protein folding and DNA hybridization (Clementi, 2008; Noid, 2013; Saunders & V oth, 2013; Kmiecik et al., 2016; Mohr et al., 2022; Shmilovich et al., 2020). However, the 1UChicago, Pritzker School of Molecular Engineering, Univer- sity of Chicago, Chicago, Illinois 60637, United States. Correspon- dence to: Andrew Ferguson <andrewferguson@uchicago.edu >. Accepted at the 1st Machine Learni",0
28281eeddd53eea7d8378c26bebc6163e4a00b0e,Hierarchical Contrastive Learning for Enzyme Function Prediction,"['Soorin Yim', 'Doyeong Hwang', 'Kiyoung Kim', 'Sehui Han']",https://openreview.net/pdf/28281eeddd53eea7d8378c26bebc6163e4a00b0e.pdf,"Hierarchical Contrastive Learning for Enzyme Function Prediction Enzymes are biological catalysts with numerous industrial applications, and they are categorized by the Enzyme Commission (EC) number system based on their catalytic activities. With over 200 million protein sequences identified, experimental characterization of enzymes is impractical, necessitating computational methods. Current approaches face challenges with class imbalance and intrinsic hierarchy of the EC number system. This study employs hierarchical contrastive learning for EC number prediction, effectively integrating the EC number hierarchy into the model. Our approach addresses severe class imbalance and improves prediction performance, particularly for higher hierarchical levels and previously unseen EC numbers, demonstrating enhanced robustness and outperforming existing methods.",28281eeddd53eea7d8378c26bebc6163e4a00b0e.pdf,"approach addresses severe class imbalance and
improves prediction performance, particularly for
higher hierarchical levels and previously unseen
EC numbers, demonstrating enhanced robustness
and outperforming existing methods.
1. Introduction
Enzymes are specialized proteins that act as biological cat-
alysts, accelerating biochemical reactions crucial for life.
They are widely used in various sectors, including food,
pharmaceutical, and energy industries (Chapman et al.,
2018), driving active research to discover or engineer en-
hanced enzymes. The Enzyme Commission (EC) number
system is a hierarchical classification system that catego-
rizes enzymes based on the reactions they catalyze, with
four levels of increasing specificity that describe their func-
tion (Tipton & McDonald, 2018). Figure 1 provides an
overview and an example of the EC number system.
As of 2023, over 200 million protein sequences have been
identified (Consortium, 2022). However, experimentally
characterizing each one to determine its function is imprac-
tical. Consequently, only a small fraction of these proteins
has been experimentally identified as enzymes with specific
EC numbers, highlighting a pressing need for computational
research in this field.
1LG AI Research, Seoul, Republic of Korea. Correspondence
to: Sehui Han <hansse.han@lgresearch.ai >.
Accepted at the 1st Machine Learning for Life and Material Sci-
ences Workshop at ICML 2024 . Copyright 2024 by the author(s).Deep learning has revolutionized the field of EC number
prediction, enabling the accurate prediction of EC numbers
for proteins based on their sequences. Initial approaches fo-
cused on training classification models, either using separate
models for each hierarchical level or by employing multi-
task learning, where each hierarchical level was treated as
a distinct task (Sureyya Rifaioglu et al., 2019; Ryu et al.,
2019; Sanderson et al., 2023). However, these models face
challenges with severe class imbalance. As of Apri","Conclusion
This study demonstrates the effectiveness of hierarchical
contrastive learning in EC number prediction. Our results
show that employing the hierarchical nature of EC number
improves the model performance and robustness over SOTA
models, particularly for higher level EC numbers and pre-
viously unseen EC numbers. These results highlight the
potential of hierarchical CL to advance the field of compu-
tational enzyme annotation, providing a robust and scalable
solution for the accurate prediction of enzyme functions.
Hierarchical Contrastive Learning for Enzyme Function Prediction
References
Chapman, J., Ismail, A. E., and Dinu, C. Z. Indus-
trial applications of enzymes: Recent advances, tech-
niques, and outlooks. Catalysts , 8(6):238, 2018. doi:
10.3390/catal8060238.
Consortium, T. U. UniProt: the Universal Pro-
tein Knowledgebase in 2023. Nucleic Acids
Research , 51(D1):D523–D531, 11 2022. ISSN
0305-1048. doi: 10.1093/nar/gkac1052. URL
https://doi.org/10.1093/nar/gkac1052 .
IUBMB. Iubmb. https://iubmb.qmul.ac.uk/ . Ac-
cessed: 2024-05-22.
Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y ., Isola,
P., Maschinot, A., Liu, C., and Krishnan, D. Supervised
contrastive learning. Advances in neural information
processing systems , 33:18661–18673, 2020.
Langley, P. Crafting papers on machine learning. In Langley,
P. (ed.), Proceedings of the 17th International Conference
on Machine Learning (ICML 2000) , pp. 1207–1216, Stan-
ford, CA, 2000. Morgan Kaufmann.
Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W.,
Smetanin, N., Verkuil, R., Kabeli, O., Shmueli, Y ., et al.
Evolutionary-scale prediction of atomic-level protein
structure with a language model. Science , 379(6637):
1123–1130, 2023.
Memon, S. A., Khan, K. A., and Naveed, H. Hecnet: a
hierarchical approach to enzyme function classification
using a siamese triplet network. Bioinformatics , 36(17):
4583–4589, 2020.
Price, M. N., Wetmore, K. M., Waters, R. J., Callaghan, M.,
Ray, J., Liu, H., Kuehl, J","approach addresses severe class imbalance and improves prediction performance, particularly for higher hierarchical levels and previously unseen EC numbers, demonstrating enhanced robustness and outperforming existing methods. 1. Introduction Enzymes are specialized proteins that act as biological cat- alysts, accelerating biochemical reactions crucial for life. They are widely used in various sectors, including food, pharmaceutical, and energy industries (Chapman et al., 2018), driving active research to discover or engineer en- hanced enzymes. The Enzyme Commission (EC) number system is a hierarchical classification system that catego- rizes enzymes based on the reactions they catalyze, with four levels of increasing specificity that describe their func- tion (Tipton & McDonald, 2018). Figure 1 provides an overview and an example of the EC number system. As of 2023, over 200 million protein sequences have been identified (Consortium, 2022). However, experimentally characterizing each one to determine its function is imprac- tical. Consequently, only a small fraction of these proteins has been experimentally identified as enzymes with specific EC numbers, highlighting a pressing need for computational research in this field. 1LG AI Research, Seoul, Republic of Korea. Correspondence to: Sehui Han <hansse.han@lgresearch.ai >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).Deep learning has revolutionized the field of EC number prediction, enabling the accurate prediction of EC numbers for proteins based on their sequences. Initial approaches fo- cused on training classification models, either using separate models for each hierarchical level or by employing multi- task learning, where each hierarchical level was treated as a distinct task (Sureyya Rifaioglu et al., 2019; Ryu et al., 2019; Sanderson et al., 2023). However, these models face challenges with severe class imbalance. As of Apri","Conclusion This study demonstrates the effectiveness of hierarchical contrastive learning in EC number prediction. Our results show that employing the hierarchical nature of EC number improves the model performance and robustness over SOTA models, particularly for higher level EC numbers and pre- viously unseen EC numbers. These results highlight the potential of hierarchical CL to advance the field of compu- tational enzyme annotation, providing a robust and scalable solution for the accurate prediction of enzyme functions. Hierarchical Contrastive Learning for Enzyme Function Prediction","Hierarchical Contrastive Learning for Enzyme Function Prediction Enzymes are biological catalysts with numerous industrial applications, and they are categorized by the Enzyme Commission (EC) number system based on their catalytic activities. With over 200 million protein sequences identified, experimental characterization of enzymes is impractical, necessitating computational methods. Current approaches face challenges with class imbalance and intrinsic hierarchy of the EC number system. This study employs hierarchical contrastive learning for EC number prediction, effectively integrating the EC number hierarchy into the model. Our approach addresses severe class imbalance and improves prediction performance, particularly for higher hierarchical levels and previously unseen EC numbers, demonstrating enhanced robustness and outperforming existing methods.","Hierarchical Contrastive Learning for Enzyme Function Prediction Enzymes are biological catalysts with numerous industrial applications, and they are categorized by the Enzyme Commission (EC) number system based on their catalytic activities. With over 200 million protein sequences identified, experimental characterization of enzymes is impractical, necessitating computational methods. Current approaches face challenges with class imbalance and intrinsic hierarchy of the EC number system. This study employs hierarchical contrastive learning for EC number prediction, effectively integrating the EC number hierarchy into the model. Our approach addresses severe class imbalance and improves prediction performance, particularly for higher hierarchical levels and previously unseen EC numbers, demonstrating enhanced robustness and outperforming existing methods. Conclusion This study demonstrates the effectiveness of hierarchical contrastive learning in EC number prediction. Our results show that employing the hierarchical nature of EC number improves the model performance and robustness over SOTA models, particularly for higher level EC numbers and pre- viously unseen EC numbers. These results highlight the potential of hierarchical CL to advance the field of compu- tational enzyme annotation, providing a robust and scalable solution for the accurate prediction of enzyme functions. Hierarchical Contrastive Learning for Enzyme Function Prediction approach addresses severe class imbalance and improves prediction performance, particularly for higher hierarchical levels and previously unseen EC numbers, demonstrating enhanced robustness and outperforming existing methods. 1. Introduction Enzymes are specialized proteins that act as biological cat- alysts, accelerating biochemical reactions crucial for life. They are widely used in various sectors, including food, pharmaceutical, and energy industries (Chapman et al., 2018), driving active research to discover or engineer en- hanced enzymes. The Enzyme Commission (EC) number system is a hierarchical classification system that catego- rizes enzymes based on the reactions they catalyze, with four levels of increasing specificity that describe their func- tion (Tipton & McDonald, 2018). Figure 1 provides an overview and an example of the EC number system. As of 2023, over 200 million protein sequences have been identified (Consortium, 2022). However, experimentally characterizing each one to determine its function is imprac- tical. Consequently, only a small fraction of these proteins has been experimentally identified as enzymes with specific EC numbers, highlighting a pressing need for computational research in this field. 1LG AI Research, Seoul, Republic of Korea. Correspondence to: Sehui Han <hansse.han@lgresearch.ai >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).Deep learning has revolutionized the field of EC number prediction, enabling the accurate prediction of EC numbers for proteins based on their sequences. Initial approaches fo- cused on training classification models, either using separate models for each hierarchical level or by employing multi- task learning, where each hierarchical level was treated as a distinct task (Sureyya Rifaioglu et al., 2019; Ryu et al., 2019; Sanderson et al., 2023). However, these models face challenges with severe class imbalance. As of Apri",0
81a11360cd14b58c8da298b3311f37d9f94859d8,Towards Linking Graph Topology to Model Performance for Biomedical Knowledge Graph Completion,"['Alberto Cattaneo', 'Thomas Martynec', 'Stephen Bonner', 'Carlo Luschi', 'Daniel Justus']",https://openreview.net/pdf/81a11360cd14b58c8da298b3311f37d9f94859d8.pdf,"Towards Linking Graph Topology to Model Performance for Biomedical Knowledge Graph Completion This study links topological properties of biomedical Knowledge Graphs to the accuracy observed in Knowledge Graph Completion tasks and provides new tools to study this connection. Knowledge Graph Completion has been increasingly adopted as a useful method for several tasks in biomedical research, like drug repurposing or drug-target identification. To that end, a variety of datasets and Knowledge Graph Embedding models has been proposed over the years. However, little is known about the properties that render a dataset useful for a given task and, even though theoretical properties of Knowledge Graph Embedding models are well understood, their practical utility in this field remains controversial. We conduct a comprehensive investigation into the topological properties of publicly available biomedical Knowledge Graphs and establish links to the accuracy observed in real-world applications. By releasing all model predictions and a new suite of analysis tools we invite the community to build upon our work and continue improving the understanding of these crucial applications.",81a11360cd14b58c8da298b3311f37d9f94859d8.pdf,"approaches able to capture all four investigated topological
patterns. Notice however that the theoretical capability of
a scoring function to model a particular edge topological
pattern is not per se a guarantee of stronger predictive per-
formance on such edges (Jin et al., 2023). Our experiments
are designed to quantify this impact.
The training scheme and hyperparameter optimisation de-
tails are presented in Appendix B. The results reported in
the following sections refer to tail predictions generated on
the held out test split, by scoring each (h, r,?)query against
all entities in the KG and computing the rank of the ground
truth tail t, after masking out scores of other (h, r, t′)triples
contained in the graph.
2In the original PrimeKG graph, for every triple (h, r, t )the
reverse triple (t, r, h )is present as well. We prepocessed PrimeKG
to remove these reverse edges.
Towards Linking Graph Topology to Model Performance for Biomedical Knowledge Graph Completion
4. Results
4.1. Effect of Topological Properties on MRR
We observe a significant variance in mean reciprocal rank
(MRR) across the different KGs and KGE models (Fig-
ure 1). In an attempt to understand its cause, we focus our
analysis on the edge cardinalities and topological patterns
described in Section 2. We find that they occur with varying
frequencies in the different KGs (Figure A.2 and Table A.2).
However, the data does not support a conclusion about the ef-
fect of these topological properties on model accuracy when
only considering their average occurrence per dataset (Fig-
ure C.1). Previous work (e.g., Teneva & Hruschka (2023a))
went one step further, classifying relation types based on
the predominant edge topological pattern/cardinality, how-
ever this also does not yield conclusive results (Figure C.2).
This is due to the fact that the confounding effects of covari-
ates, such as node degrees and different topological patterns,
remain too difficult to disentangle, as these properties are","conclusion about the ef-
fect of these topological properties on model accuracy when
only considering their average occurrence per dataset (Fig-
ure C.1). Previous work (e.g., Teneva & Hruschka (2023a))
went one step further, classifying relation types based on
the predominant edge topological pattern/cardinality, how-
ever this also does not yield conclusive results (Figure C.2).
This is due to the fact that the confounding effects of covari-
ates, such as node degrees and different topological patterns,
remain too difficult to disentangle, as these properties are
often not homogenous within a relation type. Therefore, we
dissect the link between topological properties and KGE
models accuracy on the level of individual triples to allow
for a finer-grained analysis with improved statistical power.
Hetionet
OpenBioLink PharMeBINetPharmKG PrimeKGFB15k-2370.00.10.20.30.4MRRDistMult
RotatE
TransE
TripleRE
Figure 1: Mean reciprocal rank on the test split achieved by
different models, for the six datasets.
While all investigated KGs are dominated by many-to-many
triples (Figure A.2), there is a wide variability in the ef-
fect of edge cardinality on MRR across different KGs (Fig-
ure 2). This suggests that the exact entity degrees, together
with other topological properties, might be better suited to
explain the MRR than a binary one/many cardinality clas-
sification. Indeed, for a given triple, we observe a strong
correlation between model accuracy and both the out-degree
of the head entity and the in-degree of the tail entity (Fig-
ure C.3), especially for degrees of same relation (Figures 3
and C.4; see Figure A.3 for the distribution of degrees in
the different KGs). In fact, a high in-degree of the tail node
in a tail prediction task has been linked to a higher score
(Bonner et al., 2022b), therefore increasing the likelihood of
predicting it, as confirmed in Figure C.5. On the other hand,
a high out-degree of same relation of the head node in a
1:1 1:M M:1 M:M0.00.1","approaches able to capture all four investigated topological patterns. Notice however that the theoretical capability of a scoring function to model a particular edge topological pattern is not per se a guarantee of stronger predictive per- formance on such edges (Jin et al., 2023). Our experiments are designed to quantify this impact. The training scheme and hyperparameter optimisation de- tails are presented in Appendix B. The results reported in the following sections refer to tail predictions generated on the held out test split, by scoring each (h, r,?)query against all entities in the KG and computing the rank of the ground truth tail t, after masking out scores of other (h, r, t )triples contained in the graph. 2In the original PrimeKG graph, for every triple (h, r, t )the reverse triple (t, r, h )is present as well. We prepocessed PrimeKG to remove these reverse edges. Towards Linking Graph Topology to Model Performance for Biomedical Knowledge Graph Completion 4. Results 4.1. Effect of Topological Properties on MRR We observe a significant variance in mean reciprocal rank (MRR) across the different KGs and KGE models (Fig- ure 1). In an attempt to understand its cause, we focus our analysis on the edge cardinalities and topological patterns described in Section 2. We find that they occur with varying frequencies in the different KGs (Figure A.2 and Table A.2). However, the data does not support a conclusion about the ef- fect of these topological properties on model accuracy when only considering their average occurrence per dataset (Fig- ure C.1). Previous work (e.g., Teneva & Hruschka (2023a)) went one step further, classifying relation types based on the predominant edge topological pattern/cardinality, how- ever this also does not yield conclusive results (Figure C.2). This is due to the fact that the confounding effects of covari- ates, such as node degrees and different topological patterns, remain too difficult to disentangle, as these properties are","conclusion about the ef- fect of these topological properties on model accuracy when only considering their average occurrence per dataset (Fig- ure C.1). Previous work (e.g., Teneva & Hruschka (2023a)) went one step further, classifying relation types based on the predominant edge topological pattern/cardinality, how- ever this also does not yield conclusive results (Figure C.2). This is due to the fact that the confounding effects of covari- ates, such as node degrees and different topological patterns, remain too difficult to disentangle, as these properties are often not homogenous within a relation type. Therefore, we dissect the link between topological properties and KGE models accuracy on the level of individual triples to allow for a finer-grained analysis with improved statistical power. Hetionet OpenBioLink PharMeBINetPharmKG PrimeKGFB15k-2370.00.10.20.30.4MRRDistMult RotatE TransE TripleRE Figure 1: Mean reciprocal rank on the test split achieved by different models, for the six datasets. While all investigated KGs are dominated by many-to-many triples (Figure A.2), there is a wide variability in the ef- fect of edge cardinality on MRR across different KGs (Fig- ure 2). This suggests that the exact entity degrees, together with other topological properties, might be better suited to explain the MRR than a binary one/many cardinality clas- sification. Indeed, for a given triple, we observe a strong correlation between model accuracy and both the out-degree of the head entity and the in-degree of the tail entity (Fig- ure C.3), especially for degrees of same relation (Figures 3 and C.4; see Figure A.3 for the distribution of degrees in the different KGs). In fact, a high in-degree of the tail node in a tail prediction task has been linked to a higher score (Bonner et al., 2022b), therefore increasing the likelihood of predicting it, as confirmed in Figure C.5. On the other hand, a high out-degree of same relation of the head node in a 1:1 1:M M:1 M:M0.00.1","Towards Linking Graph Topology to Model Performance for Biomedical Knowledge Graph Completion This study links topological properties of biomedical Knowledge Graphs to the accuracy observed in Knowledge Graph Completion tasks and provides new tools to study this connection. Knowledge Graph Completion has been increasingly adopted as a useful method for several tasks in biomedical research, like drug repurposing or drug-target identification. To that end, a variety of datasets and Knowledge Graph Embedding models has been proposed over the years. However, little is known about the properties that render a dataset useful for a given task and, even though theoretical properties of Knowledge Graph Embedding models are well understood, their practical utility in this field remains controversial. We conduct a comprehensive investigation into the topological properties of publicly available biomedical Knowledge Graphs and establish links to the accuracy observed in real-world applications. By releasing all model predictions and a new suite of analysis tools we invite the community to build upon our work and continue improving the understanding of these crucial applications.","Towards Linking Graph Topology to Model Performance for Biomedical Knowledge Graph Completion This study links topological properties of biomedical Knowledge Graphs to the accuracy observed in Knowledge Graph Completion tasks and provides new tools to study this connection. Knowledge Graph Completion has been increasingly adopted as a useful method for several tasks in biomedical research, like drug repurposing or drug-target identification. To that end, a variety of datasets and Knowledge Graph Embedding models has been proposed over the years. However, little is known about the properties that render a dataset useful for a given task and, even though theoretical properties of Knowledge Graph Embedding models are well understood, their practical utility in this field remains controversial. We conduct a comprehensive investigation into the topological properties of publicly available biomedical Knowledge Graphs and establish links to the accuracy observed in real-world applications. By releasing all model predictions and a new suite of analysis tools we invite the community to build upon our work and continue improving the understanding of these crucial applications. conclusion about the ef- fect of these topological properties on model accuracy when only considering their average occurrence per dataset (Fig- ure C.1). Previous work (e.g., Teneva & Hruschka (2023a)) went one step further, classifying relation types based on the predominant edge topological pattern/cardinality, how- ever this also does not yield conclusive results (Figure C.2). This is due to the fact that the confounding effects of covari- ates, such as node degrees and different topological patterns, remain too difficult to disentangle, as these properties are often not homogenous within a relation type. Therefore, we dissect the link between topological properties and KGE models accuracy on the level of individual triples to allow for a finer-grained analysis with improved statistical power. Hetionet OpenBioLink PharMeBINetPharmKG PrimeKGFB15k-2370.00.10.20.30.4MRRDistMult RotatE TransE TripleRE Figure 1: Mean reciprocal rank on the test split achieved by different models, for the six datasets. While all investigated KGs are dominated by many-to-many triples (Figure A.2), there is a wide variability in the ef- fect of edge cardinality on MRR across different KGs (Fig- ure 2). This suggests that the exact entity degrees, together with other topological properties, might be better suited to explain the MRR than a binary one/many cardinality clas- sification. Indeed, for a given triple, we observe a strong correlation between model accuracy and both the out-degree of the head entity and the in-degree of the tail entity (Fig- ure C.3), especially for degrees of same relation (Figures 3 and C.4; see Figure A.3 for the distribution of degrees in the different KGs). In fact, a high in-degree of the tail node in a tail prediction task has been linked to a higher score (Bonner et al., 2022b), therefore increasing the likelihood of predicting it, as confirmed in Figure C.5. On the other hand, a high out-degree of same relation of the head node in a 1:1 1:M M:1 M:M0.00.1 approaches able to capture all four investigated topological patterns. Notice however that the theoretical capability of a scoring function to model a particular edge topological pattern is not per se a guarantee of stronger predictive per- formance on such edges (Jin et al., 2023). Our experiments are designed to quantify this impact. The training scheme and hyperparameter optimisation de- tails are presented in Appendix B. The results reported in the following sections refer to tail predictions generated on the held out test split, by scoring each (h, r,?)query against all entities in the KG and computing the rank of the ground truth tail t, after masking out scores of other (h, r, t )triples contained in the graph. 2In the original PrimeKG graph, for every triple (h, r, t )the reverse triple (t, r, h )is present as well. We prepocessed PrimeKG to remove these reverse edges. Towards Linking Graph Topology to Model Performance for Biomedical Knowledge Graph Completion 4. Results 4.1. Effect of Topological Properties on MRR We observe a significant variance in mean reciprocal rank (MRR) across the different KGs and KGE models (Fig- ure 1). In an attempt to understand its cause, we focus our analysis on the edge cardinalities and topological patterns described in Section 2. We find that they occur with varying frequencies in the different KGs (Figure A.2 and Table A.2). However, the data does not support a conclusion about the ef- fect of these topological properties on model accuracy when only considering their average occurrence per dataset (Fig- ure C.1). Previous work (e.g., Teneva & Hruschka (2023a)) went one step further, classifying relation types based on the predominant edge topological pattern/cardinality, how- ever this also does not yield conclusive results (Figure C.2). This is due to the fact that the confounding effects of covari- ates, such as node degrees and different topological patterns, remain too difficult to disentangle, as these properties are",0
8400461e17b6fc2393358d1525de96e5a3c33344,RamanSPy: Augmenting Raman Spectroscopy Data Analysis with AI,"['Dimitar Georgiev', 'Simon Vilms Pedersen', 'Ruoxiao Xie', 'Álvaro Fernández-Galiana', 'Molly M. Stevens', 'Mauricio Barahona']",https://openreview.net/pdf/8400461e17b6fc2393358d1525de96e5a3c33344.pdf,"RamanSPy: Augmenting Raman Spectroscopy Data Analysis with AI We developed a modular, open-source Python package that streamlines the development, validation and deployment of AI-based technologies for Raman spectroscopy data analysis. Raman spectroscopy is a non-destructive and label-free chemical analysis technique, which plays a key role in the analysis and discovery cycle of various branches of life and material sciences. Recently, there has been a marked increase in the adoption of machine learning techniques in Raman spectroscopic analysis. Nonetheless, progress in the area is still impeded by the lack of software, methodological and data standardisation, and the ensuing fragmentation and lack of reproducibility of analysis workflows thereof. To address these issues, we introduce *RamanSPy*, an open-source Python package for Raman spectroscopic data analysis, which supports day-to-day tasks, integrative analyses, the development of methods and protocols, and the integration of advanced data analytics. *RamanSPy* is highly modular, not tied to a particular technology or data format, and can be readily interfaced with the burgeoning ecosystem for data science, statistical analysis and machine learning in Python. *RamanSPy* is hosted at https://github.com/barahona-research-group/RamanSPy, supplemented with extended online documentation, available at https://ramanspy.readthedocs.io, that includes tutorials, example applications, and details about the real-world research applications presented in this paper.",8400461e17b6fc2393358d1525de96e5a3c33344.pdf,"Approach for Screening
Major Carotenoids of Tomato by Handheld Raman Spec-
troscopy Using Chemometric Methods. Sensors , 20(13):
3723, 2020.
Alshdaifat, E., Alshdaifat, D., Alsarhan, A., Hussein, F.,
and El-Salhi, S. M. F. S. The effect of preprocessing
techniques, applied to numeric features, on classification
algorithms’ performance. Data , 6(2):11, 2021.
Auner, G. W., Koya, S. K., Huang, C., Broadbent, B.,
Trexler, M., Auner, Z., Elias, A., Mehne, K. C., and
Brusatori, M. A. Applications of Raman spectroscopy in
cancer diagnosis. Cancer and Metastasis Reviews , 37(4):
691–717, 2018.
Barton, B., Thomson, J., Diz, E. L., and Portela, R. Chemo-
metrics for Raman Spectroscopy Harmonization. Applied
Spectroscopy , 76(9):1021–1041, 2022.
Benjamini, Y . and Hochberg, Y . Controlling the false dis-
covery rate: A practical and powerful approach to mul-
tiple testing. Journal of the Royal Statistical Society:
Series B (Methodological) , 57(1):289–300, 1995. doi:
https://doi.org/10.1111/j.2517-6161.1995.tb02031.x.
Bergholt, M. S., St-Pierre, J.-P., Offeddu, G. S., Parmar,
P. A., Albro, M. B., Puetzer, J. L., Oyen, M. L., and
Stevens, M. M. Raman spectroscopy reveals new in-
sights into the zonal organization of native and tissue-
engineered articular cartilage. ACS central science , 2(12):
885–895, 2016.
Byrne, H. J., Knief, P., Keating, M. E., and Bonnier, F.
Spectral pre and post processing for infrared and Raman
spectroscopy of biological tissues and cells. Chemical
Society Reviews , 45(7):1865–1878, 2016.
RamanSPy: Augmenting Raman Spectroscopy Data Analysis with AI
Carey, C., Boucher, T., Mahadevan, S., Bartholomew, P., and
Dyar, M. Machine learning tools for mineral recognition
and classification from Raman spectroscopy. Journal of
Raman Spectroscopy , 46(10):894–903, 2015.
Chalmers, J. M., Edwards, H. G. M., and Hargreaves, M. D.
(eds.). Infrared and Raman Spectroscopy in Forensic
Science . Wiley Online Library, 2012.
Chang, C.-I. Spectral information divergence for","Conclusion
In this paper, we have introduced RamanSPy - a computa-
tional framework for integrative Raman spectroscopic data
analysis aimed at overcoming the limitations of currently
available commercial software tools in terms of accessi-
bility, flexibility and reproducibility, and facilitating the
adoption and validation of advanced AI technologies for
next-generation RS analysis.
RamanSPy: Augmenting Raman Spectroscopy Data Analysis with AI
The codebase of RamanSPy is fully open-source and dissem-
inated under a permissive license that allows for unrestricted
use, adaptation, and extension, including for commercial
purposes. It is supplemented with extended online documen-
tation containing a comprehensive selection of tutorials and
example applications, as well as further information about
the analyses presented in this paper. We believe this will
be critical for the continuous development of the platform
and its adoption across different scientific domains, includ-
ing biomedical research, chemistry, and materials science,
among others. Future directions include the expansion of
our suite of built-in methods, tools and datasets; the incorpo-
ration of cutting-edge AI technologies into the framework
as the field progresses; and the integration of the package
into experimental setups and other software solutions.
Data availability
All data used in this article are previously published open-
access data that have been deposited by the respective
authors online. Instructions on how to access, down-
load, and load the data sets provided in RamanSPy are
available in the documentation at https://ramanspy.
readthedocs.io/en/latest/datasets.html .
Code availability
The codebase of RamanSPy is open-source and
hosted on GitHub at https://github.com/
barahona-research-group/RamanSPy . The
package can be installed via pip using ‘pip install ramanspy’.
Documentation, including detailed tutorials and examples, is
available at https://ramanspy.readthedocs.io .
The scripts use","Approach for Screening Major Carotenoids of Tomato by Handheld Raman Spec- troscopy Using Chemometric Methods. Sensors , 20(13): 3723, 2020. Alshdaifat, E., Alshdaifat, D., Alsarhan, A., Hussein, F., and El-Salhi, S. M. F. S. The effect of preprocessing techniques, applied to numeric features, on classification algorithms performance. Data , 6(2):11, 2021. Auner, G. W., Koya, S. K., Huang, C., Broadbent, B., Trexler, M., Auner, Z., Elias, A., Mehne, K. C., and Brusatori, M. A. Applications of Raman spectroscopy in cancer diagnosis. Cancer and Metastasis Reviews , 37(4): 691 717, 2018. Barton, B., Thomson, J., Diz, E. L., and Portela, R. Chemo- metrics for Raman Spectroscopy Harmonization. Applied Spectroscopy , 76(9):1021 1041, 2022. Benjamini, Y . and Hochberg, Y . Controlling the false dis- covery rate: A practical and powerful approach to mul- tiple testing. Journal of the Royal Statistical Society: Series B (Methodological) , 57(1):289 300, 1995. doi: https://doi.org/10.1111/j.2517-6161.1995.tb02031.x. Bergholt, M. S., St-Pierre, J.-P., Offeddu, G. S., Parmar, P. A., Albro, M. B., Puetzer, J. L., Oyen, M. L., and Stevens, M. M. Raman spectroscopy reveals new in- sights into the zonal organization of native and tissue- engineered articular cartilage. ACS central science , 2(12): 885 895, 2016. Byrne, H. J., Knief, P., Keating, M. E., and Bonnier, F. Spectral pre and post processing for infrared and Raman spectroscopy of biological tissues and cells. Chemical Society Reviews , 45(7):1865 1878, 2016. RamanSPy: Augmenting Raman Spectroscopy Data Analysis with AI Carey, C., Boucher, T., Mahadevan, S., Bartholomew, P., and Dyar, M. Machine learning tools for mineral recognition and classification from Raman spectroscopy. Journal of Raman Spectroscopy , 46(10):894 903, 2015. Chalmers, J. M., Edwards, H. G. M., and Hargreaves, M. D. (eds.). Infrared and Raman Spectroscopy in Forensic Science . Wiley Online Library, 2012. Chang, C.-I. Spectral information divergence for","Conclusion In this paper, we have introduced RamanSPy - a computa- tional framework for integrative Raman spectroscopic data analysis aimed at overcoming the limitations of currently available commercial software tools in terms of accessi- bility, flexibility and reproducibility, and facilitating the adoption and validation of advanced AI technologies for next-generation RS analysis. RamanSPy: Augmenting Raman Spectroscopy Data Analysis with AI The codebase of RamanSPy is fully open-source and dissem- inated under a permissive license that allows for unrestricted use, adaptation, and extension, including for commercial purposes. It is supplemented with extended online documen- tation containing a comprehensive selection of tutorials and example applications, as well as further information about the analyses presented in this paper. We believe this will be critical for the continuous development of the platform and its adoption across different scientific domains, includ- ing biomedical research, chemistry, and materials science, among others. Future directions include the expansion of our suite of built-in methods, tools and datasets; the incorpo- ration of cutting-edge AI technologies into the framework as the field progresses; and the integration of the package into experimental setups and other software solutions. Data availability All data used in this article are previously published open- access data that have been deposited by the respective authors online. Instructions on how to access, down- load, and load the data sets provided in RamanSPy are available in the documentation at https://ramanspy. readthedocs.io/en/latest/datasets.html . Code availability The codebase of RamanSPy is open-source and hosted on GitHub at https://github.com/ barahona-research-group/RamanSPy . The package can be installed via pip using pip install ramanspy . Documentation, including detailed tutorials and examples, is available at https://ramanspy.readthedocs.io . The scripts use","RamanSPy: Augmenting Raman Spectroscopy Data Analysis with AI We developed a modular, open-source Python package that streamlines the development, validation and deployment of AI-based technologies for Raman spectroscopy data analysis. Raman spectroscopy is a non-destructive and label-free chemical analysis technique, which plays a key role in the analysis and discovery cycle of various branches of life and material sciences. Recently, there has been a marked increase in the adoption of machine learning techniques in Raman spectroscopic analysis. Nonetheless, progress in the area is still impeded by the lack of software, methodological and data standardisation, and the ensuing fragmentation and lack of reproducibility of analysis workflows thereof. To address these issues, we introduce *RamanSPy*, an open-source Python package for Raman spectroscopic data analysis, which supports day-to-day tasks, integrative analyses, the development of methods and protocols, and the integration of advanced data analytics. *RamanSPy* is highly modular, not tied to a particular technology or data format, and can be readily interfaced with the burgeoning ecosystem for data science, statistical analysis and machine learning in Python. *RamanSPy* is hosted at https://github.com/barahona-research-group/RamanSPy, supplemented with extended online documentation, available at https://ramanspy.readthedocs.io, that includes tutorials, example applications, and details about the real-world research applications presented in this paper.","RamanSPy: Augmenting Raman Spectroscopy Data Analysis with AI We developed a modular, open-source Python package that streamlines the development, validation and deployment of AI-based technologies for Raman spectroscopy data analysis. Raman spectroscopy is a non-destructive and label-free chemical analysis technique, which plays a key role in the analysis and discovery cycle of various branches of life and material sciences. Recently, there has been a marked increase in the adoption of machine learning techniques in Raman spectroscopic analysis. Nonetheless, progress in the area is still impeded by the lack of software, methodological and data standardisation, and the ensuing fragmentation and lack of reproducibility of analysis workflows thereof. To address these issues, we introduce *RamanSPy*, an open-source Python package for Raman spectroscopic data analysis, which supports day-to-day tasks, integrative analyses, the development of methods and protocols, and the integration of advanced data analytics. *RamanSPy* is highly modular, not tied to a particular technology or data format, and can be readily interfaced with the burgeoning ecosystem for data science, statistical analysis and machine learning in Python. *RamanSPy* is hosted at https://github.com/barahona-research-group/RamanSPy, supplemented with extended online documentation, available at https://ramanspy.readthedocs.io, that includes tutorials, example applications, and details about the real-world research applications presented in this paper. Conclusion In this paper, we have introduced RamanSPy - a computa- tional framework for integrative Raman spectroscopic data analysis aimed at overcoming the limitations of currently available commercial software tools in terms of accessi- bility, flexibility and reproducibility, and facilitating the adoption and validation of advanced AI technologies for next-generation RS analysis. RamanSPy: Augmenting Raman Spectroscopy Data Analysis with AI The codebase of RamanSPy is fully open-source and dissem- inated under a permissive license that allows for unrestricted use, adaptation, and extension, including for commercial purposes. It is supplemented with extended online documen- tation containing a comprehensive selection of tutorials and example applications, as well as further information about the analyses presented in this paper. We believe this will be critical for the continuous development of the platform and its adoption across different scientific domains, includ- ing biomedical research, chemistry, and materials science, among others. Future directions include the expansion of our suite of built-in methods, tools and datasets; the incorpo- ration of cutting-edge AI technologies into the framework as the field progresses; and the integration of the package into experimental setups and other software solutions. Data availability All data used in this article are previously published open- access data that have been deposited by the respective authors online. Instructions on how to access, down- load, and load the data sets provided in RamanSPy are available in the documentation at https://ramanspy. readthedocs.io/en/latest/datasets.html . Code availability The codebase of RamanSPy is open-source and hosted on GitHub at https://github.com/ barahona-research-group/RamanSPy . The package can be installed via pip using pip install ramanspy . Documentation, including detailed tutorials and examples, is available at https://ramanspy.readthedocs.io . The scripts use Approach for Screening Major Carotenoids of Tomato by Handheld Raman Spec- troscopy Using Chemometric Methods. Sensors , 20(13): 3723, 2020. Alshdaifat, E., Alshdaifat, D., Alsarhan, A., Hussein, F., and El-Salhi, S. M. F. S. The effect of preprocessing techniques, applied to numeric features, on classification algorithms performance. Data , 6(2):11, 2021. Auner, G. W., Koya, S. K., Huang, C., Broadbent, B., Trexler, M., Auner, Z., Elias, A., Mehne, K. C., and Brusatori, M. A. Applications of Raman spectroscopy in cancer diagnosis. Cancer and Metastasis Reviews , 37(4): 691 717, 2018. Barton, B., Thomson, J., Diz, E. L., and Portela, R. Chemo- metrics for Raman Spectroscopy Harmonization. Applied Spectroscopy , 76(9):1021 1041, 2022. Benjamini, Y . and Hochberg, Y . Controlling the false dis- covery rate: A practical and powerful approach to mul- tiple testing. Journal of the Royal Statistical Society: Series B (Methodological) , 57(1):289 300, 1995. doi: https://doi.org/10.1111/j.2517-6161.1995.tb02031.x. Bergholt, M. S., St-Pierre, J.-P., Offeddu, G. S., Parmar, P. A., Albro, M. B., Puetzer, J. L., Oyen, M. L., and Stevens, M. M. Raman spectroscopy reveals new in- sights into the zonal organization of native and tissue- engineered articular cartilage. ACS central science , 2(12): 885 895, 2016. Byrne, H. J., Knief, P., Keating, M. E., and Bonnier, F. Spectral pre and post processing for infrared and Raman spectroscopy of biological tissues and cells. Chemical Society Reviews , 45(7):1865 1878, 2016. RamanSPy: Augmenting Raman Spectroscopy Data Analysis with AI Carey, C., Boucher, T., Mahadevan, S., Bartholomew, P., and Dyar, M. Machine learning tools for mineral recognition and classification from Raman spectroscopy. Journal of Raman Spectroscopy , 46(10):894 903, 2015. Chalmers, J. M., Edwards, H. G. M., and Hargreaves, M. D. (eds.). Infrared and Raman Spectroscopy in Forensic Science . Wiley Online Library, 2012. Chang, C.-I. Spectral information divergence for",0
a476b1c9e6e324bdebce6109f91bc6486db309b2,Robustness of Explainable Artificial Intelligence in Industrial Process Modelling,"['Benedikt Kantz', 'Clemens Staudinger', 'Christoph Feilmayr', 'Johannes Wachlmayr', 'Alexander Haberl', 'Stefan Schuster', 'Franz Pernkopf']",https://openreview.net/pdf/a476b1c9e6e324bdebce6109f91bc6486db309b2.pdf,"Robustness of Explainable Artificial Intelligence in Industrial Process Modelling eXplainable Artificial Intelligence (XAI) aims at providing understandable explanations of black box models. In this paper, we evaluate current XAI methods by scoring them based on ground truth simulations and sensitivity analysis. To this end, we used an Electric Arc Furnace (EAF) model to better understand the limits and robustness characteristics of XAI methods such as SHapley Additive exPlanations (SHAP), Local Interpretable Model-agnostic Explanations (LIME), as well as Averaged Local Effects (ALE) or Smooth Gradients (SG) in a highly topical setting. These XAI methods were applied to various types of black-box models and then scored based on their correctness compared to the ground-truth sensitivity of the data-generating processes using a novel scoring evaluation methodology over a range of simulated additive noise. The resulting evaluation shows that the capability of the Machine Learning (ML) models to capture the process accurately is, indeed, coupled with the correctness of the explainability of the underlying data-generating process. We furthermore show the differences between XAI methods in their ability to correctly predict the true sensitivity of the modeled industrial process.",a476b1c9e6e324bdebce6109f91bc6486db309b2.pdf,"methodology over a range of
simulated additive noise. The resulting evaluation
shows that the capability of the Machine Learning
(ML) models to capture the process accurately
is, indeed, coupled with the correctness of the
explainability of the underlying data-generating
process. We furthermore show the differences
between XAI methods in their ability to correctly
predict the true sensitivity of the modeled indus-
trial process.
1. Introduction
ML approaches have the power to model complex depen-
dencies in demanding tasks such as industrial processes.
However, the behavior of these industrial processes that rely
on complex, non-linear interactions is often not fully under-
stood. This results in the need for algorithms to understand
and interpret how these ML models arrive at certain predic-
1Signal Processing and Speech Communication Laboratory,
Technical University Graz, Graz, Austria2voestalpine Stahl GmbH,
Linz, Austria3K1-MET GmbH, Linz, Austria. Correspondence
to: Benedikt Kantz <benedikt.kantz@tugraz.at >, Franz Pernkopf
<pernkopf@tugraz.at >.
Accepted at the 1st Machine Learning for Life and Material Sci-
ences Workshop at ICML 2024 . Copyright 2024 by the author(s).tions and how they might react to certain perturbances in
the input. In the last years, there has been an effort to pro-
vide explanations to the ML model predictions using XAI
(Lundberg & Lee, 2017; Ribeiro et al., 2018; Alvarez-Melis
& Jaakkola, 2018; Shrikumar et al., 2017).
Most of these works, even if they focus on the robustness
and trustworthiness of the XAI method, have the short-
coming that they can only be evaluated through surrogate
measures (Crabb ´e & van der Schaar, 2023), and the ground
truth sensitivity of the evaluated datasets cannot be prop-
erly calculated (Alvarez-Melis & Jaakkola, 2018). Some
existing approaches rather use data augmentation (Sun et al.,
2020) or create measures estimating the importance of the
features (Yeh et al., 2019); further related work is provided
i","Future Work
We showed how different XAI methods are affected based
on the predictive performance of the ML models. The focus
of this work was on model-agnostic post-hoc explanations
for local data samples, as these could be evaluated using
numeric, data-driven approaches. Of these XAI methods,
SHAP, LIME, SG, and a local version of ALE were chosen.
These were evaluated using a novel evaluation process fo-
cused on scaling the feature importance scores to a similar
magnitude within one sample, then normalizing them to the
same range as the ground truth effects, and, finally, calculat-
ing the distance to the ground truth effects reference. This
ground truth was generated using a chemical simulation ofan EAF model, providing the necessary ground truth sensi-
tivity for comparison and evaluation. This data-generating
distribution was chosen based on the maturity of the EAF
models for these real-world processes as well as current
interest in the technology due to its promise of cleaner steel
production. Additionally, a toy example was initially used
to test the approaches on a limited and known nonlinear
dataset.
Noise analysis over a range of perturbances of the initial
dataset was performed using this evaluation methodology.
The resulting analysis lends the conclusion that XGBoost
in combination with smooth gradient-based XAI methods
can approximate both the target values as well as the ground
truth interpretations very well, even in noisy environments.
LIME and SHAP, however, were not as successful in cor-
rectly finding the ground truth feature importance scores,
probably due to their differing approaches to the interpreta-
tion of the feature importance scores. Some of these XAI
methods and ML models, furthermore, showed a higher vari-
ance, indicating that they varied between sampling runs and
were affected by the high noise. Sanity checks on the valid-
ity of the evaluation process were carried out using noise,
first, as evaluation data, and then as training data","methodology over a range of simulated additive noise. The resulting evaluation shows that the capability of the Machine Learning (ML) models to capture the process accurately is, indeed, coupled with the correctness of the explainability of the underlying data-generating process. We furthermore show the differences between XAI methods in their ability to correctly predict the true sensitivity of the modeled indus- trial process. 1. Introduction ML approaches have the power to model complex depen- dencies in demanding tasks such as industrial processes. However, the behavior of these industrial processes that rely on complex, non-linear interactions is often not fully under- stood. This results in the need for algorithms to understand and interpret how these ML models arrive at certain predic- 1Signal Processing and Speech Communication Laboratory, Technical University Graz, Graz, Austria2voestalpine Stahl GmbH, Linz, Austria3K1-MET GmbH, Linz, Austria. Correspondence to: Benedikt Kantz <benedikt.kantz@tugraz.at >, Franz Pernkopf <pernkopf@tugraz.at >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).tions and how they might react to certain perturbances in the input. In the last years, there has been an effort to pro- vide explanations to the ML model predictions using XAI (Lundberg & Lee, 2017; Ribeiro et al., 2018; Alvarez-Melis & Jaakkola, 2018; Shrikumar et al., 2017). Most of these works, even if they focus on the robustness and trustworthiness of the XAI method, have the short- coming that they can only be evaluated through surrogate measures (Crabb e & van der Schaar, 2023), and the ground truth sensitivity of the evaluated datasets cannot be prop- erly calculated (Alvarez-Melis & Jaakkola, 2018). Some existing approaches rather use data augmentation (Sun et al., 2020) or create measures estimating the importance of the features (Yeh et al., 2019); further related work is provided i","Future Work We showed how different XAI methods are affected based on the predictive performance of the ML models. The focus of this work was on model-agnostic post-hoc explanations for local data samples, as these could be evaluated using numeric, data-driven approaches. Of these XAI methods, SHAP, LIME, SG, and a local version of ALE were chosen. These were evaluated using a novel evaluation process fo- cused on scaling the feature importance scores to a similar magnitude within one sample, then normalizing them to the same range as the ground truth effects, and, finally, calculat- ing the distance to the ground truth effects reference. This ground truth was generated using a chemical simulation ofan EAF model, providing the necessary ground truth sensi- tivity for comparison and evaluation. This data-generating distribution was chosen based on the maturity of the EAF models for these real-world processes as well as current interest in the technology due to its promise of cleaner steel production. Additionally, a toy example was initially used to test the approaches on a limited and known nonlinear dataset. Noise analysis over a range of perturbances of the initial dataset was performed using this evaluation methodology. The resulting analysis lends the conclusion that XGBoost in combination with smooth gradient-based XAI methods can approximate both the target values as well as the ground truth interpretations very well, even in noisy environments. LIME and SHAP, however, were not as successful in cor- rectly finding the ground truth feature importance scores, probably due to their differing approaches to the interpreta- tion of the feature importance scores. Some of these XAI methods and ML models, furthermore, showed a higher vari- ance, indicating that they varied between sampling runs and were affected by the high noise. Sanity checks on the valid- ity of the evaluation process were carried out using noise, first, as evaluation data, and then as training data","Robustness of Explainable Artificial Intelligence in Industrial Process Modelling eXplainable Artificial Intelligence (XAI) aims at providing understandable explanations of black box models. In this paper, we evaluate current XAI methods by scoring them based on ground truth simulations and sensitivity analysis. To this end, we used an Electric Arc Furnace (EAF) model to better understand the limits and robustness characteristics of XAI methods such as SHapley Additive exPlanations (SHAP), Local Interpretable Model-agnostic Explanations (LIME), as well as Averaged Local Effects (ALE) or Smooth Gradients (SG) in a highly topical setting. These XAI methods were applied to various types of black-box models and then scored based on their correctness compared to the ground-truth sensitivity of the data-generating processes using a novel scoring evaluation methodology over a range of simulated additive noise. The resulting evaluation shows that the capability of the Machine Learning (ML) models to capture the process accurately is, indeed, coupled with the correctness of the explainability of the underlying data-generating process. We furthermore show the differences between XAI methods in their ability to correctly predict the true sensitivity of the modeled industrial process.","Robustness of Explainable Artificial Intelligence in Industrial Process Modelling eXplainable Artificial Intelligence (XAI) aims at providing understandable explanations of black box models. In this paper, we evaluate current XAI methods by scoring them based on ground truth simulations and sensitivity analysis. To this end, we used an Electric Arc Furnace (EAF) model to better understand the limits and robustness characteristics of XAI methods such as SHapley Additive exPlanations (SHAP), Local Interpretable Model-agnostic Explanations (LIME), as well as Averaged Local Effects (ALE) or Smooth Gradients (SG) in a highly topical setting. These XAI methods were applied to various types of black-box models and then scored based on their correctness compared to the ground-truth sensitivity of the data-generating processes using a novel scoring evaluation methodology over a range of simulated additive noise. The resulting evaluation shows that the capability of the Machine Learning (ML) models to capture the process accurately is, indeed, coupled with the correctness of the explainability of the underlying data-generating process. We furthermore show the differences between XAI methods in their ability to correctly predict the true sensitivity of the modeled industrial process. Future Work We showed how different XAI methods are affected based on the predictive performance of the ML models. The focus of this work was on model-agnostic post-hoc explanations for local data samples, as these could be evaluated using numeric, data-driven approaches. Of these XAI methods, SHAP, LIME, SG, and a local version of ALE were chosen. These were evaluated using a novel evaluation process fo- cused on scaling the feature importance scores to a similar magnitude within one sample, then normalizing them to the same range as the ground truth effects, and, finally, calculat- ing the distance to the ground truth effects reference. This ground truth was generated using a chemical simulation ofan EAF model, providing the necessary ground truth sensi- tivity for comparison and evaluation. This data-generating distribution was chosen based on the maturity of the EAF models for these real-world processes as well as current interest in the technology due to its promise of cleaner steel production. Additionally, a toy example was initially used to test the approaches on a limited and known nonlinear dataset. Noise analysis over a range of perturbances of the initial dataset was performed using this evaluation methodology. The resulting analysis lends the conclusion that XGBoost in combination with smooth gradient-based XAI methods can approximate both the target values as well as the ground truth interpretations very well, even in noisy environments. LIME and SHAP, however, were not as successful in cor- rectly finding the ground truth feature importance scores, probably due to their differing approaches to the interpreta- tion of the feature importance scores. Some of these XAI methods and ML models, furthermore, showed a higher vari- ance, indicating that they varied between sampling runs and were affected by the high noise. Sanity checks on the valid- ity of the evaluation process were carried out using noise, first, as evaluation data, and then as training data methodology over a range of simulated additive noise. The resulting evaluation shows that the capability of the Machine Learning (ML) models to capture the process accurately is, indeed, coupled with the correctness of the explainability of the underlying data-generating process. We furthermore show the differences between XAI methods in their ability to correctly predict the true sensitivity of the modeled indus- trial process. 1. Introduction ML approaches have the power to model complex depen- dencies in demanding tasks such as industrial processes. However, the behavior of these industrial processes that rely on complex, non-linear interactions is often not fully under- stood. This results in the need for algorithms to understand and interpret how these ML models arrive at certain predic- 1Signal Processing and Speech Communication Laboratory, Technical University Graz, Graz, Austria2voestalpine Stahl GmbH, Linz, Austria3K1-MET GmbH, Linz, Austria. Correspondence to: Benedikt Kantz <benedikt.kantz@tugraz.at >, Franz Pernkopf <pernkopf@tugraz.at >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).tions and how they might react to certain perturbances in the input. In the last years, there has been an effort to pro- vide explanations to the ML model predictions using XAI (Lundberg & Lee, 2017; Ribeiro et al., 2018; Alvarez-Melis & Jaakkola, 2018; Shrikumar et al., 2017). Most of these works, even if they focus on the robustness and trustworthiness of the XAI method, have the short- coming that they can only be evaluated through surrogate measures (Crabb e & van der Schaar, 2023), and the ground truth sensitivity of the evaluated datasets cannot be prop- erly calculated (Alvarez-Melis & Jaakkola, 2018). Some existing approaches rather use data augmentation (Sun et al., 2020) or create measures estimating the importance of the features (Yeh et al., 2019); further related work is provided i",0
b09603cf77e16bc99e1c029cd1c2d1b7012a84da,Protein language models expose viral mimicry and immune escape,"['Dan Ofer', 'Michal Linial']",https://openreview.net/pdf/b09603cf77e16bc99e1c029cd1c2d1b7012a84da.pdf,"Protein language models expose viral mimicry and immune escape Protein language models to can differentiate between human and viral proteins with high accuracy, uncovering patterns of viral mimicry and immune escape, with high interpretability using a multimodal approach. Viruses elude the immune system through molecular mimicry, adopting their hosts biophysical characteristics. We adapt protein language models (PLMs) to differenti-ate between human and viral proteins. Understanding where the immune system and our models make mistakes could reveal viral immune escape mechanisms. We ap-plied pretrained PLMs to predict viral from human pro-teins. achieving a state-of-the-art results (99.7% ROCAUC). We use interpretable models to characterize viral escapers. Altogether, mistakes account for 3.9% of the sequences with viral proteins being disproportionally misclassified. Errors often involve proteins with low im-munogenic potential, human specific viruses, and reverse transcriptases. Viral families causing chronic infections and immune evasion are further enriched. Biological and ML models make similar mistakes. Integrating PLMs with explainable AI, we provide novel insights into viral im-mune escape mechanisms, enhancing strategies for vac-cine development and antiviral research.",b09603cf77e16bc99e1c029cd1c2d1b7012a84da.pdf,"approach offers novel in-
sights into the mechanisms underlying viral escape. Our 
findings reveal a striking parallel between mistakes made 
by PLMs in classifying proteins and those encountered by 
the failures of the natural immune system of the host. Mit-
igating the impact of viral infections on human health can 
benefit from inspecting the model's success and faulty clas-
sification in cases of viral mimicry.  
2 Methods  
2.1. Protein datasets  
All reviewed human proteins and virus proteins with a 
known, vertebrate host were downloaded from SwissProt 
within the UniProtKB database (https://www.uniprot.org) 
(Status: Reviewed, non -fragment, Nov 2021), along with 
their Uniref50 and 90 sequence simila rity clusters, and an-
notations: UniProt keywords, name, taxonomy, virus -host, 
and length.  
Duplicate sequences at the UniRef90 level were dropped to 
reduce redundancy. Proteins longer than 1,600 were ex-
cluded. The dataset was shuffled and partitioned by 
UniRef50 clusters into a training subset (80%) and a test 
subset (20%). Sequences sharing the  same cluster (i.e., 
greater than 50% sequence identity) were always disjoint 
between train and test sets. Protein -level embeddings were 
downloaded from UniProt. Virus family, genus, and Balti-
more classification were downloaded from ViralZone 
(Masson, et a l., 2012).  
 
2.2. Pretrained deep language models (ESM, T5)  
ESM2 is a deep learning architecture, based on the BERT 
Transformer model (Lin, et al., 2023). It was pretrained on 
the UniRef50 dataset to predict masked -out amino acids 
(tokens). It can efficiently represent amino acid sequences 
and has shown good perfor mance across different protein 
predictive tasks. We used different -sized ESM2 models.  
We use mean pooling for extracting a sequence -level rep-
resentation of each protein. Th is approach has been shown 
to yield a good representation in sequence -level problems. 
Specifically, the final dense layer of the chosen model is 
take","future work on the evolutionary 
strategies that these specific viruses developed to evade the 
host immune system, such as suppressing the adaptive im-
mune system, or strategi es that may have oncological im-
pact and relevance.  
 
 
References  
 
Bahir, I. , et al.  Viral adaptation to host: a proteome‐based 
analysis of codon usage and amino acid preferences. 
Molecular systems biology  2009;5(1):311.  
Begum, S. , et al.  Molecular Mimicry Analyses Unveiled 
the Human Herpes Simplex and Poxvirus Epitopes as 
Possible Candidates to Incite Autoimmunity. Pathogens  
2022;11(11).  
Brandes, N. , et al.  Genome -wide prediction of disease 
variant effects with a deep protein language model. Nat 
Genet  2023;55(9):1512 -1522.  
Brandes, N. and Linial, M. Gene overlapping and size 
constraints in the viral world. Biology direct  2016;11:1 -
15. 
Brandes, N. , et al.  ProteinBERT: a universal deep -
learning model of protein sequence and function. 
Bioinformatics  2022;38(8):2102 -2110.  
Cohen, J.I. Herpesvirus latency. The Journal of clinical 
investigation  2020;130(7):3361 -3369.  
 
 Elnaggar, A. , et al.  ProtTrans: Toward Understanding the 
Language of Life Through Self -Supervised Learning. 
IEEE Trans Pattern Anal Mach Intell  
2022;44(10):7112 -7127.  
Elsayed, G. , et al.  Adversarial examples that fool both 
computer vision and time -limited humans. Advances in 
neural information processing systems  2018;31.  
Geirhos, R. , et al.  ImageNet -trained CNNs are biased 
towards texture; increasing shape bias improves 
accuracy and robustness. arXiv preprint 
arXiv:1811.12231  2018.  
Hie, B. , et al.  Learning the language of viral evolution and 
escape. Science  2021;371(6526):284 -288. 
Hu, E.J. , et al.  Lora: Low -rank adaptation of large 
language models. arXiv preprint arXiv:2106.09685  
2021.  
Lin, Z. , et al.  Evolutionary -scale prediction of atomic -
level protein structure with a language model. Science  
2023;379(6637):1123 -1130.  
Mahmoudabadi","approach offers novel in- sights into the mechanisms underlying viral escape. Our findings reveal a striking parallel between mistakes made by PLMs in classifying proteins and those encountered by the failures of the natural immune system of the host. Mit- igating the impact of viral infections on human health can benefit from inspecting the model's success and faulty clas- sification in cases of viral mimicry. 2 Methods 2.1. Protein datasets All reviewed human proteins and virus proteins with a known, vertebrate host were downloaded from SwissProt within the UniProtKB database (https://www.uniprot.org) (Status: Reviewed, non -fragment, Nov 2021), along with their Uniref50 and 90 sequence simila rity clusters, and an- notations: UniProt keywords, name, taxonomy, virus -host, and length. Duplicate sequences at the UniRef90 level were dropped to reduce redundancy. Proteins longer than 1,600 were ex- cluded. The dataset was shuffled and partitioned by UniRef50 clusters into a training subset (80%) and a test subset (20%). Sequences sharing the same cluster (i.e., greater than 50% sequence identity) were always disjoint between train and test sets. Protein -level embeddings were downloaded from UniProt. Virus family, genus, and Balti- more classification were downloaded from ViralZone (Masson, et a l., 2012). 2.2. Pretrained deep language models (ESM, T5) ESM2 is a deep learning architecture, based on the BERT Transformer model (Lin, et al., 2023). It was pretrained on the UniRef50 dataset to predict masked -out amino acids (tokens). It can efficiently represent amino acid sequences and has shown good perfor mance across different protein predictive tasks. We used different -sized ESM2 models. We use mean pooling for extracting a sequence -level rep- resentation of each protein. Th is approach has been shown to yield a good representation in sequence -level problems. Specifically, the final dense layer of the chosen model is take","future work on the evolutionary strategies that these specific viruses developed to evade the host immune system, such as suppressing the adaptive im- mune system, or strategi es that may have oncological im- pact and relevance.","Protein language models expose viral mimicry and immune escape Protein language models to can differentiate between human and viral proteins with high accuracy, uncovering patterns of viral mimicry and immune escape, with high interpretability using a multimodal approach. Viruses elude the immune system through molecular mimicry, adopting their hosts biophysical characteristics. We adapt protein language models (PLMs) to differenti-ate between human and viral proteins. Understanding where the immune system and our models make mistakes could reveal viral immune escape mechanisms. We ap-plied pretrained PLMs to predict viral from human pro-teins. achieving a state-of-the-art results (99.7% ROCAUC). We use interpretable models to characterize viral escapers. Altogether, mistakes account for 3.9% of the sequences with viral proteins being disproportionally misclassified. Errors often involve proteins with low im-munogenic potential, human specific viruses, and reverse transcriptases. Viral families causing chronic infections and immune evasion are further enriched. Biological and ML models make similar mistakes. Integrating PLMs with explainable AI, we provide novel insights into viral im-mune escape mechanisms, enhancing strategies for vac-cine development and antiviral research.","Protein language models expose viral mimicry and immune escape Protein language models to can differentiate between human and viral proteins with high accuracy, uncovering patterns of viral mimicry and immune escape, with high interpretability using a multimodal approach. Viruses elude the immune system through molecular mimicry, adopting their hosts biophysical characteristics. We adapt protein language models (PLMs) to differenti-ate between human and viral proteins. Understanding where the immune system and our models make mistakes could reveal viral immune escape mechanisms. We ap-plied pretrained PLMs to predict viral from human pro-teins. achieving a state-of-the-art results (99.7% ROCAUC). We use interpretable models to characterize viral escapers. Altogether, mistakes account for 3.9% of the sequences with viral proteins being disproportionally misclassified. Errors often involve proteins with low im-munogenic potential, human specific viruses, and reverse transcriptases. Viral families causing chronic infections and immune evasion are further enriched. Biological and ML models make similar mistakes. Integrating PLMs with explainable AI, we provide novel insights into viral im-mune escape mechanisms, enhancing strategies for vac-cine development and antiviral research. future work on the evolutionary strategies that these specific viruses developed to evade the host immune system, such as suppressing the adaptive im- mune system, or strategi es that may have oncological im- pact and relevance. approach offers novel in- sights into the mechanisms underlying viral escape. Our findings reveal a striking parallel between mistakes made by PLMs in classifying proteins and those encountered by the failures of the natural immune system of the host. Mit- igating the impact of viral infections on human health can benefit from inspecting the model's success and faulty clas- sification in cases of viral mimicry. 2 Methods 2.1. Protein datasets All reviewed human proteins and virus proteins with a known, vertebrate host were downloaded from SwissProt within the UniProtKB database (https://www.uniprot.org) (Status: Reviewed, non -fragment, Nov 2021), along with their Uniref50 and 90 sequence simila rity clusters, and an- notations: UniProt keywords, name, taxonomy, virus -host, and length. Duplicate sequences at the UniRef90 level were dropped to reduce redundancy. Proteins longer than 1,600 were ex- cluded. The dataset was shuffled and partitioned by UniRef50 clusters into a training subset (80%) and a test subset (20%). Sequences sharing the same cluster (i.e., greater than 50% sequence identity) were always disjoint between train and test sets. Protein -level embeddings were downloaded from UniProt. Virus family, genus, and Balti- more classification were downloaded from ViralZone (Masson, et a l., 2012). 2.2. Pretrained deep language models (ESM, T5) ESM2 is a deep learning architecture, based on the BERT Transformer model (Lin, et al., 2023). It was pretrained on the UniRef50 dataset to predict masked -out amino acids (tokens). It can efficiently represent amino acid sequences and has shown good perfor mance across different protein predictive tasks. We used different -sized ESM2 models. We use mean pooling for extracting a sequence -level rep- resentation of each protein. Th is approach has been shown to yield a good representation in sequence -level problems. Specifically, the final dense layer of the chosen model is take",0
ea8dc3f4f9437c6ad83f3a1ab1baf06247932801,Benchmarking probabilistic machine learning in protein ﬁtness landscape predictions,"['Ningning Chen', 'Wenkai Han', 'Sai T. Reddy']",https://openreview.net/pdf/ea8dc3f4f9437c6ad83f3a1ab1baf06247932801.pdf,"Benchmarking probabilistic machine learning in protein tness landscape predictions Machine learning guided protein engineering, which consists of high-throughput screening and deep sequencing of protein mutagenesis libraries combined with machine learning is a powerful approach for engineering proteins and interrogating their tness landscapes. Uncertainty quanti cation enhances the trustworthiness of model predictions by indicating reliability and thus can be used to guide downstream experimental work. Aleatoric uncertainty identifying inherent observational noise in protein properties and epistemic uncertainty revealing gaps in the model s knowledge based on the amount of training data. Although uncertainty quanti cation has been investigated in the application of protein engineering, systematic benchmarks for probabilistic machine learning model selection and the bene ts of different types of uncertainty in protein tness predictions are lacking. Addressing this gap, our study benchmarks six advanced probabilistic modeling techniques across eleven diverse protein- tness datasets, employing evaluation metrics on prediction accuracy and uncertainty quality to assess performance for both in-distribution and out-ofdistribution scenarios. Our ndings offer valuable insights into the application of uncertaintyaware machine learning in high-throughput protein screening experiments. Our study supports more robust, ef cient experimental processes and enhances the practical usability of machine learning models in real-word protein tness related tasks such as therapeutic antibody optimization and viral evolution.",ea8dc3f4f9437c6ad83f3a1ab1baf06247932801.pdf,"approach for engineering proteins and interrogat-
ing their ﬁtness landscapes. Uncertainty quan-
tiﬁcation enhances the trustworthiness of model
predictions by indicating reliability and thus can
be used to guide downstream experimental work.
Aleatoric uncertainty identifying inherent obser-
vational noise in protein properties and epistemic
uncertainty revealing gaps in the model’s knowl-
edge based on the amount of training data. Al-
though uncertainty quantiﬁcation has been inves-
tigated in the application of protein engineering,
systematic benchmarks for probabilistic machine
learning model selection and the beneﬁts of differ-
ent types of uncertainty in protein ﬁtness predic-
tions are lacking. Addressing this gap, our study
benchmarks six advanced probabilistic modeling
techniques across eleven diverse protein-ﬁtness
datasets, employing evaluation metrics on predic-
tion accuracy and uncertainty quality to assess
performance for both in-distribution and out-of-
distribution scenarios. Our ﬁndings offer valu-
able insights into the application of uncertainty-
aware machine learning in high-throughput pro-
tein screening experiments. Our study supports
more robust, efﬁcient experimental processes and
enhances the practical usability of machine learn-
ing models in real-word protein ﬁtness related
tasks such as therapeutic antibody optimization
and viral evolution.
*Equal contribution1ETH Zurich, Department of Biosystems
Science and Engineering, Basel 4056, Switzerland2King Abdullah
University of Science and Technology, CBRC. Correspondence to:
Sai T Reddy <sai.reddy@bsse.ethz.ch >.
Accepted at the 1st Machine Learning for Life and Material Sci-
ences Workshop at ICML 2024 . Copyright 2024 by the author(s).1. Introduction
Depicting protein ﬁtness landscapes from amino acid se-
quences is crucial for both evolutionary biology and protein
engineering. Traditional experimental approaches to char-
acterize protein ﬁtness can be laborious and resource inten-
sive. Fo","future work, we plan to test the models with varying
training data sizes, hypothesizing that the GP model may
be more robust in low-data regimes, whereas Bayesian neu-
ral networks and dropout techniques might perform better
with larger datasets. Additionally, there is a need for a
standardized framework to evaluate model performance and
uncertainty estimation to accelerate protein engineering and
other real-world applications.
It is also worth exploring the application of uncertainty
values in next-round experimental design using strategies
such as active learning or Bayesian optimization.
References
Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra,
D. Weight uncertainty in neural network. In International
conference on machine learning , pp. 1613–1622. PMLR,
2015.
Fowler, D. M. and Fields, S. Deep mutational scanning:
a new style of protein science. Nature methods , 11(8):
801–807, 2014.
Gal, Y . and Ghahramani, Z. Dropout as a bayesian approx-
imation: Representing model uncertainty in deep learn-
ing. In international conference on machine learning , pp.
1050–1059. PMLR, 2016.
Greenman, K. P., Amini, A. P., and Yang, K. K. Bench-
marking uncertainty quantiﬁcation for protein engineer-
ing.bioRxiv , pp. 2023–04, 2023.
Hie, B., Bryson, B. D., and Berger, B. Leveraging uncer-
tainty in machine learning accelerates biological discov-
ery and design. Cell systems , 11(5):461–477, 2020.
Kendall, A. and Gal, Y . What uncertainties do we need in
bayesian deep learning for computer vision? Advances
in neural information processing systems , 30, 2017.Kuleshov, V ., Fenner, N., and Ermon, S. Accurate uncer-
tainties for deep learning using calibrated regression. In
International conference on machine learning , pp. 2796–
2804. PMLR, 2018.
Lakshminarayanan, B., Pritzel, A., and Blundell, C. Simple
and scalable predictive uncertainty estimation using deep
ensembles. Advances in neural information processing
systems , 30, 2017.
MacKay, D. J. et al. Introduction to g","approach for engineering proteins and interrogat- ing their tness landscapes. Uncertainty quan- ti cation enhances the trustworthiness of model predictions by indicating reliability and thus can be used to guide downstream experimental work. Aleatoric uncertainty identifying inherent obser- vational noise in protein properties and epistemic uncertainty revealing gaps in the model s knowl- edge based on the amount of training data. Al- though uncertainty quanti cation has been inves- tigated in the application of protein engineering, systematic benchmarks for probabilistic machine learning model selection and the bene ts of differ- ent types of uncertainty in protein tness predic- tions are lacking. Addressing this gap, our study benchmarks six advanced probabilistic modeling techniques across eleven diverse protein- tness datasets, employing evaluation metrics on predic- tion accuracy and uncertainty quality to assess performance for both in-distribution and out-of- distribution scenarios. Our ndings offer valu- able insights into the application of uncertainty- aware machine learning in high-throughput pro- tein screening experiments. Our study supports more robust, ef cient experimental processes and enhances the practical usability of machine learn- ing models in real-word protein tness related tasks such as therapeutic antibody optimization and viral evolution. *Equal contribution1ETH Zurich, Department of Biosystems Science and Engineering, Basel 4056, Switzerland2King Abdullah University of Science and Technology, CBRC. Correspondence to: Sai T Reddy <sai.reddy@bsse.ethz.ch >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).1. Introduction Depicting protein tness landscapes from amino acid se- quences is crucial for both evolutionary biology and protein engineering. Traditional experimental approaches to char- acterize protein tness can be laborious and resource inten- sive. Fo","future work, we plan to test the models with varying training data sizes, hypothesizing that the GP model may be more robust in low-data regimes, whereas Bayesian neu- ral networks and dropout techniques might perform better with larger datasets. Additionally, there is a need for a standardized framework to evaluate model performance and uncertainty estimation to accelerate protein engineering and other real-world applications. It is also worth exploring the application of uncertainty values in next-round experimental design using strategies such as active learning or Bayesian optimization.","Benchmarking probabilistic machine learning in protein tness landscape predictions Machine learning guided protein engineering, which consists of high-throughput screening and deep sequencing of protein mutagenesis libraries combined with machine learning is a powerful approach for engineering proteins and interrogating their tness landscapes. Uncertainty quanti cation enhances the trustworthiness of model predictions by indicating reliability and thus can be used to guide downstream experimental work. Aleatoric uncertainty identifying inherent observational noise in protein properties and epistemic uncertainty revealing gaps in the model s knowledge based on the amount of training data. Although uncertainty quanti cation has been investigated in the application of protein engineering, systematic benchmarks for probabilistic machine learning model selection and the bene ts of different types of uncertainty in protein tness predictions are lacking. Addressing this gap, our study benchmarks six advanced probabilistic modeling techniques across eleven diverse protein- tness datasets, employing evaluation metrics on prediction accuracy and uncertainty quality to assess performance for both in-distribution and out-ofdistribution scenarios. Our ndings offer valuable insights into the application of uncertaintyaware machine learning in high-throughput protein screening experiments. Our study supports more robust, ef cient experimental processes and enhances the practical usability of machine learning models in real-word protein tness related tasks such as therapeutic antibody optimization and viral evolution.","Benchmarking probabilistic machine learning in protein tness landscape predictions Machine learning guided protein engineering, which consists of high-throughput screening and deep sequencing of protein mutagenesis libraries combined with machine learning is a powerful approach for engineering proteins and interrogating their tness landscapes. Uncertainty quanti cation enhances the trustworthiness of model predictions by indicating reliability and thus can be used to guide downstream experimental work. Aleatoric uncertainty identifying inherent observational noise in protein properties and epistemic uncertainty revealing gaps in the model s knowledge based on the amount of training data. Although uncertainty quanti cation has been investigated in the application of protein engineering, systematic benchmarks for probabilistic machine learning model selection and the bene ts of different types of uncertainty in protein tness predictions are lacking. Addressing this gap, our study benchmarks six advanced probabilistic modeling techniques across eleven diverse protein- tness datasets, employing evaluation metrics on prediction accuracy and uncertainty quality to assess performance for both in-distribution and out-ofdistribution scenarios. Our ndings offer valuable insights into the application of uncertaintyaware machine learning in high-throughput protein screening experiments. Our study supports more robust, ef cient experimental processes and enhances the practical usability of machine learning models in real-word protein tness related tasks such as therapeutic antibody optimization and viral evolution. future work, we plan to test the models with varying training data sizes, hypothesizing that the GP model may be more robust in low-data regimes, whereas Bayesian neu- ral networks and dropout techniques might perform better with larger datasets. Additionally, there is a need for a standardized framework to evaluate model performance and uncertainty estimation to accelerate protein engineering and other real-world applications. It is also worth exploring the application of uncertainty values in next-round experimental design using strategies such as active learning or Bayesian optimization. approach for engineering proteins and interrogat- ing their tness landscapes. Uncertainty quan- ti cation enhances the trustworthiness of model predictions by indicating reliability and thus can be used to guide downstream experimental work. Aleatoric uncertainty identifying inherent obser- vational noise in protein properties and epistemic uncertainty revealing gaps in the model s knowl- edge based on the amount of training data. Al- though uncertainty quanti cation has been inves- tigated in the application of protein engineering, systematic benchmarks for probabilistic machine learning model selection and the bene ts of differ- ent types of uncertainty in protein tness predic- tions are lacking. Addressing this gap, our study benchmarks six advanced probabilistic modeling techniques across eleven diverse protein- tness datasets, employing evaluation metrics on predic- tion accuracy and uncertainty quality to assess performance for both in-distribution and out-of- distribution scenarios. Our ndings offer valu- able insights into the application of uncertainty- aware machine learning in high-throughput pro- tein screening experiments. Our study supports more robust, ef cient experimental processes and enhances the practical usability of machine learn- ing models in real-word protein tness related tasks such as therapeutic antibody optimization and viral evolution. *Equal contribution1ETH Zurich, Department of Biosystems Science and Engineering, Basel 4056, Switzerland2King Abdullah University of Science and Technology, CBRC. Correspondence to: Sai T Reddy <sai.reddy@bsse.ethz.ch >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).1. Introduction Depicting protein tness landscapes from amino acid se- quences is crucial for both evolutionary biology and protein engineering. Traditional experimental approaches to char- acterize protein tness can be laborious and resource inten- sive. Fo",0
15240fc1de5f04e093eb31e2ae98aa5a437d424f,GraphKAN: Graph Kolmogorov Arnold Network for Small Molecule-Protein Interaction Predictions,"['Tashin Ahmed', 'Md Habibur Rahman Sifat']",https://openreview.net/pdf/15240fc1de5f04e093eb31e2ae98aa5a437d424f.pdf,"GraphKAN: Graph Kolmogorov Arnold Network for Small Molecule-Protein Interaction Predictions Graph Kolmogorov Arnold Networks (GraphKAN) for predicting small molecule-protein binding affinities, demonstrating its potential and highlighting the need for further refinement to enhance computational drug discovery. This study presents a proof of concept for utilizing Graph Kolmogorov Arnold Networks (GraphKAN/GKAN) in predicting the binding affinity of small molecules to protein targets. Working with three protein targets, we explored the potential of GraphKAN to infer binding affinities. We compared the performance of GraphKAN with MLP-based graph neural networks, 1D convolutional neural networks (1D CNN), and machine learning algorithms like random forests. Although the model did not achieve state-of-the-art performance, our results demonstrate its feasibility and highlight its promise as a novel approach in computational drug discovery. This work opens new research directions, suggesting that further refinement and exploration of GraphKAN could significantly impact the efficiency and accuracy of binding affinity predictions, ultimately aiding in the discovery of new therapeutic agents. Source code is available at - https://github.com/TashinAhmed/ferroin.",15240fc1de5f04e093eb31e2ae98aa5a437d424f.pdf,"approach in computational drug discov-
ery. This work opens new research directions,
suggesting that further refinement and exploration
of GraphKAN could significantly impact the ef-
ficiency and accuracy of binding affinity predic-
tions, ultimately aiding in the discovery of new
therapeutic agents. Source code is available at -
https://github.com/TashinAhmed/ferroin.
1. Introduction
Kolmogorov Arnold Networks (KAN) (Liu et al., 2024),
a newly designed replacement of Multi-Layer Perceptron
(Haykin, 1998; Cybenko, 1989; Hornik et al., 1989), which
has learnable activation functions on edges (weights) instead
of fixed activation function on nodes (neurons). Also, re-
search like Liquid Time-Constant Networks (LTCs) (Hasani
et al., 2021), a new class of time-continuous recurrent neu-
ral networks with modulated linear dynamics and inter-
linked nonlinear gates, which demonstrate stable behavior,
enhanced expressivity, and superior performance in time-
series prediction tasks compared to traditional and modern
1Independent Researcher, Dhaka, Bangladesh2The Hong Kong
Polytechnic University, Hong Kong. Correspondence to: Tashin
Ahmed <tashinahmed@aol.com >, Md Habibur Rahman Sifat
<habib.sifat@connect.polyu.hk >.
Accepted at the 1st Machine Learning for Life and Material Sci-
ences Workshop at ICML 2024 . Copyright 2024 by the author(s).RNNs. These studies are pioneering new directions in AI
research, prompting us to explore innovative approaches to
developing neural networks.
In this paper, we have prepared a proof of concept employ-
ing KAN instead of MLP with Graph Neural Networks
(GNN) (Zhou et al., 2020) on a small molecule-protein inter-
action prediction problem. We have prepared a comparative
analysis on the results of a popular machine learning algo-
rithm, i.e., Random Forests (Breiman, 2001), 1 Dimensional
CNN (Kiranyaz et al., 2021), a simple MLP-based GNN,
and KAN-based GNN.
The search for effective small molecule drugs, which in-
teract with cellular protei","Future work will involve analyzing the com-
plete dataset.
GraphKAN: Graph Kolmogorov Arnold Network for Small Molecule-Protein Interaction Predictions
C#CC[C@@H](CC(=O)O)NC(=O)
OCC1c2ccccc2-c2ccccc21
C#CCOc1ccc(CN)cc1.Cl
 Br.Br.NCC1CCCN1c1cccnn1
C#CCOc1ccc(CNc2nc(NCC3CCCN3c3cccnn3)
nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1
A B C D
Figure 1. A row from the training dataset. A, B, and C are the building blocks in SMILES format. D is the structure of the fully assembled
molecule in SMILES. This includes the three building blocks and the triazine core. Note [Dy] stands for the DNA linker. Structures are
drawn with rdkit (Bento et al., 2020). For this particular example, the target protein name BRD4 and the binding will not be possible in
this case.
2. Dataset
The training dataset consists of roughly 98 million examples
per protein, with 200,000 validation examples per protein
and 360,000 test molecules per protein (Andrew Blevins,
2024). The test set contains building blocks that do not
appear in the training set to assess the generalizability of
the models. These datasets are highly imbalanced, with
approximately 0.5%of examples classified as binders. The
data collection involved three rounds of triplicate selection
to identify binders experimentally. In our PoC, we have
utilized a subset of this large dataset consisting of 20,000
samples per protein target.
2.1. Protein Targets
Three protein targets were screened in this study:
•EPHX2 (sEH): Soluble epoxide hydrolase, a poten-
tial drug target for high blood pressure and diabetes
progression.
•BRD4: Bromodomain 4, involved in cancer progres-
sion and targeted by drugs inhibiting its activities.
•ALB (HSA): Human serum albumin, the most common
protein in the blood, plays a role in drug absorption
and transport.
Details on the dataset preparation are available in Appendix:
A.3. Methods
3.1. Adaption of GNN to Incorporate KAN
In this study, we showed the adaptation of the traditional
GNN by integrating KANs. This approach aims","approach in computational drug discov- ery. This work opens new research directions, suggesting that further refinement and exploration of GraphKAN could significantly impact the ef- ficiency and accuracy of binding affinity predic- tions, ultimately aiding in the discovery of new therapeutic agents. Source code is available at - https://github.com/TashinAhmed/ferroin. 1. Introduction Kolmogorov Arnold Networks (KAN) (Liu et al., 2024), a newly designed replacement of Multi-Layer Perceptron (Haykin, 1998; Cybenko, 1989; Hornik et al., 1989), which has learnable activation functions on edges (weights) instead of fixed activation function on nodes (neurons). Also, re- search like Liquid Time-Constant Networks (LTCs) (Hasani et al., 2021), a new class of time-continuous recurrent neu- ral networks with modulated linear dynamics and inter- linked nonlinear gates, which demonstrate stable behavior, enhanced expressivity, and superior performance in time- series prediction tasks compared to traditional and modern 1Independent Researcher, Dhaka, Bangladesh2The Hong Kong Polytechnic University, Hong Kong. Correspondence to: Tashin Ahmed <tashinahmed@aol.com >, Md Habibur Rahman Sifat <habib.sifat@connect.polyu.hk >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).RNNs. These studies are pioneering new directions in AI research, prompting us to explore innovative approaches to developing neural networks. In this paper, we have prepared a proof of concept employ- ing KAN instead of MLP with Graph Neural Networks (GNN) (Zhou et al., 2020) on a small molecule-protein inter- action prediction problem. We have prepared a comparative analysis on the results of a popular machine learning algo- rithm, i.e., Random Forests (Breiman, 2001), 1 Dimensional CNN (Kiranyaz et al., 2021), a simple MLP-based GNN, and KAN-based GNN. The search for effective small molecule drugs, which in- teract with cellular protei","Future work will involve analyzing the com- plete dataset. GraphKAN: Graph Kolmogorov Arnold Network for Small Molecule-Protein Interaction Predictions C#CC[C@@H](CC(=O)O)NC(=O) OCC1c2ccccc2-c2ccccc21 C#CCOc1ccc(CN)cc1.Cl Br.Br.NCC1CCCN1c1cccnn1 C#CCOc1ccc(CNc2nc(NCC3CCCN3c3cccnn3) nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1 A B C D Figure 1. A row from the training dataset. A, B, and C are the building blocks in SMILES format. D is the structure of the fully assembled molecule in SMILES. This includes the three building blocks and the triazine core. Note [Dy] stands for the DNA linker. Structures are drawn with rdkit (Bento et al., 2020). For this particular example, the target protein name BRD4 and the binding will not be possible in this case. 2. Dataset The training dataset consists of roughly 98 million examples per protein, with 200,000 validation examples per protein and 360,000 test molecules per protein (Andrew Blevins, 2024). The test set contains building blocks that do not appear in the training set to assess the generalizability of the models. These datasets are highly imbalanced, with approximately 0.5%of examples classified as binders. The data collection involved three rounds of triplicate selection to identify binders experimentally. In our PoC, we have utilized a subset of this large dataset consisting of 20,000 samples per protein target. 2.1. Protein Targets Three protein targets were screened in this study: EPHX2 (sEH): Soluble epoxide hydrolase, a poten- tial drug target for high blood pressure and diabetes progression. BRD4: Bromodomain 4, involved in cancer progres- sion and targeted by drugs inhibiting its activities. ALB (HSA): Human serum albumin, the most common protein in the blood, plays a role in drug absorption and transport. Details on the dataset preparation are available in Appendix: A.3. Methods 3.1. Adaption of GNN to Incorporate KAN In this study, we showed the adaptation of the traditional GNN by integrating KANs. This approach aims","GraphKAN: Graph Kolmogorov Arnold Network for Small Molecule-Protein Interaction Predictions Graph Kolmogorov Arnold Networks (GraphKAN) for predicting small molecule-protein binding affinities, demonstrating its potential and highlighting the need for further refinement to enhance computational drug discovery. This study presents a proof of concept for utilizing Graph Kolmogorov Arnold Networks (GraphKAN/GKAN) in predicting the binding affinity of small molecules to protein targets. Working with three protein targets, we explored the potential of GraphKAN to infer binding affinities. We compared the performance of GraphKAN with MLP-based graph neural networks, 1D convolutional neural networks (1D CNN), and machine learning algorithms like random forests. Although the model did not achieve state-of-the-art performance, our results demonstrate its feasibility and highlight its promise as a novel approach in computational drug discovery. This work opens new research directions, suggesting that further refinement and exploration of GraphKAN could significantly impact the efficiency and accuracy of binding affinity predictions, ultimately aiding in the discovery of new therapeutic agents. Source code is available at - https://github.com/TashinAhmed/ferroin.","GraphKAN: Graph Kolmogorov Arnold Network for Small Molecule-Protein Interaction Predictions Graph Kolmogorov Arnold Networks (GraphKAN) for predicting small molecule-protein binding affinities, demonstrating its potential and highlighting the need for further refinement to enhance computational drug discovery. This study presents a proof of concept for utilizing Graph Kolmogorov Arnold Networks (GraphKAN/GKAN) in predicting the binding affinity of small molecules to protein targets. Working with three protein targets, we explored the potential of GraphKAN to infer binding affinities. We compared the performance of GraphKAN with MLP-based graph neural networks, 1D convolutional neural networks (1D CNN), and machine learning algorithms like random forests. Although the model did not achieve state-of-the-art performance, our results demonstrate its feasibility and highlight its promise as a novel approach in computational drug discovery. This work opens new research directions, suggesting that further refinement and exploration of GraphKAN could significantly impact the efficiency and accuracy of binding affinity predictions, ultimately aiding in the discovery of new therapeutic agents. Source code is available at - https://github.com/TashinAhmed/ferroin. Future work will involve analyzing the com- plete dataset. GraphKAN: Graph Kolmogorov Arnold Network for Small Molecule-Protein Interaction Predictions C#CC[C@@H](CC(=O)O)NC(=O) OCC1c2ccccc2-c2ccccc21 C#CCOc1ccc(CN)cc1.Cl Br.Br.NCC1CCCN1c1cccnn1 C#CCOc1ccc(CNc2nc(NCC3CCCN3c3cccnn3) nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1 A B C D Figure 1. A row from the training dataset. A, B, and C are the building blocks in SMILES format. D is the structure of the fully assembled molecule in SMILES. This includes the three building blocks and the triazine core. Note [Dy] stands for the DNA linker. Structures are drawn with rdkit (Bento et al., 2020). For this particular example, the target protein name BRD4 and the binding will not be possible in this case. 2. Dataset The training dataset consists of roughly 98 million examples per protein, with 200,000 validation examples per protein and 360,000 test molecules per protein (Andrew Blevins, 2024). The test set contains building blocks that do not appear in the training set to assess the generalizability of the models. These datasets are highly imbalanced, with approximately 0.5%of examples classified as binders. The data collection involved three rounds of triplicate selection to identify binders experimentally. In our PoC, we have utilized a subset of this large dataset consisting of 20,000 samples per protein target. 2.1. Protein Targets Three protein targets were screened in this study: EPHX2 (sEH): Soluble epoxide hydrolase, a poten- tial drug target for high blood pressure and diabetes progression. BRD4: Bromodomain 4, involved in cancer progres- sion and targeted by drugs inhibiting its activities. ALB (HSA): Human serum albumin, the most common protein in the blood, plays a role in drug absorption and transport. Details on the dataset preparation are available in Appendix: A.3. Methods 3.1. Adaption of GNN to Incorporate KAN In this study, we showed the adaptation of the traditional GNN by integrating KANs. This approach aims approach in computational drug discov- ery. This work opens new research directions, suggesting that further refinement and exploration of GraphKAN could significantly impact the ef- ficiency and accuracy of binding affinity predic- tions, ultimately aiding in the discovery of new therapeutic agents. Source code is available at - https://github.com/TashinAhmed/ferroin. 1. Introduction Kolmogorov Arnold Networks (KAN) (Liu et al., 2024), a newly designed replacement of Multi-Layer Perceptron (Haykin, 1998; Cybenko, 1989; Hornik et al., 1989), which has learnable activation functions on edges (weights) instead of fixed activation function on nodes (neurons). Also, re- search like Liquid Time-Constant Networks (LTCs) (Hasani et al., 2021), a new class of time-continuous recurrent neu- ral networks with modulated linear dynamics and inter- linked nonlinear gates, which demonstrate stable behavior, enhanced expressivity, and superior performance in time- series prediction tasks compared to traditional and modern 1Independent Researcher, Dhaka, Bangladesh2The Hong Kong Polytechnic University, Hong Kong. Correspondence to: Tashin Ahmed <tashinahmed@aol.com >, Md Habibur Rahman Sifat <habib.sifat@connect.polyu.hk >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).RNNs. These studies are pioneering new directions in AI research, prompting us to explore innovative approaches to developing neural networks. In this paper, we have prepared a proof of concept employ- ing KAN instead of MLP with Graph Neural Networks (GNN) (Zhou et al., 2020) on a small molecule-protein inter- action prediction problem. We have prepared a comparative analysis on the results of a popular machine learning algo- rithm, i.e., Random Forests (Breiman, 2001), 1 Dimensional CNN (Kiranyaz et al., 2021), a simple MLP-based GNN, and KAN-based GNN. The search for effective small molecule drugs, which in- teract with cellular protei",0
545770acf29134decd458b6d8e3e8f274cf26688,Scanning Tunneling Microscopy (STM) Image Segmentation Using Unsupervised and Few-shot Learning,"['Nikola Kolev', 'Emily Hofmann', 'Geoff Thornton', 'Max Trouton', 'Filippo Federici', 'David Gao', 'Steven Schofield', 'Taylor Stock', 'Neil Curson']",https://openreview.net/pdf/545770acf29134decd458b6d8e3e8f274cf26688.pdf,"Scanning Tunneling Microscopy (STM) Image Segmentation Using Unsupervised and Few-shot Learning We reduce the burden of labelling on the user by leveraging unsupervised and few shot learning for segmentation of images taken by a scanning tunelling microscope Scanning tunneling microscopy (STM) is a powerful technique for imaging surfaces with atomic resolution, providing invaluable insights into surface structure and physical and chemical processes occurring on surfaces. A regular task of STM image analysis is detecting and labelling features of interest against the background of the unperturbed surface. Performing this segmentation manually is a labor-intensive task, requiring significant human effort. In this paper, we propose an automated approach to the segmentation of STM images that leverages few-shot learning and unsupervised learning to remove the requirement for large manually annotated datasets. Our technique offers greater flexibility compared to previous supervised methods, being easier to adapt to an unseen surface while maintaining high accuracy, reaching up to 90%. We demonstrate the effectiveness of our approach on two distinct surfaces: Si(001), TiO2(110). We show that our model exhibits strong generalization capabilities, adapting well to unseen surfaces with only as little as one additional labeled data point after initial training. This work represents a significant step towards more efficient and adaptable segmentation of STM images.",545770acf29134decd458b6d8e3e8f274cf26688.pdf,"approach to the segmentation of STM images that leverages few -shot learning and unsupervised learning to  remove the 
requirement for large manually annotated datasets. 
(2) – UNet & Automated Labelling of training data
•UNet is used to produce a binary map of the all the defects 
on the surface (like the one shown in f).
•We want to reduce  the time spent manually  labelling training 
data for the UNet:
•Use a pretrained network (FCNResnet101) to extract 
feature vectors for each pixel.
•These are then clustered using k -means clustering to 
produce a segmented image.
•By varying the resolution of the input, we can change how 
detailed  the segmentation is: higher resolution highlights 
features such as atomic rows and defects , lower 
resolution focuses more on phase domains.
•These images are then augmented, and extra 
experimental noise  is added to train a UNet. In this way, 
we get a more robust , and faster , segmentation network.
(4i) – Si(001):H:AsH3
•Surface is of especial 
significance for the 
semiconductor  and quantum 
computing industry [1][2]. 
•FSL allows for flexibility to 
implement new dopant atom 
precursor types with as little as 
one new labelled data point.
•Models are trained and tested 
on data from the same surface.
(4ii) – Ge(001):AsH3 
and TiO2(3) – FSL networks
•We test multiple few -shot 
learning (FSL) networks.
•The prototypical, matching, 
relation, and simple shot 
(conv 4) all have a Conv 4 
backbone  and are trained 
using episodes on subject 
specific data .
•We test a simple shot 
network with a pretrained 
(non-subject specific ) 
Resnet 18 backbone.
•Are the embeddings 
useful/meaningful ?  - We 
compare to the accuracy of 
KNN on the bare  pixels.
Results for all tables are 
accuracies averaged 
over 100 episodes and 
with 95% confidence 
interval.(1) STM imaging
Classification on TiO2(110) data. TiO2(110) data has only filled state 
images. Trained on defects from non -TiO2(110) data.Classification on Ge(001):AsH3 data. Tr",,"approach to the segmentation of STM images that leverages few -shot learning and unsupervised learning to remove the requirement for large manually annotated datasets. (2) UNet & Automated Labelling of training data UNet is used to produce a binary map of the all the defects on the surface (like the one shown in f). We want to reduce the time spent manually labelling training data for the UNet: Use a pretrained network (FCNResnet101) to extract feature vectors for each pixel. These are then clustered using k -means clustering to produce a segmented image. By varying the resolution of the input, we can change how detailed the segmentation is: higher resolution highlights features such as atomic rows and defects , lower resolution focuses more on phase domains. These images are then augmented, and extra experimental noise is added to train a UNet. In this way, we get a more robust , and faster , segmentation network. (4i) Si(001):H:AsH3 Surface is of especial significance for the semiconductor and quantum computing industry [1][2]. FSL allows for flexibility to implement new dopant atom precursor types with as little as one new labelled data point. Models are trained and tested on data from the same surface. (4ii) Ge(001):AsH3 and TiO2(3) FSL networks We test multiple few -shot learning (FSL) networks. The prototypical, matching, relation, and simple shot (conv 4) all have a Conv 4 backbone and are trained using episodes on subject specific data . We test a simple shot network with a pretrained (non-subject specific ) Resnet 18 backbone. Are the embeddings useful/meaningful ? - We compare to the accuracy of KNN on the bare pixels. Results for all tables are accuracies averaged over 100 episodes and with 95% confidence interval.(1) STM imaging Classification on TiO2(110) data. TiO2(110) data has only filled state images. Trained on defects from non -TiO2(110) data.Classification on Ge(001):AsH3 data. Tr",,"Scanning Tunneling Microscopy (STM) Image Segmentation Using Unsupervised and Few-shot Learning We reduce the burden of labelling on the user by leveraging unsupervised and few shot learning for segmentation of images taken by a scanning tunelling microscope Scanning tunneling microscopy (STM) is a powerful technique for imaging surfaces with atomic resolution, providing invaluable insights into surface structure and physical and chemical processes occurring on surfaces. A regular task of STM image analysis is detecting and labelling features of interest against the background of the unperturbed surface. Performing this segmentation manually is a labor-intensive task, requiring significant human effort. In this paper, we propose an automated approach to the segmentation of STM images that leverages few-shot learning and unsupervised learning to remove the requirement for large manually annotated datasets. Our technique offers greater flexibility compared to previous supervised methods, being easier to adapt to an unseen surface while maintaining high accuracy, reaching up to 90%. We demonstrate the effectiveness of our approach on two distinct surfaces: Si(001), TiO2(110). We show that our model exhibits strong generalization capabilities, adapting well to unseen surfaces with only as little as one additional labeled data point after initial training. This work represents a significant step towards more efficient and adaptable segmentation of STM images.","Scanning Tunneling Microscopy (STM) Image Segmentation Using Unsupervised and Few-shot Learning We reduce the burden of labelling on the user by leveraging unsupervised and few shot learning for segmentation of images taken by a scanning tunelling microscope Scanning tunneling microscopy (STM) is a powerful technique for imaging surfaces with atomic resolution, providing invaluable insights into surface structure and physical and chemical processes occurring on surfaces. A regular task of STM image analysis is detecting and labelling features of interest against the background of the unperturbed surface. Performing this segmentation manually is a labor-intensive task, requiring significant human effort. In this paper, we propose an automated approach to the segmentation of STM images that leverages few-shot learning and unsupervised learning to remove the requirement for large manually annotated datasets. Our technique offers greater flexibility compared to previous supervised methods, being easier to adapt to an unseen surface while maintaining high accuracy, reaching up to 90%. We demonstrate the effectiveness of our approach on two distinct surfaces: Si(001), TiO2(110). We show that our model exhibits strong generalization capabilities, adapting well to unseen surfaces with only as little as one additional labeled data point after initial training. This work represents a significant step towards more efficient and adaptable segmentation of STM images.  approach to the segmentation of STM images that leverages few -shot learning and unsupervised learning to remove the requirement for large manually annotated datasets. (2) UNet & Automated Labelling of training data UNet is used to produce a binary map of the all the defects on the surface (like the one shown in f). We want to reduce the time spent manually labelling training data for the UNet: Use a pretrained network (FCNResnet101) to extract feature vectors for each pixel. These are then clustered using k -means clustering to produce a segmented image. By varying the resolution of the input, we can change how detailed the segmentation is: higher resolution highlights features such as atomic rows and defects , lower resolution focuses more on phase domains. These images are then augmented, and extra experimental noise is added to train a UNet. In this way, we get a more robust , and faster , segmentation network. (4i) Si(001):H:AsH3 Surface is of especial significance for the semiconductor and quantum computing industry [1][2]. FSL allows for flexibility to implement new dopant atom precursor types with as little as one new labelled data point. Models are trained and tested on data from the same surface. (4ii) Ge(001):AsH3 and TiO2(3) FSL networks We test multiple few -shot learning (FSL) networks. The prototypical, matching, relation, and simple shot (conv 4) all have a Conv 4 backbone and are trained using episodes on subject specific data . We test a simple shot network with a pretrained (non-subject specific ) Resnet 18 backbone. Are the embeddings useful/meaningful ? - We compare to the accuracy of KNN on the bare pixels. Results for all tables are accuracies averaged over 100 episodes and with 95% confidence interval.(1) STM imaging Classification on TiO2(110) data. TiO2(110) data has only filled state images. Trained on defects from non -TiO2(110) data.Classification on Ge(001):AsH3 data. Tr",0
21a500542c9db53d3b5b4801aa155b5718d2bd5c,DualBind: A Dual-Loss Framework for Protein-Ligand Binding Affinity Prediction,"['Meng Liu', 'Saee Gopal Paliwal']",https://openreview.net/pdf/21a500542c9db53d3b5b4801aa155b5718d2bd5c.pdf,"DualBind: A Dual-Loss Framework for Protein-Ligand Binding Affinity Prediction Accurate prediction of protein-ligand binding affinities is crucial for drug development. Recent advances in machine learning show promising results on this task. However, these methods typically rely heavily on labeled data, which can be scarce or unreliable, or they rely on assumptions like Boltzmann-distributed data that may not hold true in practice. Here, we present DualBind, a novel framework that integrates supervised mean squared error (MSE) with unsupervised denoising score matching (DSM) to accurately learn the binding energy function. DualBind not only addresses the limitations of DSM-only models by providing more accurate absolute affinity predictions but also improves generalizability and reduces reliance on labeled data compared to MSE-only models. Our experimental results demonstrate that DualBind excels in predicting binding affinities and can effectively utilize both labeled and unlabeled data to enhance performance.",21a500542c9db53d3b5b4801aa155b5718d2bd5c.pdf,"methodology. DualBind employs a dual-loss framework that combines the MSE loss LMSE
and the DSM loss LDSM. Specifically, LMSEanchors the predicted binding affinity of the crystal structure to its experimentally determined
binding affinity. Concurrently, LDSMshapes the gradient at the perturbed structure. Details are described in Section 2.
the coordinates of these atoms. We define a binding affin-
ity prediction model as Eθ(A,X) :Rn×d×Rn×3→R,
which produces a scalar energy value for any given complex.
θdenotes the learnable parameters in the model.
Intuitively, our dual-loss framework combines the DSM loss
LDSM, which learns the energy landscape by shaping the gra-
dient of the energy function, with the MSE loss LMSE, which
directly ties the predictions to known binding affinity values.
The overview of our DualBind approach is illustrated in
Figure 2.
MSE Loss. As shown in the upper branch of Figure 2, the
use of the MSE loss is straightforward. We feed a complex
crystal structure (A,X)into the model and then compute
the error between the predicted and the ground truth binding
affinity. Formally, for one data sample,
LMSE= (Eθ(A,X)−y)2, (1)
where ydenotes the the ground truth binding affinity. In-
tuitively, this MSE loss encourages that the points, which
represent observed crystal structures within the energy land-
scape, are anchored to experimental binding affinities, main-
taining their alignment with empirical observations.
DSM Loss. Training neural networks with denoising is
a well-established technique aimed at enhancing the ro-
bustness and generalization capabilities of models (Bishop,
1995; Vincent et al., 2008; Kong et al., 2020; Godwin et al.,
2021). More specifically, the denoising score matching tech-
nique has been employed to pretrain models for 3D molecu-
lar tasks, demonstrating that such objective not only simu-
lates learning a molecular force field but also significantly
boosts performance across various downstream tasks (Zaidi
et al., 2022). Fu","Conclusion
In this study, we propose DualBind, a novel dual-loss frame-
work which can perform accurate absolute binding affinity
prediction. Our evaluations on standard benchmark demon-
strate that DualBind consistently outperforms both baseline
models and its ablated variants. A key feature of DualBind
is its innovative strategy of integrating unlabeled data with
labeled examples, which has shown considerable benefits in
our initial experiments. Looking ahead, we plan to expand
our research into hybrid training approaches and explore
pretraining techniques around the DualBind framework.
Acknowledgements
We thank the entire NVIDIA BioNeMo team for the discus-
sions and support throughout this research.References
Bishop, C. M. Training with noise is equivalent to tikhonov
regularization. Neural computation , 7(1):108–116, 1995.
Chen, L., Tan, X., Wang, D., Zhong, F., Liu, X., Yang, T.,
Luo, X., Chen, K., Jiang, H., and Zheng, M. Transformer-
CPI: improving compound–protein interaction predic-
tion by sequence-based deep learning with self-attention
mechanism and label reversal experiments. Bioinformat-
ics, 36(16):4406–4414, 2020.
Eberhardt, J., Santos-Martins, D., Tillack, A. F., and Forli, S.
AutoDock Vina 1.2. 0: New docking methods, expanded
force field, and python bindings. Journal of chemical
information and modeling , 61(8):3891–3898, 2021.
Friesner, R. A., Banks, J. L., Murphy, R. B., Halgren, T. A.,
Klicic, J. J., Mainz, D. T., Repasky, M. P., Knoll, E. H.,
Shelley, M., Perry, J. K., et al. Glide: a new approach
for rapid, accurate docking and scoring. 1. method and
assessment of docking accuracy. Journal of medicinal
chemistry , 47(7):1739–1749, 2004.
Godwin, J., Schaarschmidt, M., Gaunt, A., Sanchez-
Gonzalez, A., Rubanova, Y ., Veli ˇckovi ´c, P., Kirkpatrick,
J., and Battaglia, P. Simple gnn regularisation for 3d
molecular property prediction & beyond. arXiv preprint
arXiv:2106.07971 , 2021.
Hern ´andez-Garrido, C. A. and S ´anchez-Cruz, N. Exper-
iment","methodology. DualBind employs a dual-loss framework that combines the MSE loss LMSE and the DSM loss LDSM. Specifically, LMSEanchors the predicted binding affinity of the crystal structure to its experimentally determined binding affinity. Concurrently, LDSMshapes the gradient at the perturbed structure. Details are described in Section 2. the coordinates of these atoms. We define a binding affin- ity prediction model as E (A,X) :Rn d Rn 3 R, which produces a scalar energy value for any given complex. denotes the learnable parameters in the model. Intuitively, our dual-loss framework combines the DSM loss LDSM, which learns the energy landscape by shaping the gra- dient of the energy function, with the MSE loss LMSE, which directly ties the predictions to known binding affinity values. The overview of our DualBind approach is illustrated in Figure 2. MSE Loss. As shown in the upper branch of Figure 2, the use of the MSE loss is straightforward. We feed a complex crystal structure (A,X)into the model and then compute the error between the predicted and the ground truth binding affinity. Formally, for one data sample, LMSE= (E (A,X) y)2, (1) where ydenotes the the ground truth binding affinity. In- tuitively, this MSE loss encourages that the points, which represent observed crystal structures within the energy land- scape, are anchored to experimental binding affinities, main- taining their alignment with empirical observations. DSM Loss. Training neural networks with denoising is a well-established technique aimed at enhancing the ro- bustness and generalization capabilities of models (Bishop, 1995; Vincent et al., 2008; Kong et al., 2020; Godwin et al., 2021). More specifically, the denoising score matching tech- nique has been employed to pretrain models for 3D molecu- lar tasks, demonstrating that such objective not only simu- lates learning a molecular force field but also significantly boosts performance across various downstream tasks (Zaidi et al., 2022). Fu","Conclusion In this study, we propose DualBind, a novel dual-loss frame- work which can perform accurate absolute binding affinity prediction. Our evaluations on standard benchmark demon- strate that DualBind consistently outperforms both baseline models and its ablated variants. A key feature of DualBind is its innovative strategy of integrating unlabeled data with labeled examples, which has shown considerable benefits in our initial experiments. Looking ahead, we plan to expand our research into hybrid training approaches and explore pretraining techniques around the DualBind framework.","DualBind: A Dual-Loss Framework for Protein-Ligand Binding Affinity Prediction Accurate prediction of protein-ligand binding affinities is crucial for drug development. Recent advances in machine learning show promising results on this task. However, these methods typically rely heavily on labeled data, which can be scarce or unreliable, or they rely on assumptions like Boltzmann-distributed data that may not hold true in practice. Here, we present DualBind, a novel framework that integrates supervised mean squared error (MSE) with unsupervised denoising score matching (DSM) to accurately learn the binding energy function. DualBind not only addresses the limitations of DSM-only models by providing more accurate absolute affinity predictions but also improves generalizability and reduces reliance on labeled data compared to MSE-only models. Our experimental results demonstrate that DualBind excels in predicting binding affinities and can effectively utilize both labeled and unlabeled data to enhance performance.","DualBind: A Dual-Loss Framework for Protein-Ligand Binding Affinity Prediction Accurate prediction of protein-ligand binding affinities is crucial for drug development. Recent advances in machine learning show promising results on this task. However, these methods typically rely heavily on labeled data, which can be scarce or unreliable, or they rely on assumptions like Boltzmann-distributed data that may not hold true in practice. Here, we present DualBind, a novel framework that integrates supervised mean squared error (MSE) with unsupervised denoising score matching (DSM) to accurately learn the binding energy function. DualBind not only addresses the limitations of DSM-only models by providing more accurate absolute affinity predictions but also improves generalizability and reduces reliance on labeled data compared to MSE-only models. Our experimental results demonstrate that DualBind excels in predicting binding affinities and can effectively utilize both labeled and unlabeled data to enhance performance. Conclusion In this study, we propose DualBind, a novel dual-loss frame- work which can perform accurate absolute binding affinity prediction. Our evaluations on standard benchmark demon- strate that DualBind consistently outperforms both baseline models and its ablated variants. A key feature of DualBind is its innovative strategy of integrating unlabeled data with labeled examples, which has shown considerable benefits in our initial experiments. Looking ahead, we plan to expand our research into hybrid training approaches and explore pretraining techniques around the DualBind framework. methodology. DualBind employs a dual-loss framework that combines the MSE loss LMSE and the DSM loss LDSM. Specifically, LMSEanchors the predicted binding affinity of the crystal structure to its experimentally determined binding affinity. Concurrently, LDSMshapes the gradient at the perturbed structure. Details are described in Section 2. the coordinates of these atoms. We define a binding affin- ity prediction model as E (A,X) :Rn d Rn 3 R, which produces a scalar energy value for any given complex. denotes the learnable parameters in the model. Intuitively, our dual-loss framework combines the DSM loss LDSM, which learns the energy landscape by shaping the gra- dient of the energy function, with the MSE loss LMSE, which directly ties the predictions to known binding affinity values. The overview of our DualBind approach is illustrated in Figure 2. MSE Loss. As shown in the upper branch of Figure 2, the use of the MSE loss is straightforward. We feed a complex crystal structure (A,X)into the model and then compute the error between the predicted and the ground truth binding affinity. Formally, for one data sample, LMSE= (E (A,X) y)2, (1) where ydenotes the the ground truth binding affinity. In- tuitively, this MSE loss encourages that the points, which represent observed crystal structures within the energy land- scape, are anchored to experimental binding affinities, main- taining their alignment with empirical observations. DSM Loss. Training neural networks with denoising is a well-established technique aimed at enhancing the ro- bustness and generalization capabilities of models (Bishop, 1995; Vincent et al., 2008; Kong et al., 2020; Godwin et al., 2021). More specifically, the denoising score matching tech- nique has been employed to pretrain models for 3D molecu- lar tasks, demonstrating that such objective not only simu- lates learning a molecular force field but also significantly boosts performance across various downstream tasks (Zaidi et al., 2022). Fu",0
148d89e0577cf3ddab63cab758f1d177c25f2783,Generative acceleration of molecular dynamics simulations for solid-state electrolytes,"['Juno Nam', 'Sulin Liu', 'Gavin Winter', 'Rafael Gomez-Bombarelli']",https://openreview.net/pdf/148d89e0577cf3ddab63cab758f1d177c25f2783.pdf,"Generative acceleration of molecular dynamics simulations for solid-state electrolytes We introduce LiFlow, a generative acceleration framework designed for efficiently simulating diffusive dynamics in solids, particularly lithium-based solid-state electrolytes (SSEs). LiFlow consists of two components: Propagator and Corrector, which utilize a conditional flow matching scheme to predict atomic displacements and perform denoising, respectively. Our model achieves a Spearman's rank correlation of approximately 0.7 for the lithium mean squared displacement (MSD) on test set based on composition and temperature splits and offers a substantial speedup compared to reference molecular dynamics (MD) simulations using machine learning interatomic potentials (MLIPs). This framework facilitates high-throughput virtual screening for electrolyte materials and holds promise for the optimization of the kinetic properties of crystalline solids.",148d89e0577cf3ddab63cab758f1d177c25f2783.pdf,"methodology employed
in the current study is focused on rapidly screening poten-
tial electrolyte materials, prioritizing speed over absolute
accuracy in generating dynamics. Beyond facilitating high-
throughput virtual screening for electrolyte materials, we
envision that based on an efficient and differentiable descrip-
tion of ionic transport, future application of our framework
will enable inverse design and optimization of electrolyte
materials.
Generative acceleration of molecular dynamics simulations for solid-state electrolytes
References
Bachman, J. C., Muy, S., Grimaud, A., Chang, H.-H., Pour,
N., Lux, S. F., Paschos, O., Maglia, F., Lupart, S., Lamp,
P., et al. Inorganic solid-state electrolytes for lithium
batteries: mechanisms and properties governing ion con-
duction. Chemical reviews , 116(1):140–162, 2016.
Batatia, I., Benner, P., Chiang, Y ., Elena, A. M., Kov ´acs,
D. P., Riebesell, J., Advincula, X. R., Asta, M., Avaylon,
M., Baldwin, W. J., Berger, F., Bernstein, N., Bhowmik,
A., Blau, S. M., C ˘arare, V ., Darby, J. P., De, S., Pia,
F. D., Deringer, V . L., Elijo ˇsius, R., El-Machachi, Z., Fal-
cioni, F., Fako, E., Ferrari, A. C., Genreith-Schriever,
A., George, J., Goodall, R. E. A., Grey, C. P., Grig-
orev, P., Han, S., Handley, W., Heenen, H. H., Hermans-
son, K., Holm, C., Jaafar, J., Hofmann, S., Jakob, K. S.,
Jung, H., Kapil, V ., Kaplan, A. D., Karimitari, N., Ker-
mode, J. R., Kroupa, N., Kullgren, J., Kuner, M. C.,
Kuryla, D., Liepuoniute, G., Margraf, J. T., Magd ˘au, I.-
B., Michaelides, A., Moore, J. H., Naik, A. A., Niblett,
S. P., Norwood, S. W., O’Neill, N., Ortner, C., Persson,
K. A., Reuter, K., Rosen, A. S., Schaaf, L. L., Schran,
C., Shi, B. X., Sivonxay, E., Stenczel, T. K., Svahn, V .,
Sutton, C., Swinburne, T. D., Tilly, J., van der Oord, C.,
Varga-Umbrich, E., Vegge, T., V ondr ´ak, M., Wang, Y .,
Witt, W. C., Zills, F., and Cs ´anyi, G. A foundation model
for atomistic materials chemistry, 2024.
Deng, B., Choi, Y ., Zhon","Conclusion
We proposed the LIFLOW model, a generative acceleration
framework tailored for lithium-based solid-state electrolyte
molecular dynamics (MD) simulations. The model is com-
posed of Propagator andCorrector components, which uti-
lizes a conditional flow matching scheme to predict atomic
displacements for time propagation and denoising, respec-
tively. Our model achieves Spearman’s rank correlation of
approximately 0.7 when reproducing mean squared displace-
ment (MSD) values on compositionally and thermally split
test structures. Remarkably, LIFLOW achieves a speedup
of about 300×compared to reference MD simulations with
machine learning interatomic potentials (MLIPs). While
our model exhibits a tendency to overestimate MSD values
for non-diffusing structures, we aim to address this issue
by incorporating features from pretrained MLIP models to
develop energy-aware propagation models, enhancing the
accuracy and robustness of our approach.
While the underlying assumption regarding the sufficient
accuracy of electronic structure calculations and MLIP ap-
proximations generally holds, recent reports (Deng et al.,
2024) indicate that MLIPs may smooth the potential energy
landscape and lead to an overestimation of kinetic proper-
ties. Therefore, it’s essential to consider the accuracy of the
reference dynamics. However, the methodology employed
in the current study is focused on rapidly screening poten-
tial electrolyte materials, prioritizing speed over absolute
accuracy in generating dynamics. Beyond facilitating high-
throughput virtual screening for electrolyte materials, we
envision that based on an efficient and differentiable descrip-
tion of ionic transport, future application of our framework
will enable inverse design and optimization of electrolyte
materials.
Generative acceleration of molecular dynamics simulations for solid-state electrolytes
References
Bachman, J. C., Muy, S., Grimaud, A., Chang, H.-H., Pour,
N., Lux, S. F., Paschos, O., Maglia,","methodology employed in the current study is focused on rapidly screening poten- tial electrolyte materials, prioritizing speed over absolute accuracy in generating dynamics. Beyond facilitating high- throughput virtual screening for electrolyte materials, we envision that based on an efficient and differentiable descrip- tion of ionic transport, future application of our framework will enable inverse design and optimization of electrolyte materials. Generative acceleration of molecular dynamics simulations for solid-state electrolytes","Conclusion We proposed the LIFLOW model, a generative acceleration framework tailored for lithium-based solid-state electrolyte molecular dynamics (MD) simulations. The model is com- posed of Propagator andCorrector components, which uti- lizes a conditional flow matching scheme to predict atomic displacements for time propagation and denoising, respec- tively. Our model achieves Spearman s rank correlation of approximately 0.7 when reproducing mean squared displace- ment (MSD) values on compositionally and thermally split test structures. Remarkably, LIFLOW achieves a speedup of about 300 compared to reference MD simulations with machine learning interatomic potentials (MLIPs). While our model exhibits a tendency to overestimate MSD values for non-diffusing structures, we aim to address this issue by incorporating features from pretrained MLIP models to develop energy-aware propagation models, enhancing the accuracy and robustness of our approach. While the underlying assumption regarding the sufficient accuracy of electronic structure calculations and MLIP ap- proximations generally holds, recent reports (Deng et al., 2024) indicate that MLIPs may smooth the potential energy landscape and lead to an overestimation of kinetic proper- ties. Therefore, it s essential to consider the accuracy of the reference dynamics. However, the methodology employed in the current study is focused on rapidly screening poten- tial electrolyte materials, prioritizing speed over absolute accuracy in generating dynamics. Beyond facilitating high- throughput virtual screening for electrolyte materials, we envision that based on an efficient and differentiable descrip- tion of ionic transport, future application of our framework will enable inverse design and optimization of electrolyte materials. Generative acceleration of molecular dynamics simulations for solid-state electrolytes","Generative acceleration of molecular dynamics simulations for solid-state electrolytes We introduce LiFlow, a generative acceleration framework designed for efficiently simulating diffusive dynamics in solids, particularly lithium-based solid-state electrolytes (SSEs). LiFlow consists of two components: Propagator and Corrector, which utilize a conditional flow matching scheme to predict atomic displacements and perform denoising, respectively. Our model achieves a Spearman's rank correlation of approximately 0.7 for the lithium mean squared displacement (MSD) on test set based on composition and temperature splits and offers a substantial speedup compared to reference molecular dynamics (MD) simulations using machine learning interatomic potentials (MLIPs). This framework facilitates high-throughput virtual screening for electrolyte materials and holds promise for the optimization of the kinetic properties of crystalline solids.","Generative acceleration of molecular dynamics simulations for solid-state electrolytes We introduce LiFlow, a generative acceleration framework designed for efficiently simulating diffusive dynamics in solids, particularly lithium-based solid-state electrolytes (SSEs). LiFlow consists of two components: Propagator and Corrector, which utilize a conditional flow matching scheme to predict atomic displacements and perform denoising, respectively. Our model achieves a Spearman's rank correlation of approximately 0.7 for the lithium mean squared displacement (MSD) on test set based on composition and temperature splits and offers a substantial speedup compared to reference molecular dynamics (MD) simulations using machine learning interatomic potentials (MLIPs). This framework facilitates high-throughput virtual screening for electrolyte materials and holds promise for the optimization of the kinetic properties of crystalline solids. Conclusion We proposed the LIFLOW model, a generative acceleration framework tailored for lithium-based solid-state electrolyte molecular dynamics (MD) simulations. The model is com- posed of Propagator andCorrector components, which uti- lizes a conditional flow matching scheme to predict atomic displacements for time propagation and denoising, respec- tively. Our model achieves Spearman s rank correlation of approximately 0.7 when reproducing mean squared displace- ment (MSD) values on compositionally and thermally split test structures. Remarkably, LIFLOW achieves a speedup of about 300 compared to reference MD simulations with machine learning interatomic potentials (MLIPs). While our model exhibits a tendency to overestimate MSD values for non-diffusing structures, we aim to address this issue by incorporating features from pretrained MLIP models to develop energy-aware propagation models, enhancing the accuracy and robustness of our approach. While the underlying assumption regarding the sufficient accuracy of electronic structure calculations and MLIP ap- proximations generally holds, recent reports (Deng et al., 2024) indicate that MLIPs may smooth the potential energy landscape and lead to an overestimation of kinetic proper- ties. Therefore, it s essential to consider the accuracy of the reference dynamics. However, the methodology employed in the current study is focused on rapidly screening poten- tial electrolyte materials, prioritizing speed over absolute accuracy in generating dynamics. Beyond facilitating high- throughput virtual screening for electrolyte materials, we envision that based on an efficient and differentiable descrip- tion of ionic transport, future application of our framework will enable inverse design and optimization of electrolyte materials. Generative acceleration of molecular dynamics simulations for solid-state electrolytes methodology employed in the current study is focused on rapidly screening poten- tial electrolyte materials, prioritizing speed over absolute accuracy in generating dynamics. Beyond facilitating high- throughput virtual screening for electrolyte materials, we envision that based on an efficient and differentiable descrip- tion of ionic transport, future application of our framework will enable inverse design and optimization of electrolyte materials. Generative acceleration of molecular dynamics simulations for solid-state electrolytes",0
5a743d28e1336cdcd511412bffa9854afbd5b9ce,Deep Supramolecular Language Processing for Co-crystal Prediction,"['Rebecca Birolo', 'Rıza Özçelik', 'Andrea Aramini', 'Michele R. Chierotti', 'Roberto Gobetto', 'Francesca Grisoni']",https://openreview.net/pdf/5a743d28e1336cdcd511412bffa9854afbd5b9ce.pdf,"Deep Supramolecular Language Processing for Co-crystal Prediction A deep learning model that leverages chemical language processing to accelerate co-crystallization, reducing laboratory time and resource consumption. Approximately 40% of marketed drugs exhibit suboptimal pharmacokinetic profiles. Co-crystallization, where pairs of molecules form a multicomponent crystal, constitutes a promising strategy to enhance physicochemical properties without compromising the pharmacological activity. However, finding promising co-crystal pairs is resource-intensive, due to the vast number of possible combinations. We present DeepCocrystal, a novel deep learning approach designed to predict co-crystal formation by processing the `chemical language' from a supramolecular vantage point. Rigorous validation of DeepCocrystal showed a balanced accuracy of 78% in realistic scenarios, outperforming existing models. By leveraging properties of molecular string representations, DeepCocrystal can also estimate the uncertainty of its predictions. We harness this capability in a challenging prospective study, and successfully discovered two novel co-crystal of diflunisal, an anti-inflammatory drug. This study underscores the potential of deep learning -- and in particular of chemical language processing -- to accelerate co-crystallization, and ultimately drug development, in both academic and industrial contexts.",5a743d28e1336cdcd511412bffa9854afbd5b9ce.pdf,"methodology and encod-
ing rules. Journal ofchemical information andcomputer
sciences, 28(1):31–36, 1988.
Yahfoufi, N., Alsadi, N., Jambi, M., and Matar, C. The im-
munomodulatory and anti-inflammatory role of polyphe-
nols. Nutrients, 10(11):1618, 2018.
Deep Supramolecular Language Processing for Co-crystal Prediction
Yang, D., Wang, L., Yuan, P., An, Q., Su, B., Yu, M., Chen,
T., Hu, K., Zhang, L., Lu, Y ., et al. Cocrystal virtual
screening based on the xgboost machine learning model.
Chinese Chemical Letters, 34(8):107964, 2023.
Yin, W., Kann, K., Yu, M., and Sch ¨utze, H. Comparative
study of cnn and rnn for natural language processing.
arXiv preprint arXiv:1702.01923, 2017.","Conclusions
Optimizing the pharmacokinetic properties of active com-
pounds is an ever-lasting challenge in drug discovery, and co-
crystallization is an attractive strategy to address this issue.
However, identifying suitable co-crystallization partners for
active compounds is both resource- and time-intensive. To
accelerate this process, we developed DeepCocrystal, a deep
chemical language processing approach designed to predict
the co-crystallization of any selected molecular pairs.
This study shows the potential of DeepCocrystal to advance
the state-of-the-art. DeepCocrystal owes its performance to
the intriguing properties of the SMILES language, which al-
lowed mitigating data imbalance and estimating uncertainty.
By learning (and then combining) single-molecule informa-
tion, DeepCocrystal learns elements of the “supramolecular
language” (Lehn, 1988; Brock & Dunitz, 1994; Cragg &
Cragg, 2010) of co-crystal formation. The experimental val-
idation of DeepCocrystal further corroborated its potential
and identified adenine and caffeine as two previously unre-
ported coformers of diflunisal. These results, taken together,
underscore the potential of DeepCocrystal to accelerate the
discovery of co-crystallization partners.
This first-in-time adoption of the “supramolecular language”
perspective with SMILES strings shows its potential for co-
crystalization prediction. While this study only focused on
‘two-word sentences’ ( i.e., molecule pairs ), our approach
could be extended to supramolecular interactions among
multiple molecular partners. Moreover, extensive datasets
with thorough annotations on stereochemistry might further
expand the co-crystal prediction ability of approaches based
on SMILES strings. Ultimately, extensions of DeepCocrys-
tal might open unexplored opportunities in supramolecu-
lar chemistry, e.g.for drug development(Kawakami et al.,
2012), materials discovery,(Stupp & Palmer, 2014) and be-
yond.
References
Aaker ¨oy, C. B., Grommet, A. B., an","methodology and encod- ing rules. Journal ofchemical information andcomputer sciences, 28(1):31 36, 1988. Yahfoufi, N., Alsadi, N., Jambi, M., and Matar, C. The im- munomodulatory and anti-inflammatory role of polyphe- nols. Nutrients, 10(11):1618, 2018. Deep Supramolecular Language Processing for Co-crystal Prediction Yang, D., Wang, L., Yuan, P., An, Q., Su, B., Yu, M., Chen, T., Hu, K., Zhang, L., Lu, Y ., et al. Cocrystal virtual screening based on the xgboost machine learning model. Chinese Chemical Letters, 34(8):107964, 2023. Yin, W., Kann, K., Yu, M., and Sch utze, H. Comparative study of cnn and rnn for natural language processing. arXiv preprint arXiv:1702.01923, 2017.","Conclusions Optimizing the pharmacokinetic properties of active com- pounds is an ever-lasting challenge in drug discovery, and co- crystallization is an attractive strategy to address this issue. However, identifying suitable co-crystallization partners for active compounds is both resource- and time-intensive. To accelerate this process, we developed DeepCocrystal, a deep chemical language processing approach designed to predict the co-crystallization of any selected molecular pairs. This study shows the potential of DeepCocrystal to advance the state-of-the-art. DeepCocrystal owes its performance to the intriguing properties of the SMILES language, which al- lowed mitigating data imbalance and estimating uncertainty. By learning (and then combining) single-molecule informa- tion, DeepCocrystal learns elements of the supramolecular language (Lehn, 1988; Brock & Dunitz, 1994; Cragg & Cragg, 2010) of co-crystal formation. The experimental val- idation of DeepCocrystal further corroborated its potential and identified adenine and caffeine as two previously unre- ported coformers of diflunisal. These results, taken together, underscore the potential of DeepCocrystal to accelerate the discovery of co-crystallization partners. This first-in-time adoption of the supramolecular language perspective with SMILES strings shows its potential for co- crystalization prediction. While this study only focused on two-word sentences ( i.e., molecule pairs ), our approach could be extended to supramolecular interactions among multiple molecular partners. Moreover, extensive datasets with thorough annotations on stereochemistry might further expand the co-crystal prediction ability of approaches based on SMILES strings. Ultimately, extensions of DeepCocrys- tal might open unexplored opportunities in supramolecu- lar chemistry, e.g.for drug development(Kawakami et al., 2012), materials discovery,(Stupp & Palmer, 2014) and be- yond.","Deep Supramolecular Language Processing for Co-crystal Prediction A deep learning model that leverages chemical language processing to accelerate co-crystallization, reducing laboratory time and resource consumption. Approximately 40% of marketed drugs exhibit suboptimal pharmacokinetic profiles. Co-crystallization, where pairs of molecules form a multicomponent crystal, constitutes a promising strategy to enhance physicochemical properties without compromising the pharmacological activity. However, finding promising co-crystal pairs is resource-intensive, due to the vast number of possible combinations. We present DeepCocrystal, a novel deep learning approach designed to predict co-crystal formation by processing the `chemical language' from a supramolecular vantage point. Rigorous validation of DeepCocrystal showed a balanced accuracy of 78% in realistic scenarios, outperforming existing models. By leveraging properties of molecular string representations, DeepCocrystal can also estimate the uncertainty of its predictions. We harness this capability in a challenging prospective study, and successfully discovered two novel co-crystal of diflunisal, an anti-inflammatory drug. This study underscores the potential of deep learning -- and in particular of chemical language processing -- to accelerate co-crystallization, and ultimately drug development, in both academic and industrial contexts.","Deep Supramolecular Language Processing for Co-crystal Prediction A deep learning model that leverages chemical language processing to accelerate co-crystallization, reducing laboratory time and resource consumption. Approximately 40% of marketed drugs exhibit suboptimal pharmacokinetic profiles. Co-crystallization, where pairs of molecules form a multicomponent crystal, constitutes a promising strategy to enhance physicochemical properties without compromising the pharmacological activity. However, finding promising co-crystal pairs is resource-intensive, due to the vast number of possible combinations. We present DeepCocrystal, a novel deep learning approach designed to predict co-crystal formation by processing the `chemical language' from a supramolecular vantage point. Rigorous validation of DeepCocrystal showed a balanced accuracy of 78% in realistic scenarios, outperforming existing models. By leveraging properties of molecular string representations, DeepCocrystal can also estimate the uncertainty of its predictions. We harness this capability in a challenging prospective study, and successfully discovered two novel co-crystal of diflunisal, an anti-inflammatory drug. This study underscores the potential of deep learning -- and in particular of chemical language processing -- to accelerate co-crystallization, and ultimately drug development, in both academic and industrial contexts. Conclusions Optimizing the pharmacokinetic properties of active com- pounds is an ever-lasting challenge in drug discovery, and co- crystallization is an attractive strategy to address this issue. However, identifying suitable co-crystallization partners for active compounds is both resource- and time-intensive. To accelerate this process, we developed DeepCocrystal, a deep chemical language processing approach designed to predict the co-crystallization of any selected molecular pairs. This study shows the potential of DeepCocrystal to advance the state-of-the-art. DeepCocrystal owes its performance to the intriguing properties of the SMILES language, which al- lowed mitigating data imbalance and estimating uncertainty. By learning (and then combining) single-molecule informa- tion, DeepCocrystal learns elements of the supramolecular language (Lehn, 1988; Brock & Dunitz, 1994; Cragg & Cragg, 2010) of co-crystal formation. The experimental val- idation of DeepCocrystal further corroborated its potential and identified adenine and caffeine as two previously unre- ported coformers of diflunisal. These results, taken together, underscore the potential of DeepCocrystal to accelerate the discovery of co-crystallization partners. This first-in-time adoption of the supramolecular language perspective with SMILES strings shows its potential for co- crystalization prediction. While this study only focused on two-word sentences ( i.e., molecule pairs ), our approach could be extended to supramolecular interactions among multiple molecular partners. Moreover, extensive datasets with thorough annotations on stereochemistry might further expand the co-crystal prediction ability of approaches based on SMILES strings. Ultimately, extensions of DeepCocrys- tal might open unexplored opportunities in supramolecu- lar chemistry, e.g.for drug development(Kawakami et al., 2012), materials discovery,(Stupp & Palmer, 2014) and be- yond. methodology and encod- ing rules. Journal ofchemical information andcomputer sciences, 28(1):31 36, 1988. Yahfoufi, N., Alsadi, N., Jambi, M., and Matar, C. The im- munomodulatory and anti-inflammatory role of polyphe- nols. Nutrients, 10(11):1618, 2018. Deep Supramolecular Language Processing for Co-crystal Prediction Yang, D., Wang, L., Yuan, P., An, Q., Su, B., Yu, M., Chen, T., Hu, K., Zhang, L., Lu, Y ., et al. Cocrystal virtual screening based on the xgboost machine learning model. Chinese Chemical Letters, 34(8):107964, 2023. Yin, W., Kann, K., Yu, M., and Sch utze, H. Comparative study of cnn and rnn for natural language processing. arXiv preprint arXiv:1702.01923, 2017.",0
807a265abb4706580089e96a8e3ee62d509718d9,Latent-Guided Equivariant Diffusion for Controlled Structure-Based De Novo Ligand Generation,"['Tuan Le', 'Julian Cremer', 'Djork-Arné Clevert', 'Kristof T Schütt']",https://openreview.net/pdf/807a265abb4706580089e96a8e3ee62d509718d9.pdf,"Latent-Guided Equivariant Diffusion for Controlled Structure-Based De Novo Ligand Generation We propose PoLiGenX for de novo ligand design using latent-conditioned, target-aware equivariant diffusion. Our model leverages the conditioning of the generation process on reference molecules within a protein pocket to produce shape-similar de novo ligands that can be used for target-aware hit expansion and hit optimization. The results of our study showcase the efficacy of PoLiGenX in ligand design. Docking scores indicate that the generated ligands exhibit superior binding affinity compared to the reference molecule while preserving the shape. At the same time, our model maintains chemical diversity, ensuring the exploration of diverse chemical space. The evaluation of Lipinski's rule of five suggests that the sampled molecules possess a higher drug-likeness than the reference data. This constitutes an important step towards the controlled generation of therapeutically relevant de novo ligands tailored to specific protein targets.",807a265abb4706580089e96a8e3ee62d509718d9.pdf,"approaches to
identifying novel therapeutic compounds. Among these in-
novations, AI-enabled structure-based drug discovery has
emerged as a promising research avenue, in particular in
form of equivariant target-aware diffusion models. By con-
ditioning the diffusion process on the receptors of proteins,
these models exhibit a remarkable capacity to generate de
novo ligands with enhanced affinity (Peng et al., 2022; Guan
et al., 2023; Schneuing et al., 2023; Le et al., 2024). Failing
to consider the essential chemical properties for target bind-
*Equal contribution1Pfizer Research & Development2Freie
Universit ¨at Berlin3University Pompeu Fabra. Correspon-
dence to: Tuan Le <tuan.le@pfizer.com >, Julian Cremer <ju-
lian.cremer@pfizer.com >.
Accepted at the 1st Machine Learning for Life and Material Sci-
ences Workshop at ICML 2024 . Copyright 2024 by the author(s).ing can lead to a significant lack of specificity and result
in ineffective drug candidates. Moreover, these candidates
must exhibit favorable absorption, distribution, metabolism,
excretion (ADME), and toxicity profiles. Designing ligands
from scratch without addressing these critical properties
may produce molecules with poor bioavailability or po-
tential toxicity, thereby limiting their therapeutic potential.
This challenge is further exacerbated by the often sparse
and noisy data available for developing effective machine
learning models. However, machine learning shows con-
siderable promise during the hit expansion phase of drug
discovery. This crucial stage involves enhancing and ex-
ploring the chemical space around promising hits already
identified through high-throughput screening or other meth-
ods. In this study, we introduce PoLiGenX ( Pocket-based
Ligand Generator for hit e Xpansion) that generates ligands
de novo within a protein binding pocket. Unlike previous
models, PoLiGenX starts with a seed molecule, such as a hit
candidate or an initial scaffold, and iteratively refines and
modifies","Conclusions
We have developed PoLiGenX for controlled de novo ligand
generation within a protein binding pocket. By incorporat-
ing a latent encoding from a seed molecule into the diffusion
model, we ensure that the generated ligands preserve shape
and also adhere to the structural constraints of the target
protein binding site. The effectiveness of PoLiGenX is evi-
denced by improved docking scores compared to reference
ligands. Additionally, the generated ligands conform to
Lipinski’s Rule of Five, demonstrating their drug-likeness.
Importantly, the model maintains chemical diversity, which
is essential for exploring a broad range of chemical space
and discovering novel therapeutic candidates. This integra-
tion of shape preservation, target specificity, and chemical
diversity provides a powerful approach for the targeted gen-
eration of drug candidates, particularly useful in the hit
expansion phase of drug discovery campaigns.
Latent-Conditioned Equivariant Diffusion for Controlled Structure-Based De Novo Ligand Generation
References
Adams, K. and Coley, C. W. Equivariant shape-conditioned
generation of 3d molecules for ligand-based drug design.
InThe Eleventh International Conference on Learning
Representations , 2023. URL https://openreview.
net/forum?id=4MbGnp4iPQ .
Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and van den
Berg, R. Structured denoising diffusion models in dis-
crete state-spaces. In Beygelzimer, A., Dauphin, Y .,
Liang, P., and Vaughan, J. W. (eds.), Advances in Neural
Information Processing Systems , 2021. URL https:
//openreview.net/forum?id=h7-XixPCAL .
Chen, Z., Peng, B., Parthasarathy, S., and Ning, X. Shape-
conditioned 3d molecule generation via equivariant dif-
fusion models, 2023. URL https://arxiv.org/
abs/2308.11890 .
Francoeur, P. G., Masuda, T., Sunseri, J., Jia, A., Io-
vanisci, R. B., Snyder, I., and Koes, D. R. Three-
dimensional convolutional neural networks and a cross-
docked data set for structure-based drug design. Jour","approaches to identifying novel therapeutic compounds. Among these in- novations, AI-enabled structure-based drug discovery has emerged as a promising research avenue, in particular in form of equivariant target-aware diffusion models. By con- ditioning the diffusion process on the receptors of proteins, these models exhibit a remarkable capacity to generate de novo ligands with enhanced affinity (Peng et al., 2022; Guan et al., 2023; Schneuing et al., 2023; Le et al., 2024). Failing to consider the essential chemical properties for target bind- *Equal contribution1Pfizer Research & Development2Freie Universit at Berlin3University Pompeu Fabra. Correspon- dence to: Tuan Le <tuan.le@pfizer.com >, Julian Cremer <ju- lian.cremer@pfizer.com >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).ing can lead to a significant lack of specificity and result in ineffective drug candidates. Moreover, these candidates must exhibit favorable absorption, distribution, metabolism, excretion (ADME), and toxicity profiles. Designing ligands from scratch without addressing these critical properties may produce molecules with poor bioavailability or po- tential toxicity, thereby limiting their therapeutic potential. This challenge is further exacerbated by the often sparse and noisy data available for developing effective machine learning models. However, machine learning shows con- siderable promise during the hit expansion phase of drug discovery. This crucial stage involves enhancing and ex- ploring the chemical space around promising hits already identified through high-throughput screening or other meth- ods. In this study, we introduce PoLiGenX ( Pocket-based Ligand Generator for hit e Xpansion) that generates ligands de novo within a protein binding pocket. Unlike previous models, PoLiGenX starts with a seed molecule, such as a hit candidate or an initial scaffold, and iteratively refines and modifies","Conclusions We have developed PoLiGenX for controlled de novo ligand generation within a protein binding pocket. By incorporat- ing a latent encoding from a seed molecule into the diffusion model, we ensure that the generated ligands preserve shape and also adhere to the structural constraints of the target protein binding site. The effectiveness of PoLiGenX is evi- denced by improved docking scores compared to reference ligands. Additionally, the generated ligands conform to Lipinski s Rule of Five, demonstrating their drug-likeness. Importantly, the model maintains chemical diversity, which is essential for exploring a broad range of chemical space and discovering novel therapeutic candidates. This integra- tion of shape preservation, target specificity, and chemical diversity provides a powerful approach for the targeted gen- eration of drug candidates, particularly useful in the hit expansion phase of drug discovery campaigns. Latent-Conditioned Equivariant Diffusion for Controlled Structure-Based De Novo Ligand Generation","Latent-Guided Equivariant Diffusion for Controlled Structure-Based De Novo Ligand Generation We propose PoLiGenX for de novo ligand design using latent-conditioned, target-aware equivariant diffusion. Our model leverages the conditioning of the generation process on reference molecules within a protein pocket to produce shape-similar de novo ligands that can be used for target-aware hit expansion and hit optimization. The results of our study showcase the efficacy of PoLiGenX in ligand design. Docking scores indicate that the generated ligands exhibit superior binding affinity compared to the reference molecule while preserving the shape. At the same time, our model maintains chemical diversity, ensuring the exploration of diverse chemical space. The evaluation of Lipinski's rule of five suggests that the sampled molecules possess a higher drug-likeness than the reference data. This constitutes an important step towards the controlled generation of therapeutically relevant de novo ligands tailored to specific protein targets.","Latent-Guided Equivariant Diffusion for Controlled Structure-Based De Novo Ligand Generation We propose PoLiGenX for de novo ligand design using latent-conditioned, target-aware equivariant diffusion. Our model leverages the conditioning of the generation process on reference molecules within a protein pocket to produce shape-similar de novo ligands that can be used for target-aware hit expansion and hit optimization. The results of our study showcase the efficacy of PoLiGenX in ligand design. Docking scores indicate that the generated ligands exhibit superior binding affinity compared to the reference molecule while preserving the shape. At the same time, our model maintains chemical diversity, ensuring the exploration of diverse chemical space. The evaluation of Lipinski's rule of five suggests that the sampled molecules possess a higher drug-likeness than the reference data. This constitutes an important step towards the controlled generation of therapeutically relevant de novo ligands tailored to specific protein targets. Conclusions We have developed PoLiGenX for controlled de novo ligand generation within a protein binding pocket. By incorporat- ing a latent encoding from a seed molecule into the diffusion model, we ensure that the generated ligands preserve shape and also adhere to the structural constraints of the target protein binding site. The effectiveness of PoLiGenX is evi- denced by improved docking scores compared to reference ligands. Additionally, the generated ligands conform to Lipinski s Rule of Five, demonstrating their drug-likeness. Importantly, the model maintains chemical diversity, which is essential for exploring a broad range of chemical space and discovering novel therapeutic candidates. This integra- tion of shape preservation, target specificity, and chemical diversity provides a powerful approach for the targeted gen- eration of drug candidates, particularly useful in the hit expansion phase of drug discovery campaigns. Latent-Conditioned Equivariant Diffusion for Controlled Structure-Based De Novo Ligand Generation approaches to identifying novel therapeutic compounds. Among these in- novations, AI-enabled structure-based drug discovery has emerged as a promising research avenue, in particular in form of equivariant target-aware diffusion models. By con- ditioning the diffusion process on the receptors of proteins, these models exhibit a remarkable capacity to generate de novo ligands with enhanced affinity (Peng et al., 2022; Guan et al., 2023; Schneuing et al., 2023; Le et al., 2024). Failing to consider the essential chemical properties for target bind- *Equal contribution1Pfizer Research & Development2Freie Universit at Berlin3University Pompeu Fabra. Correspon- dence to: Tuan Le <tuan.le@pfizer.com >, Julian Cremer <ju- lian.cremer@pfizer.com >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).ing can lead to a significant lack of specificity and result in ineffective drug candidates. Moreover, these candidates must exhibit favorable absorption, distribution, metabolism, excretion (ADME), and toxicity profiles. Designing ligands from scratch without addressing these critical properties may produce molecules with poor bioavailability or po- tential toxicity, thereby limiting their therapeutic potential. This challenge is further exacerbated by the often sparse and noisy data available for developing effective machine learning models. However, machine learning shows con- siderable promise during the hit expansion phase of drug discovery. This crucial stage involves enhancing and ex- ploring the chemical space around promising hits already identified through high-throughput screening or other meth- ods. In this study, we introduce PoLiGenX ( Pocket-based Ligand Generator for hit e Xpansion) that generates ligands de novo within a protein binding pocket. Unlike previous models, PoLiGenX starts with a seed molecule, such as a hit candidate or an initial scaffold, and iteratively refines and modifies",0
dea9eb2b229805646eb0f2581984ac8b3fc941d9,CellFlows: Inferring Splicing Kinetics from Latent and Mechanistic Cellular Dynamics,"['Sei Chang', 'Zaiqian Chen', 'Bianca Dumitrascu', 'David A. Knowles']",https://openreview.net/pdf/dea9eb2b229805646eb0f2581984ac8b3fc941d9.pdf,"CellFlows: Inferring Splicing Kinetics from Latent and Mechanistic Cellular Dynamics CellFlows is a novel architecture that infers mechanistic cell and gene-specific transcription, splicing, and decay kinetics to regularize the latent representations learned through a VAE and neural ODEs. RNA velocity-based methods estimate cellular dynamics and cell developmental trajectories based on spliced and unspliced RNA counts. Although numerous methods have been proposed, RNA velocity-based models vary greatly in their biophysical assumptions, architectures, and use cases. In this work, we introduce a new architecture, CellFlows, which incorporates self-supervised neural dimensionality reduction with the flexibility of neural-based latent time estimation into a mechanistic model, improving model interpretability and accuracy. CellFlows models splicing dynamics to infer gene and context-specific kinetic rates at single-cell resolution and correctly identifies both linear and branching cellular differentiation pathways originating from mouse embryonic stem cells.",dea9eb2b229805646eb0f2581984ac8b3fc941d9.pdf,"approaches (Li, 2023).
The computational methods that aim to apply the RNA ve-
locity framework to infer and predict developmental trajec-
tories vary significantly in their architectures, parameter as-
sumptions, and mechanistic formulations. Here, we propose
CellFlows , a novel architecture that infers mechanistic cell
and gene-specific transcription, splicing, and decay kinetics
to regularize the latent representations learned through a
variational autoencoder (V AE) and neural ordinary differ-
ential equations (ODEs). We compare the model’s perfor-
mance on publicly available single-cell RNA sequencing
(scRNA-seq) datasets that capture the dynamics of mouse
embryonic stem cell differentiation and gastrulation ery-
throid maturation against recently published methods.
CellFlows: Inferring Splicing Kinetics from Latent and Mechanistic Cellular Dynamics
2. Related Work
Learning cell-gene splicing kinetics . cellDancer (Li et al.,
2023) uses a “relay” model to infer velocity locally for
each cell based on its neighbors, rather than assuming the
same kinetics globally for all cells. Local velocity inference
based on gene-specific kinetic parameters enables the model
to handle heterogeneous cell populations. Prior velocity
methods such as scVelo (Bergen et al., 2020) rely on strong
biological assumptions such as uniform gene-specific kinet-
ics across all cells, which fails to account for differences in
kinetics between cell subpopulations. cellDancer addresses
the limitations directly by predicting cell and gene-specific
transcription, splicing, and degradation kinetic rates using a
multi-layer perception (MLP). The kinetic rates parameter-
ize the RNA velocity through a system of linear ODEs:
du
dt=α(t)−β(t)u(t)
ds
dt=β(t)u(t)−γ(t)s(t)(1)
where tis time, u(t)is the unspliced abundance, s(t)is
the spliced mRNA abundance, α(t)is the transcription rate,
β(t)is the splicing rate, and γ(t)is the decay rate, all at
time t. The MLP is trained with a loss function based
on","limitations directly by predicting cell and gene-specific
transcription, splicing, and degradation kinetic rates using a
multi-layer perception (MLP). The kinetic rates parameter-
ize the RNA velocity through a system of linear ODEs:
du
dt=α(t)−β(t)u(t)
ds
dt=β(t)u(t)−γ(t)s(t)(1)
where tis time, u(t)is the unspliced abundance, s(t)is
the spliced mRNA abundance, α(t)is the transcription rate,
β(t)is the splicing rate, and γ(t)is the decay rate, all at
time t. The MLP is trained with a loss function based
on velocity vector similarity to its k-nearest neighbors in
gene expression. cellDancer uniquely learns biologically
interpretable parameters to constrain its black-box nonlinear
approximation of the dynamics.
Joint inference of latent space and latent dynamics. sc-
Tour (Li, 2023) jointly infers developmental pseudotime,
a transcriptomic vector field, and latent representations of
cells. Unlike cellDancer and other RNA velocity methods,
scTour does not consider splicing dynamics and instead
leverages its unique model architecture to derive cell repre-
sentations. scTour applies a V AE to denoise and extract a
latent representation from the single-cell data. The model
simultaneously uses a second encoder to extract a pseudo-
time label for each cell. The latent state of the cell with the
earliest pseudotime, along with the encoded pseudotimes of
all cells, are fed directly into a neural ODE to predict future
gene expression-based cell states. scTour then aligns the
neural ODE-generated predictions with the V AE-based la-
tent representations of the measured cells to find the optimal
vector field that describes the developmental trajectory.
Key Limitations . cellDancer learns its transcriptomic vec-
tor field and cell-specific pseudotime in a two-stage process
similar to scVelo (Li et al., 2023). In the first stage, cell-
Dancer’s MLP is trained to learn RNA velocity and inter-
pretable kinetics. In the second stage, the velocity estimates
are used to construct cell-t","approaches (Li, 2023). The computational methods that aim to apply the RNA ve- locity framework to infer and predict developmental trajec- tories vary significantly in their architectures, parameter as- sumptions, and mechanistic formulations. Here, we propose CellFlows , a novel architecture that infers mechanistic cell and gene-specific transcription, splicing, and decay kinetics to regularize the latent representations learned through a variational autoencoder (V AE) and neural ordinary differ- ential equations (ODEs). We compare the model s perfor- mance on publicly available single-cell RNA sequencing (scRNA-seq) datasets that capture the dynamics of mouse embryonic stem cell differentiation and gastrulation ery- throid maturation against recently published methods. CellFlows: Inferring Splicing Kinetics from Latent and Mechanistic Cellular Dynamics 2. Related Work Learning cell-gene splicing kinetics . cellDancer (Li et al., 2023) uses a relay model to infer velocity locally for each cell based on its neighbors, rather than assuming the same kinetics globally for all cells. Local velocity inference based on gene-specific kinetic parameters enables the model to handle heterogeneous cell populations. Prior velocity methods such as scVelo (Bergen et al., 2020) rely on strong biological assumptions such as uniform gene-specific kinet- ics across all cells, which fails to account for differences in kinetics between cell subpopulations. cellDancer addresses the limitations directly by predicting cell and gene-specific transcription, splicing, and degradation kinetic rates using a multi-layer perception (MLP). The kinetic rates parameter- ize the RNA velocity through a system of linear ODEs: du dt= (t) (t)u(t) ds dt= (t)u(t) (t)s(t)(1) where tis time, u(t)is the unspliced abundance, s(t)is the spliced mRNA abundance, (t)is the transcription rate, (t)is the splicing rate, and (t)is the decay rate, all at time t. The MLP is trained with a loss function based on","limitations directly by predicting cell and gene-specific transcription, splicing, and degradation kinetic rates using a multi-layer perception (MLP). The kinetic rates parameter- ize the RNA velocity through a system of linear ODEs: du dt= (t) (t)u(t) ds dt= (t)u(t) (t)s(t)(1) where tis time, u(t)is the unspliced abundance, s(t)is the spliced mRNA abundance, (t)is the transcription rate, (t)is the splicing rate, and (t)is the decay rate, all at time t. The MLP is trained with a loss function based on velocity vector similarity to its k-nearest neighbors in gene expression. cellDancer uniquely learns biologically interpretable parameters to constrain its black-box nonlinear approximation of the dynamics. Joint inference of latent space and latent dynamics. sc- Tour (Li, 2023) jointly infers developmental pseudotime, a transcriptomic vector field, and latent representations of cells. Unlike cellDancer and other RNA velocity methods, scTour does not consider splicing dynamics and instead leverages its unique model architecture to derive cell repre- sentations. scTour applies a V AE to denoise and extract a latent representation from the single-cell data. The model simultaneously uses a second encoder to extract a pseudo- time label for each cell. The latent state of the cell with the earliest pseudotime, along with the encoded pseudotimes of all cells, are fed directly into a neural ODE to predict future gene expression-based cell states. scTour then aligns the neural ODE-generated predictions with the V AE-based la- tent representations of the measured cells to find the optimal vector field that describes the developmental trajectory. Key Limitations . cellDancer learns its transcriptomic vec- tor field and cell-specific pseudotime in a two-stage process similar to scVelo (Li et al., 2023). In the first stage, cell- Dancer s MLP is trained to learn RNA velocity and inter- pretable kinetics. In the second stage, the velocity estimates are used to construct cell-t","CellFlows: Inferring Splicing Kinetics from Latent and Mechanistic Cellular Dynamics CellFlows is a novel architecture that infers mechanistic cell and gene-specific transcription, splicing, and decay kinetics to regularize the latent representations learned through a VAE and neural ODEs. RNA velocity-based methods estimate cellular dynamics and cell developmental trajectories based on spliced and unspliced RNA counts. Although numerous methods have been proposed, RNA velocity-based models vary greatly in their biophysical assumptions, architectures, and use cases. In this work, we introduce a new architecture, CellFlows, which incorporates self-supervised neural dimensionality reduction with the flexibility of neural-based latent time estimation into a mechanistic model, improving model interpretability and accuracy. CellFlows models splicing dynamics to infer gene and context-specific kinetic rates at single-cell resolution and correctly identifies both linear and branching cellular differentiation pathways originating from mouse embryonic stem cells.","CellFlows: Inferring Splicing Kinetics from Latent and Mechanistic Cellular Dynamics CellFlows is a novel architecture that infers mechanistic cell and gene-specific transcription, splicing, and decay kinetics to regularize the latent representations learned through a VAE and neural ODEs. RNA velocity-based methods estimate cellular dynamics and cell developmental trajectories based on spliced and unspliced RNA counts. Although numerous methods have been proposed, RNA velocity-based models vary greatly in their biophysical assumptions, architectures, and use cases. In this work, we introduce a new architecture, CellFlows, which incorporates self-supervised neural dimensionality reduction with the flexibility of neural-based latent time estimation into a mechanistic model, improving model interpretability and accuracy. CellFlows models splicing dynamics to infer gene and context-specific kinetic rates at single-cell resolution and correctly identifies both linear and branching cellular differentiation pathways originating from mouse embryonic stem cells. limitations directly by predicting cell and gene-specific transcription, splicing, and degradation kinetic rates using a multi-layer perception (MLP). The kinetic rates parameter- ize the RNA velocity through a system of linear ODEs: du dt= (t) (t)u(t) ds dt= (t)u(t) (t)s(t)(1) where tis time, u(t)is the unspliced abundance, s(t)is the spliced mRNA abundance, (t)is the transcription rate, (t)is the splicing rate, and (t)is the decay rate, all at time t. The MLP is trained with a loss function based on velocity vector similarity to its k-nearest neighbors in gene expression. cellDancer uniquely learns biologically interpretable parameters to constrain its black-box nonlinear approximation of the dynamics. Joint inference of latent space and latent dynamics. sc- Tour (Li, 2023) jointly infers developmental pseudotime, a transcriptomic vector field, and latent representations of cells. Unlike cellDancer and other RNA velocity methods, scTour does not consider splicing dynamics and instead leverages its unique model architecture to derive cell repre- sentations. scTour applies a V AE to denoise and extract a latent representation from the single-cell data. The model simultaneously uses a second encoder to extract a pseudo- time label for each cell. The latent state of the cell with the earliest pseudotime, along with the encoded pseudotimes of all cells, are fed directly into a neural ODE to predict future gene expression-based cell states. scTour then aligns the neural ODE-generated predictions with the V AE-based la- tent representations of the measured cells to find the optimal vector field that describes the developmental trajectory. Key Limitations . cellDancer learns its transcriptomic vec- tor field and cell-specific pseudotime in a two-stage process similar to scVelo (Li et al., 2023). In the first stage, cell- Dancer s MLP is trained to learn RNA velocity and inter- pretable kinetics. In the second stage, the velocity estimates are used to construct cell-t approaches (Li, 2023). The computational methods that aim to apply the RNA ve- locity framework to infer and predict developmental trajec- tories vary significantly in their architectures, parameter as- sumptions, and mechanistic formulations. Here, we propose CellFlows , a novel architecture that infers mechanistic cell and gene-specific transcription, splicing, and decay kinetics to regularize the latent representations learned through a variational autoencoder (V AE) and neural ordinary differ- ential equations (ODEs). We compare the model s perfor- mance on publicly available single-cell RNA sequencing (scRNA-seq) datasets that capture the dynamics of mouse embryonic stem cell differentiation and gastrulation ery- throid maturation against recently published methods. CellFlows: Inferring Splicing Kinetics from Latent and Mechanistic Cellular Dynamics 2. Related Work Learning cell-gene splicing kinetics . cellDancer (Li et al., 2023) uses a relay model to infer velocity locally for each cell based on its neighbors, rather than assuming the same kinetics globally for all cells. Local velocity inference based on gene-specific kinetic parameters enables the model to handle heterogeneous cell populations. Prior velocity methods such as scVelo (Bergen et al., 2020) rely on strong biological assumptions such as uniform gene-specific kinet- ics across all cells, which fails to account for differences in kinetics between cell subpopulations. cellDancer addresses the limitations directly by predicting cell and gene-specific transcription, splicing, and degradation kinetic rates using a multi-layer perception (MLP). The kinetic rates parameter- ize the RNA velocity through a system of linear ODEs: du dt= (t) (t)u(t) ds dt= (t)u(t) (t)s(t)(1) where tis time, u(t)is the unspliced abundance, s(t)is the spliced mRNA abundance, (t)is the transcription rate, (t)is the splicing rate, and (t)is the decay rate, all at time t. The MLP is trained with a loss function based on",0
c5478757496342894bea58f7ea6b30656da1a426,"Mirror, Mirror on the Wall: Automating Dental Smile Analysis in Smart Mirrors with CNN and Diffusion Model",[],https://openreview.net/pdf/c5478757496342894bea58f7ea6b30656da1a426.pdf,"Mirror, Mirror on the Wall: Automating Dental Smile Analysis in Smart Mirrors with CNN and Diffusion Model Automating dental smile analysis using a CNN trained on real and generated data with a vision to be integrated into an innovative Internet-of-Mirrors network of smart mirrors. This paper presents a smart diagnostic framework for dental smile analysis. To accurately and efficiently identify esthetic issues from a single image of a smile, a convolutional neural network (CNN) was trained. To overcome the limitations of scarce data, a diffusion model was employed to generate dental smile images in addition to manually curated data. The CNN was trained and evaluated on three datasets: all real images, all generated images, and a hybrid dataset of equal proportions of real-to-generated images. All three models demonstrate accuracy significantly above the baseline in detecting excessive gingival display, unlocking a novel diagnostic method in smile analysis. Notably, the hybrid model achieved the highest accuracy of 81.61\% ( value 0.01), highlighting the effectiveness of generative data augmentation for machine learning. The proposed solution could be part of a standalone home-deployed smart mirror or connected to a network of an innovative Internet-of-Mirrors to facilitate patient-dentist communication.",c5478757496342894bea58f7ea6b30656da1a426.pdf,"Methodology
2.1. Dataset
In the context of dental smiles, the absence of a suitable
dataset in open-source repositories and the ethical collection
of a diverse array of dental images encompassing propor-
tional ethnicities, genders, and degrees of gingival display
became a persistent endeavor posed a significant challenge.
In total, two sets of 256 (counting 512 combined) images
were collected and evenly distributed into ”gummy” and
”normal” classes. The first set was manually curated from
publicly available pictures scrapped from online frontal face
images of people smiling. These included highly positive
excessive gingival display samples as well as more discrete
ones. The second set was collected from text-to-image AI
generated images using Adobe Firefly’s DM (ado). The text
prompts for the ”normal” class varied from ”frontal portrait
of a person smiling with teeth; this person does not have
a gummy smile or braces” to simply picking out relevant
images in ”frontal portrait of a person smiling”. Despite
some filtering being required, this process still consumed
less time and resources than manual annotation.
Both sets contributed 56 randomly chosen images each to
the testing dataset, setting aside 112 of the original 512
images for testing. The remaining 400 (200 from real, 200
AI-generated by the DM) images were randomly split with
80% assigned to training and 20% to validation. In total,
three separate datasets were curated: real images (256 im-
ages), AI-generated images (256 images), and combinedimages (512 images). The data was balanced evenly be-
tween the two classes, and the correctness of the labels was
validated by a professional dentist.
2.2. Preprocessing
To mitigate overfitting, all images were normalized and
grayscaled. Subsequently, each image underwent a process
where a pre-trained facial landmark detector from the Dlib
library was utilized to pinpoint the coordinates of the mouth,
followed by cropping every image to a 28 by 28 pixels cen-
tered a","Conclusion
We opted to evaluate each model using a combined dataset
comprising both real and AI-generated images, rather than
relying solely on real images. This decision stems from the
recognition that both types of images may harbor biases,
though potentially in divergent directions. The manually
curated ’real’ dataset exhibits significant variation in im-
age quality but may lack proportional ethnic representation,
potentially resulting in the model’s underperformance for
minority groups. Conversely, the DM yielded diverse im-
ages, but it may lack certain nuanced human characteristics,
the absence of which could disrupt the model’s accuracy.
Lacking a precise quantitative method to gauge these biases,
it would be imprudent to assume that either dataset thor-
oughly captures reality. While the ideal future testing data
would mirror reality in terms of camera quality, lighting, and
angle, our current approach involves making estimations
based on the combined dataset, relying on data augmenta-
tion to compensate for potential inconsistencies.
Automating Dental Smile Analysis in Smart Mirrors with CNN and Diffusion Model
We have not only presented a novel framework that resolves
all the current drawbacks of traditional smile analysis, but
also provided a high-performing model and a dental smile
dataset. The research represents a significant step forward
in applying ML to dental smile analysis, underscoring the
potential of utilizing IoT and ML techniques in real-time
healthcare applications.
References
Adobe firefly. https://firefly.adobe.com/ . Ac-
cessed on January 23, 2024.
Aggarwal, A., Choudhury, A., Fearnhead, N., Kearns, P.,
Kirby, A., Lawler, M., Quinlan, S., Palmieri, C., Roques,
T., Simcock, R., Walter, F., Price, P., and Sullivan, R.
The future of cancer care in the uk-time for a radical and
sustainable national cancer plan. Lancet Oncol , 25(1):
e6–e17, Jan 2024. doi: 10.1016/S1470-2045(23)00511-9.
Epub 2023 Nov 14. Erratum in: Lancet Oncol. 2024
Jan;25(","Methodology 2.1. Dataset In the context of dental smiles, the absence of a suitable dataset in open-source repositories and the ethical collection of a diverse array of dental images encompassing propor- tional ethnicities, genders, and degrees of gingival display became a persistent endeavor posed a significant challenge. In total, two sets of 256 (counting 512 combined) images were collected and evenly distributed into gummy and normal classes. The first set was manually curated from publicly available pictures scrapped from online frontal face images of people smiling. These included highly positive excessive gingival display samples as well as more discrete ones. The second set was collected from text-to-image AI generated images using Adobe Firefly s DM (ado). The text prompts for the normal class varied from frontal portrait of a person smiling with teeth; this person does not have a gummy smile or braces to simply picking out relevant images in frontal portrait of a person smiling . Despite some filtering being required, this process still consumed less time and resources than manual annotation. Both sets contributed 56 randomly chosen images each to the testing dataset, setting aside 112 of the original 512 images for testing. The remaining 400 (200 from real, 200 AI-generated by the DM) images were randomly split with 80% assigned to training and 20% to validation. In total, three separate datasets were curated: real images (256 im- ages), AI-generated images (256 images), and combinedimages (512 images). The data was balanced evenly be- tween the two classes, and the correctness of the labels was validated by a professional dentist. 2.2. Preprocessing To mitigate overfitting, all images were normalized and grayscaled. Subsequently, each image underwent a process where a pre-trained facial landmark detector from the Dlib library was utilized to pinpoint the coordinates of the mouth, followed by cropping every image to a 28 by 28 pixels cen- tered a","Conclusion We opted to evaluate each model using a combined dataset comprising both real and AI-generated images, rather than relying solely on real images. This decision stems from the recognition that both types of images may harbor biases, though potentially in divergent directions. The manually curated real dataset exhibits significant variation in im- age quality but may lack proportional ethnic representation, potentially resulting in the model s underperformance for minority groups. Conversely, the DM yielded diverse im- ages, but it may lack certain nuanced human characteristics, the absence of which could disrupt the model s accuracy. Lacking a precise quantitative method to gauge these biases, it would be imprudent to assume that either dataset thor- oughly captures reality. While the ideal future testing data would mirror reality in terms of camera quality, lighting, and angle, our current approach involves making estimations based on the combined dataset, relying on data augmenta- tion to compensate for potential inconsistencies. Automating Dental Smile Analysis in Smart Mirrors with CNN and Diffusion Model We have not only presented a novel framework that resolves all the current drawbacks of traditional smile analysis, but also provided a high-performing model and a dental smile dataset. The research represents a significant step forward in applying ML to dental smile analysis, underscoring the potential of utilizing IoT and ML techniques in real-time healthcare applications.","Mirror, Mirror on the Wall: Automating Dental Smile Analysis in Smart Mirrors with CNN and Diffusion Model Automating dental smile analysis using a CNN trained on real and generated data with a vision to be integrated into an innovative Internet-of-Mirrors network of smart mirrors. This paper presents a smart diagnostic framework for dental smile analysis. To accurately and efficiently identify esthetic issues from a single image of a smile, a convolutional neural network (CNN) was trained. To overcome the limitations of scarce data, a diffusion model was employed to generate dental smile images in addition to manually curated data. The CNN was trained and evaluated on three datasets: all real images, all generated images, and a hybrid dataset of equal proportions of real-to-generated images. All three models demonstrate accuracy significantly above the baseline in detecting excessive gingival display, unlocking a novel diagnostic method in smile analysis. Notably, the hybrid model achieved the highest accuracy of 81.61\% ( value 0.01), highlighting the effectiveness of generative data augmentation for machine learning. The proposed solution could be part of a standalone home-deployed smart mirror or connected to a network of an innovative Internet-of-Mirrors to facilitate patient-dentist communication.","Mirror, Mirror on the Wall: Automating Dental Smile Analysis in Smart Mirrors with CNN and Diffusion Model Automating dental smile analysis using a CNN trained on real and generated data with a vision to be integrated into an innovative Internet-of-Mirrors network of smart mirrors. This paper presents a smart diagnostic framework for dental smile analysis. To accurately and efficiently identify esthetic issues from a single image of a smile, a convolutional neural network (CNN) was trained. To overcome the limitations of scarce data, a diffusion model was employed to generate dental smile images in addition to manually curated data. The CNN was trained and evaluated on three datasets: all real images, all generated images, and a hybrid dataset of equal proportions of real-to-generated images. All three models demonstrate accuracy significantly above the baseline in detecting excessive gingival display, unlocking a novel diagnostic method in smile analysis. Notably, the hybrid model achieved the highest accuracy of 81.61\% ( value 0.01), highlighting the effectiveness of generative data augmentation for machine learning. The proposed solution could be part of a standalone home-deployed smart mirror or connected to a network of an innovative Internet-of-Mirrors to facilitate patient-dentist communication. Conclusion We opted to evaluate each model using a combined dataset comprising both real and AI-generated images, rather than relying solely on real images. This decision stems from the recognition that both types of images may harbor biases, though potentially in divergent directions. The manually curated real dataset exhibits significant variation in im- age quality but may lack proportional ethnic representation, potentially resulting in the model s underperformance for minority groups. Conversely, the DM yielded diverse im- ages, but it may lack certain nuanced human characteristics, the absence of which could disrupt the model s accuracy. Lacking a precise quantitative method to gauge these biases, it would be imprudent to assume that either dataset thor- oughly captures reality. While the ideal future testing data would mirror reality in terms of camera quality, lighting, and angle, our current approach involves making estimations based on the combined dataset, relying on data augmenta- tion to compensate for potential inconsistencies. Automating Dental Smile Analysis in Smart Mirrors with CNN and Diffusion Model We have not only presented a novel framework that resolves all the current drawbacks of traditional smile analysis, but also provided a high-performing model and a dental smile dataset. The research represents a significant step forward in applying ML to dental smile analysis, underscoring the potential of utilizing IoT and ML techniques in real-time healthcare applications. Methodology 2.1. Dataset In the context of dental smiles, the absence of a suitable dataset in open-source repositories and the ethical collection of a diverse array of dental images encompassing propor- tional ethnicities, genders, and degrees of gingival display became a persistent endeavor posed a significant challenge. In total, two sets of 256 (counting 512 combined) images were collected and evenly distributed into gummy and normal classes. The first set was manually curated from publicly available pictures scrapped from online frontal face images of people smiling. These included highly positive excessive gingival display samples as well as more discrete ones. The second set was collected from text-to-image AI generated images using Adobe Firefly s DM (ado). The text prompts for the normal class varied from frontal portrait of a person smiling with teeth; this person does not have a gummy smile or braces to simply picking out relevant images in frontal portrait of a person smiling . Despite some filtering being required, this process still consumed less time and resources than manual annotation. Both sets contributed 56 randomly chosen images each to the testing dataset, setting aside 112 of the original 512 images for testing. The remaining 400 (200 from real, 200 AI-generated by the DM) images were randomly split with 80% assigned to training and 20% to validation. In total, three separate datasets were curated: real images (256 im- ages), AI-generated images (256 images), and combinedimages (512 images). The data was balanced evenly be- tween the two classes, and the correctness of the labels was validated by a professional dentist. 2.2. Preprocessing To mitigate overfitting, all images were normalized and grayscaled. Subsequently, each image underwent a process where a pre-trained facial landmark detector from the Dlib library was utilized to pinpoint the coordinates of the mouth, followed by cropping every image to a 28 by 28 pixels cen- tered a",0
735f60043bf3cd6e067f0ecdf6a647142d451a78,Quality-Diversity for One-Shot Biological Sequence Design,"['Jérémie DONA', 'Arthur Flajolet', 'Andrei Marginean', 'Antoine Cully', 'Thomas PIERROT']",https://openreview.net/pdf/735f60043bf3cd6e067f0ecdf6a647142d451a78.pdf,"Quality-Diversity for One-Shot Biological Sequence Design Quality-Diversity for One-Shot Biological Sequence Design Designing sequences with desired properties is a common problem in Biology. Relying exclusively on wet-lab experiments to select sequences is costly and time-consuming so in-silico design is often used as a preliminary step. The latter is hard for three reasons. First, the search space is discrete and large. Second, scoring functions quantifying target properties may be inaccurate, especially if fitted on a limited dataset. Third, not all properties can be modeled in silico or measured in vitro, thus requiring in-vivo experiments for evaluation. Strategies have been developed in the literature to address the first two challenges. As for the third one, there is a consensus that concurrently evaluating batches of sequences, supposedly high-performing and diverse, is a good strategy to maximize the chances that at least one design will meet all desidera. Ideally, this is achieved in one shot. We develop a Quality Diversity approach, to guarantee diversity for any batch size. We show that our method outperforms existing ones in terms of diversity, performance, and hyperparameter sensitivity on three datasets from the literature.",735f60043bf3cd6e067f0ecdf6a647142d451a78.pdf,"approach, to guar-
antee diversity for any batch size. We show that
our method outperforms existing ones in terms of
diversity, performance, and hyperparameter sensi-
tivity on three datasets from the literature.
1. Introduction
The problem of designing biological sequences (e.g. pro-
teins, RNAs, or DNA) with desired properties is com-
monplace in Biology, in particular in therapeutic applica-
tions such as when designing mRNA vaccines (Miao et al .,
2021), antibodies with high affinity to disease-associated
targets (Liu et al ., 2020), or anti-microbial peptides (Fjell
et al., 2012). Originally, practitioners relied on a combi-
nation of human expert knowledge, heuristics, or random
mutations to iteratively select design candidates and evalu-
ate them in expensive and time-consuming wet-lab experi-
ments until one meeting all requirements was found (Cobb
*Equal contribution1InstaDeep, Paris, France2Imperial Col-
lege, London, United Kingdom. Correspondence to: J ´er´emie Don `a
<j.dona@instadeep.com >.
Accepted at the 1st Machine Learning for Life and Material Sci-
ences Workshop at ICML 2024 . Copyright 2024 by the author(s).et al., 2013). With the advent of modern Bioinformatics
tools, the increased availability of biological data due to im-
provements in high-throughput assays, and recent advances
in AI for protein folding and function prediction (Jumper
et al., 2021; Rives et al ., 2021), RNA folding (Townshend
et al., 2021), or molecular phenotype predictions from DNA
sequence (Dalla-Torre et al ., 2023), in-silico screening is
becoming a trustworthy tool that holds the promise of sig-
nificant cost savings and shortened development times.
Modern sequence design pipelines are typically broken
down in four stages (Jain et al ., 2022). In a first stage, in-
silico screening is carried out to select a batch of promising
design candidates. In a second stage, some of the key desired
properties are evaluated experimentally in vitro for all candi-
dates. Finally, se","future work.
Quality-Diversity for One-Shot Biological Sequence Design
References
Moustafa Alzantot, Yash Sharma, Supriyo Chakraborty,
Huan Zhang, Cho-Jui Hsieh, and Mani B. Srivas-
tava. 2019. GenAttack: Practical Black-Box Attacks
with Gradient-Free Optimization. In Proceedings of
the Genetic and Evolutionary Computation Confer-
ence (Prague, Czech Republic) (GECCO ’19) . As-
sociation for Computing Machinery, New York, NY ,
USA, 1111–1119. https://doi.org/10.1145/
3321707.3321749
Christof Angermueller, David Belanger, Andreea Gane,
Zelda Mariet, David Dohan, Kevin Murphy, Lucy Col-
well, and D Sculley. 2020a. Population-based black-box
optimization for biological sequence design. In Interna-
tional Conference on Machine Learning . 324–334.
Christof Angermueller, David Dohan, David Belanger,
Ramya Deshpande, Kevin Murphy, and Lucy Colwell.
2020b. Model-based reinforcement learning for biolog-
ical sequence design. In International Conference on
Learning Representations .
Raphael Boige, Guillaume Richard, J ´er´emie Dona, Thomas
Pierrot, and Antoine Cully. 2023. Gradient-Informed
Quality Diversity for the Illumination of Discrete Spaces.
InProceedings of the Genetic and Evolutionary Compu-
tation Conference .
James Bradbury, Roy Frostig, Peter Hawkins,
Matthew James Johnson, Chris Leary, Dougal Maclaurin,
George Necula, Adam Paszke, Jake VanderPlas, Skye
Wanderman-Milne, and Qiao Zhang. 2018. JAX: com-
posable transformations of Python+NumPy programs .
http://github.com/google/jax
David Brookes, Hahnbeom Park, and Jennifer Listgarten.
2019. Conditioning by adaptive sampling for robust de-
sign. In International conference on machine learning ,
V ol. 97. 773–782.
Drew H Bryant, Ali Bashir, Sam Sinai, Nina K Jain, Pierce J
Ogden, Patrick F Riley, George M Church, Lucy J Col-
well, and Eric D Kelsic. 2021. Deep diversification of
an AA V capsid protein by machine learning. Nature
Biotechnology 39, 6 (2021), 691–696.
Egbert Castro, Abhinav Godavarthi, Julian Rubinfien,","approach, to guar- antee diversity for any batch size. We show that our method outperforms existing ones in terms of diversity, performance, and hyperparameter sensi- tivity on three datasets from the literature. 1. Introduction The problem of designing biological sequences (e.g. pro- teins, RNAs, or DNA) with desired properties is com- monplace in Biology, in particular in therapeutic applica- tions such as when designing mRNA vaccines (Miao et al ., 2021), antibodies with high affinity to disease-associated targets (Liu et al ., 2020), or anti-microbial peptides (Fjell et al., 2012). Originally, practitioners relied on a combi- nation of human expert knowledge, heuristics, or random mutations to iteratively select design candidates and evalu- ate them in expensive and time-consuming wet-lab experi- ments until one meeting all requirements was found (Cobb *Equal contribution1InstaDeep, Paris, France2Imperial Col- lege, London, United Kingdom. Correspondence to: J er emie Don `a <j.dona@instadeep.com >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).et al., 2013). With the advent of modern Bioinformatics tools, the increased availability of biological data due to im- provements in high-throughput assays, and recent advances in AI for protein folding and function prediction (Jumper et al., 2021; Rives et al ., 2021), RNA folding (Townshend et al., 2021), or molecular phenotype predictions from DNA sequence (Dalla-Torre et al ., 2023), in-silico screening is becoming a trustworthy tool that holds the promise of sig- nificant cost savings and shortened development times. Modern sequence design pipelines are typically broken down in four stages (Jain et al ., 2022). In a first stage, in- silico screening is carried out to select a batch of promising design candidates. In a second stage, some of the key desired properties are evaluated experimentally in vitro for all candi- dates. Finally, se",future work. Quality-Diversity for One-Shot Biological Sequence Design,"Quality-Diversity for One-Shot Biological Sequence Design Quality-Diversity for One-Shot Biological Sequence Design Designing sequences with desired properties is a common problem in Biology. Relying exclusively on wet-lab experiments to select sequences is costly and time-consuming so in-silico design is often used as a preliminary step. The latter is hard for three reasons. First, the search space is discrete and large. Second, scoring functions quantifying target properties may be inaccurate, especially if fitted on a limited dataset. Third, not all properties can be modeled in silico or measured in vitro, thus requiring in-vivo experiments for evaluation. Strategies have been developed in the literature to address the first two challenges. As for the third one, there is a consensus that concurrently evaluating batches of sequences, supposedly high-performing and diverse, is a good strategy to maximize the chances that at least one design will meet all desidera. Ideally, this is achieved in one shot. We develop a Quality Diversity approach, to guarantee diversity for any batch size. We show that our method outperforms existing ones in terms of diversity, performance, and hyperparameter sensitivity on three datasets from the literature.","Quality-Diversity for One-Shot Biological Sequence Design Quality-Diversity for One-Shot Biological Sequence Design Designing sequences with desired properties is a common problem in Biology. Relying exclusively on wet-lab experiments to select sequences is costly and time-consuming so in-silico design is often used as a preliminary step. The latter is hard for three reasons. First, the search space is discrete and large. Second, scoring functions quantifying target properties may be inaccurate, especially if fitted on a limited dataset. Third, not all properties can be modeled in silico or measured in vitro, thus requiring in-vivo experiments for evaluation. Strategies have been developed in the literature to address the first two challenges. As for the third one, there is a consensus that concurrently evaluating batches of sequences, supposedly high-performing and diverse, is a good strategy to maximize the chances that at least one design will meet all desidera. Ideally, this is achieved in one shot. We develop a Quality Diversity approach, to guarantee diversity for any batch size. We show that our method outperforms existing ones in terms of diversity, performance, and hyperparameter sensitivity on three datasets from the literature. future work. Quality-Diversity for One-Shot Biological Sequence Design approach, to guar- antee diversity for any batch size. We show that our method outperforms existing ones in terms of diversity, performance, and hyperparameter sensi- tivity on three datasets from the literature. 1. Introduction The problem of designing biological sequences (e.g. pro- teins, RNAs, or DNA) with desired properties is com- monplace in Biology, in particular in therapeutic applica- tions such as when designing mRNA vaccines (Miao et al ., 2021), antibodies with high affinity to disease-associated targets (Liu et al ., 2020), or anti-microbial peptides (Fjell et al., 2012). Originally, practitioners relied on a combi- nation of human expert knowledge, heuristics, or random mutations to iteratively select design candidates and evalu- ate them in expensive and time-consuming wet-lab experi- ments until one meeting all requirements was found (Cobb *Equal contribution1InstaDeep, Paris, France2Imperial Col- lege, London, United Kingdom. Correspondence to: J er emie Don `a <j.dona@instadeep.com >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).et al., 2013). With the advent of modern Bioinformatics tools, the increased availability of biological data due to im- provements in high-throughput assays, and recent advances in AI for protein folding and function prediction (Jumper et al., 2021; Rives et al ., 2021), RNA folding (Townshend et al., 2021), or molecular phenotype predictions from DNA sequence (Dalla-Torre et al ., 2023), in-silico screening is becoming a trustworthy tool that holds the promise of sig- nificant cost savings and shortened development times. Modern sequence design pipelines are typically broken down in four stages (Jain et al ., 2022). In a first stage, in- silico screening is carried out to select a batch of promising design candidates. In a second stage, some of the key desired properties are evaluated experimentally in vitro for all candi- dates. Finally, se",0
daa98278d16a9d575661b7f03924be50f9afef3a,Out-of-Distribution Validation for Bioactivity Prediction  in Drug Discovery: Lessons from Materials Science,"['Udit Surya Saha', 'Michele Vendruscolo', 'Anne E Carpenter', 'Shantanu Singh', 'Andreas Bender', 'Srijit Seal']",https://openreview.net/pdf/daa98278d16a9d575661b7f03924be50f9afef3a.pdf,"Out-of-Distribution Validation for Bioactivity Prediction in Drug Discovery: Lessons from Materials Science Building on MLmethods popular in life sciences, we have adapted them for drug discovery, specifically focusing on assessing performance on out-of-distribution data Recent advances in machine learning for materials science have significantly improved the prediction of novel materials. Building on these methods, we have adapted them for drug discovery, specifically focusing on assessing performance on out-of-distribution data. We found this approach more effective than conventional cross-validation methods by employing k-fold n-step forward cross-validation (SFCV) for predicting small molecules. Additionally, we introduced two new metrics: discovery yield and novelty error. These metrics provide deeper insights into model applicability and prediction accuracy for drug-like molecules. Based on our findings, we recommend incorporating these metrics into state-of-the-art bioactivity prediction models for drug discovery.",daa98278d16a9d575661b7f03924be50f9afef3a.pdf,"approach more
effective than conventional cross-validation meth-
ods by employing k-fold n-step forward cross-
validation (SFCV) for predicting small molecules.
Additionally, we introduced two new metrics: dis-
covery yield and novelty error. These metrics
provide deeper insights into model applicability
and prediction accuracy for drug-like molecules.
Based on our findings, we recommend incorporat-
ing these metrics into state-of-the-art bioactivity
prediction models for drug discovery.
1. Introduction
Recently, many advancements have been made in devel-
oping computational methods for predicting properties in
materials science. Suitable validation methods have also
been introduced to estimate the performance of these predic-
tive models [18, 3, 4]. Here, we investigated whether these
validation methods can be translated into the field of drug
discovery. Here, we address the problem of prospective val-
idation. Since predictive models are trained and validated
on the experimentally measured activity of libraries of com-
pounds, real-world use in drug discovery requires strong
performance on out-of-distribution data [11]. This is be-
cause the goal is often to accurately predict the properties of
compounds that have not been synthesized yet. Inadequate
prospective validation is a common issue in the drug discov-
ery literature, often creating a mismatch between published
*Equal contribution1Yusuf Hamied Department of Chemistry,
University of Cambridge, UK2Broad Institute of MIT and Har-
vard, Cambridge, MA, US3STAR-UBB Institute, Babe s ¸-Bolyai
University, Cluj-Napoca, Romania . Correspondence to: Srijit Seal
<seal@broadinstitute.org >.
Accepted at the 1st Machine Learning for Life and Material Sci-
ences Workshop at ICML 2024 . Copyright 2024 by the author(s).studies and real-world use [1, 2]. This problem is less severe
in domains such as materials science, where the underlying
physical principles are often known [4, 37], and protein fold-
ing, where evolution le","Conclusion
We have investigated the potential of applying machine
learning validation techniques developed in materials sci-ence to molecular property prediction in drug discovery.
We made a conceptual argument that k-fold n-step forward
cross-validation (SFCV) [34] better matches real-world
use and is more stringent than conventional k-fold cross-
validation (CV). We show that sorted SFCV is better at
selecting novel structures where the biological activity is of
interest compared to the second-best validation approach of
CV with scaffold-based splitting. We have also translated
discovery yield [3] and novelty error [3] into the drug discov-
ery field as metrics to evaluate models to accurately predict
properties of compounds that diverge from the training data
in terms of biological property and chemical structure.
Using a sorted SFCV , we have demonstrated that it is chal-
lenging for any model to perform consistently in a varying
chemical space that systematically changes towards a de-
sired property, such as logP. The differing discovery yields
observed across different validation methods highlight the
influence of dataset composition and model training strate-
gies on predictive accuracy. Models trained using a sorted
SFCV are less prone to overfitting and yield lower nov-
elty errors. In the future, models optimized using sorted
SFCV—implemented in DMTA cycles [24]—could poten-
tially generalize better for diverse compounds. In the future,
SFCV could be implemented for other activity measures,
such as inhibition constants (K i) and potency measures from
ChEMBL, offering additional insights. Various activity mea-
sures are crucial for understanding compound-protein inter-
actions in drug discovery, and findings from other activity
measures could help elucidate how they relate to IC 50, the
property explored in the current work.
Overall, we recommend the techniques presented in this
study to align model testing with the directional nature of
drug discovery, whe","approach more effective than conventional cross-validation meth- ods by employing k-fold n-step forward cross- validation (SFCV) for predicting small molecules. Additionally, we introduced two new metrics: dis- covery yield and novelty error. These metrics provide deeper insights into model applicability and prediction accuracy for drug-like molecules. Based on our findings, we recommend incorporat- ing these metrics into state-of-the-art bioactivity prediction models for drug discovery. 1. Introduction Recently, many advancements have been made in devel- oping computational methods for predicting properties in materials science. Suitable validation methods have also been introduced to estimate the performance of these predic- tive models [18, 3, 4]. Here, we investigated whether these validation methods can be translated into the field of drug discovery. Here, we address the problem of prospective val- idation. Since predictive models are trained and validated on the experimentally measured activity of libraries of com- pounds, real-world use in drug discovery requires strong performance on out-of-distribution data [11]. This is be- cause the goal is often to accurately predict the properties of compounds that have not been synthesized yet. Inadequate prospective validation is a common issue in the drug discov- ery literature, often creating a mismatch between published *Equal contribution1Yusuf Hamied Department of Chemistry, University of Cambridge, UK2Broad Institute of MIT and Har- vard, Cambridge, MA, US3STAR-UBB Institute, Babe s -Bolyai University, Cluj-Napoca, Romania . Correspondence to: Srijit Seal <seal@broadinstitute.org >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).studies and real-world use [1, 2]. This problem is less severe in domains such as materials science, where the underlying physical principles are often known [4, 37], and protein fold- ing, where evolution le","Conclusion We have investigated the potential of applying machine learning validation techniques developed in materials sci-ence to molecular property prediction in drug discovery. We made a conceptual argument that k-fold n-step forward cross-validation (SFCV) [34] better matches real-world use and is more stringent than conventional k-fold cross- validation (CV). We show that sorted SFCV is better at selecting novel structures where the biological activity is of interest compared to the second-best validation approach of CV with scaffold-based splitting. We have also translated discovery yield [3] and novelty error [3] into the drug discov- ery field as metrics to evaluate models to accurately predict properties of compounds that diverge from the training data in terms of biological property and chemical structure. Using a sorted SFCV , we have demonstrated that it is chal- lenging for any model to perform consistently in a varying chemical space that systematically changes towards a de- sired property, such as logP. The differing discovery yields observed across different validation methods highlight the influence of dataset composition and model training strate- gies on predictive accuracy. Models trained using a sorted SFCV are less prone to overfitting and yield lower nov- elty errors. In the future, models optimized using sorted SFCV implemented in DMTA cycles [24] could poten- tially generalize better for diverse compounds. In the future, SFCV could be implemented for other activity measures, such as inhibition constants (K i) and potency measures from ChEMBL, offering additional insights. Various activity mea- sures are crucial for understanding compound-protein inter- actions in drug discovery, and findings from other activity measures could help elucidate how they relate to IC 50, the property explored in the current work. Overall, we recommend the techniques presented in this study to align model testing with the directional nature of drug discovery, whe","Out-of-Distribution Validation for Bioactivity Prediction in Drug Discovery: Lessons from Materials Science Building on MLmethods popular in life sciences, we have adapted them for drug discovery, specifically focusing on assessing performance on out-of-distribution data Recent advances in machine learning for materials science have significantly improved the prediction of novel materials. Building on these methods, we have adapted them for drug discovery, specifically focusing on assessing performance on out-of-distribution data. We found this approach more effective than conventional cross-validation methods by employing k-fold n-step forward cross-validation (SFCV) for predicting small molecules. Additionally, we introduced two new metrics: discovery yield and novelty error. These metrics provide deeper insights into model applicability and prediction accuracy for drug-like molecules. Based on our findings, we recommend incorporating these metrics into state-of-the-art bioactivity prediction models for drug discovery.","Out-of-Distribution Validation for Bioactivity Prediction in Drug Discovery: Lessons from Materials Science Building on MLmethods popular in life sciences, we have adapted them for drug discovery, specifically focusing on assessing performance on out-of-distribution data Recent advances in machine learning for materials science have significantly improved the prediction of novel materials. Building on these methods, we have adapted them for drug discovery, specifically focusing on assessing performance on out-of-distribution data. We found this approach more effective than conventional cross-validation methods by employing k-fold n-step forward cross-validation (SFCV) for predicting small molecules. Additionally, we introduced two new metrics: discovery yield and novelty error. These metrics provide deeper insights into model applicability and prediction accuracy for drug-like molecules. Based on our findings, we recommend incorporating these metrics into state-of-the-art bioactivity prediction models for drug discovery. Conclusion We have investigated the potential of applying machine learning validation techniques developed in materials sci-ence to molecular property prediction in drug discovery. We made a conceptual argument that k-fold n-step forward cross-validation (SFCV) [34] better matches real-world use and is more stringent than conventional k-fold cross- validation (CV). We show that sorted SFCV is better at selecting novel structures where the biological activity is of interest compared to the second-best validation approach of CV with scaffold-based splitting. We have also translated discovery yield [3] and novelty error [3] into the drug discov- ery field as metrics to evaluate models to accurately predict properties of compounds that diverge from the training data in terms of biological property and chemical structure. Using a sorted SFCV , we have demonstrated that it is chal- lenging for any model to perform consistently in a varying chemical space that systematically changes towards a de- sired property, such as logP. The differing discovery yields observed across different validation methods highlight the influence of dataset composition and model training strate- gies on predictive accuracy. Models trained using a sorted SFCV are less prone to overfitting and yield lower nov- elty errors. In the future, models optimized using sorted SFCV implemented in DMTA cycles [24] could poten- tially generalize better for diverse compounds. In the future, SFCV could be implemented for other activity measures, such as inhibition constants (K i) and potency measures from ChEMBL, offering additional insights. Various activity mea- sures are crucial for understanding compound-protein inter- actions in drug discovery, and findings from other activity measures could help elucidate how they relate to IC 50, the property explored in the current work. Overall, we recommend the techniques presented in this study to align model testing with the directional nature of drug discovery, whe approach more effective than conventional cross-validation meth- ods by employing k-fold n-step forward cross- validation (SFCV) for predicting small molecules. Additionally, we introduced two new metrics: dis- covery yield and novelty error. These metrics provide deeper insights into model applicability and prediction accuracy for drug-like molecules. Based on our findings, we recommend incorporat- ing these metrics into state-of-the-art bioactivity prediction models for drug discovery. 1. Introduction Recently, many advancements have been made in devel- oping computational methods for predicting properties in materials science. Suitable validation methods have also been introduced to estimate the performance of these predic- tive models [18, 3, 4]. Here, we investigated whether these validation methods can be translated into the field of drug discovery. Here, we address the problem of prospective val- idation. Since predictive models are trained and validated on the experimentally measured activity of libraries of com- pounds, real-world use in drug discovery requires strong performance on out-of-distribution data [11]. This is be- cause the goal is often to accurately predict the properties of compounds that have not been synthesized yet. Inadequate prospective validation is a common issue in the drug discov- ery literature, often creating a mismatch between published *Equal contribution1Yusuf Hamied Department of Chemistry, University of Cambridge, UK2Broad Institute of MIT and Har- vard, Cambridge, MA, US3STAR-UBB Institute, Babe s -Bolyai University, Cluj-Napoca, Romania . Correspondence to: Srijit Seal <seal@broadinstitute.org >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).studies and real-world use [1, 2]. This problem is less severe in domains such as materials science, where the underlying physical principles are often known [4, 37], and protein fold- ing, where evolution le",0
e9a0c02ea3a486fa2c74a8d3caa2bd9fb190af57,Multi-Modal and Multi-Task Transformer for Small Molecule Drug Discovery,"['Sai Krishna Sirumalla', 'David Stephen Farina Jr', 'Zhuoran Qiao', 'Daniele Alessandro Di Cesare', 'Felipe Costas Farias', 'Michael Bernard O’Connor', 'Peter John Bygrave', 'Feizhi Ding', 'Thomas Dresselhaus', 'Marcelo Gomes Pereira de Lacerda', 'Jason Matthew Swails', 'Daniel Miles', 'Matthew Welborn', 'Fred Manby', 'Thomas Miller']",https://openreview.net/pdf/e9a0c02ea3a486fa2c74a8d3caa2bd9fb190af57.pdf,"Multi-Modal and Multi-Task Transformer for Small Molecule Drug Discovery We introduce a 1B-parameter transformer model pre-trained from scratch on 2.25T tokens from a massive mixture of datasets centered around drug discovery. These datasets are heterogeneous, coming from dozens of sources and covering 15 data modalities. We demonstrate the model s capability on various molecular assay prediction tasks, including public benchmarks and internally generated holdouts from real-world drug discovery programs. Following parameter-efficient fine-tuning, the multi-modal transformer excels at multi-task predictions compared to strong molecular property prediction baselines including XGBoost and Chemprop.",e9a0c02ea3a486fa2c74a8d3caa2bd9fb190af57.pdf,"methodology and encoding
rules. J. Phys. Chem. A , 28(1):31–36, February 1988a.
Weininger, D. Smiles, a chemical language and information
system. 1. introduction to methodology and encoding
rules. Journal of chemical information and computer
sciences , 28(1):31–36, 1988b.
Yasunaga, M., Aghajanyan, A., Shi, W., James, R.,
Leskovec, J., Liang, P., Lewis, M., Zettlemoyer, L., and
Yih, W.-t. Retrieval-augmented multimodal language
modeling. arXiv preprint arXiv:2211.12561 , 2022.
Yoon, W., Jackson, R., Ford, E., Poroshin, V ., and
Kang, J. Biomedical ner for the enterprise with dis-
tillated bern2 and the kazu framework. arXiv preprint
arXiv:2212.00223 , 2022.
Zhang, P., Zeng, G., Wang, T., and Lu, W. Tinyllama:
An open-source small language model. arXiv preprint
arXiv:2401.02385 , 2024.
Zhao, Y ., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M.,
Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al.
Pytorch fsdp: experiences on scaling fully sharded data
parallel. arXiv preprint arXiv:2304.11277 , 2023.","Future work could
include predictions involving multiple chemical entities,
such as reaction yields, and could further extend to include
structured data, as in retrosynthesis prediction. The model,
being generative, also lends itself naturally to generative
tasks, including inverse design of molecules with desired
properties.
Finally, it is well established that transformer models im-
prove as the number of training data and model parameters
Multi-Modal and Multi-Task Transformer for Small Molecule Drug Discovery
are increased. Our model is relatively small, and there is
headroom to explore significantly larger models. In addition,
the universe of public biomedical data is vast, and could
readily provide orders of magnitude more training tokens.
6. Acknowledgments
This work used computational resources provided by the
National Energy Research Scientific Computing Center
(NERSC), under Contract No. ERCAP0029432.
References
OpenAI Documentation: GPT-3.5 Turbo.
https://platform.openai.com/docs/models/gpt-3-5-turbo.
Accessed: 2024-05-23.
Protein data bank: the single global archive for 3d macro-
molecular structure data. Nucleic acids research , 47(D1):
D520–D528, 2019.
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,
Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,
Anadkat, S., et al. Gpt-4 technical report. arXiv preprint
arXiv:2303.08774 , 2023.
Aghajanyan, A., Huang, B., Ross, C., Karpukhin, V ., Xu,
H., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,
M., et al. Cm3: A causal masked multimodal model of
the internet. arXiv preprint arXiv:2201.07520 , 2022.
Askr, H., Elgeldawi, E., Aboul Ella, H., Elshaier, Y . A., Go-
maa, M. M., and Hassanien, A. E. Deep learning in drug
discovery: an integrative review and future challenges.
Artificial Intelligence Review , 56(7):5975–6037, 2023.
Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,
L., Golding, L., He, H., Leahy, C., McDonell, K., Phang,
J., et al. Gpt-neox-20b: An open-source autoregres","methodology and encoding rules. J. Phys. Chem. A , 28(1):31 36, February 1988a. Weininger, D. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences , 28(1):31 36, 1988b. Yasunaga, M., Aghajanyan, A., Shi, W., James, R., Leskovec, J., Liang, P., Lewis, M., Zettlemoyer, L., and Yih, W.-t. Retrieval-augmented multimodal language modeling. arXiv preprint arXiv:2211.12561 , 2022. Yoon, W., Jackson, R., Ford, E., Poroshin, V ., and Kang, J. Biomedical ner for the enterprise with dis- tillated bern2 and the kazu framework. arXiv preprint arXiv:2212.00223 , 2022. Zhang, P., Zeng, G., Wang, T., and Lu, W. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385 , 2024. Zhao, Y ., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277 , 2023.","Future work could include predictions involving multiple chemical entities, such as reaction yields, and could further extend to include structured data, as in retrosynthesis prediction. The model, being generative, also lends itself naturally to generative tasks, including inverse design of molecules with desired properties. Finally, it is well established that transformer models im- prove as the number of training data and model parameters Multi-Modal and Multi-Task Transformer for Small Molecule Drug Discovery are increased. Our model is relatively small, and there is headroom to explore significantly larger models. In addition, the universe of public biomedical data is vast, and could readily provide orders of magnitude more training tokens. 6. Acknowledgments This work used computational resources provided by the National Energy Research Scientific Computing Center (NERSC), under Contract No. ERCAP0029432.","Multi-Modal and Multi-Task Transformer for Small Molecule Drug Discovery We introduce a 1B-parameter transformer model pre-trained from scratch on 2.25T tokens from a massive mixture of datasets centered around drug discovery. These datasets are heterogeneous, coming from dozens of sources and covering 15 data modalities. We demonstrate the model s capability on various molecular assay prediction tasks, including public benchmarks and internally generated holdouts from real-world drug discovery programs. Following parameter-efficient fine-tuning, the multi-modal transformer excels at multi-task predictions compared to strong molecular property prediction baselines including XGBoost and Chemprop.","Multi-Modal and Multi-Task Transformer for Small Molecule Drug Discovery We introduce a 1B-parameter transformer model pre-trained from scratch on 2.25T tokens from a massive mixture of datasets centered around drug discovery. These datasets are heterogeneous, coming from dozens of sources and covering 15 data modalities. We demonstrate the model s capability on various molecular assay prediction tasks, including public benchmarks and internally generated holdouts from real-world drug discovery programs. Following parameter-efficient fine-tuning, the multi-modal transformer excels at multi-task predictions compared to strong molecular property prediction baselines including XGBoost and Chemprop. Future work could include predictions involving multiple chemical entities, such as reaction yields, and could further extend to include structured data, as in retrosynthesis prediction. The model, being generative, also lends itself naturally to generative tasks, including inverse design of molecules with desired properties. Finally, it is well established that transformer models im- prove as the number of training data and model parameters Multi-Modal and Multi-Task Transformer for Small Molecule Drug Discovery are increased. Our model is relatively small, and there is headroom to explore significantly larger models. In addition, the universe of public biomedical data is vast, and could readily provide orders of magnitude more training tokens. 6. Acknowledgments This work used computational resources provided by the National Energy Research Scientific Computing Center (NERSC), under Contract No. ERCAP0029432. methodology and encoding rules. J. Phys. Chem. A , 28(1):31 36, February 1988a. Weininger, D. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences , 28(1):31 36, 1988b. Yasunaga, M., Aghajanyan, A., Shi, W., James, R., Leskovec, J., Liang, P., Lewis, M., Zettlemoyer, L., and Yih, W.-t. Retrieval-augmented multimodal language modeling. arXiv preprint arXiv:2211.12561 , 2022. Yoon, W., Jackson, R., Ford, E., Poroshin, V ., and Kang, J. Biomedical ner for the enterprise with dis- tillated bern2 and the kazu framework. arXiv preprint arXiv:2212.00223 , 2022. Zhang, P., Zeng, G., Wang, T., and Lu, W. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385 , 2024. Zhao, Y ., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277 , 2023.",0
09f7e6a51ed8e01872f4b297a78c78105d242e94,"Navigating Trustworthiness of Deep Learning in ∆∆G prediction : Addressing Data Bias, Model Evaluation, and Interpretation","['Ruochi Zhang', 'Ningning Chen', 'Fengfeng Zhou', 'Xin Gao']",https://openreview.net/pdf/09f7e6a51ed8e01872f4b297a78c78105d242e94.pdf,"Navigating Trustworthiness of Deep Learning in G prediction : Addressing Data Bias, Model Evaluation, and Interpretation Arti cial intelligence has emerged as an epicenter of global attention, given the rapid proliferation of cutting-edge AI tools. One promising avenue of application is the leveraging of deep learning methodologies to resolve complex biological conundrums. However, an essential question arises about the reliability and utility of deep learning models in the context of biosciences, where experimental data are often limited, especially in comparison to the vast data troves available in other domains. In this work, we focus on the task of identifying the change of binding af nity ( G) induced by mutations in protein-protein interaction, exploring the impact of the data bias, the methods of model evaluation and interpretation. Surprisingly, we nd that deep learning models may only learn the unintentional bias in the dataset instead of intrinsic principles, therefore proper data analysis and model evaluation should be applied not just focusing on improving the evaluation metrics. Our work provides a guideline to navigate the trustworthiness challenges in deep learning in bioscience and brings forth suggestions for future improvements.",09f7e6a51ed8e01872f4b297a78c78105d242e94.pdf,"approach, we
ﬁrst calculated the pairwise sequence similarity of the s1w
ands2wusing a normalized Smith-Waterman algorithm and
then clustered all the sequences based on the similarity by
Hierarchical clustering algorithm. The total 381 unique se-
quences in the set fs1w; s2wgare clustered into 209 clusters
by the threshold of similarity 0.4. Each sequence pair pw
is assigned a unique id based on the clustering results and
the whole dataset is divided into ﬁve folds based on the
clustering id.
Metrics determination The metrics to evaluate the model
performance should be carefully chosen. It should align
with the real world needs, which will be helpful for model
users to choose the right model. In the task of estimating
the change of binding afﬁnity by mutations in PPIs, the
possible application will be to optimize a protein to better
bind a target or to introduce a mutation that might decrease
the binding afﬁnity between a protein and target to achieve
a speciﬁc goal like increasing the speciﬁcity. Therefore,
instead of the Pearson correlation and RMSE used by most
of the models in this ﬁeld, we used Spearman correlation
as the metrics because the ranking between the mutations
is more meaningful than predicting the accurate numbers
of afﬁnity. Additionally, the aggregate metric might only
provide the ”at a glance” performance of the model, but
granular evaluations are needed (Burnell et al., 2023). In
this task setting, the predictions inside one type of protein
or protein family are more meaningful than the overall per-
formance, i.e. the rankings between the different protein
families might not be helpful.
Held-out sanity check Beside the traditional steps in deep
learning, we also propose some approaches to sanity check
that the model learns the basic knowledge but not something
unexpected(e.g. data bias) like the phenomenon of ”Clever
Hans”. Here, we propose two methods:•Dataset features regression test: applying simple ma-
chine learning on the features of the d","Conclusion and Discussion
This study explored the application of deep learning models
in biosciences, speciﬁcally focusing on identifying changes
in binding afﬁnity( G) caused by mutations in protein-
protein interaction. We underscored the importance of
proper data analysis and model evaluation as crucial deter-
minants of the trustworthiness of such models. Our ﬁndings
unveiled that deep learning models might be learning unin-
tentional biases present in the dataset rather than the actual
biological relationships, thereby questioning their practi-
cal utility. The results depicted that models like ProtMut
performed signiﬁcantly better than other baseline models
under both random and cluster-level split conditions. How-
ever, the implementation of the cluster-level split strategy
led to a marked decrease in performance across all models.
This trend highlighted the sensitivity of these models to the
partitioning strategy and the potential pitfalls of bias in the
data. It also emphasized the necessity of rigorous model
evaluation approaches that extend beyond mere metric im-
Submission and Formatting Instructions for ML4LMS @ ICML 2024
provement.
In conclusion, while deep learning offers immense potential
in biosciences, its applications should be approached with
caution. A thorough understanding of the underlying data
and appropriate evaluation methods is essential to avoid
misleading conclusions and to truly harness the power of
deep learning in this ﬁeld.
References
Bruzzoni-Giovanelli, H., Alezra, V ., Wolff, N., Dong, C.-Z.,
Tuffery, P., and Rebollo, A. Interfering peptides target-
ing protein–protein interactions: the next generation of
drugs? Drug Discovery Today , 23(2):272–285, 2018.
Burnell, R., Schellaert, W., Burden, J., Ullman, T. D.,
Martinez-Plumed, F., Tenenbaum, J. B., Rutar, D., Cheke,
L. G., Sohl-Dickstein, J., Mitchell, M., Kiela, D., Shana-
han, M., V oorhees, E. M., Cohn, A. G., Leibo, J. Z.,
and Hernandez-Orallo, J. Rethink reporting of evalu","approach, we rst calculated the pairwise sequence similarity of the s1w ands2wusing a normalized Smith-Waterman algorithm and then clustered all the sequences based on the similarity by Hierarchical clustering algorithm. The total 381 unique se- quences in the set fs1w; s2wgare clustered into 209 clusters by the threshold of similarity 0.4. Each sequence pair pw is assigned a unique id based on the clustering results and the whole dataset is divided into ve folds based on the clustering id. Metrics determination The metrics to evaluate the model performance should be carefully chosen. It should align with the real world needs, which will be helpful for model users to choose the right model. In the task of estimating the change of binding af nity by mutations in PPIs, the possible application will be to optimize a protein to better bind a target or to introduce a mutation that might decrease the binding af nity between a protein and target to achieve a speci c goal like increasing the speci city. Therefore, instead of the Pearson correlation and RMSE used by most of the models in this eld, we used Spearman correlation as the metrics because the ranking between the mutations is more meaningful than predicting the accurate numbers of af nity. Additionally, the aggregate metric might only provide the at a glance performance of the model, but granular evaluations are needed (Burnell et al., 2023). In this task setting, the predictions inside one type of protein or protein family are more meaningful than the overall per- formance, i.e. the rankings between the different protein families might not be helpful. Held-out sanity check Beside the traditional steps in deep learning, we also propose some approaches to sanity check that the model learns the basic knowledge but not something unexpected(e.g. data bias) like the phenomenon of Clever Hans . Here, we propose two methods: Dataset features regression test: applying simple ma- chine learning on the features of the d","Conclusion and Discussion This study explored the application of deep learning models in biosciences, speci cally focusing on identifying changes in binding af nity( G) caused by mutations in protein- protein interaction. We underscored the importance of proper data analysis and model evaluation as crucial deter- minants of the trustworthiness of such models. Our ndings unveiled that deep learning models might be learning unin- tentional biases present in the dataset rather than the actual biological relationships, thereby questioning their practi- cal utility. The results depicted that models like ProtMut performed signi cantly better than other baseline models under both random and cluster-level split conditions. How- ever, the implementation of the cluster-level split strategy led to a marked decrease in performance across all models. This trend highlighted the sensitivity of these models to the partitioning strategy and the potential pitfalls of bias in the data. It also emphasized the necessity of rigorous model evaluation approaches that extend beyond mere metric im- Submission and Formatting Instructions for ML4LMS @ ICML 2024 provement. In conclusion, while deep learning offers immense potential in biosciences, its applications should be approached with caution. A thorough understanding of the underlying data and appropriate evaluation methods is essential to avoid misleading conclusions and to truly harness the power of deep learning in this eld.","Navigating Trustworthiness of Deep Learning in G prediction : Addressing Data Bias, Model Evaluation, and Interpretation Arti cial intelligence has emerged as an epicenter of global attention, given the rapid proliferation of cutting-edge AI tools. One promising avenue of application is the leveraging of deep learning methodologies to resolve complex biological conundrums. However, an essential question arises about the reliability and utility of deep learning models in the context of biosciences, where experimental data are often limited, especially in comparison to the vast data troves available in other domains. In this work, we focus on the task of identifying the change of binding af nity ( G) induced by mutations in protein-protein interaction, exploring the impact of the data bias, the methods of model evaluation and interpretation. Surprisingly, we nd that deep learning models may only learn the unintentional bias in the dataset instead of intrinsic principles, therefore proper data analysis and model evaluation should be applied not just focusing on improving the evaluation metrics. Our work provides a guideline to navigate the trustworthiness challenges in deep learning in bioscience and brings forth suggestions for future improvements.","Navigating Trustworthiness of Deep Learning in G prediction : Addressing Data Bias, Model Evaluation, and Interpretation Arti cial intelligence has emerged as an epicenter of global attention, given the rapid proliferation of cutting-edge AI tools. One promising avenue of application is the leveraging of deep learning methodologies to resolve complex biological conundrums. However, an essential question arises about the reliability and utility of deep learning models in the context of biosciences, where experimental data are often limited, especially in comparison to the vast data troves available in other domains. In this work, we focus on the task of identifying the change of binding af nity ( G) induced by mutations in protein-protein interaction, exploring the impact of the data bias, the methods of model evaluation and interpretation. Surprisingly, we nd that deep learning models may only learn the unintentional bias in the dataset instead of intrinsic principles, therefore proper data analysis and model evaluation should be applied not just focusing on improving the evaluation metrics. Our work provides a guideline to navigate the trustworthiness challenges in deep learning in bioscience and brings forth suggestions for future improvements. Conclusion and Discussion This study explored the application of deep learning models in biosciences, speci cally focusing on identifying changes in binding af nity( G) caused by mutations in protein- protein interaction. We underscored the importance of proper data analysis and model evaluation as crucial deter- minants of the trustworthiness of such models. Our ndings unveiled that deep learning models might be learning unin- tentional biases present in the dataset rather than the actual biological relationships, thereby questioning their practi- cal utility. The results depicted that models like ProtMut performed signi cantly better than other baseline models under both random and cluster-level split conditions. How- ever, the implementation of the cluster-level split strategy led to a marked decrease in performance across all models. This trend highlighted the sensitivity of these models to the partitioning strategy and the potential pitfalls of bias in the data. It also emphasized the necessity of rigorous model evaluation approaches that extend beyond mere metric im- Submission and Formatting Instructions for ML4LMS @ ICML 2024 provement. In conclusion, while deep learning offers immense potential in biosciences, its applications should be approached with caution. A thorough understanding of the underlying data and appropriate evaluation methods is essential to avoid misleading conclusions and to truly harness the power of deep learning in this eld. approach, we rst calculated the pairwise sequence similarity of the s1w ands2wusing a normalized Smith-Waterman algorithm and then clustered all the sequences based on the similarity by Hierarchical clustering algorithm. The total 381 unique se- quences in the set fs1w; s2wgare clustered into 209 clusters by the threshold of similarity 0.4. Each sequence pair pw is assigned a unique id based on the clustering results and the whole dataset is divided into ve folds based on the clustering id. Metrics determination The metrics to evaluate the model performance should be carefully chosen. It should align with the real world needs, which will be helpful for model users to choose the right model. In the task of estimating the change of binding af nity by mutations in PPIs, the possible application will be to optimize a protein to better bind a target or to introduce a mutation that might decrease the binding af nity between a protein and target to achieve a speci c goal like increasing the speci city. Therefore, instead of the Pearson correlation and RMSE used by most of the models in this eld, we used Spearman correlation as the metrics because the ranking between the mutations is more meaningful than predicting the accurate numbers of af nity. Additionally, the aggregate metric might only provide the at a glance performance of the model, but granular evaluations are needed (Burnell et al., 2023). In this task setting, the predictions inside one type of protein or protein family are more meaningful than the overall per- formance, i.e. the rankings between the different protein families might not be helpful. Held-out sanity check Beside the traditional steps in deep learning, we also propose some approaches to sanity check that the model learns the basic knowledge but not something unexpected(e.g. data bias) like the phenomenon of Clever Hans . Here, we propose two methods: Dataset features regression test: applying simple ma- chine learning on the features of the d",0
0850980b2bbdf94f1c479f4dc9b98e8fe7707066,From Laboratory to Everyday Life: Personalized Stress Prediction via Smartwatches,"['Batuhan Koyuncu', 'Aleyna Dilan Kıran', 'Katja Heilmann', 'Laith Hamid', 'Anja Buder', 'Veronika Engert', 'Martin Walter', 'Isabel Valera']",https://openreview.net/pdf/0850980b2bbdf94f1c479f4dc9b98e8fe7707066.pdf,"From Laboratory to Everyday Life: Personalized Stress Prediction via Smartwatches This paper introduces MUSTP, a machine learning pipeline that predicts stress using heart rate and ECG data from smartwatches, achieving higher F1 score through model transfer and personalization in everday setting. Accurate prediction of stress in everyday life is essential to prevent chronic stress and maintain health and well-being through early and personalized intervention. With the goal of enabling reliable prediction suitable for everyday life, we present MuStP, a two-stage machine learning pipeline designed to predict stress from low-resolution heart rate (HR) and high-resolution electrocardiography (ECG) measurements from commercial smartwatches. Our model is pre-trained with labeled data collected in a controlled laboratory stress study. Subsequently, we transfer the model for everyday use, enabling it to operate with everyday smartwatch data in various environments. The model transfer strategy effectively addresses the domain shift from laboratory data to highly imbalanced smartwatch data and allows personalization. The empirical results on smartwatch data show that MuStP can predict stress everyday with an F1 score of , despite the measurements having sparse labels for stress.",0850980b2bbdf94f1c479f4dc9b98e8fe7707066.pdf,"Methodology
2.1. L ABDATA- Dataset Description
The LABDATA dataset contains psychological data from
108 participants who underwent stress exposure via the
TSST procedure. Due to recording issues, we eliminated 9
users; therefore, our dataset has effectively 99 users. The
dataset includes ECG signals, salivary cortisol levels, and
self-report measures of stress. ECG measurements are col-
lected during the baseline and stress (TSST) conditions will
be used as hard labels of stress for training our model. More
details about the dataset can be found in Table 1, the source
of HR modality described in Subsection 2.5. Each record of
participants in the LABDATAis appropriately anonymized.
The dataset is planned to be released in the future.2.2. E VERYDAY DATA- Dataset Description
TheEVERYDAY DATAdataset consists of 131 users whose
data is collected for approximately two months via WITH-
INGS SCANWATCH . Users were advised to wear their smart-
watches throughout the day and record an ECG along with
completing an EMA three times per day during the desig-
nated windows of 7-10 AM, 12-3 PM, and 7-10 PM. Users
are supposed to rest their arms on a table and hold the
top electrode with their thumb and index finger for 30 sec-
onds during ECG measurement. The measurements that are
logged shortly after a workout or activity are discarded. For
(soft) stress labels , we are using subjective stress from Af-
fect Grid metrics from collected EMAs (see Appendix A.2
for more details), which are 19,289 in total. More details
about the dataset can be found in Table 1. Similarly, each
record of participants in EVERYDAY DATAis appropriately
anonymized. The dataset is planned to be released in the
future.
2.3. Preprocessing
In the following analysis, HR measurements are taken over
30-minute windows at a frequency of 1/600Hz, resulting
in four measurements per window. ECG measurements,
on the other hand, are taken in non-overlapping 30-second
intervals, with the frequency specific to each datase","future work, we plan to extend our current scheme
using online reinforcement learning-based approaches to
learn user-specific adaptive decision thresholds aiming for
increased personalization without requiring further training
or offline optimization. Lastly, we want to improve the
reliability of our EMA-based soft labels using both Affect
Grid and questionnaires and decide the contribution of each
component by investigating the correlation between cortisol
levels and soft labels.
5. Acknowledgments
The authors acknowledge the support by the German Fed-
eral Ministry of Education and Research through the Cello
project under the project number: 16SV8590.
We thank Dr.-Ing. Mario Aehnelt and Nicola Marlene
Dr¨ueke from Visual Assistance Technologies, Fraunhofer-
Institut f ¨ur Graphische Datenverarbeitung IGD Rostock, for
their assistance in developing the models and processing
EMAs, Dimitri Kraft for his initial model developments, and
Paul Burggraf and Hannes Schenk from Thryve, mHealth
Pioneers GmbH, Berlin, for application development, data
processing and storage. We also appreciate the contributions
of Prof. Dr. Birgit Derntl, Sabrina Eutebach, and Juliane
K¨ostlin from the Department of Psychiatry and Psychother-
apy, University of T ¨ubingen, for data collection in the Cello
field study.
References
Acharya, U. R., Joseph, P. K., Kannathal, N., Lim,
C. M., and Suri, J. S. Heart rate variability: a re-
view. Medical and Biological Engineering and Com-
puting , 44:1031–1051, 2006. URL https://api.
semanticscholar.org/CorpusID:2970664 .
Al-Atawi, A. A., Alyahyan, S., Alatawi, M. N., Sadad,T., Manzoor, T., i Azam, M. F., and Khan, Z. H.
Stress monitoring using machine learning, iot and
wearable sensors. Sensors (Basel, Switzerland) , 23,
2023. URL https://api.semanticscholar.
org/CorpusID:264888849 .
Behinaein, B., Bhatti, A., Rodenburg, D., Hungler,
P., and Etemad, A. A transformer architecture for
stress detection from ecg. Proceedings of the 2021
ACM International","Methodology 2.1. L ABDATA- Dataset Description The LABDATA dataset contains psychological data from 108 participants who underwent stress exposure via the TSST procedure. Due to recording issues, we eliminated 9 users; therefore, our dataset has effectively 99 users. The dataset includes ECG signals, salivary cortisol levels, and self-report measures of stress. ECG measurements are col- lected during the baseline and stress (TSST) conditions will be used as hard labels of stress for training our model. More details about the dataset can be found in Table 1, the source of HR modality described in Subsection 2.5. Each record of participants in the LABDATAis appropriately anonymized. The dataset is planned to be released in the future.2.2. E VERYDAY DATA- Dataset Description TheEVERYDAY DATAdataset consists of 131 users whose data is collected for approximately two months via WITH- INGS SCANWATCH . Users were advised to wear their smart- watches throughout the day and record an ECG along with completing an EMA three times per day during the desig- nated windows of 7-10 AM, 12-3 PM, and 7-10 PM. Users are supposed to rest their arms on a table and hold the top electrode with their thumb and index finger for 30 sec- onds during ECG measurement. The measurements that are logged shortly after a workout or activity are discarded. For (soft) stress labels , we are using subjective stress from Af- fect Grid metrics from collected EMAs (see Appendix A.2 for more details), which are 19,289 in total. More details about the dataset can be found in Table 1. Similarly, each record of participants in EVERYDAY DATAis appropriately anonymized. The dataset is planned to be released in the future. 2.3. Preprocessing In the following analysis, HR measurements are taken over 30-minute windows at a frequency of 1/600Hz, resulting in four measurements per window. ECG measurements, on the other hand, are taken in non-overlapping 30-second intervals, with the frequency specific to each datase","future work, we plan to extend our current scheme using online reinforcement learning-based approaches to learn user-specific adaptive decision thresholds aiming for increased personalization without requiring further training or offline optimization. Lastly, we want to improve the reliability of our EMA-based soft labels using both Affect Grid and questionnaires and decide the contribution of each component by investigating the correlation between cortisol levels and soft labels. 5. Acknowledgments The authors acknowledge the support by the German Fed- eral Ministry of Education and Research through the Cello project under the project number: 16SV8590. We thank Dr.-Ing. Mario Aehnelt and Nicola Marlene Dr ueke from Visual Assistance Technologies, Fraunhofer- Institut f ur Graphische Datenverarbeitung IGD Rostock, for their assistance in developing the models and processing EMAs, Dimitri Kraft for his initial model developments, and Paul Burggraf and Hannes Schenk from Thryve, mHealth Pioneers GmbH, Berlin, for application development, data processing and storage. We also appreciate the contributions of Prof. Dr. Birgit Derntl, Sabrina Eutebach, and Juliane K ostlin from the Department of Psychiatry and Psychother- apy, University of T ubingen, for data collection in the Cello field study.","From Laboratory to Everyday Life: Personalized Stress Prediction via Smartwatches This paper introduces MUSTP, a machine learning pipeline that predicts stress using heart rate and ECG data from smartwatches, achieving higher F1 score through model transfer and personalization in everday setting. Accurate prediction of stress in everyday life is essential to prevent chronic stress and maintain health and well-being through early and personalized intervention. With the goal of enabling reliable prediction suitable for everyday life, we present MuStP, a two-stage machine learning pipeline designed to predict stress from low-resolution heart rate (HR) and high-resolution electrocardiography (ECG) measurements from commercial smartwatches. Our model is pre-trained with labeled data collected in a controlled laboratory stress study. Subsequently, we transfer the model for everyday use, enabling it to operate with everyday smartwatch data in various environments. The model transfer strategy effectively addresses the domain shift from laboratory data to highly imbalanced smartwatch data and allows personalization. The empirical results on smartwatch data show that MuStP can predict stress everyday with an F1 score of , despite the measurements having sparse labels for stress.","From Laboratory to Everyday Life: Personalized Stress Prediction via Smartwatches This paper introduces MUSTP, a machine learning pipeline that predicts stress using heart rate and ECG data from smartwatches, achieving higher F1 score through model transfer and personalization in everday setting. Accurate prediction of stress in everyday life is essential to prevent chronic stress and maintain health and well-being through early and personalized intervention. With the goal of enabling reliable prediction suitable for everyday life, we present MuStP, a two-stage machine learning pipeline designed to predict stress from low-resolution heart rate (HR) and high-resolution electrocardiography (ECG) measurements from commercial smartwatches. Our model is pre-trained with labeled data collected in a controlled laboratory stress study. Subsequently, we transfer the model for everyday use, enabling it to operate with everyday smartwatch data in various environments. The model transfer strategy effectively addresses the domain shift from laboratory data to highly imbalanced smartwatch data and allows personalization. The empirical results on smartwatch data show that MuStP can predict stress everyday with an F1 score of , despite the measurements having sparse labels for stress. future work, we plan to extend our current scheme using online reinforcement learning-based approaches to learn user-specific adaptive decision thresholds aiming for increased personalization without requiring further training or offline optimization. Lastly, we want to improve the reliability of our EMA-based soft labels using both Affect Grid and questionnaires and decide the contribution of each component by investigating the correlation between cortisol levels and soft labels. 5. Acknowledgments The authors acknowledge the support by the German Fed- eral Ministry of Education and Research through the Cello project under the project number: 16SV8590. We thank Dr.-Ing. Mario Aehnelt and Nicola Marlene Dr ueke from Visual Assistance Technologies, Fraunhofer- Institut f ur Graphische Datenverarbeitung IGD Rostock, for their assistance in developing the models and processing EMAs, Dimitri Kraft for his initial model developments, and Paul Burggraf and Hannes Schenk from Thryve, mHealth Pioneers GmbH, Berlin, for application development, data processing and storage. We also appreciate the contributions of Prof. Dr. Birgit Derntl, Sabrina Eutebach, and Juliane K ostlin from the Department of Psychiatry and Psychother- apy, University of T ubingen, for data collection in the Cello field study. Methodology 2.1. L ABDATA- Dataset Description The LABDATA dataset contains psychological data from 108 participants who underwent stress exposure via the TSST procedure. Due to recording issues, we eliminated 9 users; therefore, our dataset has effectively 99 users. The dataset includes ECG signals, salivary cortisol levels, and self-report measures of stress. ECG measurements are col- lected during the baseline and stress (TSST) conditions will be used as hard labels of stress for training our model. More details about the dataset can be found in Table 1, the source of HR modality described in Subsection 2.5. Each record of participants in the LABDATAis appropriately anonymized. The dataset is planned to be released in the future.2.2. E VERYDAY DATA- Dataset Description TheEVERYDAY DATAdataset consists of 131 users whose data is collected for approximately two months via WITH- INGS SCANWATCH . Users were advised to wear their smart- watches throughout the day and record an ECG along with completing an EMA three times per day during the desig- nated windows of 7-10 AM, 12-3 PM, and 7-10 PM. Users are supposed to rest their arms on a table and hold the top electrode with their thumb and index finger for 30 sec- onds during ECG measurement. The measurements that are logged shortly after a workout or activity are discarded. For (soft) stress labels , we are using subjective stress from Af- fect Grid metrics from collected EMAs (see Appendix A.2 for more details), which are 19,289 in total. More details about the dataset can be found in Table 1. Similarly, each record of participants in EVERYDAY DATAis appropriately anonymized. The dataset is planned to be released in the future. 2.3. Preprocessing In the following analysis, HR measurements are taken over 30-minute windows at a frequency of 1/600Hz, resulting in four measurements per window. ECG measurements, on the other hand, are taken in non-overlapping 30-second intervals, with the frequency specific to each datase",1
eda336669cdc68ca4ecaa9542326e504191a71b6,Improving Performance Prediction of Electrolyte Formulations with Transformer-based Molecular Representation Model,"['Indra Priyadarsini', 'Vidushi Sharma', 'Seiji Takeda', 'Akihiro Kishimoto', 'Lisa Hamada', 'Hajime Shinohara']",https://openreview.net/pdf/eda336669cdc68ca4ecaa9542326e504191a71b6.pdf,"Improving Performance Prediction of Electrolyte Formulations with Transformer-based Molecular Representation Model Development of efficient and high-performing electrolytes is crucial for advancing energy storage technologies, particularly in batteries. Predicting the performance of battery electrolytes rely on complex interactions between the individual constituents. Consequently, a strategy that adeptly captures these relationships and forms a robust representation of the formulation is essential for integrating with machine learning models to predict properties accurately. In this paper, we introduce a novel approach leveraging a transformer-based molecular representation model to effectively and efficiently capture the representation of electrolyte formulations. The performance of the proposed approach is evaluated on two battery property prediction tasks and the results show superior performance compared to the state-of-the-art methods.",eda336669cdc68ca4ecaa9542326e504191a71b6.pdf,"methodology and encoding
rules. Journal of chemical information and computer
sciences , 28(1):31–36, 1988.
Y¨uksel, A., Ulusoy, E., ¨Unl¨u, A., and Do ˘gan, T. Selformer:
molecular representation learning via selfies language
models. Machine Learning: Science and Technology , 4
(2):025035, 2023.","Conclusion
This paper introduced a novel approach leveraging a
transformer-based model to create a comprehensive feature
vector for electrolyte formulations. This was achieved by
obtaining molecular representations of individual electrolyte
components from the pretrained BART model, scaling these
Improving Performance Prediction of Electrolyte Formulations with Transformer-based Molecular Representation Model
representations based on their respective concentrations in
the formulation, and summing the scaled vectors to form a
unified feature vector. This process ensured that the final
feature vector effectively captured the compositional infor-
mation of the formulation while maintaining a consistent
dimensionality. The final feature vector was used in the eval-
uation of two battery property prediction tasks. The results
showed superior performance compared to state-of-the-art
methods. The superior performance can be speculated as a
result of a better molecular representation obtained from the
BART model pretrained with SELFIES. Further scaling the
molecular representation with the concentration and adding
aids to a better representation of the mixture as a whole.
Overall, our proposed approach showed significant promise
in advancing the field of electrolyte formulation by provid-
ing a robust, scalable, and efficient method for predicting
properties.
References
Akiba, T., Sano, S., Yanase, T., Ohta, T., and Koyama, M.
Optuna: A next-generation hyperparameter optimization
framework. In Proceedings of the 25th ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining , 2019.
Chen, T. and Guestrin, C. XGBoost: A scalable tree boost-
ing system. In Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining , KDD ’16, pp. 785–794, New York, NY ,
USA, 2016. ACM. ISBN 978-1-4503-4232-2. doi:
10.1145/2939672.2939785. URL http://doi.acm.
org/10.1145/2939672.2939785 .
Chithrananda, S., Grand, G., and Ramsundar, B. Chem","methodology and encoding rules. Journal of chemical information and computer sciences , 28(1):31 36, 1988. Y uksel, A., Ulusoy, E., Unl u, A., and Do gan, T. Selformer: molecular representation learning via selfies language models. Machine Learning: Science and Technology , 4 (2):025035, 2023.","Conclusion This paper introduced a novel approach leveraging a transformer-based model to create a comprehensive feature vector for electrolyte formulations. This was achieved by obtaining molecular representations of individual electrolyte components from the pretrained BART model, scaling these Improving Performance Prediction of Electrolyte Formulations with Transformer-based Molecular Representation Model representations based on their respective concentrations in the formulation, and summing the scaled vectors to form a unified feature vector. This process ensured that the final feature vector effectively captured the compositional infor- mation of the formulation while maintaining a consistent dimensionality. The final feature vector was used in the eval- uation of two battery property prediction tasks. The results showed superior performance compared to state-of-the-art methods. The superior performance can be speculated as a result of a better molecular representation obtained from the BART model pretrained with SELFIES. Further scaling the molecular representation with the concentration and adding aids to a better representation of the mixture as a whole. Overall, our proposed approach showed significant promise in advancing the field of electrolyte formulation by provid- ing a robust, scalable, and efficient method for predicting properties.","Improving Performance Prediction of Electrolyte Formulations with Transformer-based Molecular Representation Model Development of efficient and high-performing electrolytes is crucial for advancing energy storage technologies, particularly in batteries. Predicting the performance of battery electrolytes rely on complex interactions between the individual constituents. Consequently, a strategy that adeptly captures these relationships and forms a robust representation of the formulation is essential for integrating with machine learning models to predict properties accurately. In this paper, we introduce a novel approach leveraging a transformer-based molecular representation model to effectively and efficiently capture the representation of electrolyte formulations. The performance of the proposed approach is evaluated on two battery property prediction tasks and the results show superior performance compared to the state-of-the-art methods.","Improving Performance Prediction of Electrolyte Formulations with Transformer-based Molecular Representation Model Development of efficient and high-performing electrolytes is crucial for advancing energy storage technologies, particularly in batteries. Predicting the performance of battery electrolytes rely on complex interactions between the individual constituents. Consequently, a strategy that adeptly captures these relationships and forms a robust representation of the formulation is essential for integrating with machine learning models to predict properties accurately. In this paper, we introduce a novel approach leveraging a transformer-based molecular representation model to effectively and efficiently capture the representation of electrolyte formulations. The performance of the proposed approach is evaluated on two battery property prediction tasks and the results show superior performance compared to the state-of-the-art methods. Conclusion This paper introduced a novel approach leveraging a transformer-based model to create a comprehensive feature vector for electrolyte formulations. This was achieved by obtaining molecular representations of individual electrolyte components from the pretrained BART model, scaling these Improving Performance Prediction of Electrolyte Formulations with Transformer-based Molecular Representation Model representations based on their respective concentrations in the formulation, and summing the scaled vectors to form a unified feature vector. This process ensured that the final feature vector effectively captured the compositional infor- mation of the formulation while maintaining a consistent dimensionality. The final feature vector was used in the eval- uation of two battery property prediction tasks. The results showed superior performance compared to state-of-the-art methods. The superior performance can be speculated as a result of a better molecular representation obtained from the BART model pretrained with SELFIES. Further scaling the molecular representation with the concentration and adding aids to a better representation of the mixture as a whole. Overall, our proposed approach showed significant promise in advancing the field of electrolyte formulation by provid- ing a robust, scalable, and efficient method for predicting properties. methodology and encoding rules. Journal of chemical information and computer sciences , 28(1):31 36, 1988. Y uksel, A., Ulusoy, E., Unl u, A., and Do gan, T. Selformer: molecular representation learning via selfies language models. Machine Learning: Science and Technology , 4 (2):025035, 2023.",0
93ddd5c97e40a5766f88dcdd97495389a518990c,A Bayesian Approach to Adversarially Robust Life Testing,"['Dorina Weichert', 'Alexander Kister', 'Sebastian Houben', 'Gunar Ernis', 'Tim Wirtz']",https://openreview.net/pdf/93ddd5c97e40a5766f88dcdd97495389a518990c.pdf,"A Bayesian Approach to Adversarially Robust Life Testing In materials science and engineering, the lifetime of materials and products is tested by costly manual characterization procedures that are standardized only in certain cases. In this paper, we investigate a modular Bayesian approach to lifetime testing that can reduce the number of experiments and, thus, the overall cost of experiments. The approach is based on the correct definition of the probability of the outcome of an experiment, e.g., its likelihood. Since this is usually unknown, we extend it to the adversarial setting, finding an experimental procedure that is robust to a given set of probabilities in the worst case. By simulations, we empirically show the advantages of this procedure over the state-of-the-art and the basic approach, potentially reducing the number of costly experiments.",93ddd5c97e40a5766f88dcdd97495389a518990c.pdf,"Approach to Adversarially Robust Life Testing
Dorina Weichert1Sebastian Houben2Alexander Kister3Gunar Ernis1Tim Wirtz1
Abstract
In materials science and engineering, the lifetime
of materials and products is tested by costly man-
ual characterization procedures that are standard-
ized only in certain cases. In this paper, we inves-
tigate a modular Bayesian approach to lifetime
testing that can reduce the number of experiments
and, thus, the overall cost of experiments. The
approach is based on the correct definition of the
probability of the outcome of an experiment, e.g.,
its likelihood. Since this is usually unknown, we
extend it to the adversarial setting, finding an ex-
perimental procedure that is robust to a given set
of probabilities in the worst case. By simulations,
we empirically show the advantages of this pro-
cedure over the state-of-the-art and the basic ap-
proach, potentially reducing the number of costly
experiments.
1. Introduction
Life testing describes the planning, execution, and analysis
of product tests to estimate a product’s expected lifetime,
e.g., in engineering or material science.
To estimate a product’s lifetime, first, the most critical fac-
tors stressing the product are identified. In practice, these
typically refer to alternating stresses, e.g., alternating me-
chanical loads, alternating temperatures, or alternating elec-
trical loads. Then, so-called accelerated tests are taken out,
where the identified stresses are cyclically applied to a sam-
ple in short intervals to determine the maximum stress a
sample can withstand. We generate a test statistic to find
this maximum stress: we apply different stress levels to dif-
ferent samples and record if they break or not - a so-called
accelerated binary test (Escobar & Meeker, 2006, p. 4).
1Fraunhofer Institute for Intelligent Analysis and Informa-
tion Systems IAIS, Sankt Augustin, Germany2University of Ap-
plied Sciences Bonn-Rhein-Sieg, Sankt Augustin, Germany3VP.1
eScience, Federal","future work, we would like to enhance
our approach by taking into account more potential failure
models and additionally perform model selection, further
lowering the number of required samples to find a good
estimate of a product’s expected lifetime.
References
Agrawal, A., Deshpande, P. D., Cecen, A., Basavarsu, G. P.,
Choudhary, A. N., and Kalidindi, S. R. Exploration of
data science techniques to predict fatigue strength of steel
from composition and processing parameters. Integrating
Materials and Manufacturing Innovation , 3(1):90–108,2014.
ASTM E1823-24a. Standard terminology relating to fatigue
and fracture testing, 2024.
Barnett, V . D. A bayesian sequential life test. Technomet-
rics, 14(2):453–468, 1972. doi: 10.1080/00401706.1972.
10488929.
DIN 50100:2016-12. Load controlled fatigue testing – Ex-
ecution and evaluation of cyclic tests at constant load
amplitudes on metallic specimens and components, 2016.
Escobar, L. A. and Meeker, W. Q. A review of accelerated
test models. Statistical Science , 21(4):552–577, 2006.
Garnett, R. Bayesian optimization . Cambridge University
Press, 2023.
Insua, D. R., Ruggeri, F., Soyer, R., and Wilson, S. Ad-
vances in bayesian decision making in reliability. Eu-
ropean Journal of Operational Research , 282(1):1–18,
2020.
Li, M. and Meeker, W. Q. Application of bayesian methods
in reliability data analyses. Journal of Quality Technology ,
46(1):1–23, 2014.
Limon, S., Yadav, O. P., and Liao, H. A literature review
on planning and analysis of accelerated testing for reli-
ability assessment. Quality and Reliability Engineering
International , 33(8):2361–2383, 2017.
Lindley, D. V . On a measure of the information provided by
an experiment. The Annals of Mathematical Statistics , 27
(4):986–1005, 1956.
Meeker, W. Q., Escobar, L. A., and Pascual, F. G. Statistical
methods for reliability data . John Wiley & Sons, 2022.
Settles, B. Active Learning . Springer Cham, 2012.
Shuto, S. and Amemiya, T. Sequential bayesian inference
for","Approach to Adversarially Robust Life Testing Dorina Weichert1Sebastian Houben2Alexander Kister3Gunar Ernis1Tim Wirtz1 Abstract In materials science and engineering, the lifetime of materials and products is tested by costly man- ual characterization procedures that are standard- ized only in certain cases. In this paper, we inves- tigate a modular Bayesian approach to lifetime testing that can reduce the number of experiments and, thus, the overall cost of experiments. The approach is based on the correct definition of the probability of the outcome of an experiment, e.g., its likelihood. Since this is usually unknown, we extend it to the adversarial setting, finding an ex- perimental procedure that is robust to a given set of probabilities in the worst case. By simulations, we empirically show the advantages of this pro- cedure over the state-of-the-art and the basic ap- proach, potentially reducing the number of costly experiments. 1. Introduction Life testing describes the planning, execution, and analysis of product tests to estimate a product s expected lifetime, e.g., in engineering or material science. To estimate a product s lifetime, first, the most critical fac- tors stressing the product are identified. In practice, these typically refer to alternating stresses, e.g., alternating me- chanical loads, alternating temperatures, or alternating elec- trical loads. Then, so-called accelerated tests are taken out, where the identified stresses are cyclically applied to a sam- ple in short intervals to determine the maximum stress a sample can withstand. We generate a test statistic to find this maximum stress: we apply different stress levels to dif- ferent samples and record if they break or not - a so-called accelerated binary test (Escobar & Meeker, 2006, p. 4). 1Fraunhofer Institute for Intelligent Analysis and Informa- tion Systems IAIS, Sankt Augustin, Germany2University of Ap- plied Sciences Bonn-Rhein-Sieg, Sankt Augustin, Germany3VP.1 eScience, Federal","future work, we would like to enhance our approach by taking into account more potential failure models and additionally perform model selection, further lowering the number of required samples to find a good estimate of a product s expected lifetime.","A Bayesian Approach to Adversarially Robust Life Testing In materials science and engineering, the lifetime of materials and products is tested by costly manual characterization procedures that are standardized only in certain cases. In this paper, we investigate a modular Bayesian approach to lifetime testing that can reduce the number of experiments and, thus, the overall cost of experiments. The approach is based on the correct definition of the probability of the outcome of an experiment, e.g., its likelihood. Since this is usually unknown, we extend it to the adversarial setting, finding an experimental procedure that is robust to a given set of probabilities in the worst case. By simulations, we empirically show the advantages of this procedure over the state-of-the-art and the basic approach, potentially reducing the number of costly experiments.","A Bayesian Approach to Adversarially Robust Life Testing In materials science and engineering, the lifetime of materials and products is tested by costly manual characterization procedures that are standardized only in certain cases. In this paper, we investigate a modular Bayesian approach to lifetime testing that can reduce the number of experiments and, thus, the overall cost of experiments. The approach is based on the correct definition of the probability of the outcome of an experiment, e.g., its likelihood. Since this is usually unknown, we extend it to the adversarial setting, finding an experimental procedure that is robust to a given set of probabilities in the worst case. By simulations, we empirically show the advantages of this procedure over the state-of-the-art and the basic approach, potentially reducing the number of costly experiments. future work, we would like to enhance our approach by taking into account more potential failure models and additionally perform model selection, further lowering the number of required samples to find a good estimate of a product s expected lifetime. Approach to Adversarially Robust Life Testing Dorina Weichert1Sebastian Houben2Alexander Kister3Gunar Ernis1Tim Wirtz1 Abstract In materials science and engineering, the lifetime of materials and products is tested by costly man- ual characterization procedures that are standard- ized only in certain cases. In this paper, we inves- tigate a modular Bayesian approach to lifetime testing that can reduce the number of experiments and, thus, the overall cost of experiments. The approach is based on the correct definition of the probability of the outcome of an experiment, e.g., its likelihood. Since this is usually unknown, we extend it to the adversarial setting, finding an ex- perimental procedure that is robust to a given set of probabilities in the worst case. By simulations, we empirically show the advantages of this pro- cedure over the state-of-the-art and the basic ap- proach, potentially reducing the number of costly experiments. 1. Introduction Life testing describes the planning, execution, and analysis of product tests to estimate a product s expected lifetime, e.g., in engineering or material science. To estimate a product s lifetime, first, the most critical fac- tors stressing the product are identified. In practice, these typically refer to alternating stresses, e.g., alternating me- chanical loads, alternating temperatures, or alternating elec- trical loads. Then, so-called accelerated tests are taken out, where the identified stresses are cyclically applied to a sam- ple in short intervals to determine the maximum stress a sample can withstand. We generate a test statistic to find this maximum stress: we apply different stress levels to dif- ferent samples and record if they break or not - a so-called accelerated binary test (Escobar & Meeker, 2006, p. 4). 1Fraunhofer Institute for Intelligent Analysis and Informa- tion Systems IAIS, Sankt Augustin, Germany2University of Ap- plied Sciences Bonn-Rhein-Sieg, Sankt Augustin, Germany3VP.1 eScience, Federal",0
8b542cadc616220ef6b49aa0fda49f6663720811,CodonMPNN for Organism Specific and Codon Optimal Inverse Folding,"['Hannes Stark', 'Umesh Padia', 'Julia Balla', 'Cameron Diao']",https://openreview.net/pdf/8b542cadc616220ef6b49aa0fda49f6663720811.pdf,"CodonMPNN for Organism Specific and Codon Optimal Inverse Folding Predict Codons instead of amino acids with ProteinMPNN Generating protein sequences conditioned on protein structures is an impactful technique for protein engineering. When synthesizing engineered proteins, they are commonly translated into DNA and expressed in an organism such as yeast. One difficulty in this process is that the expression rates can be low due to suboptimal codon sequences for expressing a protein in a host organism. We propose CodonMPNN, which generates a codon sequence conditioned on a protein backbone structure and an organism label. If naturally occurring DNA sequences are close to codon optimality, CodonMPNN could learn to generate codon sequences with higher expression yields than heuristic codon choices for generated amino acid sequences. Experiments show that CodonMPNN retains the performance of previous inverse folding approaches and recovers wild-type codons more frequently than baselines. Furthermore, CodonMPNN has a higher likelihood of generating high-fitness codon sequences than low-fitness codon sequences for the same protein sequence.",8b542cadc616220ef6b49aa0fda49f6663720811.pdf,"approaches and recovers wild-
type codons more frequently than baselines. Fur-
thermore, CodonMPNN has a higher likelihood
of generating high-fitness codon sequences than
low-fitness codon sequences for the same pro-
tein sequence. Code is available at https://
github.com/HannesStark/CodonMPNN .
1. Introduction
A significant barrier for protein engineering and protein
production is the expression of engineered RNA in host
systems such as yeast (Presnyak et al., 2015). One aspect
leading toward low expression yields is the suboptimality
of the DNA/codon sequence used to express the protein: as
illustrated in Figure 1, multiple codon sequences can encode
the same protein sequence, but each codon sequence can
interact differently with its environment in a cell. This can
lead to different behaviors between codon sequences that
encode the same protein, such as different expression levels.
*Equal contribution1CSAIL, Massachuetts Institute of Tech-
nology, Cambridge, MA, USA. Correspondence to: Hannes Stark
<hstark@mit.edu >.
Accepted at the 1st Machine Learning for Life and Material Sci-
ences Workshop at ICML 2024 . Copyright 2024 by the author(s).
Figure 1. Amino acid sequences have corresponding DNA se-
quences with triplets of nucleotides (A, C, G, U) corresponding to
amino acids. Since there are 64 possible triplets called codons and
only 20 amino acids, there are multiple codon sequences for each
protein sequence. Some have higher expression rates than others,
and some are not expressed at all. This expression level depends
on the host in which the codon sequence is expressed.
This suboptimality of codon sequences is also a bottleneck
in the recently prominent protein engineering approach of
generating protein sequences conditioned on a protein back-
bone structure with tools such as ProteinMPNN (Dauparas
et al., 2022) to improve stability, function, or to use in
denovo protein design (Watson et al., 2023). When validat-
ing the proteins obtained from such tools by","Future Work
To aid structure-based protein engineering via inverse fold-
ing methods, we developed CodonMPNN as a drop-in re-
placement for ProteinMPNN. By directly generating codon
sequences instead of amino acid sequences, CodonMPNN
directly generates codon sequences that are closer to codon
optimality than naively choosing the most frequent codon
per amino acid. Further, the user can condition on the host
system in which they aim to express the generated codon
sequence - information that often is available to experimen-
talists but has not been used in previous inverse folding
approaches. We experimentally verified that CodonMPNN
retains ProteinMPNN’s performance and assigns higher like-
lihoods to more highly expressed codon sequences than
lower expression codon sequences that encode the same
amino acid sequence.
Future Work. In many codon optimization tasks, no pro-
tein structure is available, and the goal is to obtain an amino
acid sequence’s most highly expressed codon sequence for
a given host system. While CodonMPNN can be a useful
drop-in replacement for inverse folding models such as Pro-
4
CodonMPNN for Organism Specific and Codon Optimal Inverse Folding
teinMPNN, it fails to address this related task. Thus, we
aim to fine-tune a protein language model to generate codon
sequences conditioned on amino acid sequences and our
taxon label.
6. Acknowledgements
We thank Sergey Ovchinnikov for his advice and Yehlin Cho,
Peter Mikhael, Felix Faltings, Bowen Jing, Tommi Jaakkola,
Bonnie Berger, and Eric Alm for insightful discussion.
References
Ahdritz, G., Bouatta, N., Floristean, C., Kadyan, S., Xia,
Q., Gerecke, W., O’Donnell, T. J., Berenberg, D., Fisk,
I., Zanichelli, N., Zhang, B., Nowaczynski, A., Wang,
B., Stepniewska-Dziubinska, M. M., Zhang, S., Ojewole,
A., Guney, M. E., Biderman, S., Watkins, A. M., Ra, S.,
Lorenzo, P. R., Nivon, L., Weitzner, B., Ban, Y .-E. A.,
Sorger, P. K., Mostaque, E., Zhang, Z., Bonneau, R., and
AlQuraishi, M. OpenFold: Retrai","approaches and recovers wild- type codons more frequently than baselines. Fur- thermore, CodonMPNN has a higher likelihood of generating high-fitness codon sequences than low-fitness codon sequences for the same pro- tein sequence. Code is available at https:// github.com/HannesStark/CodonMPNN . 1. Introduction A significant barrier for protein engineering and protein production is the expression of engineered RNA in host systems such as yeast (Presnyak et al., 2015). One aspect leading toward low expression yields is the suboptimality of the DNA/codon sequence used to express the protein: as illustrated in Figure 1, multiple codon sequences can encode the same protein sequence, but each codon sequence can interact differently with its environment in a cell. This can lead to different behaviors between codon sequences that encode the same protein, such as different expression levels. *Equal contribution1CSAIL, Massachuetts Institute of Tech- nology, Cambridge, MA, USA. Correspondence to: Hannes Stark <hstark@mit.edu >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s). Figure 1. Amino acid sequences have corresponding DNA se- quences with triplets of nucleotides (A, C, G, U) corresponding to amino acids. Since there are 64 possible triplets called codons and only 20 amino acids, there are multiple codon sequences for each protein sequence. Some have higher expression rates than others, and some are not expressed at all. This expression level depends on the host in which the codon sequence is expressed. This suboptimality of codon sequences is also a bottleneck in the recently prominent protein engineering approach of generating protein sequences conditioned on a protein back- bone structure with tools such as ProteinMPNN (Dauparas et al., 2022) to improve stability, function, or to use in denovo protein design (Watson et al., 2023). When validat- ing the proteins obtained from such tools by","Future Work To aid structure-based protein engineering via inverse fold- ing methods, we developed CodonMPNN as a drop-in re- placement for ProteinMPNN. By directly generating codon sequences instead of amino acid sequences, CodonMPNN directly generates codon sequences that are closer to codon optimality than naively choosing the most frequent codon per amino acid. Further, the user can condition on the host system in which they aim to express the generated codon sequence - information that often is available to experimen- talists but has not been used in previous inverse folding approaches. We experimentally verified that CodonMPNN retains ProteinMPNN s performance and assigns higher like- lihoods to more highly expressed codon sequences than lower expression codon sequences that encode the same amino acid sequence. Future Work. In many codon optimization tasks, no pro- tein structure is available, and the goal is to obtain an amino acid sequence s most highly expressed codon sequence for a given host system. While CodonMPNN can be a useful drop-in replacement for inverse folding models such as Pro- 4 CodonMPNN for Organism Specific and Codon Optimal Inverse Folding teinMPNN, it fails to address this related task. Thus, we aim to fine-tune a protein language model to generate codon sequences conditioned on amino acid sequences and our taxon label. 6.","CodonMPNN for Organism Specific and Codon Optimal Inverse Folding Predict Codons instead of amino acids with ProteinMPNN Generating protein sequences conditioned on protein structures is an impactful technique for protein engineering. When synthesizing engineered proteins, they are commonly translated into DNA and expressed in an organism such as yeast. One difficulty in this process is that the expression rates can be low due to suboptimal codon sequences for expressing a protein in a host organism. We propose CodonMPNN, which generates a codon sequence conditioned on a protein backbone structure and an organism label. If naturally occurring DNA sequences are close to codon optimality, CodonMPNN could learn to generate codon sequences with higher expression yields than heuristic codon choices for generated amino acid sequences. Experiments show that CodonMPNN retains the performance of previous inverse folding approaches and recovers wild-type codons more frequently than baselines. Furthermore, CodonMPNN has a higher likelihood of generating high-fitness codon sequences than low-fitness codon sequences for the same protein sequence.","CodonMPNN for Organism Specific and Codon Optimal Inverse Folding Predict Codons instead of amino acids with ProteinMPNN Generating protein sequences conditioned on protein structures is an impactful technique for protein engineering. When synthesizing engineered proteins, they are commonly translated into DNA and expressed in an organism such as yeast. One difficulty in this process is that the expression rates can be low due to suboptimal codon sequences for expressing a protein in a host organism. We propose CodonMPNN, which generates a codon sequence conditioned on a protein backbone structure and an organism label. If naturally occurring DNA sequences are close to codon optimality, CodonMPNN could learn to generate codon sequences with higher expression yields than heuristic codon choices for generated amino acid sequences. Experiments show that CodonMPNN retains the performance of previous inverse folding approaches and recovers wild-type codons more frequently than baselines. Furthermore, CodonMPNN has a higher likelihood of generating high-fitness codon sequences than low-fitness codon sequences for the same protein sequence. Future Work To aid structure-based protein engineering via inverse fold- ing methods, we developed CodonMPNN as a drop-in re- placement for ProteinMPNN. By directly generating codon sequences instead of amino acid sequences, CodonMPNN directly generates codon sequences that are closer to codon optimality than naively choosing the most frequent codon per amino acid. Further, the user can condition on the host system in which they aim to express the generated codon sequence - information that often is available to experimen- talists but has not been used in previous inverse folding approaches. We experimentally verified that CodonMPNN retains ProteinMPNN s performance and assigns higher like- lihoods to more highly expressed codon sequences than lower expression codon sequences that encode the same amino acid sequence. Future Work. In many codon optimization tasks, no pro- tein structure is available, and the goal is to obtain an amino acid sequence s most highly expressed codon sequence for a given host system. While CodonMPNN can be a useful drop-in replacement for inverse folding models such as Pro- 4 CodonMPNN for Organism Specific and Codon Optimal Inverse Folding teinMPNN, it fails to address this related task. Thus, we aim to fine-tune a protein language model to generate codon sequences conditioned on amino acid sequences and our taxon label. 6. approaches and recovers wild- type codons more frequently than baselines. Fur- thermore, CodonMPNN has a higher likelihood of generating high-fitness codon sequences than low-fitness codon sequences for the same pro- tein sequence. Code is available at https:// github.com/HannesStark/CodonMPNN . 1. Introduction A significant barrier for protein engineering and protein production is the expression of engineered RNA in host systems such as yeast (Presnyak et al., 2015). One aspect leading toward low expression yields is the suboptimality of the DNA/codon sequence used to express the protein: as illustrated in Figure 1, multiple codon sequences can encode the same protein sequence, but each codon sequence can interact differently with its environment in a cell. This can lead to different behaviors between codon sequences that encode the same protein, such as different expression levels. *Equal contribution1CSAIL, Massachuetts Institute of Tech- nology, Cambridge, MA, USA. Correspondence to: Hannes Stark <hstark@mit.edu >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s). Figure 1. Amino acid sequences have corresponding DNA se- quences with triplets of nucleotides (A, C, G, U) corresponding to amino acids. Since there are 64 possible triplets called codons and only 20 amino acids, there are multiple codon sequences for each protein sequence. Some have higher expression rates than others, and some are not expressed at all. This expression level depends on the host in which the codon sequence is expressed. This suboptimality of codon sequences is also a bottleneck in the recently prominent protein engineering approach of generating protein sequences conditioned on a protein back- bone structure with tools such as ProteinMPNN (Dauparas et al., 2022) to improve stability, function, or to use in denovo protein design (Watson et al., 2023). When validat- ing the proteins obtained from such tools by",0
c02342d63d04171f07a8ea50b1321b2b52f8be4e,Limitations of scRNA-seq Zero-Imputation Methods for Network Inference,"['Ankit Bhardwaj', 'Joshua Weiner', 'Preetha Balasubramanian', 'Lakshmi Subramanian']",https://openreview.net/pdf/c02342d63d04171f07a8ea50b1321b2b52f8be4e.pdf,"Limitations of scRNA-seq Zero-Imputation Methods for Network Inference Zero-imputation methods are widely applied to address non-biological zeros in scRNA-seq data. However, these methods can introduce artificial signals, skewing the results of downstream analysis to match initial assumptions rather than emulate the underlying biological processes. This paper makes a simple but surprising observation: we demonstrate that several popular zero imputation techniques provide significantly varied results on the downstream network inference tasks over the same real-world scRNA datasets. Benchmarking their performance on synthetically controlled simulated scRNA datasets using the SERGIO simulator and the GENIE3 network inference algorithm, we observed poor metrics across the board. A key takeaway from our analysis is both unearthing the unreliability of existing imputation techniques and the inability to define a uniform gold-standard for zero imputation.",c02342d63d04171f07a8ea50b1321b2b52f8be4e.pdf,"methodology. The details of the noise iteration process,
and comparison between datasets can be found in the SER-
GIO paper.
One important aspect that needs to be highlighted is the
sparsity of the edges in the gene regulatory networks used
in the datasets. The prevalence of edges in the graph is less
than 0.2%, which makes the network inference essentially a
needle-in-haystack problem.
Table 1. Properties of different datasets used for our experiments.
Dataset # Cells # Genes # Cell Types # Regulators # Edges Species
DS1 2700 100 9 10 258 E. coli
DS2 2700 400 9 37 1155 S. cerevisiae
DS3 2700 1200 9 127 2713 E. coli
4. Methodology
4.1. Experiment Pipeline
The different datasets produced using the SERGIO simula-
tor (DS1, DS2, DS3) are ultimately used as the input for our
network inference method to infer a predicted gene regula-
tory network. The predicted GRNs and the original input
GRNs for the datasets are used to compute the AUC-ROC
metric, which is the final performance metric for the task.
For the clean versions of the datasets, no changes are made
and they are directly fed in as input to the network inference
algorithm. The resulting AUC-ROC is considered as the
ideal performance for different Zero Imputation methods.
For the noisy versions of the datasets, we repeat the same
experiment to obtain the lower bound on the performance.
Then, for different zero-imputation methods, we apply them
to the noisy versions of the three datasets, and the outputs
are fed as input to the network inference algorithm, and
the resulting AUC-ROC values are used as the metric of
performance. The pipeline is better understood from Figure
3.
Figure 3. Experiment pipeline flowchart.
4.2. Network Inference Algorithm
Similar to the SERGIO paper, we chose to use the GENIE3
algorithm (Huynh-Thu et al., 2010) for network inference.
This choice was made to ensure that the data reproduction
and network inference pipeline were in-line with the results
obtained by Dibaeinia et. al. GENIE3 c","conclusions. Moreover, imputation can obscure the natural
sparsity of single-cell data, which is a characteristic feature
reflecting the stochastic nature of gene expression. Masking
this sparsity with imputed values can reduce the data’s abil-
ity to reveal true biological differences between cell types
or states.
We acknowledge that our work may have several limitations
in our methodology given the complexity of the underly-
ing biological processes, data generation methodologies
and the nuances in the individual algorithmic pipelines.
Our benchmark uses the SERGIO simulator, which has
a particular set of assumptions and its own limitations.
We also note the scarcity of gene-regulation based simu-
lation frameworks that are used for benchmarking scRNA-
seq imputation methods as a potential research direction
worthy of further thought. We also believe that further
work is required on defining mechanisms to standardize
pre-processing pipelines including the right way to perform
zero imputation for scRNA-seq data.
7. Reproducibility
The code for our experiments is available at https://
github.com/ankitbha/dfdl_imputation/ .
References
Hu0379 blood gse172495. http://husch.
comp-genomics.org/#/detail/HU_0379_
Blood_GSE172495 .
Hu0325 esophagus gse159929. http://husch.
comp-genomics.org/#/detail/HU_0325_
Esophagus_GSE159929 .
Hu0231 pancreas gse134355. http://husch.
comp-genomics.org/#/detail/HU_0231_
Pancreas_GSE134355 .
Hu0243 prostate gse134355. http://husch.
comp-genomics.org/#/detail/HU_0243_
Prostate_GSE134355 .
Hu0269 uterus gse134355. http://husch.
comp-genomics.org/#/detail/HU_0269_
Uterus_GSE134355 .
Amodio, M., van Dijk, D., Srinivasan, K., Chen, W. S.,
Mohsen, H., Moon, K. R., Campbell, A., Zhao, Y ., Wang,
X., Venkataswamy, M., Desai, A., Ravi, V ., Kumar, P.,
Montgomery, R., Wolf, G., and Krishnaswamy, S. Ex-
ploring single-cell data with deep multitasking neural
networks. Nat. Methods , 16(11):1139–1145, November
2019.
Arisdakessian, C., Poirion, O., Yuni","methodology. The details of the noise iteration process, and comparison between datasets can be found in the SER- GIO paper. One important aspect that needs to be highlighted is the sparsity of the edges in the gene regulatory networks used in the datasets. The prevalence of edges in the graph is less than 0.2%, which makes the network inference essentially a needle-in-haystack problem. Table 1. Properties of different datasets used for our experiments. Dataset # Cells # Genes # Cell Types # Regulators # Edges Species DS1 2700 100 9 10 258 E. coli DS2 2700 400 9 37 1155 S. cerevisiae DS3 2700 1200 9 127 2713 E. coli 4. Methodology 4.1. Experiment Pipeline The different datasets produced using the SERGIO simula- tor (DS1, DS2, DS3) are ultimately used as the input for our network inference method to infer a predicted gene regula- tory network. The predicted GRNs and the original input GRNs for the datasets are used to compute the AUC-ROC metric, which is the final performance metric for the task. For the clean versions of the datasets, no changes are made and they are directly fed in as input to the network inference algorithm. The resulting AUC-ROC is considered as the ideal performance for different Zero Imputation methods. For the noisy versions of the datasets, we repeat the same experiment to obtain the lower bound on the performance. Then, for different zero-imputation methods, we apply them to the noisy versions of the three datasets, and the outputs are fed as input to the network inference algorithm, and the resulting AUC-ROC values are used as the metric of performance. The pipeline is better understood from Figure 3. Figure 3. Experiment pipeline flowchart. 4.2. Network Inference Algorithm Similar to the SERGIO paper, we chose to use the GENIE3 algorithm (Huynh-Thu et al., 2010) for network inference. This choice was made to ensure that the data reproduction and network inference pipeline were in-line with the results obtained by Dibaeinia et. al. GENIE3 c","conclusions. Moreover, imputation can obscure the natural sparsity of single-cell data, which is a characteristic feature reflecting the stochastic nature of gene expression. Masking this sparsity with imputed values can reduce the data s abil- ity to reveal true biological differences between cell types or states. We acknowledge that our work may have several limitations in our methodology given the complexity of the underly- ing biological processes, data generation methodologies and the nuances in the individual algorithmic pipelines. Our benchmark uses the SERGIO simulator, which has a particular set of assumptions and its own limitations. We also note the scarcity of gene-regulation based simu- lation frameworks that are used for benchmarking scRNA- seq imputation methods as a potential research direction worthy of further thought. We also believe that further work is required on defining mechanisms to standardize pre-processing pipelines including the right way to perform zero imputation for scRNA-seq data. 7. Reproducibility The code for our experiments is available at https:// github.com/ankitbha/dfdl_imputation/ .","Limitations of scRNA-seq Zero-Imputation Methods for Network Inference Zero-imputation methods are widely applied to address non-biological zeros in scRNA-seq data. However, these methods can introduce artificial signals, skewing the results of downstream analysis to match initial assumptions rather than emulate the underlying biological processes. This paper makes a simple but surprising observation: we demonstrate that several popular zero imputation techniques provide significantly varied results on the downstream network inference tasks over the same real-world scRNA datasets. Benchmarking their performance on synthetically controlled simulated scRNA datasets using the SERGIO simulator and the GENIE3 network inference algorithm, we observed poor metrics across the board. A key takeaway from our analysis is both unearthing the unreliability of existing imputation techniques and the inability to define a uniform gold-standard for zero imputation.","Limitations of scRNA-seq Zero-Imputation Methods for Network Inference Zero-imputation methods are widely applied to address non-biological zeros in scRNA-seq data. However, these methods can introduce artificial signals, skewing the results of downstream analysis to match initial assumptions rather than emulate the underlying biological processes. This paper makes a simple but surprising observation: we demonstrate that several popular zero imputation techniques provide significantly varied results on the downstream network inference tasks over the same real-world scRNA datasets. Benchmarking their performance on synthetically controlled simulated scRNA datasets using the SERGIO simulator and the GENIE3 network inference algorithm, we observed poor metrics across the board. A key takeaway from our analysis is both unearthing the unreliability of existing imputation techniques and the inability to define a uniform gold-standard for zero imputation. conclusions. Moreover, imputation can obscure the natural sparsity of single-cell data, which is a characteristic feature reflecting the stochastic nature of gene expression. Masking this sparsity with imputed values can reduce the data s abil- ity to reveal true biological differences between cell types or states. We acknowledge that our work may have several limitations in our methodology given the complexity of the underly- ing biological processes, data generation methodologies and the nuances in the individual algorithmic pipelines. Our benchmark uses the SERGIO simulator, which has a particular set of assumptions and its own limitations. We also note the scarcity of gene-regulation based simu- lation frameworks that are used for benchmarking scRNA- seq imputation methods as a potential research direction worthy of further thought. We also believe that further work is required on defining mechanisms to standardize pre-processing pipelines including the right way to perform zero imputation for scRNA-seq data. 7. Reproducibility The code for our experiments is available at https:// github.com/ankitbha/dfdl_imputation/ . methodology. The details of the noise iteration process, and comparison between datasets can be found in the SER- GIO paper. One important aspect that needs to be highlighted is the sparsity of the edges in the gene regulatory networks used in the datasets. The prevalence of edges in the graph is less than 0.2%, which makes the network inference essentially a needle-in-haystack problem. Table 1. Properties of different datasets used for our experiments. Dataset # Cells # Genes # Cell Types # Regulators # Edges Species DS1 2700 100 9 10 258 E. coli DS2 2700 400 9 37 1155 S. cerevisiae DS3 2700 1200 9 127 2713 E. coli 4. Methodology 4.1. Experiment Pipeline The different datasets produced using the SERGIO simula- tor (DS1, DS2, DS3) are ultimately used as the input for our network inference method to infer a predicted gene regula- tory network. The predicted GRNs and the original input GRNs for the datasets are used to compute the AUC-ROC metric, which is the final performance metric for the task. For the clean versions of the datasets, no changes are made and they are directly fed in as input to the network inference algorithm. The resulting AUC-ROC is considered as the ideal performance for different Zero Imputation methods. For the noisy versions of the datasets, we repeat the same experiment to obtain the lower bound on the performance. Then, for different zero-imputation methods, we apply them to the noisy versions of the three datasets, and the outputs are fed as input to the network inference algorithm, and the resulting AUC-ROC values are used as the metric of performance. The pipeline is better understood from Figure 3. Figure 3. Experiment pipeline flowchart. 4.2. Network Inference Algorithm Similar to the SERGIO paper, we chose to use the GENIE3 algorithm (Huynh-Thu et al., 2010) for network inference. This choice was made to ensure that the data reproduction and network inference pipeline were in-line with the results obtained by Dibaeinia et. al. GENIE3 c",0
ed545c1c3af1003877fe505eb36ddb7bf58cde7b,Exploring sequence landscape of biosynthetic gene clusters with protein language models,"['Tatiana Malygina', 'Olga Kalinina']",https://openreview.net/pdf/ed545c1c3af1003877fe505eb36ddb7bf58cde7b.pdf,"Exploring sequence landscape of biosynthetic gene clusters with protein language models Many organisms, such as bacteria, fungi, and plants, produce intricate chemicals that are not needed for their growth and reproduction, and thus are called secondary metabolites or natural products (NPs). NPs are a rich source of drugs, with most antibiotics being derivatives of NPs. In a producer organism, NPs are synthesized by a set of enzymes encoded by genes that often lie near each other on the chromosome and are called a biosynthetic gene cluster (BGC). In this work, we explore the capability of protein language models (PLMs) to produce meaningful representations of BGCs. We employ transfer learning to train models to predict the chemical class of the produced compound and explore the topological properties of the produced embeddings. The code is available at project's GitHub repository: https://github.com/kalininalab/NaturalPPLuM.",ed545c1c3af1003877fe505eb36ddb7bf58cde7b.pdf,"approaches is that they are only
able to detect known types of BGCs. ClusterFinder (Cimer-
mancic et al., 2014) was the first attempt to generalize this
idea to unseen combinations of BGC-relevant protein do-
mains, but was not successful in the community.
Recently, machine-learning (ML) and deep-learning (DL)
methods are gaining more attention in natural-product re-
Exploring sequence landscape of biosynthetic gene clusters with protein language models
search (Mullowney et al., 2023). In particular, for the de-
tection of BGCs, a host of methods became available re-
cently: DeepBGC (Hannigan et al., 2019), GECCO (Carroll
et al., 2021), SanntiS (Fragoso et al., 2023). They all em-
ploy different DL techniques – bidirectional long short-term
memory residual neural networks (DeepBGC), conditional
random fields (GECCO), or a combination of bidirectional
long short-term memory and convolutional layers (SanntiS)
– to predict BGCs from genome sequences. In all cases,
they featurize these sequences by predicting genes in them,
and then either working directly with their sequence, using
transformation such as word2vec (DeepBGC), or predicting
domains (Pfam or InterPro) and using these as features.
1.3. Protein language models
Protein language models (PLMs ) have been recently devel-
oped to create embeddings of protein sequences that can
be used in a variety of downstream tasks. Taking inspira-
tion from large language models, PLM is trained to predict
the most probable amino acid given the sequence context.
Specifically, a fraction of amino acids in a protein’s se-
quence are masked, and the model is trained to predict the
identity of masked amino acids. PLMs are typically trained
on very large sets of unlabeled sequences, such as around
250 million protein sequences in UniProt (ESM2 (Rives
et al., 2019; Lin et al., 2022), Ankh (Elnaggar et al., 2023),
etc.)
The major contribution of this study is employing and fine-
tuning PLMs for natural product research and exploring t","Discussion
In this study, we present an exploratory analysis of PLM-
based representation of genetic fragments encoding for the
biosynthesis of natural products.
The experiments show that PLM-based embeddings are ca-
pable of separating different classes of BGCs corresponding
to chemically different classes of compounds. The fine-
tuning of one of the protein foundational models allowed
us to use all available information on protein universe and
to apply it to our specific dataset at the same time. We ob-
served that this combination improved separation between
different biosynthetic classes.
Our results of the experiments with the leave-one-class-out
(LOCO) cross-validation demonstrated that biosynthetic
classes with a well-defined genomic architecture, such as
NRP and polyketide synthases, can be detected by the mod-
els as a cluster in the embedding space, even when they are
not present in training. This means that the model is capable
of finding a strong genetic signal, even if it has not observed
it before. For the example of NRP and polyketide synthases,
despite sharing a homologous domain in their core structure,
each class forms a defined cluster in the embedding space
even when absent from training. Should another example of
a biosynthetic class exist that also has a comparably strong
genomic structure, our model should generate embeddings
that cluster equally well, and indicates that new unknown
BGC classes may potentially be identified with this method.5. Acknowledgements
We are grateful to Amay Ajaykumar Agrawal and Guangyi
Chen for inspiring discussion and a critical reading of the
manuscript.
References
Atanasov, A. G., Zotchev, S. B., Dirsch, V . M., et al. Natural
products in drug discovery: advances and opportunities.
Nature Reviews Drug Discovery , 20:200–216, 2021. doi:
10.1038/s41573-020-00114-z.
Blin, K., Shaw, S., Augustijn, H. E., Reitz, Z. L., Biermann,
F., Alanjary, M., Fetter, A., Terlouw, B. R., Metcalf,
W. W., Helfrich, E. J. N., van Weze","approaches is that they are only able to detect known types of BGCs. ClusterFinder (Cimer- mancic et al., 2014) was the first attempt to generalize this idea to unseen combinations of BGC-relevant protein do- mains, but was not successful in the community. Recently, machine-learning (ML) and deep-learning (DL) methods are gaining more attention in natural-product re- Exploring sequence landscape of biosynthetic gene clusters with protein language models search (Mullowney et al., 2023). In particular, for the de- tection of BGCs, a host of methods became available re- cently: DeepBGC (Hannigan et al., 2019), GECCO (Carroll et al., 2021), SanntiS (Fragoso et al., 2023). They all em- ploy different DL techniques bidirectional long short-term memory residual neural networks (DeepBGC), conditional random fields (GECCO), or a combination of bidirectional long short-term memory and convolutional layers (SanntiS) to predict BGCs from genome sequences. In all cases, they featurize these sequences by predicting genes in them, and then either working directly with their sequence, using transformation such as word2vec (DeepBGC), or predicting domains (Pfam or InterPro) and using these as features. 1.3. Protein language models Protein language models (PLMs ) have been recently devel- oped to create embeddings of protein sequences that can be used in a variety of downstream tasks. Taking inspira- tion from large language models, PLM is trained to predict the most probable amino acid given the sequence context. Specifically, a fraction of amino acids in a protein s se- quence are masked, and the model is trained to predict the identity of masked amino acids. PLMs are typically trained on very large sets of unlabeled sequences, such as around 250 million protein sequences in UniProt (ESM2 (Rives et al., 2019; Lin et al., 2022), Ankh (Elnaggar et al., 2023), etc.) The major contribution of this study is employing and fine- tuning PLMs for natural product research and exploring t","Discussion In this study, we present an exploratory analysis of PLM- based representation of genetic fragments encoding for the biosynthesis of natural products. The experiments show that PLM-based embeddings are ca- pable of separating different classes of BGCs corresponding to chemically different classes of compounds. The fine- tuning of one of the protein foundational models allowed us to use all available information on protein universe and to apply it to our specific dataset at the same time. We ob- served that this combination improved separation between different biosynthetic classes. Our results of the experiments with the leave-one-class-out (LOCO) cross-validation demonstrated that biosynthetic classes with a well-defined genomic architecture, such as NRP and polyketide synthases, can be detected by the mod- els as a cluster in the embedding space, even when they are not present in training. This means that the model is capable of finding a strong genetic signal, even if it has not observed it before. For the example of NRP and polyketide synthases, despite sharing a homologous domain in their core structure, each class forms a defined cluster in the embedding space even when absent from training. Should another example of a biosynthetic class exist that also has a comparably strong genomic structure, our model should generate embeddings that cluster equally well, and indicates that new unknown BGC classes may potentially be identified with this method.5.","Exploring sequence landscape of biosynthetic gene clusters with protein language models Many organisms, such as bacteria, fungi, and plants, produce intricate chemicals that are not needed for their growth and reproduction, and thus are called secondary metabolites or natural products (NPs). NPs are a rich source of drugs, with most antibiotics being derivatives of NPs. In a producer organism, NPs are synthesized by a set of enzymes encoded by genes that often lie near each other on the chromosome and are called a biosynthetic gene cluster (BGC). In this work, we explore the capability of protein language models (PLMs) to produce meaningful representations of BGCs. We employ transfer learning to train models to predict the chemical class of the produced compound and explore the topological properties of the produced embeddings. The code is available at project's GitHub repository: https://github.com/kalininalab/NaturalPPLuM.","Exploring sequence landscape of biosynthetic gene clusters with protein language models Many organisms, such as bacteria, fungi, and plants, produce intricate chemicals that are not needed for their growth and reproduction, and thus are called secondary metabolites or natural products (NPs). NPs are a rich source of drugs, with most antibiotics being derivatives of NPs. In a producer organism, NPs are synthesized by a set of enzymes encoded by genes that often lie near each other on the chromosome and are called a biosynthetic gene cluster (BGC). In this work, we explore the capability of protein language models (PLMs) to produce meaningful representations of BGCs. We employ transfer learning to train models to predict the chemical class of the produced compound and explore the topological properties of the produced embeddings. The code is available at project's GitHub repository: https://github.com/kalininalab/NaturalPPLuM. Discussion In this study, we present an exploratory analysis of PLM- based representation of genetic fragments encoding for the biosynthesis of natural products. The experiments show that PLM-based embeddings are ca- pable of separating different classes of BGCs corresponding to chemically different classes of compounds. The fine- tuning of one of the protein foundational models allowed us to use all available information on protein universe and to apply it to our specific dataset at the same time. We ob- served that this combination improved separation between different biosynthetic classes. Our results of the experiments with the leave-one-class-out (LOCO) cross-validation demonstrated that biosynthetic classes with a well-defined genomic architecture, such as NRP and polyketide synthases, can be detected by the mod- els as a cluster in the embedding space, even when they are not present in training. This means that the model is capable of finding a strong genetic signal, even if it has not observed it before. For the example of NRP and polyketide synthases, despite sharing a homologous domain in their core structure, each class forms a defined cluster in the embedding space even when absent from training. Should another example of a biosynthetic class exist that also has a comparably strong genomic structure, our model should generate embeddings that cluster equally well, and indicates that new unknown BGC classes may potentially be identified with this method.5. approaches is that they are only able to detect known types of BGCs. ClusterFinder (Cimer- mancic et al., 2014) was the first attempt to generalize this idea to unseen combinations of BGC-relevant protein do- mains, but was not successful in the community. Recently, machine-learning (ML) and deep-learning (DL) methods are gaining more attention in natural-product re- Exploring sequence landscape of biosynthetic gene clusters with protein language models search (Mullowney et al., 2023). In particular, for the de- tection of BGCs, a host of methods became available re- cently: DeepBGC (Hannigan et al., 2019), GECCO (Carroll et al., 2021), SanntiS (Fragoso et al., 2023). They all em- ploy different DL techniques bidirectional long short-term memory residual neural networks (DeepBGC), conditional random fields (GECCO), or a combination of bidirectional long short-term memory and convolutional layers (SanntiS) to predict BGCs from genome sequences. In all cases, they featurize these sequences by predicting genes in them, and then either working directly with their sequence, using transformation such as word2vec (DeepBGC), or predicting domains (Pfam or InterPro) and using these as features. 1.3. Protein language models Protein language models (PLMs ) have been recently devel- oped to create embeddings of protein sequences that can be used in a variety of downstream tasks. Taking inspira- tion from large language models, PLM is trained to predict the most probable amino acid given the sequence context. Specifically, a fraction of amino acids in a protein s se- quence are masked, and the model is trained to predict the identity of masked amino acids. PLMs are typically trained on very large sets of unlabeled sequences, such as around 250 million protein sequences in UniProt (ESM2 (Rives et al., 2019; Lin et al., 2022), Ankh (Elnaggar et al., 2023), etc.) The major contribution of this study is employing and fine- tuning PLMs for natural product research and exploring t",0
1caf4827280a8542994f400472d4b40284b243cd,Cell Morphology-Guided Small Molecule Generation with GFlowNets,"['Stephen Zhewen Lu', 'Ziqing Lu', 'Ehsan Hajiramezanali', 'Tommaso Biancalani', 'Yoshua Bengio', 'Gabriele Scalia', 'Michał Koziarski']",https://openreview.net/pdf/1caf4827280a8542994f400472d4b40284b243cd.pdf,"Cell Morphology-Guided Small Molecule Generation with GFlowNets Molecular generation using GFlowNets guided by the latent representation from a cross-modal contrastive model High-content phenotypic screening, including high-content imaging (HCI), has gained popularity in the last few years for its ability to characterize novel therapeutics without prior knowledge of the protein target. This work focuses on the novel task of HCI-guided molecular design. We consider an approach in which we leverage an unsupervised multimodal joint embedding to define a latent similarity as a reward for GFlowNets. The proposed model learns to generate new molecules that could produce phenotypic effects similar to those of the given image target, without relying on pre-annotated phenotypic labels. We demonstrate that our method generates molecules with high morphological and structural similarity to the target, increasing the likelihood of similar biological activity.",1caf4827280a8542994f400472d4b40284b243cd.pdf,"methodology, e.g. variational autoencoders (Jin et al., 2018;
Maziarka et al., 2020), reinforcement learning (RL) (Loef-
fler et al., 2024) or diffusion models (Runcie & Mey, 2023;
Uehara et al., 2024). These methods have found applica-
tions in drug discovery, reporting successes in several areas
such as immunology and infectious diseases (Godinez et al.,
2022; Moret et al., 2023). Recently, Generative Flow Net-
works (GFlowNets) (Bengio et al., 2021; Nica et al., 2022;
Shen et al., 2023; V olokhova et al., 2024; Koziarski et al.,
2024a; Gai ´nski et al., 2024; Koziarski et al., 2024b) emerged
as a promising paradigm for molecular generation due to
the ability to sample diverse candidate molecules, which is
crucial in the drug discovery process. Importantly, similar
to RL, GFlowNets can be trained based on the specified
reward function, which makes them suitable for phenotypic
discovery. Compared to existing methods, which focus on
conditional generation based on a single property or mul-
tiple properties of interest, we tackle generation guided by
high-content readouts, which we achieve through a multi-
modal latent joint representation.
Deep learning for high-content molecular perturbations.
High-content phenotypic screening, particularly high-
content imaging (HCI), has become crucial in drug dis-
covery for characterizing molecular effects in cells and elu-
cidating targets, gene programs, and biological functions
(Moffat et al., 2017; Chandrasekaran et al., 2021). Recent
advances propose deep learning techniques to accelerate and
enhance these processes (Gavriilidis et al., 2024). Predic-
tive models to infer the outcome of molecular effects have
been developed both for transcriptomic readouts (Lotfollahi
et al., 2019; Hetzel et al., 2022; Piran et al., 2024) and HCI
readouts (Palma et al., 2023). In these models, the output
is highly multi-dimensional, capturing the full readout of
the high-throughput experiment, thus potentially requiring a
large amount of d","Conclusions
In this paper, we consider the task of designing a generative
model able to produce molecules that can induce a cell mor-
phology profile similar to a given target. Such framework
is broadly applicable, for example to design drugs mim-
icking the effect of a genetic perturbation, designing drug
analogs, or, more generally, molecular design guided by
phenotypic readouts. The proposed approach relies on the
GFlowNet framework for molecular generation, and uses a
reward based on the latent similarity of representations from
a multi-modal contrastive learning model. To the best of our
knowledge, this is the first published attempt at the challeng-
ing task of guiding the generative molecular model with the
expected image morphology outcome. We experimentally
Cell Morphology-Guided Small Molecule Generation with GFlowNets
demonstrate the usefulness of the proposed approach for
generating diverse drug candidates, which was shown to
increase the likelihood of producing molecules with similar
biological activity when compared to random screening.
Acknowledgements
The research was supported by funding from CQDM Fonds
d’Accélération des Collaborations en Santé (FACS) / Acuité
Québec and Genentech.
References
Arús-Pous, J., Patronov, A., Bjerrum, E. J., Tyrchan, C.,
Reymond, J.-L., Chen, H., and Engkvist, O. Smiles-
based deep generative scaffold decorator for de-novo drug
design. Journal of cheminformatics , 12:1–18, 2020.
Bengio, E., Jain, M., Korablyov, M., Precup, D., and Ben-
gio, Y . Flow network based generative models for non-
iterative diverse candidate generation. Advances in Neu-
ral Information Processing Systems , 34:27381–27394,
2021.
Bengio, Y ., Lahlou, S., Deleu, T., Hu, E. J., Tiwari, M., and
Bengio, E. GFlowNet foundations. Journal of Machine
Learning Research , 24(210):1–55, 2023.
Bilodeau, C., Jin, W., Jaakkola, T., Barzilay, R., and Jensen,
K. F. Generative models for molecular discovery: Recent
advances and challenges. Wiley Interdisciplinary","methodology, e.g. variational autoencoders (Jin et al., 2018; Maziarka et al., 2020), reinforcement learning (RL) (Loef- fler et al., 2024) or diffusion models (Runcie & Mey, 2023; Uehara et al., 2024). These methods have found applica- tions in drug discovery, reporting successes in several areas such as immunology and infectious diseases (Godinez et al., 2022; Moret et al., 2023). Recently, Generative Flow Net- works (GFlowNets) (Bengio et al., 2021; Nica et al., 2022; Shen et al., 2023; V olokhova et al., 2024; Koziarski et al., 2024a; Gai nski et al., 2024; Koziarski et al., 2024b) emerged as a promising paradigm for molecular generation due to the ability to sample diverse candidate molecules, which is crucial in the drug discovery process. Importantly, similar to RL, GFlowNets can be trained based on the specified reward function, which makes them suitable for phenotypic discovery. Compared to existing methods, which focus on conditional generation based on a single property or mul- tiple properties of interest, we tackle generation guided by high-content readouts, which we achieve through a multi- modal latent joint representation. Deep learning for high-content molecular perturbations. High-content phenotypic screening, particularly high- content imaging (HCI), has become crucial in drug dis- covery for characterizing molecular effects in cells and elu- cidating targets, gene programs, and biological functions (Moffat et al., 2017; Chandrasekaran et al., 2021). Recent advances propose deep learning techniques to accelerate and enhance these processes (Gavriilidis et al., 2024). Predic- tive models to infer the outcome of molecular effects have been developed both for transcriptomic readouts (Lotfollahi et al., 2019; Hetzel et al., 2022; Piran et al., 2024) and HCI readouts (Palma et al., 2023). In these models, the output is highly multi-dimensional, capturing the full readout of the high-throughput experiment, thus potentially requiring a large amount of d","Conclusions In this paper, we consider the task of designing a generative model able to produce molecules that can induce a cell mor- phology profile similar to a given target. Such framework is broadly applicable, for example to design drugs mim- icking the effect of a genetic perturbation, designing drug analogs, or, more generally, molecular design guided by phenotypic readouts. The proposed approach relies on the GFlowNet framework for molecular generation, and uses a reward based on the latent similarity of representations from a multi-modal contrastive learning model. To the best of our knowledge, this is the first published attempt at the challeng- ing task of guiding the generative molecular model with the expected image morphology outcome. We experimentally Cell Morphology-Guided Small Molecule Generation with GFlowNets demonstrate the usefulness of the proposed approach for generating diverse drug candidates, which was shown to increase the likelihood of producing molecules with similar biological activity when compared to random screening.","Cell Morphology-Guided Small Molecule Generation with GFlowNets Molecular generation using GFlowNets guided by the latent representation from a cross-modal contrastive model High-content phenotypic screening, including high-content imaging (HCI), has gained popularity in the last few years for its ability to characterize novel therapeutics without prior knowledge of the protein target. This work focuses on the novel task of HCI-guided molecular design. We consider an approach in which we leverage an unsupervised multimodal joint embedding to define a latent similarity as a reward for GFlowNets. The proposed model learns to generate new molecules that could produce phenotypic effects similar to those of the given image target, without relying on pre-annotated phenotypic labels. We demonstrate that our method generates molecules with high morphological and structural similarity to the target, increasing the likelihood of similar biological activity.","Cell Morphology-Guided Small Molecule Generation with GFlowNets Molecular generation using GFlowNets guided by the latent representation from a cross-modal contrastive model High-content phenotypic screening, including high-content imaging (HCI), has gained popularity in the last few years for its ability to characterize novel therapeutics without prior knowledge of the protein target. This work focuses on the novel task of HCI-guided molecular design. We consider an approach in which we leverage an unsupervised multimodal joint embedding to define a latent similarity as a reward for GFlowNets. The proposed model learns to generate new molecules that could produce phenotypic effects similar to those of the given image target, without relying on pre-annotated phenotypic labels. We demonstrate that our method generates molecules with high morphological and structural similarity to the target, increasing the likelihood of similar biological activity. Conclusions In this paper, we consider the task of designing a generative model able to produce molecules that can induce a cell mor- phology profile similar to a given target. Such framework is broadly applicable, for example to design drugs mim- icking the effect of a genetic perturbation, designing drug analogs, or, more generally, molecular design guided by phenotypic readouts. The proposed approach relies on the GFlowNet framework for molecular generation, and uses a reward based on the latent similarity of representations from a multi-modal contrastive learning model. To the best of our knowledge, this is the first published attempt at the challeng- ing task of guiding the generative molecular model with the expected image morphology outcome. We experimentally Cell Morphology-Guided Small Molecule Generation with GFlowNets demonstrate the usefulness of the proposed approach for generating diverse drug candidates, which was shown to increase the likelihood of producing molecules with similar biological activity when compared to random screening. methodology, e.g. variational autoencoders (Jin et al., 2018; Maziarka et al., 2020), reinforcement learning (RL) (Loef- fler et al., 2024) or diffusion models (Runcie & Mey, 2023; Uehara et al., 2024). These methods have found applica- tions in drug discovery, reporting successes in several areas such as immunology and infectious diseases (Godinez et al., 2022; Moret et al., 2023). Recently, Generative Flow Net- works (GFlowNets) (Bengio et al., 2021; Nica et al., 2022; Shen et al., 2023; V olokhova et al., 2024; Koziarski et al., 2024a; Gai nski et al., 2024; Koziarski et al., 2024b) emerged as a promising paradigm for molecular generation due to the ability to sample diverse candidate molecules, which is crucial in the drug discovery process. Importantly, similar to RL, GFlowNets can be trained based on the specified reward function, which makes them suitable for phenotypic discovery. Compared to existing methods, which focus on conditional generation based on a single property or mul- tiple properties of interest, we tackle generation guided by high-content readouts, which we achieve through a multi- modal latent joint representation. Deep learning for high-content molecular perturbations. High-content phenotypic screening, particularly high- content imaging (HCI), has become crucial in drug dis- covery for characterizing molecular effects in cells and elu- cidating targets, gene programs, and biological functions (Moffat et al., 2017; Chandrasekaran et al., 2021). Recent advances propose deep learning techniques to accelerate and enhance these processes (Gavriilidis et al., 2024). Predic- tive models to infer the outcome of molecular effects have been developed both for transcriptomic readouts (Lotfollahi et al., 2019; Hetzel et al., 2022; Piran et al., 2024) and HCI readouts (Palma et al., 2023). In these models, the output is highly multi-dimensional, capturing the full readout of the high-throughput experiment, thus potentially requiring a large amount of d",0
3bdd88e3daf9558e9c3294f56efca5f3f8e2d31b,Score-Based Generative Models For Binding Peptide Backbones,[],https://openreview.net/pdf/3bdd88e3daf9558e9c3294f56efca5f3f8e2d31b.pdf,"Score-Based Generative Models For Binding Peptide Backbones We develop a score-based generative model (SGM) and evaluation pipeline for binding peptide structures and use the framework to explore key design choices for SGMs and develop improved metrics for generated binder structures. Score-based generative models (SGMs) have emerged as powerful tools for protein design, capable of generating protein structures for a variety of biologically relevant design specifications. Among these, the ability to generate structures capable of binding a specified target holds particular relevance for a range of applications. Despite the success of SGMs in this domain, there has been little systematic exploration of the impact of model design choices for protein binder backbone generation, in part due to the lack of appropriate metrics for generated backbones and their complementarity to the target protein. Here we present LoopGen, a flexible SGM framework for the generation and evaluation of de novo binding protein backbones in the absence of inverse folding/folding models. This decoupling from existing inverse folding/folding models not only provides an orthogonal set of metrics but also enables the evaluation of protein structure SGMs in domains where such models are difficult to obtain (e.g. peptide design). We apply our framework to design antibody CDR loop structures, a class of peptides with notable structural diversity, and evaluate a variety of model design choices, showing that choices of structural representation and variance schedule have dramatic impacts on model performance. Furthermore, we propose three novel metrics for testing the dependency of a generated binder structure on its target protein, and demonstrate that LoopGen's generated backbones are indeed conditioned on the sequence, structure, and position of its input epitope. Our results identify promising avenues for further development of SGMs for protein design.",3bdd88e3daf9558e9c3294f56efca5f3f8e2d31b.pdf,"methodology between works have made con-
trolled comparisons of model design choices challenging
thus far, further exacerbated by the fact that standard met-
rics for evaluating designed binders in silico have not been
established. Therefore, we sought to develop a framework
for training and evaluating different SGMs in the context
of an important task: generating binding loop structures
in antibodies, a key class of biomolecules widely applied
as therapeutics, diagnostics, and research tools (Sormanni
et al., 2018; Lu et al., 2020). Antibody binding is mediated
primarily through interactions between the target and short
loop regions called complementarity-determining-regions
(CDRs), which often lack secondary structure and can ex-
hibit extreme sequence and structural diversity (Fern ´andez-
Quintero et al., 2020), posing challenges in the generative
modelling setting. Designing CDR structures for binding
is also challenging due to limited data availability; around
8000 total antibody structures are available in the PDB,
many of which are redundant or lack a binding partner
(Dunbar et al., 2014). These challenges, combined with
the general utility of antibodies (Sormanni et al., 2018; Lu
et al., 2020), motivate the development of novel methods
for generating and evaluating CDR structures in silico .
Our main contributions are the following:
1.We formulate a flexible, architecture-agnostic SGM
that allows rotational information to be straightfor-
wardly added/removed from the model, providing the
first controlled comparison, to our knowledge, of pro-
tein diffusion models trained on residue orientations
and coordinates versus coordinates alone. We demon-
strate that reasoning over rotations is essential for gen-
erating diverse CDR loop structures.
2.We investigate the use of different variance schedules
for coordinates and rotations, observing patterns that
motivate the development of new schedules.
3.We identify that ground-truth RMSD, the most com-
mon metric in","Conclusion
While SGMs for protein structure generation have catalysed
great advances in de novo protein design, relatively little
is known about the key components affecting their perfor-
mance, and even less so in the context of designing binding
proteins. Here we introduce LoopGen, a generative model
and evaluation pipeline that designs CDR loop structures
and enables direct comparison of various hyperparameter
choices in the SGM setting. We use LoopGen to conduct,
to our knowledge, the first direct comparison between mod-
elling entire residue frames versus C αcoordinates alone.
We hypothesised that modelling frames would provide spe-
cific benefits in designing binding peptides, which have
highly variable structures and therefore significant degrees
of freedom in their backbone dihedral angles. Indeed, re-
sults show that the C αtraces from structures generated
using frame diffusion are significantly more diverse than
structures generated solely using diffusion over C αcoor-
dinates. We also compare models trained under different
variance schedules and show that, while schedules do not
have a significant effect on the commonly used ground truth
RMSD metric, they do significantly influence the rate of
Score-Based Generative Models for Binding Peptide Backbones
Figure 2. Left: Ramachandran Distribution of the generated CDRs compared to ground truth. Right: The minimum distance between an
Cαon the CDR and the epitope for the ground truth and generated CDRs. Stratification by length reveals better performance on shorter
CDRs.
Figure 3. Analyzing the dependence of the generated CDR structures on the epitope. A set of ten CDR structures is generated for each of
687 test set epitopes (wild-type (WT) epitopes). Another ten are generated under each of three types of perturbation to each epitope:
permutation (swapping the WT epitope with a random one from test set), scrambling (permuting residue identities within the WT epitope
structure), and translation (translating the","methodology between works have made con- trolled comparisons of model design choices challenging thus far, further exacerbated by the fact that standard met- rics for evaluating designed binders in silico have not been established. Therefore, we sought to develop a framework for training and evaluating different SGMs in the context of an important task: generating binding loop structures in antibodies, a key class of biomolecules widely applied as therapeutics, diagnostics, and research tools (Sormanni et al., 2018; Lu et al., 2020). Antibody binding is mediated primarily through interactions between the target and short loop regions called complementarity-determining-regions (CDRs), which often lack secondary structure and can ex- hibit extreme sequence and structural diversity (Fern andez- Quintero et al., 2020), posing challenges in the generative modelling setting. Designing CDR structures for binding is also challenging due to limited data availability; around 8000 total antibody structures are available in the PDB, many of which are redundant or lack a binding partner (Dunbar et al., 2014). These challenges, combined with the general utility of antibodies (Sormanni et al., 2018; Lu et al., 2020), motivate the development of novel methods for generating and evaluating CDR structures in silico . Our main contributions are the following: 1.We formulate a flexible, architecture-agnostic SGM that allows rotational information to be straightfor- wardly added/removed from the model, providing the first controlled comparison, to our knowledge, of pro- tein diffusion models trained on residue orientations and coordinates versus coordinates alone. We demon- strate that reasoning over rotations is essential for gen- erating diverse CDR loop structures. 2.We investigate the use of different variance schedules for coordinates and rotations, observing patterns that motivate the development of new schedules. 3.We identify that ground-truth RMSD, the most com- mon metric in","Conclusion While SGMs for protein structure generation have catalysed great advances in de novo protein design, relatively little is known about the key components affecting their perfor- mance, and even less so in the context of designing binding proteins. Here we introduce LoopGen, a generative model and evaluation pipeline that designs CDR loop structures and enables direct comparison of various hyperparameter choices in the SGM setting. We use LoopGen to conduct, to our knowledge, the first direct comparison between mod- elling entire residue frames versus C coordinates alone. We hypothesised that modelling frames would provide spe- cific benefits in designing binding peptides, which have highly variable structures and therefore significant degrees of freedom in their backbone dihedral angles. Indeed, re- sults show that the C traces from structures generated using frame diffusion are significantly more diverse than structures generated solely using diffusion over C coor- dinates. We also compare models trained under different variance schedules and show that, while schedules do not have a significant effect on the commonly used ground truth RMSD metric, they do significantly influence the rate of Score-Based Generative Models for Binding Peptide Backbones Figure 2. Left: Ramachandran Distribution of the generated CDRs compared to ground truth. Right: The minimum distance between an C on the CDR and the epitope for the ground truth and generated CDRs. Stratification by length reveals better performance on shorter CDRs. Figure 3. Analyzing the dependence of the generated CDR structures on the epitope. A set of ten CDR structures is generated for each of 687 test set epitopes (wild-type (WT) epitopes). Another ten are generated under each of three types of perturbation to each epitope: permutation (swapping the WT epitope with a random one from test set), scrambling (permuting residue identities within the WT epitope structure), and translation (translating the","Score-Based Generative Models For Binding Peptide Backbones We develop a score-based generative model (SGM) and evaluation pipeline for binding peptide structures and use the framework to explore key design choices for SGMs and develop improved metrics for generated binder structures. Score-based generative models (SGMs) have emerged as powerful tools for protein design, capable of generating protein structures for a variety of biologically relevant design specifications. Among these, the ability to generate structures capable of binding a specified target holds particular relevance for a range of applications. Despite the success of SGMs in this domain, there has been little systematic exploration of the impact of model design choices for protein binder backbone generation, in part due to the lack of appropriate metrics for generated backbones and their complementarity to the target protein. Here we present LoopGen, a flexible SGM framework for the generation and evaluation of de novo binding protein backbones in the absence of inverse folding/folding models. This decoupling from existing inverse folding/folding models not only provides an orthogonal set of metrics but also enables the evaluation of protein structure SGMs in domains where such models are difficult to obtain (e.g. peptide design). We apply our framework to design antibody CDR loop structures, a class of peptides with notable structural diversity, and evaluate a variety of model design choices, showing that choices of structural representation and variance schedule have dramatic impacts on model performance. Furthermore, we propose three novel metrics for testing the dependency of a generated binder structure on its target protein, and demonstrate that LoopGen's generated backbones are indeed conditioned on the sequence, structure, and position of its input epitope. Our results identify promising avenues for further development of SGMs for protein design.","Score-Based Generative Models For Binding Peptide Backbones We develop a score-based generative model (SGM) and evaluation pipeline for binding peptide structures and use the framework to explore key design choices for SGMs and develop improved metrics for generated binder structures. Score-based generative models (SGMs) have emerged as powerful tools for protein design, capable of generating protein structures for a variety of biologically relevant design specifications. Among these, the ability to generate structures capable of binding a specified target holds particular relevance for a range of applications. Despite the success of SGMs in this domain, there has been little systematic exploration of the impact of model design choices for protein binder backbone generation, in part due to the lack of appropriate metrics for generated backbones and their complementarity to the target protein. Here we present LoopGen, a flexible SGM framework for the generation and evaluation of de novo binding protein backbones in the absence of inverse folding/folding models. This decoupling from existing inverse folding/folding models not only provides an orthogonal set of metrics but also enables the evaluation of protein structure SGMs in domains where such models are difficult to obtain (e.g. peptide design). We apply our framework to design antibody CDR loop structures, a class of peptides with notable structural diversity, and evaluate a variety of model design choices, showing that choices of structural representation and variance schedule have dramatic impacts on model performance. Furthermore, we propose three novel metrics for testing the dependency of a generated binder structure on its target protein, and demonstrate that LoopGen's generated backbones are indeed conditioned on the sequence, structure, and position of its input epitope. Our results identify promising avenues for further development of SGMs for protein design. Conclusion While SGMs for protein structure generation have catalysed great advances in de novo protein design, relatively little is known about the key components affecting their perfor- mance, and even less so in the context of designing binding proteins. Here we introduce LoopGen, a generative model and evaluation pipeline that designs CDR loop structures and enables direct comparison of various hyperparameter choices in the SGM setting. We use LoopGen to conduct, to our knowledge, the first direct comparison between mod- elling entire residue frames versus C coordinates alone. We hypothesised that modelling frames would provide spe- cific benefits in designing binding peptides, which have highly variable structures and therefore significant degrees of freedom in their backbone dihedral angles. Indeed, re- sults show that the C traces from structures generated using frame diffusion are significantly more diverse than structures generated solely using diffusion over C coor- dinates. We also compare models trained under different variance schedules and show that, while schedules do not have a significant effect on the commonly used ground truth RMSD metric, they do significantly influence the rate of Score-Based Generative Models for Binding Peptide Backbones Figure 2. Left: Ramachandran Distribution of the generated CDRs compared to ground truth. Right: The minimum distance between an C on the CDR and the epitope for the ground truth and generated CDRs. Stratification by length reveals better performance on shorter CDRs. Figure 3. Analyzing the dependence of the generated CDR structures on the epitope. A set of ten CDR structures is generated for each of 687 test set epitopes (wild-type (WT) epitopes). Another ten are generated under each of three types of perturbation to each epitope: permutation (swapping the WT epitope with a random one from test set), scrambling (permuting residue identities within the WT epitope structure), and translation (translating the methodology between works have made con- trolled comparisons of model design choices challenging thus far, further exacerbated by the fact that standard met- rics for evaluating designed binders in silico have not been established. Therefore, we sought to develop a framework for training and evaluating different SGMs in the context of an important task: generating binding loop structures in antibodies, a key class of biomolecules widely applied as therapeutics, diagnostics, and research tools (Sormanni et al., 2018; Lu et al., 2020). Antibody binding is mediated primarily through interactions between the target and short loop regions called complementarity-determining-regions (CDRs), which often lack secondary structure and can ex- hibit extreme sequence and structural diversity (Fern andez- Quintero et al., 2020), posing challenges in the generative modelling setting. Designing CDR structures for binding is also challenging due to limited data availability; around 8000 total antibody structures are available in the PDB, many of which are redundant or lack a binding partner (Dunbar et al., 2014). These challenges, combined with the general utility of antibodies (Sormanni et al., 2018; Lu et al., 2020), motivate the development of novel methods for generating and evaluating CDR structures in silico . Our main contributions are the following: 1.We formulate a flexible, architecture-agnostic SGM that allows rotational information to be straightfor- wardly added/removed from the model, providing the first controlled comparison, to our knowledge, of pro- tein diffusion models trained on residue orientations and coordinates versus coordinates alone. We demon- strate that reasoning over rotations is essential for gen- erating diverse CDR loop structures. 2.We investigate the use of different variance schedules for coordinates and rotations, observing patterns that motivate the development of new schedules. 3.We identify that ground-truth RMSD, the most com- mon metric in",0
50130326aeb9657fc4d250893f22aa563d87ac42,Flexible Docking via Unbalanced Flow Matching,"['Gabriele Corso', 'Vignesh Ram Somnath', 'Noah Getz', 'Regina Barzilay', 'Tommi Jaakkola', 'Andreas Krause']",https://openreview.net/pdf/50130326aeb9657fc4d250893f22aa563d87ac42.pdf,"Flexible Docking via Unbalanced Flow Matching flexible protein-ligand docking via unbalanced relaxations of flow matching Diffusion models have emerged as a recent successful paradigm for molecular docking. However, these methods treat the protein either as a rigid structure, or force the model to fold proteins from unstructured noise. In this work, we instead focus on flexible docking, leveraging the unbound distribution of proteins to model the precise effect(s) of ligand binding. While Flow Matching (FM) presents an attractive option for this task, we show that a naive application of flow matching results in a complex learning task with poor performance. We thus propose Unbalanced Flow Matching, a generalization of flow matching that allows us to tradeoff sample efficiency with approximation accuracy by relaxing the marginal constraints. Empirically, we validate our framework on flexible docking, demonstrating strong improvements in protein conformation prediction while retaining comparable docking accuracy.",50130326aeb9657fc4d250893f22aa563d87ac42.pdf,"approach FLEXDOCK improves the proportion of very ac-
curate protein structure predictions (all-atom RMSD <1Å)
from 39.8% to 44.1%, while retaining comparable docking
Flexible Docking via Unbalanced Flow Matching
accuracy (ligand RMSD <2Å). On the PoseBusters bench-
mark dataset, FLEXDOCK outperforms most co-folding
methods, despite only being trained on the PDBBind dataset.
2. Background and Related Work
Flow matching (FM) (Lipman et al., 2022; Albergo et al.,
2023) is a generative modeling paradigm that was intro-
duced as a flexible generalization of diffusion models, and
allows learning a transport between arbitrary distributions
with a simulation-free objective. Given two distributions
q0andq1, FM provides a way of learning a vector field vt
which induces a continuous normalizing flow ψt(x)that
transports q0toq1, i.e., q1(x) = [ ψ1]#q0(x)), where #
denotes the pushforward operator.
The key idea in FM is defining a conditional flow ψt(x0|x1)
interpolating between x0∼q0andx1∼q1, and its as-
sociated vector field ut(xt|x1) =d
dtψt(xt|x1). One can
then learn the marginal vector field ˆvt(x, t;θ)with a neural
network ( ˆvt(x, t;θ)≈vt(x, t), by regressing against the
conditional vector field with the CFM objective:
LCFM=Et,x0∼q0,x1∼q1∥vt(xt;θ)−ut(xt|x1)∥2(1)
FM was further generalized by Pooladian et al. (2023) and
Tong et al. (2023) which showed that the sampling distribu-
tion in the CFM objective, which we will refer to as coupling
distribution, does not have to be independent samples from
q0andq1and can be an arbitrary joint distribution q(x0, x1)
as long as it satisfies the marginal constraints being q0and
q1respectively. This formulation enabled drawing a connec-
tion between FM and optimal transport (OT). When using
OT to define the coupling distribution q, the flows become
straight and the transport cost Eq0(x0)∥ψ1(x0)−x0∥2is
the OT cost W2
2(q0, q1).
Protein-ligand binding When proteins bind to small
molecules, their structural distribution adjusts to fit th","Conclusion
We propose Unbalanced Flow Matching, a generalization
of Flow Matching that allows us to relax the marginal con-
straints and learn simpler flows. We theoretically analyze
the tradeoffs between sample efficiency and approxima-
tion capabilities these relaxations induce. Empirically, we
validate our framework on flexible docking, with strong
improvements in modelling protein conformational changes,
while retaining comparable docking accuracy.
References
Abramson, J., Adler, J., Dunger, J., Evans, R., Green, T.,
Pritzel, A., Ronneberger, O., Willmore, L., Ballard, A. J.,
Bambrick, J., et al. Accurate structure prediction of
biomolecular interactions with alphafold 3. Nature , pp.
1–3, 2024.
Flexible Docking via Unbalanced Flow Matching
Albergo, M. S., Boffi, N. M., and Vanden-Eijnden, E.
Stochastic interpolants: A unifying framework for flows
and diffusions. arXiv preprint arXiv:2303.08797 , 2023.
Alhossary, A., Handoko, S. D., Mu, Y ., and Kwoh, C.-
K. Fast, accurate, and reliable molecular docking with
quickvina 2. Bioinformatics , 31(13):2214–2216, 2015.
Benton, J., Deligiannidis, G., and Doucet, A. Error
bounds for flow matching methods. arXiv preprint
arXiv:2305.16860 , 2023.
Bryant, P., Kelkar, A., Guljas, A., Clementi, C., and Noé,
F. Structure prediction of protein-ligand complexes from
sequence information with umol. bioRxiv , pp. 2023–11,
2023.
Buttenschoen, M., Morris, G. M., and Deane, C. M. Pose-
busters: Ai-based docking methods fail to generate physi-
cally valid poses or generalise to novel sequences. Chem-
ical Science , 2024.
Chen, R. T. and Lipman, Y . Flow matching on general
geometries. In The Twelfth International Conference on
Learning Representations , 2024.
Corso, G. Modeling molecular structures with intrinsic
diffusion models . Massachusetts Institute of Technology,
2023.
Corso, G., Stärk, H., Jing, B., Barzilay, R., and Jaakkola, T.
Diffdock: Diffusion steps, twists, and turns for molecular
docking. arXiv preprint arXiv:2210.0177","approach FLEXDOCK improves the proportion of very ac- curate protein structure predictions (all-atom RMSD <1 ) from 39.8% to 44.1%, while retaining comparable docking Flexible Docking via Unbalanced Flow Matching accuracy (ligand RMSD <2 ). On the PoseBusters bench- mark dataset, FLEXDOCK outperforms most co-folding methods, despite only being trained on the PDBBind dataset. 2. Background and Related Work Flow matching (FM) (Lipman et al., 2022; Albergo et al., 2023) is a generative modeling paradigm that was intro- duced as a flexible generalization of diffusion models, and allows learning a transport between arbitrary distributions with a simulation-free objective. Given two distributions q0andq1, FM provides a way of learning a vector field vt which induces a continuous normalizing flow t(x)that transports q0toq1, i.e., q1(x) = [ 1]#q0(x)), where # denotes the pushforward operator. The key idea in FM is defining a conditional flow t(x0|x1) interpolating between x0 q0andx1 q1, and its as- sociated vector field ut(xt|x1) =d dt t(xt|x1). One can then learn the marginal vector field vt(x, t; )with a neural network ( vt(x, t; ) vt(x, t), by regressing against the conditional vector field with the CFM objective: LCFM=Et,x0 q0,x1 q1 vt(xt; ) ut(xt|x1) 2(1) FM was further generalized by Pooladian et al. (2023) and Tong et al. (2023) which showed that the sampling distribu- tion in the CFM objective, which we will refer to as coupling distribution, does not have to be independent samples from q0andq1and can be an arbitrary joint distribution q(x0, x1) as long as it satisfies the marginal constraints being q0and q1respectively. This formulation enabled drawing a connec- tion between FM and optimal transport (OT). When using OT to define the coupling distribution q, the flows become straight and the transport cost Eq0(x0) 1(x0) x0 2is the OT cost W2 2(q0, q1). Protein-ligand binding When proteins bind to small molecules, their structural distribution adjusts to fit th","Conclusion We propose Unbalanced Flow Matching, a generalization of Flow Matching that allows us to relax the marginal con- straints and learn simpler flows. We theoretically analyze the tradeoffs between sample efficiency and approxima- tion capabilities these relaxations induce. Empirically, we validate our framework on flexible docking, with strong improvements in modelling protein conformational changes, while retaining comparable docking accuracy.","Flexible Docking via Unbalanced Flow Matching flexible protein-ligand docking via unbalanced relaxations of flow matching Diffusion models have emerged as a recent successful paradigm for molecular docking. However, these methods treat the protein either as a rigid structure, or force the model to fold proteins from unstructured noise. In this work, we instead focus on flexible docking, leveraging the unbound distribution of proteins to model the precise effect(s) of ligand binding. While Flow Matching (FM) presents an attractive option for this task, we show that a naive application of flow matching results in a complex learning task with poor performance. We thus propose Unbalanced Flow Matching, a generalization of flow matching that allows us to tradeoff sample efficiency with approximation accuracy by relaxing the marginal constraints. Empirically, we validate our framework on flexible docking, demonstrating strong improvements in protein conformation prediction while retaining comparable docking accuracy.","Flexible Docking via Unbalanced Flow Matching flexible protein-ligand docking via unbalanced relaxations of flow matching Diffusion models have emerged as a recent successful paradigm for molecular docking. However, these methods treat the protein either as a rigid structure, or force the model to fold proteins from unstructured noise. In this work, we instead focus on flexible docking, leveraging the unbound distribution of proteins to model the precise effect(s) of ligand binding. While Flow Matching (FM) presents an attractive option for this task, we show that a naive application of flow matching results in a complex learning task with poor performance. We thus propose Unbalanced Flow Matching, a generalization of flow matching that allows us to tradeoff sample efficiency with approximation accuracy by relaxing the marginal constraints. Empirically, we validate our framework on flexible docking, demonstrating strong improvements in protein conformation prediction while retaining comparable docking accuracy. Conclusion We propose Unbalanced Flow Matching, a generalization of Flow Matching that allows us to relax the marginal con- straints and learn simpler flows. We theoretically analyze the tradeoffs between sample efficiency and approxima- tion capabilities these relaxations induce. Empirically, we validate our framework on flexible docking, with strong improvements in modelling protein conformational changes, while retaining comparable docking accuracy. approach FLEXDOCK improves the proportion of very ac- curate protein structure predictions (all-atom RMSD <1 ) from 39.8% to 44.1%, while retaining comparable docking Flexible Docking via Unbalanced Flow Matching accuracy (ligand RMSD <2 ). On the PoseBusters bench- mark dataset, FLEXDOCK outperforms most co-folding methods, despite only being trained on the PDBBind dataset. 2. Background and Related Work Flow matching (FM) (Lipman et al., 2022; Albergo et al., 2023) is a generative modeling paradigm that was intro- duced as a flexible generalization of diffusion models, and allows learning a transport between arbitrary distributions with a simulation-free objective. Given two distributions q0andq1, FM provides a way of learning a vector field vt which induces a continuous normalizing flow t(x)that transports q0toq1, i.e., q1(x) = [ 1]#q0(x)), where # denotes the pushforward operator. The key idea in FM is defining a conditional flow t(x0|x1) interpolating between x0 q0andx1 q1, and its as- sociated vector field ut(xt|x1) =d dt t(xt|x1). One can then learn the marginal vector field vt(x, t; )with a neural network ( vt(x, t; ) vt(x, t), by regressing against the conditional vector field with the CFM objective: LCFM=Et,x0 q0,x1 q1 vt(xt; ) ut(xt|x1) 2(1) FM was further generalized by Pooladian et al. (2023) and Tong et al. (2023) which showed that the sampling distribu- tion in the CFM objective, which we will refer to as coupling distribution, does not have to be independent samples from q0andq1and can be an arbitrary joint distribution q(x0, x1) as long as it satisfies the marginal constraints being q0and q1respectively. This formulation enabled drawing a connec- tion between FM and optimal transport (OT). When using OT to define the coupling distribution q, the flows become straight and the transport cost Eq0(x0) 1(x0) x0 2is the OT cost W2 2(q0, q1). Protein-ligand binding When proteins bind to small molecules, their structural distribution adjusts to fit th",0
fcb112534337fb21bfbf3c9272afde3a382068f6,Multi-Objective Guidance via Importance Sampling for Target-Aware Diffusion-based De Novo Ligand Generation,"['Julian Cremer', 'Tuan Le', 'Frank Noe', 'Djork-Arné Clevert', 'Kristof T Schütt']",https://openreview.net/pdf/fcb112534337fb21bfbf3c9272afde3a382068f6.pdf,"Multi-Objective Guidance via Importance Sampling for Target-Aware Diffusion-based De Novo Ligand Generation The generation of ligands that both are tailored to a given protein pocket and exhibit a range of desired chemical properties is a major challenge in structure-based drug design. Here, we propose an in-silico approach for the de novo generation of 3D ligand structures using the equivariant diffusion model PILOT, combining pocket conditioning with a large-scale pre-training and property guidance. Its multi-objective trajectory-based importance sampling strategy is designed to direct the model towards molecules that not only exhibit desired characteristics such as increased binding affinity for a given protein pocket but also maintains high synthetic accessibility. This ensures the practicality of sampled molecules, thus maximizing their potential for the drug discovery pipeline. PILOT significantly outperforms existing methods across various metrics on the common benchmark dataset CrossDocked2020. Moreover, we employ PILOT to generate novel ligands for unseen protein pockets from the Kinodata-3D dataset, which encompasses a substantial portion of the human kinome. The generated structures exhibit predicted IC50 values indicative of potent biological activity, which highlights the potential of PILOT as a powerful tool for structure-based drug design.",fcb112534337fb21bfbf3c9272afde3a382068f6.pdf,"Methodology , 68(3):411–436,
05 2006. ISSN 1369-7412. doi: 10.1111/j.1467-9868.
2006.00553.x. URL https://doi.org/10.1111/
j.1467-9868.2006.00553.x .
Dhariwal, P. and Nichol, A. Q. Diffusion models beat GANs
on image synthesis. In Beygelzimer, A., Dauphin, Y .,
Liang, P., and Vaughan, J. W. (eds.), Advances in Neural
Information Processing Systems , 2021. URL https:
//openreview.net/forum?id=AAWuCvzaVt .
Doucet, A., de Freitas, N., and Gordon, N. An Introduction
to Sequential Monte Carlo Methods , pp. 3–14. Springer
New York, New York, NY , 2001. ISBN 978-1-4757-3437-
9. doi: 10.1007/978-1-4757-3437-9 1. URL https://
doi.org/10.1007/978-1-4757-3437-9_1 .
Green, H., Koes, D. R., and Durrant, J. D. Deepfrag: a
deep convolutional neural network for fragment-based
lead optimization. Chem. Sci. , 12:8036–8047, 2021. doi:
10.1039/D1SC00163A. URL http://dx.doi.org/
10.1039/D1SC00163A .
Guan, J., Qian, W. W., Peng, X., Su, Y ., Peng, J., and Ma,
J. 3d equivariant diffusion for target-aware molecule
generation and affinity prediction. In The Eleventh In-
ternational Conference on Learning Representations ,
2023. URL https://openreview.net/forum?
id=kJqXEPXMsE0 .
G´omez-Bombarelli, R., Wei, J. N., Duvenaud, D.,
Hern ´andez-Lobato, J. M., S ´anchez-Lengeling, B., She-
berla, D., Aguilera-Iparraguirre, J., Hirzel, T. D., Adams,
R. P., and Aspuru-Guzik, A. Automatic chemical de-
sign using a data-driven continuous representation of
molecules. ACS Central Science , 4(2):268–276, 2018.
doi: 10.1021/acscentsci.7b00572. URL https://doi.
org/10.1021/acscentsci.7b00572 . PMID:
29532027.
Landrum, G. A. and Riniker, S. Combining ic50 or ki
values from different sources is a source of significant
noise. Journal of Chemical Information and Modeling ,
64(5):1560–1567, Mar 2024. ISSN 1549-9596. doi:
10.1021/acs.jcim.4c00049. URL https://doi.org/
10.1021/acs.jcim.4c00049 .Le, T., Cremer, J., No ´e, F., Clevert, D.-A., and Sch ¨utt,
K. Navigating the design space of equivariant diffusion-
bas","Conclusions
We have introduced PILOT, a novel equivariant diffusion-
based model tailored for de novo ligand generation condi-
tioned on protein pockets in three-dimensional space. Our
research demonstrates the superior performance of PILOT
compared to existing state-of-the-art models in this domain,
as evidenced by a comprehensive evaluation across a spec-
trum of metrics critical in medicinal chemistry and drug
design. We have proposed a trajectory-based importance
sampling strategy, which enables targeted steering of lig-
and generation towards desired chemical properties. This
technique guides the generation process towards ligands
with properties such as synthetic accessibility, drug-likeness,
docking scores and potency by using surrogate models. This
strategy represents a significant advancement in structure-
based drug discovery, offering researchers a powerful tool to
design molecules with tailored properties. The dependency
on the availability and quality of training data remain a criti-
cal challenge for deploying AI models like PILOT in drug
discovery pipelines. In the domain of structure-based drug
design, data can often be sparse, noisy, and of varying qual-
ity, which significantly impacts the learning and predictive
capabilities of ML models. While our method relies heavily
on surrogate models and proxies like the RDKit synthetic
accessibility (SA) scores to estimate the synthesizability of
generated ligands, these scores may not fully capture the
complexities and practical challenges of synthetic chemistry.
Addressing these challenges will require a concerted effort
to enhance data collection practices, improve data quality,
and expand the variety of data sources. Moving forward, we
see potential applications of PILOT in the drug discovery
pipeline by integrating this model with other AI-driven tools
and technologies, such as automated synthesis platforms
and high-throughput screening to accelerate drug design.
Furthermore, the scope of our model may","Methodology , 68(3):411 436, 05 2006. ISSN 1369-7412. doi: 10.1111/j.1467-9868. 2006.00553.x. URL https://doi.org/10.1111/ j.1467-9868.2006.00553.x . Dhariwal, P. and Nichol, A. Q. Diffusion models beat GANs on image synthesis. In Beygelzimer, A., Dauphin, Y ., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems , 2021. URL https: //openreview.net/forum?id=AAWuCvzaVt . Doucet, A., de Freitas, N., and Gordon, N. An Introduction to Sequential Monte Carlo Methods , pp. 3 14. Springer New York, New York, NY , 2001. ISBN 978-1-4757-3437- 9. doi: 10.1007/978-1-4757-3437-9 1. URL https:// doi.org/10.1007/978-1-4757-3437-9_1 . Green, H., Koes, D. R., and Durrant, J. D. Deepfrag: a deep convolutional neural network for fragment-based lead optimization. Chem. Sci. , 12:8036 8047, 2021. doi: 10.1039/D1SC00163A. URL http://dx.doi.org/ 10.1039/D1SC00163A . Guan, J., Qian, W. W., Peng, X., Su, Y ., Peng, J., and Ma, J. 3d equivariant diffusion for target-aware molecule generation and affinity prediction. In The Eleventh In- ternational Conference on Learning Representations , 2023. URL https://openreview.net/forum? id=kJqXEPXMsE0 . G omez-Bombarelli, R., Wei, J. N., Duvenaud, D., Hern andez-Lobato, J. M., S anchez-Lengeling, B., She- berla, D., Aguilera-Iparraguirre, J., Hirzel, T. D., Adams, R. P., and Aspuru-Guzik, A. Automatic chemical de- sign using a data-driven continuous representation of molecules. ACS Central Science , 4(2):268 276, 2018. doi: 10.1021/acscentsci.7b00572. URL https://doi. org/10.1021/acscentsci.7b00572 . PMID: 29532027. Landrum, G. A. and Riniker, S. Combining ic50 or ki values from different sources is a source of significant noise. Journal of Chemical Information and Modeling , 64(5):1560 1567, Mar 2024. ISSN 1549-9596. doi: 10.1021/acs.jcim.4c00049. URL https://doi.org/ 10.1021/acs.jcim.4c00049 .Le, T., Cremer, J., No e, F., Clevert, D.-A., and Sch utt, K. Navigating the design space of equivariant diffusion- bas","Conclusions We have introduced PILOT, a novel equivariant diffusion- based model tailored for de novo ligand generation condi- tioned on protein pockets in three-dimensional space. Our research demonstrates the superior performance of PILOT compared to existing state-of-the-art models in this domain, as evidenced by a comprehensive evaluation across a spec- trum of metrics critical in medicinal chemistry and drug design. We have proposed a trajectory-based importance sampling strategy, which enables targeted steering of lig- and generation towards desired chemical properties. This technique guides the generation process towards ligands with properties such as synthetic accessibility, drug-likeness, docking scores and potency by using surrogate models. This strategy represents a significant advancement in structure- based drug discovery, offering researchers a powerful tool to design molecules with tailored properties. The dependency on the availability and quality of training data remain a criti- cal challenge for deploying AI models like PILOT in drug discovery pipelines. In the domain of structure-based drug design, data can often be sparse, noisy, and of varying qual- ity, which significantly impacts the learning and predictive capabilities of ML models. While our method relies heavily on surrogate models and proxies like the RDKit synthetic accessibility (SA) scores to estimate the synthesizability of generated ligands, these scores may not fully capture the complexities and practical challenges of synthetic chemistry. Addressing these challenges will require a concerted effort to enhance data collection practices, improve data quality, and expand the variety of data sources. Moving forward, we see potential applications of PILOT in the drug discovery pipeline by integrating this model with other AI-driven tools and technologies, such as automated synthesis platforms and high-throughput screening to accelerate drug design. Furthermore, the scope of our model may","Multi-Objective Guidance via Importance Sampling for Target-Aware Diffusion-based De Novo Ligand Generation The generation of ligands that both are tailored to a given protein pocket and exhibit a range of desired chemical properties is a major challenge in structure-based drug design. Here, we propose an in-silico approach for the de novo generation of 3D ligand structures using the equivariant diffusion model PILOT, combining pocket conditioning with a large-scale pre-training and property guidance. Its multi-objective trajectory-based importance sampling strategy is designed to direct the model towards molecules that not only exhibit desired characteristics such as increased binding affinity for a given protein pocket but also maintains high synthetic accessibility. This ensures the practicality of sampled molecules, thus maximizing their potential for the drug discovery pipeline. PILOT significantly outperforms existing methods across various metrics on the common benchmark dataset CrossDocked2020. Moreover, we employ PILOT to generate novel ligands for unseen protein pockets from the Kinodata-3D dataset, which encompasses a substantial portion of the human kinome. The generated structures exhibit predicted IC50 values indicative of potent biological activity, which highlights the potential of PILOT as a powerful tool for structure-based drug design.","Multi-Objective Guidance via Importance Sampling for Target-Aware Diffusion-based De Novo Ligand Generation The generation of ligands that both are tailored to a given protein pocket and exhibit a range of desired chemical properties is a major challenge in structure-based drug design. Here, we propose an in-silico approach for the de novo generation of 3D ligand structures using the equivariant diffusion model PILOT, combining pocket conditioning with a large-scale pre-training and property guidance. Its multi-objective trajectory-based importance sampling strategy is designed to direct the model towards molecules that not only exhibit desired characteristics such as increased binding affinity for a given protein pocket but also maintains high synthetic accessibility. This ensures the practicality of sampled molecules, thus maximizing their potential for the drug discovery pipeline. PILOT significantly outperforms existing methods across various metrics on the common benchmark dataset CrossDocked2020. Moreover, we employ PILOT to generate novel ligands for unseen protein pockets from the Kinodata-3D dataset, which encompasses a substantial portion of the human kinome. The generated structures exhibit predicted IC50 values indicative of potent biological activity, which highlights the potential of PILOT as a powerful tool for structure-based drug design. Conclusions We have introduced PILOT, a novel equivariant diffusion- based model tailored for de novo ligand generation condi- tioned on protein pockets in three-dimensional space. Our research demonstrates the superior performance of PILOT compared to existing state-of-the-art models in this domain, as evidenced by a comprehensive evaluation across a spec- trum of metrics critical in medicinal chemistry and drug design. We have proposed a trajectory-based importance sampling strategy, which enables targeted steering of lig- and generation towards desired chemical properties. This technique guides the generation process towards ligands with properties such as synthetic accessibility, drug-likeness, docking scores and potency by using surrogate models. This strategy represents a significant advancement in structure- based drug discovery, offering researchers a powerful tool to design molecules with tailored properties. The dependency on the availability and quality of training data remain a criti- cal challenge for deploying AI models like PILOT in drug discovery pipelines. In the domain of structure-based drug design, data can often be sparse, noisy, and of varying qual- ity, which significantly impacts the learning and predictive capabilities of ML models. While our method relies heavily on surrogate models and proxies like the RDKit synthetic accessibility (SA) scores to estimate the synthesizability of generated ligands, these scores may not fully capture the complexities and practical challenges of synthetic chemistry. Addressing these challenges will require a concerted effort to enhance data collection practices, improve data quality, and expand the variety of data sources. Moving forward, we see potential applications of PILOT in the drug discovery pipeline by integrating this model with other AI-driven tools and technologies, such as automated synthesis platforms and high-throughput screening to accelerate drug design. Furthermore, the scope of our model may Methodology , 68(3):411 436, 05 2006. ISSN 1369-7412. doi: 10.1111/j.1467-9868. 2006.00553.x. URL https://doi.org/10.1111/ j.1467-9868.2006.00553.x . Dhariwal, P. and Nichol, A. Q. Diffusion models beat GANs on image synthesis. In Beygelzimer, A., Dauphin, Y ., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems , 2021. URL https: //openreview.net/forum?id=AAWuCvzaVt . Doucet, A., de Freitas, N., and Gordon, N. An Introduction to Sequential Monte Carlo Methods , pp. 3 14. Springer New York, New York, NY , 2001. ISBN 978-1-4757-3437- 9. doi: 10.1007/978-1-4757-3437-9 1. URL https:// doi.org/10.1007/978-1-4757-3437-9_1 . Green, H., Koes, D. R., and Durrant, J. D. Deepfrag: a deep convolutional neural network for fragment-based lead optimization. Chem. Sci. , 12:8036 8047, 2021. doi: 10.1039/D1SC00163A. URL http://dx.doi.org/ 10.1039/D1SC00163A . Guan, J., Qian, W. W., Peng, X., Su, Y ., Peng, J., and Ma, J. 3d equivariant diffusion for target-aware molecule generation and affinity prediction. In The Eleventh In- ternational Conference on Learning Representations , 2023. URL https://openreview.net/forum? id=kJqXEPXMsE0 . G omez-Bombarelli, R., Wei, J. N., Duvenaud, D., Hern andez-Lobato, J. M., S anchez-Lengeling, B., She- berla, D., Aguilera-Iparraguirre, J., Hirzel, T. D., Adams, R. P., and Aspuru-Guzik, A. Automatic chemical de- sign using a data-driven continuous representation of molecules. ACS Central Science , 4(2):268 276, 2018. doi: 10.1021/acscentsci.7b00572. URL https://doi. org/10.1021/acscentsci.7b00572 . PMID: 29532027. Landrum, G. A. and Riniker, S. Combining ic50 or ki values from different sources is a source of significant noise. Journal of Chemical Information and Modeling , 64(5):1560 1567, Mar 2024. ISSN 1549-9596. doi: 10.1021/acs.jcim.4c00049. URL https://doi.org/ 10.1021/acs.jcim.4c00049 .Le, T., Cremer, J., No e, F., Clevert, D.-A., and Sch utt, K. Navigating the design space of equivariant diffusion- bas",0
20d8fcfa598dcb32afebb7f86d46e01b83efe8d8,Detecting critical treatment effect bias in small subgroups,"['Piersilvio De Bartolomeis', 'Javier Abad', 'Konstantin Donhauser', 'Fanny Yang']",https://openreview.net/pdf/20d8fcfa598dcb32afebb7f86d46e01b83efe8d8.pdf,"Detecting critical treatment effect bias in small subgroups Randomized trials are considered the gold standard for making informed decisions in medicine, yet they often lack generalizability to the patient populations in clinical practice. Observational studies, on the other hand, cover a broader patient population but are prone to various biases. Thus, before using an observational study for decision-making, it is crucial to benchmark its treatment effect estimates against those derived from a randomized trial. We propose a novel strategy to benchmark observational studies beyond the average treatment effect. First, we design a statistical test for the null hypothesis that the treatment effects estimated from the two studies, conditioned on a set of relevant features, differ up to some tolerance. We then estimate an asymptotically valid lower bound on the maximum bias strength for any subgroup in the observational study. Finally, we validate our benchmarking strategy in a real-world setting and show that it leads to conclusions that align with established medical knowledge.",20d8fcfa598dcb32afebb7f86d46e01b83efe8d8.pdf,"Methodology
In this section, we rewrite the null hypothesis from Equa-
tion (1) in terms of a signal function that captures the bias be-
tween τosandτrct. Then, we propose an oracle test statistic
assuming that the tolerance functions τos
±are known. Finally,
we provide asymptotic guarantees for the finite-sample test
statistic where the tolerance functions are estimated.
3.1. Null hypothesis using signal function
We first observe that, for some tolerance functions τos
±, Equa-
tion (1) is equivalent to stating that there exists a function
g:R|J|→[0,1]such that τos
g(X) :=g 
XJ
τos
+(X) + 
1−g 
XJ
τos
−(X)satisfies
EPrct
τrct(X)|XJ
=EPrct[τos
g(X)|XJ],Prct
XJ−a.s.
We test a slightly more restrictive hypothesis by assuming
thatglies in a sufficiently rich function class G:
HG
0:EPrct
τrct(X)|XJ
=EPrct
τos
g⋆(X)|XJ
,
for some g⋆∈ G,Prct
XJ−a.s.
In practice, one can either restrict Gto a particular func-
tion class if domain knowledge is available or use neural
networks as general function approximations.
We can then rewrite the null hypothesis above using a
signal function that captures the bias between the esti-
mates from observational and randomized data. Recall that
Z= (X, Y, T )is the vector of observed variables, we define
ψg(Z) =YT
π−1−T
1−π
−τos
g(X)
and finally arrive at the null hypothesis
HG
0:EPrct
ψg⋆(Z)|XJ
= 0, (2)
for some g⋆∈ G,Prct
XJ−a.s.
At first glance, testing the null hypothesis in Equation (2)
may seem equivalent to testing equality of conditional
means (Delgado, 1993; Neumeyer & Dette, 2003; Racine
et al., 2006; Luedtke et al., 2019; Muandet et al., 2020);
however, we remark that this equivalence holds only if the
function g⋆is known, and to our knowledge, the scenario
where g⋆is unknown has not been previously explored.3.2. Oracle test statistic
We now derive a kernelized test statistic for the null hypothe-
sis in Equation (2). First, we observe that the hypothesis HG
0
implies an infinite set of unconditional moment constraints,","conclusions
that align with established medical knowledge.
1. Introduction
Randomized trials have traditionally been the gold stan-
dard for informed decision-making in medicine, as they
allow for unbiased estimation of treatment effects under
mild assumptions. However, there is often a significant
discrepancy between the patients observed in clinical prac-
tice and those enrolled in randomized trials, limiting the
generalizability of the trial results (Rothwell, 2005; Duma
et al., 2018). To address this issue, the U.S. Food and Drug
Administration advocates for using observational data, as it
is usually more representative of the patient population in
clinical practice (Platt et al., 2018; Klonoff, 2020). Yet, a
major caveat to this recommendation is that several sources
of bias, including hidden confounding, can compromise the
1Department of Computer Science, ETH Zurich.
Correspondence to: Piersilvio de Bartolomeis <piersil-
vio.debartolomeis@inf.ethz.ch >.
Accepted at the 1st Machine Learning for Life and Material Sci-
ences Workshop at ICML 2024 . Copyright 2024 by the author(s).causal conclusions drawn from observational data.
In light of the inherent limitations of randomized and obser-
vational data, it has become a popular strategy to benchmark
observational studies against existing randomized trials to
assess their quality (Dahabreh et al., 2020; Forbes & Da-
habreh, 2020). The main idea behind this approach is first
to emulate the procedures adopted in the randomized trial
within the observational study; see e.g. (Hern ´an & Robins,
2016) for a detailed explanation. Then, the treatment effect
estimates from the observational data are compared with
those from the randomized data. If the estimates are similar,
we may be willing to trust the observational study for patient
populations where the randomized data is insufficient.
To support the benchmarking framework, several works
propose statistical tests that compare treatment effect esti-
mates between rando","Methodology In this section, we rewrite the null hypothesis from Equa- tion (1) in terms of a signal function that captures the bias be- tween osand rct. Then, we propose an oracle test statistic assuming that the tolerance functions os are known. Finally, we provide asymptotic guarantees for the finite-sample test statistic where the tolerance functions are estimated. 3.1. Null hypothesis using signal function We first observe that, for some tolerance functions os , Equa- tion (1) is equivalent to stating that there exists a function g:R|J| [0,1]such that os g(X) :=g  XJ os +(X) +  1 g  XJ os (X)satisfies EPrct rct(X)|XJ =EPrct[ os g(X)|XJ],Prct XJ a.s. We test a slightly more restrictive hypothesis by assuming thatglies in a sufficiently rich function class G: HG 0:EPrct rct(X)|XJ =EPrct os g (X)|XJ , for some g G,Prct XJ a.s. In practice, one can either restrict Gto a particular func- tion class if domain knowledge is available or use neural networks as general function approximations. We can then rewrite the null hypothesis above using a signal function that captures the bias between the esti- mates from observational and randomized data. Recall that Z= (X, Y, T )is the vector of observed variables, we define g(Z) =YT 1 T 1  os g(X) and finally arrive at the null hypothesis HG 0:EPrct g (Z)|XJ = 0, (2) for some g G,Prct XJ a.s. At first glance, testing the null hypothesis in Equation (2) may seem equivalent to testing equality of conditional means (Delgado, 1993; Neumeyer & Dette, 2003; Racine et al., 2006; Luedtke et al., 2019; Muandet et al., 2020); however, we remark that this equivalence holds only if the function g is known, and to our knowledge, the scenario where g is unknown has not been previously explored.3.2. Oracle test statistic We now derive a kernelized test statistic for the null hypothe- sis in Equation (2). First, we observe that the hypothesis HG 0 implies an infinite set of unconditional moment constraints,","conclusions that align with established medical knowledge. 1. Introduction Randomized trials have traditionally been the gold stan- dard for informed decision-making in medicine, as they allow for unbiased estimation of treatment effects under mild assumptions. However, there is often a significant discrepancy between the patients observed in clinical prac- tice and those enrolled in randomized trials, limiting the generalizability of the trial results (Rothwell, 2005; Duma et al., 2018). To address this issue, the U.S. Food and Drug Administration advocates for using observational data, as it is usually more representative of the patient population in clinical practice (Platt et al., 2018; Klonoff, 2020). Yet, a major caveat to this recommendation is that several sources of bias, including hidden confounding, can compromise the 1Department of Computer Science, ETH Zurich. Correspondence to: Piersilvio de Bartolomeis <piersil- vio.debartolomeis@inf.ethz.ch >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).causal conclusions drawn from observational data. In light of the inherent limitations of randomized and obser- vational data, it has become a popular strategy to benchmark observational studies against existing randomized trials to assess their quality (Dahabreh et al., 2020; Forbes & Da- habreh, 2020). The main idea behind this approach is first to emulate the procedures adopted in the randomized trial within the observational study; see e.g. (Hern an & Robins, 2016) for a detailed explanation. Then, the treatment effect estimates from the observational data are compared with those from the randomized data. If the estimates are similar, we may be willing to trust the observational study for patient populations where the randomized data is insufficient. To support the benchmarking framework, several works propose statistical tests that compare treatment effect esti- mates between rando","Detecting critical treatment effect bias in small subgroups Randomized trials are considered the gold standard for making informed decisions in medicine, yet they often lack generalizability to the patient populations in clinical practice. Observational studies, on the other hand, cover a broader patient population but are prone to various biases. Thus, before using an observational study for decision-making, it is crucial to benchmark its treatment effect estimates against those derived from a randomized trial. We propose a novel strategy to benchmark observational studies beyond the average treatment effect. First, we design a statistical test for the null hypothesis that the treatment effects estimated from the two studies, conditioned on a set of relevant features, differ up to some tolerance. We then estimate an asymptotically valid lower bound on the maximum bias strength for any subgroup in the observational study. Finally, we validate our benchmarking strategy in a real-world setting and show that it leads to conclusions that align with established medical knowledge.","Detecting critical treatment effect bias in small subgroups Randomized trials are considered the gold standard for making informed decisions in medicine, yet they often lack generalizability to the patient populations in clinical practice. Observational studies, on the other hand, cover a broader patient population but are prone to various biases. Thus, before using an observational study for decision-making, it is crucial to benchmark its treatment effect estimates against those derived from a randomized trial. We propose a novel strategy to benchmark observational studies beyond the average treatment effect. First, we design a statistical test for the null hypothesis that the treatment effects estimated from the two studies, conditioned on a set of relevant features, differ up to some tolerance. We then estimate an asymptotically valid lower bound on the maximum bias strength for any subgroup in the observational study. Finally, we validate our benchmarking strategy in a real-world setting and show that it leads to conclusions that align with established medical knowledge. conclusions that align with established medical knowledge. 1. Introduction Randomized trials have traditionally been the gold stan- dard for informed decision-making in medicine, as they allow for unbiased estimation of treatment effects under mild assumptions. However, there is often a significant discrepancy between the patients observed in clinical prac- tice and those enrolled in randomized trials, limiting the generalizability of the trial results (Rothwell, 2005; Duma et al., 2018). To address this issue, the U.S. Food and Drug Administration advocates for using observational data, as it is usually more representative of the patient population in clinical practice (Platt et al., 2018; Klonoff, 2020). Yet, a major caveat to this recommendation is that several sources of bias, including hidden confounding, can compromise the 1Department of Computer Science, ETH Zurich. Correspondence to: Piersilvio de Bartolomeis <piersil- vio.debartolomeis@inf.ethz.ch >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).causal conclusions drawn from observational data. In light of the inherent limitations of randomized and obser- vational data, it has become a popular strategy to benchmark observational studies against existing randomized trials to assess their quality (Dahabreh et al., 2020; Forbes & Da- habreh, 2020). The main idea behind this approach is first to emulate the procedures adopted in the randomized trial within the observational study; see e.g. (Hern an & Robins, 2016) for a detailed explanation. Then, the treatment effect estimates from the observational data are compared with those from the randomized data. If the estimates are similar, we may be willing to trust the observational study for patient populations where the randomized data is insufficient. To support the benchmarking framework, several works propose statistical tests that compare treatment effect esti- mates between rando Methodology In this section, we rewrite the null hypothesis from Equa- tion (1) in terms of a signal function that captures the bias be- tween osand rct. Then, we propose an oracle test statistic assuming that the tolerance functions os are known. Finally, we provide asymptotic guarantees for the finite-sample test statistic where the tolerance functions are estimated. 3.1. Null hypothesis using signal function We first observe that, for some tolerance functions os , Equa- tion (1) is equivalent to stating that there exists a function g:R|J| [0,1]such that os g(X) :=g  XJ os +(X) +  1 g  XJ os (X)satisfies EPrct rct(X)|XJ =EPrct[ os g(X)|XJ],Prct XJ a.s. We test a slightly more restrictive hypothesis by assuming thatglies in a sufficiently rich function class G: HG 0:EPrct rct(X)|XJ =EPrct os g (X)|XJ , for some g G,Prct XJ a.s. In practice, one can either restrict Gto a particular func- tion class if domain knowledge is available or use neural networks as general function approximations. We can then rewrite the null hypothesis above using a signal function that captures the bias between the esti- mates from observational and randomized data. Recall that Z= (X, Y, T )is the vector of observed variables, we define g(Z) =YT 1 T 1  os g(X) and finally arrive at the null hypothesis HG 0:EPrct g (Z)|XJ = 0, (2) for some g G,Prct XJ a.s. At first glance, testing the null hypothesis in Equation (2) may seem equivalent to testing equality of conditional means (Delgado, 1993; Neumeyer & Dette, 2003; Racine et al., 2006; Luedtke et al., 2019; Muandet et al., 2020); however, we remark that this equivalence holds only if the function g is known, and to our knowledge, the scenario where g is unknown has not been previously explored.3.2. Oracle test statistic We now derive a kernelized test statistic for the null hypothe- sis in Equation (2). First, we observe that the hypothesis HG 0 implies an infinite set of unconditional moment constraints,",0
ed6cad328f2a543a392506dd263a2d51cacdb756,Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge,"['Julien Delile', 'Srayanta Mukherjee', 'Anton Van Pamel', 'Leonid Zhukov']",https://openreview.net/pdf/ed6cad328f2a543a392506dd263a2d51cacdb756.pdf,"Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge Discovering new associations between biomedical entities - drugs, genes, diseases, with LLMs and knowledge graphs Large language models (LLMs) are transforming the way information is retrieved with vast amounts of knowledge being summarized and presented via natural language conversations. Yet, LLMs are prone to highlight the most frequently seen pieces of information from the training set and to neglect the rare ones. In biomedical research, latest discoveries are key to academic and industrial actors and are obscured by the abundance of an ever-increasing literature corpus (the information overload problem). Surfacing new associations between biomedical entities, e.g., drugs, genes, diseases, with LLMs becomes a challenge of capturing the long-tail knowledge of the biomedical scientific production. In this study, we show that typical RAG methods may leave out a significant proportion of relevant information due to clusters of over-represented concepts in the biomedical literature. We introduce a novel method that leverages a knowledge graph to down-sample these clusters and mitigate the information overload problem. Its retrieval performance is about twice better than embedding similarity alternatives on both precision and recall. Finally, we demonstrate that both embedding similarity and knowledge graph retrieval methods can be combined into a hybrid model that outperforms both, enabling potential improvements to biomedical question-answering models.",ed6cad328f2a543a392506dd263a2d51cacdb756.pdf,"approaches (Zhang
et al., 2023) and Retrieval-Augmentation Generation (RAG)
(Lewis et al., 2020). QA tasks, where specific text 'chunks '
need to be retrieved with high accuracy, require a broader
pull of information that cover a wider spectrum of the
query 's nuances. While the LLM 's reasoning capability
and increased context length enables the ability to respond
to more comprehensive queries, retrieving larger amount
of information into the synthesizer context presents a dis-
tinct challenge, e.g., multi-document question-answering
performance is degraded as the context grows longer (Liu
et al., 2023). While new architectures designed to deal
with very large context window may prevent performance
drops (Yu et al., 2023), an efficient selection of the most
relevant information would also reduce latency, cost, and
energy consumption. This biomedical research text cor-
pus also presents an information overload problem, where
rare and recent yet important information is dominated by
over-represented older concepts.
In this study, leaving aside the generative side of RAG, we
introduce a novel knowledge-graph-based retrieval approach
that enables access to the long tail of biomedical knowledge.
We demonstrate that RAG retrieval approaches, leave out
a significant proportion of relevant information because of
the data imbalance in a queried text corpus such as Pubmed.
Some over-represented topics can preclude the RAG synthe-
sizer to access more recent discoveries by monopolizing the
list of most similar text chunks. We propose to perform a
rebalancing of the retrieved text chunks by under-sampling
Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge
these larger clusters of information, and to do so by struc-
turing the text corpus with a Knowledge Graph (KG) of
biomedical entities (genes, diseases and diseases). In ad-
dition, our method also provides control mechanisms to
prioritize the retrieval of recent and impactful discoveries.
Finally, we built","conclusion that, in contrast to
ES IR, the data balancing mechanism of KG IR allows it
to go beyond the immediate surrounding of the question
neighborhood to retrieve relevant information thus facilitat-
ing the capture of the long-tail knowledge of biomedical
information.
3.3. ES and KG IR are highly complementary
To combine the strengths of both methods in order to max-
imize the retrieval performance, we used a new ranking
score averaging the min-max-normalized ES IR and KG IR
scores.
Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge
1
Copyright © 2023 by Boston Consulting Group. All rights reserved.
A
UMAP  1UMAP  2B
 D C
Text chunk
similarity withquestion embedding
LowHighSimilarity Landscape
Knowledge Graph IRCurated chunksQuestion embedding
Embedding Similarity IRNervous System
RespiratoryCardiovascular
Digestive SystemMental DisordersDisease area
Disease
Disease- GeneGene
Disease- DrugDrug
Gene -DrugText chunk entity
No entityRetrieval  Landscape
Figure 2. Characterization of differences between the ES IR and KG IR methods over the text embedding landscape. Each plot represents
the ˜731k 1536-dimensional text chunk embeddings in two dimensions via UMAP transformation.
Performing the same experiments as in 3.1., we observe that
Hybrid IR strongly outperforms both ES IR and KG IR for
smaller volume of retrieved information (K <100) and mod-
erately when the retrieval window increases beyond K=250
(Fig.1A-B). Both recall and precision are about twice higher
for Hybrid IR compared to KG IR for K=50. This indicates
that each base retrieval method provides a complementary
mechanism: data rebalancing from KG IR is not sufficient to
identify the most relevant pieces of information and benefits
from adding the semantic filter provided by ES.
4. Conclusion
To aid contextual synthesis by LLMs, IR plays a pre-eminent
role to maintain a balanced and unbiased selection of re-
trieved information which entails extracting the long tail
of biomedical infor","approaches (Zhang et al., 2023) and Retrieval-Augmentation Generation (RAG) (Lewis et al., 2020). QA tasks, where specific text 'chunks ' need to be retrieved with high accuracy, require a broader pull of information that cover a wider spectrum of the query 's nuances. While the LLM 's reasoning capability and increased context length enables the ability to respond to more comprehensive queries, retrieving larger amount of information into the synthesizer context presents a dis- tinct challenge, e.g., multi-document question-answering performance is degraded as the context grows longer (Liu et al., 2023). While new architectures designed to deal with very large context window may prevent performance drops (Yu et al., 2023), an efficient selection of the most relevant information would also reduce latency, cost, and energy consumption. This biomedical research text cor- pus also presents an information overload problem, where rare and recent yet important information is dominated by over-represented older concepts. In this study, leaving aside the generative side of RAG, we introduce a novel knowledge-graph-based retrieval approach that enables access to the long tail of biomedical knowledge. We demonstrate that RAG retrieval approaches, leave out a significant proportion of relevant information because of the data imbalance in a queried text corpus such as Pubmed. Some over-represented topics can preclude the RAG synthe- sizer to access more recent discoveries by monopolizing the list of most similar text chunks. We propose to perform a rebalancing of the retrieved text chunks by under-sampling Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge these larger clusters of information, and to do so by struc- turing the text corpus with a Knowledge Graph (KG) of biomedical entities (genes, diseases and diseases). In ad- dition, our method also provides control mechanisms to prioritize the retrieval of recent and impactful discoveries. Finally, we built","conclusion that, in contrast to ES IR, the data balancing mechanism of KG IR allows it to go beyond the immediate surrounding of the question neighborhood to retrieve relevant information thus facilitat- ing the capture of the long-tail knowledge of biomedical information. 3.3. ES and KG IR are highly complementary To combine the strengths of both methods in order to max- imize the retrieval performance, we used a new ranking score averaging the min-max-normalized ES IR and KG IR scores. Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge 1 Copyright 2023 by Boston Consulting Group. All rights reserved. A UMAP 1UMAP 2B D C Text chunk similarity withquestion embedding LowHighSimilarity Landscape Knowledge Graph IRCurated chunksQuestion embedding Embedding Similarity IRNervous System RespiratoryCardiovascular Digestive SystemMental DisordersDisease area Disease Disease- GeneGene Disease- DrugDrug Gene -DrugText chunk entity No entityRetrieval Landscape Figure 2. Characterization of differences between the ES IR and KG IR methods over the text embedding landscape. Each plot represents the 731k 1536-dimensional text chunk embeddings in two dimensions via UMAP transformation. Performing the same experiments as in 3.1., we observe that Hybrid IR strongly outperforms both ES IR and KG IR for smaller volume of retrieved information (K <100) and mod- erately when the retrieval window increases beyond K=250 (Fig.1A-B). Both recall and precision are about twice higher for Hybrid IR compared to KG IR for K=50. This indicates that each base retrieval method provides a complementary mechanism: data rebalancing from KG IR is not sufficient to identify the most relevant pieces of information and benefits from adding the semantic filter provided by ES. 4. Conclusion To aid contextual synthesis by LLMs, IR plays a pre-eminent role to maintain a balanced and unbiased selection of re- trieved information which entails extracting the long tail of biomedical infor","Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge Discovering new associations between biomedical entities - drugs, genes, diseases, with LLMs and knowledge graphs Large language models (LLMs) are transforming the way information is retrieved with vast amounts of knowledge being summarized and presented via natural language conversations. Yet, LLMs are prone to highlight the most frequently seen pieces of information from the training set and to neglect the rare ones. In biomedical research, latest discoveries are key to academic and industrial actors and are obscured by the abundance of an ever-increasing literature corpus (the information overload problem). Surfacing new associations between biomedical entities, e.g., drugs, genes, diseases, with LLMs becomes a challenge of capturing the long-tail knowledge of the biomedical scientific production. In this study, we show that typical RAG methods may leave out a significant proportion of relevant information due to clusters of over-represented concepts in the biomedical literature. We introduce a novel method that leverages a knowledge graph to down-sample these clusters and mitigate the information overload problem. Its retrieval performance is about twice better than embedding similarity alternatives on both precision and recall. Finally, we demonstrate that both embedding similarity and knowledge graph retrieval methods can be combined into a hybrid model that outperforms both, enabling potential improvements to biomedical question-answering models.","Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge Discovering new associations between biomedical entities - drugs, genes, diseases, with LLMs and knowledge graphs Large language models (LLMs) are transforming the way information is retrieved with vast amounts of knowledge being summarized and presented via natural language conversations. Yet, LLMs are prone to highlight the most frequently seen pieces of information from the training set and to neglect the rare ones. In biomedical research, latest discoveries are key to academic and industrial actors and are obscured by the abundance of an ever-increasing literature corpus (the information overload problem). Surfacing new associations between biomedical entities, e.g., drugs, genes, diseases, with LLMs becomes a challenge of capturing the long-tail knowledge of the biomedical scientific production. In this study, we show that typical RAG methods may leave out a significant proportion of relevant information due to clusters of over-represented concepts in the biomedical literature. We introduce a novel method that leverages a knowledge graph to down-sample these clusters and mitigate the information overload problem. Its retrieval performance is about twice better than embedding similarity alternatives on both precision and recall. Finally, we demonstrate that both embedding similarity and knowledge graph retrieval methods can be combined into a hybrid model that outperforms both, enabling potential improvements to biomedical question-answering models. conclusion that, in contrast to ES IR, the data balancing mechanism of KG IR allows it to go beyond the immediate surrounding of the question neighborhood to retrieve relevant information thus facilitat- ing the capture of the long-tail knowledge of biomedical information. 3.3. ES and KG IR are highly complementary To combine the strengths of both methods in order to max- imize the retrieval performance, we used a new ranking score averaging the min-max-normalized ES IR and KG IR scores. Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge 1 Copyright 2023 by Boston Consulting Group. All rights reserved. A UMAP 1UMAP 2B D C Text chunk similarity withquestion embedding LowHighSimilarity Landscape Knowledge Graph IRCurated chunksQuestion embedding Embedding Similarity IRNervous System RespiratoryCardiovascular Digestive SystemMental DisordersDisease area Disease Disease- GeneGene Disease- DrugDrug Gene -DrugText chunk entity No entityRetrieval Landscape Figure 2. Characterization of differences between the ES IR and KG IR methods over the text embedding landscape. Each plot represents the 731k 1536-dimensional text chunk embeddings in two dimensions via UMAP transformation. Performing the same experiments as in 3.1., we observe that Hybrid IR strongly outperforms both ES IR and KG IR for smaller volume of retrieved information (K <100) and mod- erately when the retrieval window increases beyond K=250 (Fig.1A-B). Both recall and precision are about twice higher for Hybrid IR compared to KG IR for K=50. This indicates that each base retrieval method provides a complementary mechanism: data rebalancing from KG IR is not sufficient to identify the most relevant pieces of information and benefits from adding the semantic filter provided by ES. 4. Conclusion To aid contextual synthesis by LLMs, IR plays a pre-eminent role to maintain a balanced and unbiased selection of re- trieved information which entails extracting the long tail of biomedical infor approaches (Zhang et al., 2023) and Retrieval-Augmentation Generation (RAG) (Lewis et al., 2020). QA tasks, where specific text 'chunks ' need to be retrieved with high accuracy, require a broader pull of information that cover a wider spectrum of the query 's nuances. While the LLM 's reasoning capability and increased context length enables the ability to respond to more comprehensive queries, retrieving larger amount of information into the synthesizer context presents a dis- tinct challenge, e.g., multi-document question-answering performance is degraded as the context grows longer (Liu et al., 2023). While new architectures designed to deal with very large context window may prevent performance drops (Yu et al., 2023), an efficient selection of the most relevant information would also reduce latency, cost, and energy consumption. This biomedical research text cor- pus also presents an information overload problem, where rare and recent yet important information is dominated by over-represented older concepts. In this study, leaving aside the generative side of RAG, we introduce a novel knowledge-graph-based retrieval approach that enables access to the long tail of biomedical knowledge. We demonstrate that RAG retrieval approaches, leave out a significant proportion of relevant information because of the data imbalance in a queried text corpus such as Pubmed. Some over-represented topics can preclude the RAG synthe- sizer to access more recent discoveries by monopolizing the list of most similar text chunks. We propose to perform a rebalancing of the retrieved text chunks by under-sampling Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge these larger clusters of information, and to do so by struc- turing the text corpus with a Knowledge Graph (KG) of biomedical entities (genes, diseases and diseases). In ad- dition, our method also provides control mechanisms to prioritize the retrieval of recent and impactful discoveries. Finally, we built",0
1f184552c1866dee70bbd9601eb158140fc7649c,Predicting metal-protein interactions using cofolding methods: Status quo,"['Simon L. Dürr', 'Ursula Rothlisberger']",https://openreview.net/pdf/1f184552c1866dee70bbd9601eb158140fc7649c.pdf,"Predicting metal-protein interactions using cofolding methods: Status quo We analyze how well AlphaFold3 and RoseTTAfold-All Atom handle metal ions finding that only AlphaFold3 provides realistic predictions on par with more specialized models. Metals play important roles for enzyme function and many therapeutically relevant proteins. Despite the fact that the first drugs developed via computer aided drug design were metalloprotein inhibitors, many computational pipelines still discard metalloproteins due to the difficulties of modelling them computationally. New ""cofolding"" methods such as AlphaFold3 (AF3) and RoseTTAfold-AllAtom (RFAA) promise to improve this issue by being able to dock small molecules in presence of multiple complex cofactors including metals or covalent modifications. Here, we analyze the current status for metal ion prediction using these methods. We find that currently only AF3 provides realistic predictions for metal ions, RFAA in contrast does perform worse than more specialized models such as AllMetal3D in predicting the location of metal ions accurately. We find that AF3 predictions are consistent with expected physico-chemical trends/intuition whereas RFAA often also predicts unrealistic metal ion locations.",1f184552c1866dee70bbd9601eb158140fc7649c.pdf,"methods: Status quo
Anonymous Authors1
Abstract
Metals play important roles for enzyme func-
tion and many therapeutically relevant proteins.
Despite the fact that the first drugs developed
via computer aided drug design were metallo-
protein inhibitors, many computational pipelines
still discard metalloproteins due to the difficulties
of modelling them computationally. New ”co-
folding” methods such as AlphaFold3 (AF3)[1]
and RoseTTAfold-AllAtom (RFAA)[2] promise
to improve this issue by being able to dock small
molecules in presence of multiple complex co-
factors including metals or covalent modifica-
tions. Here, we analyze the current status for
metal ion prediction using these methods. We
find that currently only AF3 provides realistic
predictions for metal ions, RFAA in contrast does
perform worse than more specialized models such
as AllMetal3D in predicting the location of metal
ions accurately. We find that AF3 predictions
are consistent with expected physico-chemical
trends/intuition whereas RFAA often also predicts
unrealistic metal ion locations.
1. Introduction
Metals are versatile and indispensable cofactors for many
proteins and a lot of DNA/RNA chemistry[3]. A major cate-
gory of biological metals are transition metals such as zinc
(used in enzymes or for structural stability such as e.g in
zinc finger domains or as Lewis acid in catalysis), as well as
iron and copper (e.g for electron transport). Earth alkali ions
such as magnesium (used in ATP and nucleic acid chemistry)
and calcium (used for signal transduction or coagulation)
also play important roles. About 10% of enzyme reactions
depend on zinc alone [4]. For this reason many metallopro-
teins are also therapeutically relevant targets. Among the
first drugs developed using structure based drug design were
1Anonymous Institution, Anonymous City, Anonymous Region,
Anonymous Country. Correspondence to: Anonymous Author
<anon.email@domain.com >.
Preliminary work. Under review by the Machine Learnin","Conclusion
AlphaFold3 demonstrates that methods to predict the struc-
ture of a protein together with its metal ion ligands now
are at the level of more specialized predictors such as
AllMetal3D if the stoichiometry of binding is known.
RoseTTAfold All-Atom does not perform at the same level
likely due some design choices when defining the loss func-
tions used to train the model. The sensitivity of AlphaFold3
to few point mutations in the metal-coordinating residues is
encouraging. At the same time limitations due to the PDB
containing mainly ordered proteins are also evident even for
relatively simple proteins where the metal ion mediates the
conformational flexibility or oligomeric state of proteins.
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329Cofolding with metal ions
6. Data Availability
All predictions and scripts to generate figures will be made
available on Zenodo. RFAA predictions will be available
under CC BY , Code under MIT. AF3 predictions will be
available under AlphaFold Server Terms.
References
(1) Abramson, J. et al. Nature 2024 , Publisher: Nature
Publishing Group, 1–3.
(2) Krishna, R. et al. Science 2024 ,384, eadl2528.
(3) Berg, J. M.; Tymoczko, J. L.; Gatto jr., G. J.; Stryer,
L.,Stryer Biochemie , 8th ed.; Springer-Verlag GmbH:
2017.
(4) Andreini, C.; Bertini, I.; Cavallaro, G.; Holliday,
G. L.; Thornton, J. M. J. Biol. Inorg. Chem. 2008 ,13,
DOI: 10.1007/s00775-008-0404-5 .
(5) Cushman, D. W.; Cheung, H. S.; Sabo, E. F.; Ondetti,
M. A. Biochemistry 1977 ,16, Publisher: American
Chemical Society, 5484–5491.
(6) Navia, M. A.; Peattie, D. A. Trends in Pharmacologi-
cal Sciences 1993 ,14, 189–195.
(7) Brunk, E.; Rothlisberger, U. Chem. Rev. 2015 ,115,
DOI: 10.1021/cr500628b .
(8) Melse, O.; Antes, I.; Kaila, V . R. I.;
Zacharias, M. Journal of Computa-
tional Chemistry 2023 ,","methods: Status quo Anonymous Authors1 Abstract Metals play important roles for enzyme func- tion and many therapeutically relevant proteins. Despite the fact that the first drugs developed via computer aided drug design were metallo- protein inhibitors, many computational pipelines still discard metalloproteins due to the difficulties of modelling them computationally. New co- folding methods such as AlphaFold3 (AF3)[1] and RoseTTAfold-AllAtom (RFAA)[2] promise to improve this issue by being able to dock small molecules in presence of multiple complex co- factors including metals or covalent modifica- tions. Here, we analyze the current status for metal ion prediction using these methods. We find that currently only AF3 provides realistic predictions for metal ions, RFAA in contrast does perform worse than more specialized models such as AllMetal3D in predicting the location of metal ions accurately. We find that AF3 predictions are consistent with expected physico-chemical trends/intuition whereas RFAA often also predicts unrealistic metal ion locations. 1. Introduction Metals are versatile and indispensable cofactors for many proteins and a lot of DNA/RNA chemistry[3]. A major cate- gory of biological metals are transition metals such as zinc (used in enzymes or for structural stability such as e.g in zinc finger domains or as Lewis acid in catalysis), as well as iron and copper (e.g for electron transport). Earth alkali ions such as magnesium (used in ATP and nucleic acid chemistry) and calcium (used for signal transduction or coagulation) also play important roles. About 10% of enzyme reactions depend on zinc alone [4]. For this reason many metallopro- teins are also therapeutically relevant targets. Among the first drugs developed using structure based drug design were 1Anonymous Institution, Anonymous City, Anonymous Region, Anonymous Country. Correspondence to: Anonymous Author <anon.email@domain.com >. Preliminary work. Under review by the Machine Learnin","Conclusion AlphaFold3 demonstrates that methods to predict the struc- ture of a protein together with its metal ion ligands now are at the level of more specialized predictors such as AllMetal3D if the stoichiometry of binding is known. RoseTTAfold All-Atom does not perform at the same level likely due some design choices when defining the loss func- tions used to train the model. The sensitivity of AlphaFold3 to few point mutations in the metal-coordinating residues is encouraging. At the same time limitations due to the PDB containing mainly ordered proteins are also evident even for relatively simple proteins where the metal ion mediates the conformational flexibility or oligomeric state of proteins. 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329Cofolding with metal ions 6. Data Availability All predictions and scripts to generate figures will be made available on Zenodo. RFAA predictions will be available under CC BY , Code under MIT. AF3 predictions will be available under AlphaFold Server Terms.","Predicting metal-protein interactions using cofolding methods: Status quo We analyze how well AlphaFold3 and RoseTTAfold-All Atom handle metal ions finding that only AlphaFold3 provides realistic predictions on par with more specialized models. Metals play important roles for enzyme function and many therapeutically relevant proteins. Despite the fact that the first drugs developed via computer aided drug design were metalloprotein inhibitors, many computational pipelines still discard metalloproteins due to the difficulties of modelling them computationally. New ""cofolding"" methods such as AlphaFold3 (AF3) and RoseTTAfold-AllAtom (RFAA) promise to improve this issue by being able to dock small molecules in presence of multiple complex cofactors including metals or covalent modifications. Here, we analyze the current status for metal ion prediction using these methods. We find that currently only AF3 provides realistic predictions for metal ions, RFAA in contrast does perform worse than more specialized models such as AllMetal3D in predicting the location of metal ions accurately. We find that AF3 predictions are consistent with expected physico-chemical trends/intuition whereas RFAA often also predicts unrealistic metal ion locations.","Predicting metal-protein interactions using cofolding methods: Status quo We analyze how well AlphaFold3 and RoseTTAfold-All Atom handle metal ions finding that only AlphaFold3 provides realistic predictions on par with more specialized models. Metals play important roles for enzyme function and many therapeutically relevant proteins. Despite the fact that the first drugs developed via computer aided drug design were metalloprotein inhibitors, many computational pipelines still discard metalloproteins due to the difficulties of modelling them computationally. New ""cofolding"" methods such as AlphaFold3 (AF3) and RoseTTAfold-AllAtom (RFAA) promise to improve this issue by being able to dock small molecules in presence of multiple complex cofactors including metals or covalent modifications. Here, we analyze the current status for metal ion prediction using these methods. We find that currently only AF3 provides realistic predictions for metal ions, RFAA in contrast does perform worse than more specialized models such as AllMetal3D in predicting the location of metal ions accurately. We find that AF3 predictions are consistent with expected physico-chemical trends/intuition whereas RFAA often also predicts unrealistic metal ion locations. Conclusion AlphaFold3 demonstrates that methods to predict the struc- ture of a protein together with its metal ion ligands now are at the level of more specialized predictors such as AllMetal3D if the stoichiometry of binding is known. RoseTTAfold All-Atom does not perform at the same level likely due some design choices when defining the loss func- tions used to train the model. The sensitivity of AlphaFold3 to few point mutations in the metal-coordinating residues is encouraging. At the same time limitations due to the PDB containing mainly ordered proteins are also evident even for relatively simple proteins where the metal ion mediates the conformational flexibility or oligomeric state of proteins. 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329Cofolding with metal ions 6. Data Availability All predictions and scripts to generate figures will be made available on Zenodo. RFAA predictions will be available under CC BY , Code under MIT. AF3 predictions will be available under AlphaFold Server Terms. methods: Status quo Anonymous Authors1 Abstract Metals play important roles for enzyme func- tion and many therapeutically relevant proteins. Despite the fact that the first drugs developed via computer aided drug design were metallo- protein inhibitors, many computational pipelines still discard metalloproteins due to the difficulties of modelling them computationally. New co- folding methods such as AlphaFold3 (AF3)[1] and RoseTTAfold-AllAtom (RFAA)[2] promise to improve this issue by being able to dock small molecules in presence of multiple complex co- factors including metals or covalent modifica- tions. Here, we analyze the current status for metal ion prediction using these methods. We find that currently only AF3 provides realistic predictions for metal ions, RFAA in contrast does perform worse than more specialized models such as AllMetal3D in predicting the location of metal ions accurately. We find that AF3 predictions are consistent with expected physico-chemical trends/intuition whereas RFAA often also predicts unrealistic metal ion locations. 1. Introduction Metals are versatile and indispensable cofactors for many proteins and a lot of DNA/RNA chemistry[3]. A major cate- gory of biological metals are transition metals such as zinc (used in enzymes or for structural stability such as e.g in zinc finger domains or as Lewis acid in catalysis), as well as iron and copper (e.g for electron transport). Earth alkali ions such as magnesium (used in ATP and nucleic acid chemistry) and calcium (used for signal transduction or coagulation) also play important roles. About 10% of enzyme reactions depend on zinc alone [4]. For this reason many metallopro- teins are also therapeutically relevant targets. Among the first drugs developed using structure based drug design were 1Anonymous Institution, Anonymous City, Anonymous Region, Anonymous Country. Correspondence to: Anonymous Author <anon.email@domain.com >. Preliminary work. Under review by the Machine Learnin",0
c453ff086a56b4a28950335cb48519a976c255f2,Batch-effect invariant graph neural networks for predicting chemotherapy response in triple-negative breast cancer patients,"['Asif Khan', 'Giuseppe Torrisi', 'Luciana Luque', 'Claudia Owczarek', 'Maddy Parsons', 'Chris Sander', 'Linus Schumacher']",https://openreview.net/pdf/c453ff086a56b4a28950335cb48519a976c255f2.pdf,"Batch-effect invariant graph neural networks for predicting chemotherapy response in triple-negative breast cancer patients This paper introduces a machine learning framework using imaging mass cytometry data and GNN to predict chemotherapy response in TNBC, addressing batch effects and integrating protein expression with spatial cell relationships for improved accuracy. Triple-negative breast cancer (TNBC) is a particularly aggressive subtype of breast cancer that is usually treated with chemotherapy. However, the effectiveness of the treatment can vary widely. Accurate prediction of the response to chemotherapy is crucial in preparing effective personalized treatment. This paper introduces a machine learning framework that uses imaging mass cytometry (IMC) data from clinical trials to train graph neural networks (GNNs) to predict whether a patient will respond to chemotherapy. Our approach combines single-cell protein expression and spatial cell-cell contact information extracted from IMC images. To account for staining variability known as batch effects, we introduce a surrogate loss function that enables learning of a representation space predictive of response, yet invariant to batch artefacts. We investigate different graph construction methods (k-nearest neighbors, k-atmost neighbors, Delaunay triangulation) to capture cell-cell contact delineating tumor microenvironment. Our framework demonstrates improved predictive performance through batch effect correction and effective integration of protein expression with spatial cellular relationships.",c453ff086a56b4a28950335cb48519a976c255f2.pdf,"Methodology
We start by preprocessing IMC images to extract protein
expression profiles and centroids of individual cells. Then,
we introduce our method that uses a GNN to predict whether
patients will respond to chemotherapy, while also removing
batch effects using a surrogate optimization objective. The
complete framework for predicting the response of TNBC
patients to chemotherapy based on IMC imaging data is
illustrated in Figure 1. Next, we introduce the real-world
dataset of IMC images utilized in our experiments.
TNBC IMC Dataset We use a real-world dataset gener-
ated as part of the Wellcome Leap Delta Tissue program1,
mostly from retrospective samples from a large number
of patients (via biobanking consent) and a few from the
FORCE clinical trial. A cohort of patients diagnosed with
triple negative breast cancer (TNBC) was recruited and tis-
sue biopsy taken before starting neoadjuvant chemotherapy.
A pathologist identified a set of regions of interest (ROIs)
based on H&E staining, which are then analyzed with IMC.
Here, IMC measures protein abundance using a panel of
35 metal-tagged antibody markers for tumoral, immune,
and stromal cells (CD3, CD4, CD8a, CD11b, CD14, CD16,
CD20, CD27, CD31, CD38, CD44, CD45, CD45RO, CD68,
CD107a, CD163, CD366, Beta-Catenin, E-Cadherin, Pan-
1https://wellcomeleap.org/delta-tissue/
TNBC Response Prediction to Chemotherapy
Keratin, Vimentin, Tbet, FOXP3, HLA-DR-DQ-DP, Alpha-
SMA, Granzyme-B, B7-H4, Ki-67, PD1, PD-L1, PD-L2,
p53, Collagen Type I, EGFR, VEGF) as well as two anti-
bodies for DNA2. This results in an image with 35 channels
(one for each protein marker).
Patients are classified based on their residual cancer bur-
den (RCB) score at the end of the treatment. Here, we
adopt a binary label for the patient response, defining a
pathological, complete responder (pCR) if the RCB is 0and
non-responder otherwise (nR). We implement strict quality
control metrics on cells and remove ROIs with less than
1000 cells. From 445 R","future work, we aim
to combine our predictions with clinicians to take a step
towards the effective translation of IMC-derived insights
into clinical practice.
4. Conclusion
This paper introduced a GNN-based approach for predicting
chemotherapy response in TNBC patients using a novel IMC
dataset. A key challenge addressed was mitigating batch
effects from staining variability that can obscure biologically
relevant signals. We proposed a surrogate objective function
implemented via gradient reversal, allowing the model to
learn a representation space predictive of response while
invariant to batch artefacts. Our framework demonstrates the
potential of GNNs in using spatially-resolved IMC data for
accurate response prediction by integrating spatial context
and protein expression features within a unified graph-based
framework. This approach offers a promising avenue for
advancing personalized treatment strategies and improving
clinical outcomes for TNBC patients through effectively
integrating multichannel imaging data.
TNBC Response Prediction to Chemotherapy
References
Bianchini, G., Balko, J. M., Mayer, I. A., Sanders, M. E.,
and Gianni, L. Triple-negative breast cancer: challenges
and opportunities of a heterogeneous disease. Nature
reviews Clinical oncology , 13(11):674–690, 2016.
Fey, M. and Lenssen, J. E. Fast graph representation learning
with pytorch geometric. arXiv preprint arXiv:1903.02428 ,
2019.
Giesen, C., Wang, H. A., Schapiro, D., Zivanovic, N., Ja-
cobs, A., Hattendorf, B., Sch ¨uffler, P. J., Grolimund, D.,
Buhmann, J. M., Brandt, S., et al. Highly multiplexed
imaging of tumor tissues with subcellular resolution by
mass cytometry. Nature methods , 11(4):417–422, 2014.
Glasson, Y ., Ch ´epeaux, L.-A., Dum ´e, A.-S., Lafont, V .,
Faget, J., Bonnefoy, N., and Michaud, H.-A. Single-
cell high-dimensional imaging mass cytometry: one step
beyond in oncology. In Seminars in Immunopathology ,
volume 45, pp. 17–28. Springer, 2023.
Greenwald, N. F., Miller, G","Methodology We start by preprocessing IMC images to extract protein expression profiles and centroids of individual cells. Then, we introduce our method that uses a GNN to predict whether patients will respond to chemotherapy, while also removing batch effects using a surrogate optimization objective. The complete framework for predicting the response of TNBC patients to chemotherapy based on IMC imaging data is illustrated in Figure 1. Next, we introduce the real-world dataset of IMC images utilized in our experiments. TNBC IMC Dataset We use a real-world dataset gener- ated as part of the Wellcome Leap Delta Tissue program1, mostly from retrospective samples from a large number of patients (via biobanking consent) and a few from the FORCE clinical trial. A cohort of patients diagnosed with triple negative breast cancer (TNBC) was recruited and tis- sue biopsy taken before starting neoadjuvant chemotherapy. A pathologist identified a set of regions of interest (ROIs) based on H&E staining, which are then analyzed with IMC. Here, IMC measures protein abundance using a panel of 35 metal-tagged antibody markers for tumoral, immune, and stromal cells (CD3, CD4, CD8a, CD11b, CD14, CD16, CD20, CD27, CD31, CD38, CD44, CD45, CD45RO, CD68, CD107a, CD163, CD366, Beta-Catenin, E-Cadherin, Pan- 1https://wellcomeleap.org/delta-tissue/ TNBC Response Prediction to Chemotherapy Keratin, Vimentin, Tbet, FOXP3, HLA-DR-DQ-DP, Alpha- SMA, Granzyme-B, B7-H4, Ki-67, PD1, PD-L1, PD-L2, p53, Collagen Type I, EGFR, VEGF) as well as two anti- bodies for DNA2. This results in an image with 35 channels (one for each protein marker). Patients are classified based on their residual cancer bur- den (RCB) score at the end of the treatment. Here, we adopt a binary label for the patient response, defining a pathological, complete responder (pCR) if the RCB is 0and non-responder otherwise (nR). We implement strict quality control metrics on cells and remove ROIs with less than 1000 cells. From 445 R","future work, we aim to combine our predictions with clinicians to take a step towards the effective translation of IMC-derived insights into clinical practice. 4. Conclusion This paper introduced a GNN-based approach for predicting chemotherapy response in TNBC patients using a novel IMC dataset. A key challenge addressed was mitigating batch effects from staining variability that can obscure biologically relevant signals. We proposed a surrogate objective function implemented via gradient reversal, allowing the model to learn a representation space predictive of response while invariant to batch artefacts. Our framework demonstrates the potential of GNNs in using spatially-resolved IMC data for accurate response prediction by integrating spatial context and protein expression features within a unified graph-based framework. This approach offers a promising avenue for advancing personalized treatment strategies and improving clinical outcomes for TNBC patients through effectively integrating multichannel imaging data. TNBC Response Prediction to Chemotherapy","Batch-effect invariant graph neural networks for predicting chemotherapy response in triple-negative breast cancer patients This paper introduces a machine learning framework using imaging mass cytometry data and GNN to predict chemotherapy response in TNBC, addressing batch effects and integrating protein expression with spatial cell relationships for improved accuracy. Triple-negative breast cancer (TNBC) is a particularly aggressive subtype of breast cancer that is usually treated with chemotherapy. However, the effectiveness of the treatment can vary widely. Accurate prediction of the response to chemotherapy is crucial in preparing effective personalized treatment. This paper introduces a machine learning framework that uses imaging mass cytometry (IMC) data from clinical trials to train graph neural networks (GNNs) to predict whether a patient will respond to chemotherapy. Our approach combines single-cell protein expression and spatial cell-cell contact information extracted from IMC images. To account for staining variability known as batch effects, we introduce a surrogate loss function that enables learning of a representation space predictive of response, yet invariant to batch artefacts. We investigate different graph construction methods (k-nearest neighbors, k-atmost neighbors, Delaunay triangulation) to capture cell-cell contact delineating tumor microenvironment. Our framework demonstrates improved predictive performance through batch effect correction and effective integration of protein expression with spatial cellular relationships.","Batch-effect invariant graph neural networks for predicting chemotherapy response in triple-negative breast cancer patients This paper introduces a machine learning framework using imaging mass cytometry data and GNN to predict chemotherapy response in TNBC, addressing batch effects and integrating protein expression with spatial cell relationships for improved accuracy. Triple-negative breast cancer (TNBC) is a particularly aggressive subtype of breast cancer that is usually treated with chemotherapy. However, the effectiveness of the treatment can vary widely. Accurate prediction of the response to chemotherapy is crucial in preparing effective personalized treatment. This paper introduces a machine learning framework that uses imaging mass cytometry (IMC) data from clinical trials to train graph neural networks (GNNs) to predict whether a patient will respond to chemotherapy. Our approach combines single-cell protein expression and spatial cell-cell contact information extracted from IMC images. To account for staining variability known as batch effects, we introduce a surrogate loss function that enables learning of a representation space predictive of response, yet invariant to batch artefacts. We investigate different graph construction methods (k-nearest neighbors, k-atmost neighbors, Delaunay triangulation) to capture cell-cell contact delineating tumor microenvironment. Our framework demonstrates improved predictive performance through batch effect correction and effective integration of protein expression with spatial cellular relationships. future work, we aim to combine our predictions with clinicians to take a step towards the effective translation of IMC-derived insights into clinical practice. 4. Conclusion This paper introduced a GNN-based approach for predicting chemotherapy response in TNBC patients using a novel IMC dataset. A key challenge addressed was mitigating batch effects from staining variability that can obscure biologically relevant signals. We proposed a surrogate objective function implemented via gradient reversal, allowing the model to learn a representation space predictive of response while invariant to batch artefacts. Our framework demonstrates the potential of GNNs in using spatially-resolved IMC data for accurate response prediction by integrating spatial context and protein expression features within a unified graph-based framework. This approach offers a promising avenue for advancing personalized treatment strategies and improving clinical outcomes for TNBC patients through effectively integrating multichannel imaging data. TNBC Response Prediction to Chemotherapy Methodology We start by preprocessing IMC images to extract protein expression profiles and centroids of individual cells. Then, we introduce our method that uses a GNN to predict whether patients will respond to chemotherapy, while also removing batch effects using a surrogate optimization objective. The complete framework for predicting the response of TNBC patients to chemotherapy based on IMC imaging data is illustrated in Figure 1. Next, we introduce the real-world dataset of IMC images utilized in our experiments. TNBC IMC Dataset We use a real-world dataset gener- ated as part of the Wellcome Leap Delta Tissue program1, mostly from retrospective samples from a large number of patients (via biobanking consent) and a few from the FORCE clinical trial. A cohort of patients diagnosed with triple negative breast cancer (TNBC) was recruited and tis- sue biopsy taken before starting neoadjuvant chemotherapy. A pathologist identified a set of regions of interest (ROIs) based on H&E staining, which are then analyzed with IMC. Here, IMC measures protein abundance using a panel of 35 metal-tagged antibody markers for tumoral, immune, and stromal cells (CD3, CD4, CD8a, CD11b, CD14, CD16, CD20, CD27, CD31, CD38, CD44, CD45, CD45RO, CD68, CD107a, CD163, CD366, Beta-Catenin, E-Cadherin, Pan- 1https://wellcomeleap.org/delta-tissue/ TNBC Response Prediction to Chemotherapy Keratin, Vimentin, Tbet, FOXP3, HLA-DR-DQ-DP, Alpha- SMA, Granzyme-B, B7-H4, Ki-67, PD1, PD-L1, PD-L2, p53, Collagen Type I, EGFR, VEGF) as well as two anti- bodies for DNA2. This results in an image with 35 channels (one for each protein marker). Patients are classified based on their residual cancer bur- den (RCB) score at the end of the treatment. Here, we adopt a binary label for the patient response, defining a pathological, complete responder (pCR) if the RCB is 0and non-responder otherwise (nR). We implement strict quality control metrics on cells and remove ROIs with less than 1000 cells. From 445 R",0
b2eb11c3441d940de862e7e81dff696a779cb5db,Doob's Lagrangian: A Sample-Efficient Variational Approach to Transition Path Sampling,"['Yuanqi Du', 'Michael Plainer', 'Rob Brekelmans', 'Chenru Duan', 'Frank Noe', 'Carla P Gomes', 'Alan Aspuru-Guzik', 'Kirill Neklyudov']",https://openreview.net/pdf/b2eb11c3441d940de862e7e81dff696a779cb5db.pdf,"Doob's Lagrangian: A Sample-Efficient Variational Approach to Transition Path Sampling Lagrangian formulation of Doob's h-transform allowing for an efficient rare event sampling Rare event sampling in dynamical systems is a fundamental problem arising in the natural sciences, which poses significant computational challenges due to an exponentially large space of trajectories. For settings where the dynamical system of interest follows a Brownian motion with known drift, the question of conditioning the process to reach a given endpoint or desired rare event is definitively answered by Doob's -transform. However, the naive simulation of this transform is infeasible, as it requires sufficiently many forward trajectories to estimate rare event probabilities. In this work, we propose a variational formulation of Doob's -transform --- an optimization problem over trajectories between a given initial point and the desired ending point. To solve this optimization, we propose a simulation-free training objective with a model parameterization that imposes the desired boundary conditions by design. Our approach significantly reduces the search space over trajectories and avoids expensive trajectory simulation and inefficient importance sampling estimators which are required in existing methods. We demonstrate the ability of our method to find feasible transition paths on real-world molecular simulation and protein folding tasks.",b2eb11c3441d940de862e7e81dff696a779cb5db.pdf,"Approach to Transition Path Sampling
Yuanqi Du* 1Michael Plainer* 2Rob Brekelmans* 3Chenru Duan4Frank Noe5 6 7Carla P. Gomes1
Alan Apsuru-Guzik3 8Kirill Neklyudov9 10
Abstract
Rare event sampling in dynamical systems is a
fundamental problem arising in the natural sci-
ences, which poses significant computational chal-
lenges due to an exponentially large space of tra-
jectories. For settings where the dynamical sys-
tem of interest follows a Brownian motion with
known drift, the question of conditioning the pro-
cess to reach a given endpoint or desired rare event
is definitively answered by Doob’s h-transform.
However, the naive simulation of this transform is
infeasible, as it requires sufficiently many forward
trajectories to estimate rare event probabilities. In
this work, we propose a variational formulation of
Doob’s h-transform — an optimization problem
over trajectories between a given initial point and
the desired ending point. To solve this optimiza-
tion, we propose a simulation-free training objec-
tive with a model parameterization that imposes
the desired boundary conditions by design. Our
approach significantly reduces the search space
over trajectories and avoids expensive trajectory
simulation and inefficient importance sampling
estimators which are required in existing methods.
We demonstrate the ability of our method to find
feasible transition paths on real-world molecular
simulation and protein folding tasks.
1. Introduction
Conditioning a stochastic process to obey a particular end-
point distribution, satisfy desired terminal conditions, or
observe a rare event is a problem with a long history
(Schrödinger, 1932; Doob, 1957) and wide-ranging appli-
*Equal contribution1Cornell University2Technische Univer-
sität Berlin3Vector Institute4Massachusetts Institute of Technol-
ogy5Freie Universität Berlin6Rice University7Microsoft Re-
search AI4Science8University of Toronto9Université de Montréal
10MILA Quebec AI Institute. Contact: yuanqidu@cs.corn","Future Work
In this paper, we propose an efficient computational frame-
work for transition path sampling with Brownian dynamics.
We formulate the transition path sampling problem by us-
ing Doob’s h-transform to condition a reference stochastic
process, and propose a variational formulation for efficient
optimization. Specifically, we propose a simulation-free
training objective and model parameterization that imposes
boundary conditions as hard constraints. We compare our
methods with MCMC-based baselines and show comparable
accuracy with lower computational costs on both synthetic
datasets and real-world molecular systems. Finally, our
method might be improved or extended by (1) accounting
for conditioning on a set of terminal events, (2) amortiz-
ing over many state pairs or systems and finally learning
an unconditioned process, and (3) accommodating variable
length paths.
References
Anderson, J. B. (2007). Predicting rare events in molecular
dynamics. Advances in Chemical Physics , 91:381–431.
Batatia, I., Benner, P., Chiang, Y ., Elena, A. M., Kovács,
D. P., Riebesell, J., Advincula, X. R., Asta, M., Bald-
win, W. J., Bernstein, N., et al. (2023). A foundation
model for atomistic materials chemistry. arXiv preprint
arXiv:2401.00096 .
Blau, S. M., Patel, H. D., Spotte-Smith, E. W. C., Xie, X.,
Dwaraknath, S., and Persson, K. A. (2021). A chemically
consistent graph architecture for massive reaction net-
works applied to solid-electrolyte interphase formation.
Chemical science , 12(13):4931–4939.
Bolhuis, P. G. and Swenson, D. W. H. (2021). Transition
path sampling as markov chain monte carlo of trajec-
tories: Recent algorithms, software, applications, and
future outlook. Advanced Theory and Simulations , 4(4).
Borrero, E. and Dellago, C. (2016). Avoiding traps in tra-
jectory space: Metadynamics enhanced transition path
sampling. The European Physical Journal Special Topics ,
225(8-9):1609–1620.
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary,
C.","Approach to Transition Path Sampling Yuanqi Du* 1Michael Plainer* 2Rob Brekelmans* 3Chenru Duan4Frank Noe5 6 7Carla P. Gomes1 Alan Apsuru-Guzik3 8Kirill Neklyudov9 10 Abstract Rare event sampling in dynamical systems is a fundamental problem arising in the natural sci- ences, which poses significant computational chal- lenges due to an exponentially large space of tra- jectories. For settings where the dynamical sys- tem of interest follows a Brownian motion with known drift, the question of conditioning the pro- cess to reach a given endpoint or desired rare event is definitively answered by Doob s h-transform. However, the naive simulation of this transform is infeasible, as it requires sufficiently many forward trajectories to estimate rare event probabilities. In this work, we propose a variational formulation of Doob s h-transform an optimization problem over trajectories between a given initial point and the desired ending point. To solve this optimiza- tion, we propose a simulation-free training objec- tive with a model parameterization that imposes the desired boundary conditions by design. Our approach significantly reduces the search space over trajectories and avoids expensive trajectory simulation and inefficient importance sampling estimators which are required in existing methods. We demonstrate the ability of our method to find feasible transition paths on real-world molecular simulation and protein folding tasks. 1. Introduction Conditioning a stochastic process to obey a particular end- point distribution, satisfy desired terminal conditions, or observe a rare event is a problem with a long history (Schr dinger, 1932; Doob, 1957) and wide-ranging appli- *Equal contribution1Cornell University2Technische Univer- sit t Berlin3Vector Institute4Massachusetts Institute of Technol- ogy5Freie Universit t Berlin6Rice University7Microsoft Re- search AI4Science8University of Toronto9Universit de Montr al 10MILA Quebec AI Institute. Contact: yuanqidu@cs.corn","Future Work In this paper, we propose an efficient computational frame- work for transition path sampling with Brownian dynamics. We formulate the transition path sampling problem by us- ing Doob s h-transform to condition a reference stochastic process, and propose a variational formulation for efficient optimization. Specifically, we propose a simulation-free training objective and model parameterization that imposes boundary conditions as hard constraints. We compare our methods with MCMC-based baselines and show comparable accuracy with lower computational costs on both synthetic datasets and real-world molecular systems. Finally, our method might be improved or extended by (1) accounting for conditioning on a set of terminal events, (2) amortiz- ing over many state pairs or systems and finally learning an unconditioned process, and (3) accommodating variable length paths.","Doob's Lagrangian: A Sample-Efficient Variational Approach to Transition Path Sampling Lagrangian formulation of Doob's h-transform allowing for an efficient rare event sampling Rare event sampling in dynamical systems is a fundamental problem arising in the natural sciences, which poses significant computational challenges due to an exponentially large space of trajectories. For settings where the dynamical system of interest follows a Brownian motion with known drift, the question of conditioning the process to reach a given endpoint or desired rare event is definitively answered by Doob's -transform. However, the naive simulation of this transform is infeasible, as it requires sufficiently many forward trajectories to estimate rare event probabilities. In this work, we propose a variational formulation of Doob's -transform --- an optimization problem over trajectories between a given initial point and the desired ending point. To solve this optimization, we propose a simulation-free training objective with a model parameterization that imposes the desired boundary conditions by design. Our approach significantly reduces the search space over trajectories and avoids expensive trajectory simulation and inefficient importance sampling estimators which are required in existing methods. We demonstrate the ability of our method to find feasible transition paths on real-world molecular simulation and protein folding tasks.","Doob's Lagrangian: A Sample-Efficient Variational Approach to Transition Path Sampling Lagrangian formulation of Doob's h-transform allowing for an efficient rare event sampling Rare event sampling in dynamical systems is a fundamental problem arising in the natural sciences, which poses significant computational challenges due to an exponentially large space of trajectories. For settings where the dynamical system of interest follows a Brownian motion with known drift, the question of conditioning the process to reach a given endpoint or desired rare event is definitively answered by Doob's -transform. However, the naive simulation of this transform is infeasible, as it requires sufficiently many forward trajectories to estimate rare event probabilities. In this work, we propose a variational formulation of Doob's -transform --- an optimization problem over trajectories between a given initial point and the desired ending point. To solve this optimization, we propose a simulation-free training objective with a model parameterization that imposes the desired boundary conditions by design. Our approach significantly reduces the search space over trajectories and avoids expensive trajectory simulation and inefficient importance sampling estimators which are required in existing methods. We demonstrate the ability of our method to find feasible transition paths on real-world molecular simulation and protein folding tasks. Future Work In this paper, we propose an efficient computational frame- work for transition path sampling with Brownian dynamics. We formulate the transition path sampling problem by us- ing Doob s h-transform to condition a reference stochastic process, and propose a variational formulation for efficient optimization. Specifically, we propose a simulation-free training objective and model parameterization that imposes boundary conditions as hard constraints. We compare our methods with MCMC-based baselines and show comparable accuracy with lower computational costs on both synthetic datasets and real-world molecular systems. Finally, our method might be improved or extended by (1) accounting for conditioning on a set of terminal events, (2) amortiz- ing over many state pairs or systems and finally learning an unconditioned process, and (3) accommodating variable length paths. Approach to Transition Path Sampling Yuanqi Du* 1Michael Plainer* 2Rob Brekelmans* 3Chenru Duan4Frank Noe5 6 7Carla P. Gomes1 Alan Apsuru-Guzik3 8Kirill Neklyudov9 10 Abstract Rare event sampling in dynamical systems is a fundamental problem arising in the natural sci- ences, which poses significant computational chal- lenges due to an exponentially large space of tra- jectories. For settings where the dynamical sys- tem of interest follows a Brownian motion with known drift, the question of conditioning the pro- cess to reach a given endpoint or desired rare event is definitively answered by Doob s h-transform. However, the naive simulation of this transform is infeasible, as it requires sufficiently many forward trajectories to estimate rare event probabilities. In this work, we propose a variational formulation of Doob s h-transform an optimization problem over trajectories between a given initial point and the desired ending point. To solve this optimiza- tion, we propose a simulation-free training objec- tive with a model parameterization that imposes the desired boundary conditions by design. Our approach significantly reduces the search space over trajectories and avoids expensive trajectory simulation and inefficient importance sampling estimators which are required in existing methods. We demonstrate the ability of our method to find feasible transition paths on real-world molecular simulation and protein folding tasks. 1. Introduction Conditioning a stochastic process to obey a particular end- point distribution, satisfy desired terminal conditions, or observe a rare event is a problem with a long history (Schr dinger, 1932; Doob, 1957) and wide-ranging appli- *Equal contribution1Cornell University2Technische Univer- sit t Berlin3Vector Institute4Massachusetts Institute of Technol- ogy5Freie Universit t Berlin6Rice University7Microsoft Re- search AI4Science8University of Toronto9Universit de Montr al 10MILA Quebec AI Institute. Contact: yuanqidu@cs.corn",0
0d7e6a3239499e6b00d6be3584512dacfe9ff0c3,EvoSBDD: Latent Evolution for Accurate and Efficient Structure-Based Drug Design,['Danny Reidenbach'],https://openreview.net/pdf/0d7e6a3239499e6b00d6be3584512dacfe9ff0c3.pdf,"EvoSBDD: Latent Evolution for Accurate and Efficient Structure-Based Drug Design EvoSBDD presents and efficient black-box optimization solution for 3D structure-based drug design by effectively using docking oracle functions over pretrained 1D molecule latent space. Structure-based Drug Design (SBDD), the task of designing 3D molecules (ligands) to bind with a target protein pocket, is a fundamental task in drug discovery. Recent geometric deep learning methods for SBDD fail to accurately generate valid docked structures without relying on physics-based post-processing (ie AutoDock Vina redocking), which resamples all the important geometric qualities of the molecule. Without 3D structure information or additional training on protein-ligand complexes as required by prior methods, EvoSBDD attains a state-of-the-art success rate of 86.4%, an average binding affinity of -10.27 kcal/mol, and demonstrates speed improvements up to 25.6x compared to the prior best method. EvoSBDD is the first method to maintain 100% generated molecule validity, novelty, and uniqueness and also excels in real-world off-target(s) binding prevention.",0d7e6a3239499e6b00d6be3584512dacfe9ff0c3.pdf,"methodology is to inhibit enzymes (proteins) that are related to the disease of interest.
Several quantities, such as Ki, Kd, and IC50, are used to measure the drug potency. All of which are inversely proportional
to potency.
•Ki is the equilibrium constant for the inhibition of an enzyme by a specific inhibitor. It represents the concentration of
an inhibitor required to inhibit an enzyme-catalyzed reaction by 50%.
•Kd is the equilibrium constant for the dissociation of a complex, such as a protein-ligand complex. Kd represents the
concentration of ligand at which half of the protein binding sites are occupied.
•IC50 is the half-maximal inhibitory concentration. It is commonly used in pharmacology to measure the potency of a
drug or inhibitor. In the context of protein-ligand binding, it represents the concentration of a ligand required to inhibit
a specific biological process by 50%. In the context of SBDD binding affinity <-8.18 kcal/mol corresponds to an IC50
of 1µM.
Fig. 5 demonstrates how in the simplest uncompetitive case IC50 and Ki are identical (with others being a scalar multiple
21
Latent Evolution for Accurate and Efficient Structure-Based Drug Design
close to 1 of Ki)3. Based on the thermodynamic relationship between free energy and Ki we plot the AutoDock Vina Score
in units that can better measure binding effectiveness and thus drug potency. Specifically, Fig. 5 a and b show how once a
ligand surpasses -8.18 kcal/mol the change in potency is quite minimal. This is important to understand as for EvoSBDD
when given a strong binding reference (Fig. 3(e)), although most molecules improve upon the reference the ones that do
not are still <-8.18 with very strong potency. Furthermore Fig. 5 a can be extrapolated to binding affinities >-3 kcal/mol.
In this area (a single molecule in the CrossDocked test) set, EvoSBDD is unable to generate <-8.18 but does improve
molecules by 5.4 kcal/mol which in potency concentrations is an increase of 110 µM which is very","future work to devise more realistic optimizations that cover multiple
properties as improving the score to -11 while on paper may seem impressive has an exponentially small real-world impact.
F. Further Off Target Binding Prevention Benchmarks
Background At a high level, a drug that has a perfect binding affinity to its desired target is useless if it also readily
binds to other dangerous sites. Unlike prior methods that blindly sample given a protein of interest, EvoSBDD can easily
customize its optimization goal to encourage on-target binding while also prohibiting off-target binding, a major focus in
real-world drug discovery.
Table 7: Pairwise Vina Dock scores of COX-1 and COX-2 ligand references from PDB. COX-2 Chain E is the primary
binding site with binding data from BindDB (Gilson et al., 2016).
COX-1 Binding Affinity ( ↓)COX-2 Binding Affinity (Chain G) ( ↓) COX-2 Binding Affinity (Chain I) ( ↓) COX-2 Binding Affinity (Chain E) ( ↓)
COX-1 Reference (Chain E) -4.411 -3.348 -3.301 -5.454
COX-2 Reference (Chain G) -4.986 -4.919 -4.432 -4.565
COX-2 Reference (Chain I) -5.372 -3.615 -4.984 -6.008
COX-2 Reference (Chain E) -6.012 -5.630 -6.777 -8.897
Figure 6: Top generated molecules of a single CMA-ES restart optimized to bind to COX-1 and avoid off-target binding to
COX-2 (chain G). Average affinity gap -1.513 kcal/mol favoring COX-1. 10 iterations, pop size 20, using UniDock for all
docking evaluations.
3Figures c and d from https://www.sciencesnail.com/science/the-difference-between-ki-kd-ic50-and-ec50-values
22
Latent Evolution for Accurate and Efficient Structure-Based Drug Design
Figure 7: Top generated molecules of a single CMA-ES restart optimized to bind to COX-1 and avoid off-target binding
to COX-2 (chains G, I, E). 10 iterations, pop size 20, using UniDock for all docking evaluations. Out of 200 sampled
molecules, 46 have COX-1 >all COX-2 chains. This is an example of optimizing for 4 binding sites at the same time (1 on
target, 3 preventative off t","methodology is to inhibit enzymes (proteins) that are related to the disease of interest. Several quantities, such as Ki, Kd, and IC50, are used to measure the drug potency. All of which are inversely proportional to potency. Ki is the equilibrium constant for the inhibition of an enzyme by a specific inhibitor. It represents the concentration of an inhibitor required to inhibit an enzyme-catalyzed reaction by 50%. Kd is the equilibrium constant for the dissociation of a complex, such as a protein-ligand complex. Kd represents the concentration of ligand at which half of the protein binding sites are occupied. IC50 is the half-maximal inhibitory concentration. It is commonly used in pharmacology to measure the potency of a drug or inhibitor. In the context of protein-ligand binding, it represents the concentration of a ligand required to inhibit a specific biological process by 50%. In the context of SBDD binding affinity <-8.18 kcal/mol corresponds to an IC50 of 1 M. Fig. 5 demonstrates how in the simplest uncompetitive case IC50 and Ki are identical (with others being a scalar multiple 21 Latent Evolution for Accurate and Efficient Structure-Based Drug Design close to 1 of Ki)3. Based on the thermodynamic relationship between free energy and Ki we plot the AutoDock Vina Score in units that can better measure binding effectiveness and thus drug potency. Specifically, Fig. 5 a and b show how once a ligand surpasses -8.18 kcal/mol the change in potency is quite minimal. This is important to understand as for EvoSBDD when given a strong binding reference (Fig. 3(e)), although most molecules improve upon the reference the ones that do not are still <-8.18 with very strong potency. Furthermore Fig. 5 a can be extrapolated to binding affinities >-3 kcal/mol. In this area (a single molecule in the CrossDocked test) set, EvoSBDD is unable to generate <-8.18 but does improve molecules by 5.4 kcal/mol which in potency concentrations is an increase of 110 M which is very","future work to devise more realistic optimizations that cover multiple properties as improving the score to -11 while on paper may seem impressive has an exponentially small real-world impact. F. Further Off Target Binding Prevention Benchmarks Background At a high level, a drug that has a perfect binding affinity to its desired target is useless if it also readily binds to other dangerous sites. Unlike prior methods that blindly sample given a protein of interest, EvoSBDD can easily customize its optimization goal to encourage on-target binding while also prohibiting off-target binding, a major focus in real-world drug discovery. Table 7: Pairwise Vina Dock scores of COX-1 and COX-2 ligand","EvoSBDD: Latent Evolution for Accurate and Efficient Structure-Based Drug Design EvoSBDD presents and efficient black-box optimization solution for 3D structure-based drug design by effectively using docking oracle functions over pretrained 1D molecule latent space. Structure-based Drug Design (SBDD), the task of designing 3D molecules (ligands) to bind with a target protein pocket, is a fundamental task in drug discovery. Recent geometric deep learning methods for SBDD fail to accurately generate valid docked structures without relying on physics-based post-processing (ie AutoDock Vina redocking), which resamples all the important geometric qualities of the molecule. Without 3D structure information or additional training on protein-ligand complexes as required by prior methods, EvoSBDD attains a state-of-the-art success rate of 86.4%, an average binding affinity of -10.27 kcal/mol, and demonstrates speed improvements up to 25.6x compared to the prior best method. EvoSBDD is the first method to maintain 100% generated molecule validity, novelty, and uniqueness and also excels in real-world off-target(s) binding prevention.","EvoSBDD: Latent Evolution for Accurate and Efficient Structure-Based Drug Design EvoSBDD presents and efficient black-box optimization solution for 3D structure-based drug design by effectively using docking oracle functions over pretrained 1D molecule latent space. Structure-based Drug Design (SBDD), the task of designing 3D molecules (ligands) to bind with a target protein pocket, is a fundamental task in drug discovery. Recent geometric deep learning methods for SBDD fail to accurately generate valid docked structures without relying on physics-based post-processing (ie AutoDock Vina redocking), which resamples all the important geometric qualities of the molecule. Without 3D structure information or additional training on protein-ligand complexes as required by prior methods, EvoSBDD attains a state-of-the-art success rate of 86.4%, an average binding affinity of -10.27 kcal/mol, and demonstrates speed improvements up to 25.6x compared to the prior best method. EvoSBDD is the first method to maintain 100% generated molecule validity, novelty, and uniqueness and also excels in real-world off-target(s) binding prevention. future work to devise more realistic optimizations that cover multiple properties as improving the score to -11 while on paper may seem impressive has an exponentially small real-world impact. F. Further Off Target Binding Prevention Benchmarks Background At a high level, a drug that has a perfect binding affinity to its desired target is useless if it also readily binds to other dangerous sites. Unlike prior methods that blindly sample given a protein of interest, EvoSBDD can easily customize its optimization goal to encourage on-target binding while also prohibiting off-target binding, a major focus in real-world drug discovery. Table 7: Pairwise Vina Dock scores of COX-1 and COX-2 ligand methodology is to inhibit enzymes (proteins) that are related to the disease of interest. Several quantities, such as Ki, Kd, and IC50, are used to measure the drug potency. All of which are inversely proportional to potency. Ki is the equilibrium constant for the inhibition of an enzyme by a specific inhibitor. It represents the concentration of an inhibitor required to inhibit an enzyme-catalyzed reaction by 50%. Kd is the equilibrium constant for the dissociation of a complex, such as a protein-ligand complex. Kd represents the concentration of ligand at which half of the protein binding sites are occupied. IC50 is the half-maximal inhibitory concentration. It is commonly used in pharmacology to measure the potency of a drug or inhibitor. In the context of protein-ligand binding, it represents the concentration of a ligand required to inhibit a specific biological process by 50%. In the context of SBDD binding affinity <-8.18 kcal/mol corresponds to an IC50 of 1 M. Fig. 5 demonstrates how in the simplest uncompetitive case IC50 and Ki are identical (with others being a scalar multiple 21 Latent Evolution for Accurate and Efficient Structure-Based Drug Design close to 1 of Ki)3. Based on the thermodynamic relationship between free energy and Ki we plot the AutoDock Vina Score in units that can better measure binding effectiveness and thus drug potency. Specifically, Fig. 5 a and b show how once a ligand surpasses -8.18 kcal/mol the change in potency is quite minimal. This is important to understand as for EvoSBDD when given a strong binding reference (Fig. 3(e)), although most molecules improve upon the reference the ones that do not are still <-8.18 with very strong potency. Furthermore Fig. 5 a can be extrapolated to binding affinities >-3 kcal/mol. In this area (a single molecule in the CrossDocked test) set, EvoSBDD is unable to generate <-8.18 but does improve molecules by 5.4 kcal/mol which in potency concentrations is an increase of 110 M which is very",0
d8364397676bcb675e6b342a0c9c639b9ac1a62b,Likelihood-based fine-tuning of protein language models for few-shot fitness prediction and design,"['Alex Hawkins-Hooker', 'Jakub Kmec', 'Oliver Bent', 'Paul Duckworth']",https://openreview.net/pdf/d8364397676bcb675e6b342a0c9c639b9ac1a62b.pdf,"Likelihood-based fine-tuning of protein language models for few-shot fitness prediction and design Although various schemes have been proposed for exploiting the distributional knowledge captured by protein language models (PLMs) to enhance supervised fitness prediction and design, lack of head-to-head comparison across different prediction strategies and different classes of PLM has made it challenging to identify the best-performing methods, and to understand the factors contributing to performance. Here, we extend previously proposed ranking-based loss functions to adapt the likelihoods of family-based and masked protein language models, and demonstrate that the best configurations outperform state-of-the-art approaches based on frozen embeddings in the low-data setting. Furthermore, we propose ensembling strategies that exploit the strong dependence of the mutational distributions learned by PLMs on sequence context, showing that they can be used to guide efficient optimisation strategies over fitness landscapes.",d8364397676bcb675e6b342a0c9c639b9ac1a62b.pdf,"approaches based on frozen embeddings in
the low-data setting. Furthermore, we propose
ensembling strategies that exploit the strong de-
pendence of the mutational distributions learned
by PLMs on sequence context, showing that they
can be used to guide efficient optimisation strate-
gies over fitness landscapes.
1. Introduction
Protein language models (PLMs) fit to the distribution of
natural sequences learn to implicitly model functional and
structural constraints relevant to protein function, with their
likelihoods forming effective zero-shot predictors of the fit-
ness effects of mutations (Meier et al., 2021; Notin et al.,
2022). In practical protein design scenarios, it is often pos-
sible to use experimental techniques to generate labelled
datasets associating sets of sequences with quantitative mea-
surements of biological properties of interest, however ex-
perimental constraints mean that it might only be feasible
to generate measurements for tens or hundreds of proteins
*Work completed during an internship at InstaDeep.
1AI Centre, University College London, London, United
Kingdom2InstaDeep Ltd.. Correspondence to: Alex
Hawkins-Hooker <ucabawk@ucl.ac.uk >, Paul Duckworth
<p.duckworth@instadeep.com >.
Accepted at the 1st Machine Learning for Life and Material Sci-
ences Workshop at ICML 2024 . Copyright 2024 by the author(s).at a time (Biswas et al., 2021). It is therefore of consider-
able interest to ask how the zero-shot prediction capacities
of PLMs can be combined with small labelled datasets to
achieve improved predictive performance.
One popular paradigm for exploiting the information in
pretrained PLMs involves extracting sequence representa-
tions and feeding these as inputs into task-specific down-
stream predictive models (Alley et al., 2019; Biswas et al.,
2021; Rao et al., 2019; Dallago et al., 2021; Notin et al.,
2023b). However, recent trends in natural language process-
ing have shown the benefits of directly adapting the distri-
butions of","Conclusion
The ability of language models to learn distributional con-
straints governing natural protein sequences makes them
powerful zero-shot predictors of the effects of mutations.
Here we show that their learned distributions can also be
rapidly adapted via feedback from relatively few experimen-
tal measurements. Even 128 sequences - of the order of a
typical batch size in wet lab experiments - allow significant
improvements over zero-shot performance. While previous
works have also suggested the effectiveness of directly fine-
tuning likelihoods, we extend this strategy to the classes
of PLM whose distributions best reflect fitness, and find
that doing so is crucial to obtaining performance surpass-
ing leading approaches based on frozen embeddings across
supervised and multi-round design settings. Notably, fine-tuning is also dramatically more computationally efficient
than the leading embedding-based approaches (Table 4). An
intriguing possibility is that when generative PLMs are fine-
tuned via likelihood-based loss functions, they may retain
their generative capacity, and we believe studying this pos-
sibility by leveraging the connection to methods like DPO
(Rafailov et al., 2023) to be a promising avenue for future
work.
References
Alley, E. C., Khimulya, G., Biswas, S., AlQuraishi, M.,
and Church, G. M. Unified rational protein engineering
with sequence-based deep representation learning. Nature
Methods , 16(12):1315–1322, 2019.
Biswas, S., Khimulya, G., Alley, E. C., Esvelt, K. M., and
Church, G. M. Low-N protein engineering with data-
efficient deep learning. Nature Methods , 18(4):389–396,
2021.
Bradley, R. A. and Terry, M. E. Rank analysis of incom-
plete block designs: the mehod of paired comparisons.
Biometrika , 39(3-4):324–345, 1952.
Brookes, D. H., Otwinowski, J., and Sinai, S. Contrastive
Likelihood-based fine-tuning of protein language models for few-shot fitness prediction and design
losses as generalized models of global epistasis. arXiv","approaches based on frozen embeddings in the low-data setting. Furthermore, we propose ensembling strategies that exploit the strong de- pendence of the mutational distributions learned by PLMs on sequence context, showing that they can be used to guide efficient optimisation strate- gies over fitness landscapes. 1. Introduction Protein language models (PLMs) fit to the distribution of natural sequences learn to implicitly model functional and structural constraints relevant to protein function, with their likelihoods forming effective zero-shot predictors of the fit- ness effects of mutations (Meier et al., 2021; Notin et al., 2022). In practical protein design scenarios, it is often pos- sible to use experimental techniques to generate labelled datasets associating sets of sequences with quantitative mea- surements of biological properties of interest, however ex- perimental constraints mean that it might only be feasible to generate measurements for tens or hundreds of proteins *Work completed during an internship at InstaDeep. 1AI Centre, University College London, London, United Kingdom2InstaDeep Ltd.. Correspondence to: Alex Hawkins-Hooker <ucabawk@ucl.ac.uk >, Paul Duckworth <p.duckworth@instadeep.com >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).at a time (Biswas et al., 2021). It is therefore of consider- able interest to ask how the zero-shot prediction capacities of PLMs can be combined with small labelled datasets to achieve improved predictive performance. One popular paradigm for exploiting the information in pretrained PLMs involves extracting sequence representa- tions and feeding these as inputs into task-specific down- stream predictive models (Alley et al., 2019; Biswas et al., 2021; Rao et al., 2019; Dallago et al., 2021; Notin et al., 2023b). However, recent trends in natural language process- ing have shown the benefits of directly adapting the distri- butions of","Conclusion The ability of language models to learn distributional con- straints governing natural protein sequences makes them powerful zero-shot predictors of the effects of mutations. Here we show that their learned distributions can also be rapidly adapted via feedback from relatively few experimen- tal measurements. Even 128 sequences - of the order of a typical batch size in wet lab experiments - allow significant improvements over zero-shot performance. While previous works have also suggested the effectiveness of directly fine- tuning likelihoods, we extend this strategy to the classes of PLM whose distributions best reflect fitness, and find that doing so is crucial to obtaining performance surpass- ing leading approaches based on frozen embeddings across supervised and multi-round design settings. Notably, fine-tuning is also dramatically more computationally efficient than the leading embedding-based approaches (Table 4). An intriguing possibility is that when generative PLMs are fine- tuned via likelihood-based loss functions, they may retain their generative capacity, and we believe studying this pos- sibility by leveraging the connection to methods like DPO (Rafailov et al., 2023) to be a promising avenue for future work.","Likelihood-based fine-tuning of protein language models for few-shot fitness prediction and design Although various schemes have been proposed for exploiting the distributional knowledge captured by protein language models (PLMs) to enhance supervised fitness prediction and design, lack of head-to-head comparison across different prediction strategies and different classes of PLM has made it challenging to identify the best-performing methods, and to understand the factors contributing to performance. Here, we extend previously proposed ranking-based loss functions to adapt the likelihoods of family-based and masked protein language models, and demonstrate that the best configurations outperform state-of-the-art approaches based on frozen embeddings in the low-data setting. Furthermore, we propose ensembling strategies that exploit the strong dependence of the mutational distributions learned by PLMs on sequence context, showing that they can be used to guide efficient optimisation strategies over fitness landscapes.","Likelihood-based fine-tuning of protein language models for few-shot fitness prediction and design Although various schemes have been proposed for exploiting the distributional knowledge captured by protein language models (PLMs) to enhance supervised fitness prediction and design, lack of head-to-head comparison across different prediction strategies and different classes of PLM has made it challenging to identify the best-performing methods, and to understand the factors contributing to performance. Here, we extend previously proposed ranking-based loss functions to adapt the likelihoods of family-based and masked protein language models, and demonstrate that the best configurations outperform state-of-the-art approaches based on frozen embeddings in the low-data setting. Furthermore, we propose ensembling strategies that exploit the strong dependence of the mutational distributions learned by PLMs on sequence context, showing that they can be used to guide efficient optimisation strategies over fitness landscapes. Conclusion The ability of language models to learn distributional con- straints governing natural protein sequences makes them powerful zero-shot predictors of the effects of mutations. Here we show that their learned distributions can also be rapidly adapted via feedback from relatively few experimen- tal measurements. Even 128 sequences - of the order of a typical batch size in wet lab experiments - allow significant improvements over zero-shot performance. While previous works have also suggested the effectiveness of directly fine- tuning likelihoods, we extend this strategy to the classes of PLM whose distributions best reflect fitness, and find that doing so is crucial to obtaining performance surpass- ing leading approaches based on frozen embeddings across supervised and multi-round design settings. Notably, fine-tuning is also dramatically more computationally efficient than the leading embedding-based approaches (Table 4). An intriguing possibility is that when generative PLMs are fine- tuned via likelihood-based loss functions, they may retain their generative capacity, and we believe studying this pos- sibility by leveraging the connection to methods like DPO (Rafailov et al., 2023) to be a promising avenue for future work. approaches based on frozen embeddings in the low-data setting. Furthermore, we propose ensembling strategies that exploit the strong de- pendence of the mutational distributions learned by PLMs on sequence context, showing that they can be used to guide efficient optimisation strate- gies over fitness landscapes. 1. Introduction Protein language models (PLMs) fit to the distribution of natural sequences learn to implicitly model functional and structural constraints relevant to protein function, with their likelihoods forming effective zero-shot predictors of the fit- ness effects of mutations (Meier et al., 2021; Notin et al., 2022). In practical protein design scenarios, it is often pos- sible to use experimental techniques to generate labelled datasets associating sets of sequences with quantitative mea- surements of biological properties of interest, however ex- perimental constraints mean that it might only be feasible to generate measurements for tens or hundreds of proteins *Work completed during an internship at InstaDeep. 1AI Centre, University College London, London, United Kingdom2InstaDeep Ltd.. Correspondence to: Alex Hawkins-Hooker <ucabawk@ucl.ac.uk >, Paul Duckworth <p.duckworth@instadeep.com >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).at a time (Biswas et al., 2021). It is therefore of consider- able interest to ask how the zero-shot prediction capacities of PLMs can be combined with small labelled datasets to achieve improved predictive performance. One popular paradigm for exploiting the information in pretrained PLMs involves extracting sequence representa- tions and feeding these as inputs into task-specific down- stream predictive models (Alley et al., 2019; Biswas et al., 2021; Rao et al., 2019; Dallago et al., 2021; Notin et al., 2023b). However, recent trends in natural language process- ing have shown the benefits of directly adapting the distri- butions of",0
0bc1083ba49f7c63d995796df8a0d770e9f0f983,Generative Modeling of Molecular Dynamics Trajectories,"['Bowen Jing', 'Hannes Stark', 'Tommi Jaakkola', 'Bonnie Berger']",https://openreview.net/pdf/0bc1083ba49f7c63d995796df8a0d770e9f0f983.pdf,"Generative Modeling of Molecular Dynamics Trajectories Molecular dynamics (MD) is a powerful technique for studying microscopic phenomena, but its computational cost has driven significant interest in the development of deep learning-based surrogate models. We introduce generative modeling of molecular trajectories as a paradigm for learning flexible multi-task surrogate models of MD from data. By conditioning on appropriately chosen frames of the trajectory, we show such generative models can be adapted to diverse tasks such as forward simulation, transition path sampling, and trajectory upsampling. By alternatively conditioning on part of the molecular system and inpainting the rest, we also demonstrate the first steps towards dynamics-conditioned molecular design. We validate the full set of these capabilities on tetrapeptide simulations and show that our model can produce reasonable ensembles of protein monomers. Altogether, our work illustrates how generative modeling can unlock value from MD data towards diverse downstream tasks that are not straightforward to address with existing methods or even MD itself.",0bc1083ba49f7c63d995796df8a0d770e9f0f983.pdf,"approaching the accuracy of replicate
100-ns simulations. To more stringently assess the ability
to locate and populate modes in the joint distribution over
state space, we build Markov State Models (MSMs) for each
test peptide using the MD trajectory, extract the correspond-
ing metastable states, and compare the ground-truth and
emulated distributions over metastable states. Our model
captures the relative ranking of states (Figure 2D).
Dynamical content. We compute the dynamical properties
of each tetrapeptide in terms of the decorrelation time ofeach torsion angle from the MD simulation and from our
sampled trajectory. Intuitively, this assesses if our model
can discriminate between slow- and fast-relaxing torsional
barriers. The correlation between true and predicted re-
laxation timescales is plotted in Figure 2F, showing excel-
lent agreement for sidechain torsions. To assess coarser
but higher-dimensional dynamical content, we compute the
flux matrix between all pairs of distinct metastable states
using ground-truth and sampled trajectories and find sub-
stantial Spearman correlation between their entries (mean
ρ= 0.67±0.01; Figure 8).
Sampling speed. Averaged across test peptides, our model
samples 100 ns-equivalent trajectories in ≈60 GPU-seconds,
compared to ≈3 GPU-hours for MD. To quantify the
speedup more rigorously, we compute the decorrelation
wall-clock times along the slowest independent component
from TICA, capturing how quickly the simulation traverses
the highest barriers in state space. These times are plotted
in Figure 2E, showing that our model achieves an speedup
of 10x–1000x over the MD simulation for 78 out of 100
peptides (the other 22 peptides did not fully decorrelate).
3.2. Interpolation
In the interpolation ortransition path sampling setting, we
train a model to sample 1 ns trajectories conditioned on the
first and last frames. For evaluation, we identify the two
most well-separated states (i.e., with the least flux between
them) for e","future work.
Protein Simulation. To demonstrate the applicability of our method for larger systems such as proteins, we train a model
to emulate explicit-solvent, all-atom simulations of proteins from the ATLAS dataset (Vander Meersche et al., 2024)
conditioned on the first frame (i.e., forward simulation). We follow the same splits as Jing et al. (2024). Due to the much
larger number of residues, we generate samples with 250 frames and 400 ps timestep, such that a single sample emulates
the 100 ns ATLAS reference trajectory. The difficulty of running fully equilibrated trajectories for proteins prevents the
16
Table 4. Median results on test protein ensembles ( n= 82 ). Runtimes are reported per sample structure or frame.
MDG ENAlphaFlow MSA sub.
Pairwise RMSD r↑ 0.48 0.48 0.22
Global RMSF r↑ 0.50 0.60 0.29
Per-target RMSF r↑ 0.71 0.85 0.55
Root mean W2dist.↓ 2.69 2.61 3.62
MD PCA W2dist.↓ 1.89 1.52 1.88
% PC-sim ≥0.5↑ 10 44 21
Weak contacts J↑ 0.51 0.62 0.40
Exposed residue J↑ 0.29 0.41 0.27
Runtime (s) 0.2 70 4
MD
 Ours
02
Figure 6. MD vs generated ensembles for 6uof A, with C αRMSFs plotted by residue index (Pearson r= 0.74).
construction of Markov state models used in our main evaluations. Instead, we compare statistical properties of forward
simulation ensembles following Jing et al. (2024). Our ensembles successfully emulate the ground-truth ensembles at a
level of accuracy between AlphaFlow and MSA subsampling while being orders of magnitude faster per generated structure
than either (Table 4).
17
D.2. Forward Simulation
LIRHBackbone torsions
 MD FES
 Sample FES
LIFE
 MAFM
 EHEV
 ESIC
IVMABackbone torsions
 MD FES
 Sample FES
FLRH
 VDRN
 WSAQ
 CSYR
Figure 7. Additional backbone torsion angle distributions (orange from MD, blue from samples) and free energy surfaces along the top
two TICA components for 10 randomly chosen test peptides.
LIRH =0.40
LIFE =0.80
MAFM =0.68
EHEV =0.65
ESIC =0.54
IVMA =0.79
FLRH =0.64
VDRN =0.40
WSAQ =0.47
CSYR =0.60
Figure 8. Flux","approaching the accuracy of replicate 100-ns simulations. To more stringently assess the ability to locate and populate modes in the joint distribution over state space, we build Markov State Models (MSMs) for each test peptide using the MD trajectory, extract the correspond- ing metastable states, and compare the ground-truth and emulated distributions over metastable states. Our model captures the relative ranking of states (Figure 2D). Dynamical content. We compute the dynamical properties of each tetrapeptide in terms of the decorrelation time ofeach torsion angle from the MD simulation and from our sampled trajectory. Intuitively, this assesses if our model can discriminate between slow- and fast-relaxing torsional barriers. The correlation between true and predicted re- laxation timescales is plotted in Figure 2F, showing excel- lent agreement for sidechain torsions. To assess coarser but higher-dimensional dynamical content, we compute the flux matrix between all pairs of distinct metastable states using ground-truth and sampled trajectories and find sub- stantial Spearman correlation between their entries (mean = 0.67 0.01; Figure 8). Sampling speed. Averaged across test peptides, our model samples 100 ns-equivalent trajectories in 60 GPU-seconds, compared to 3 GPU-hours for MD. To quantify the speedup more rigorously, we compute the decorrelation wall-clock times along the slowest independent component from TICA, capturing how quickly the simulation traverses the highest barriers in state space. These times are plotted in Figure 2E, showing that our model achieves an speedup of 10x 1000x over the MD simulation for 78 out of 100 peptides (the other 22 peptides did not fully decorrelate). 3.2. Interpolation In the interpolation ortransition path sampling setting, we train a model to sample 1 ns trajectories conditioned on the first and last frames. For evaluation, we identify the two most well-separated states (i.e., with the least flux between them) for e","future work. Protein Simulation. To demonstrate the applicability of our method for larger systems such as proteins, we train a model to emulate explicit-solvent, all-atom simulations of proteins from the ATLAS dataset (Vander Meersche et al., 2024) conditioned on the first frame (i.e., forward simulation). We follow the same splits as Jing et al. (2024). Due to the much larger number of residues, we generate samples with 250 frames and 400 ps timestep, such that a single sample emulates the 100 ns ATLAS reference trajectory. The difficulty of running fully equilibrated trajectories for proteins prevents the 16 Table 4. Median results on test protein ensembles ( n= 82 ). Runtimes are reported per sample structure or frame. MDG ENAlphaFlow MSA sub. Pairwise RMSD r 0.48 0.48 0.22 Global RMSF r 0.50 0.60 0.29 Per-target RMSF r 0.71 0.85 0.55 Root mean W2dist. 2.69 2.61 3.62 MD PCA W2dist. 1.89 1.52 1.88 % PC-sim 0.5 10 44 21 Weak contacts J 0.51 0.62 0.40 Exposed residue J 0.29 0.41 0.27 Runtime (s) 0.2 70 4 MD Ours 02 Figure 6. MD vs generated ensembles for 6uof A, with C RMSFs plotted by residue index (Pearson r= 0.74). construction of Markov state models used in our main evaluations. Instead, we compare statistical properties of forward simulation ensembles following Jing et al. (2024). Our ensembles successfully emulate the ground-truth ensembles at a level of accuracy between AlphaFlow and MSA subsampling while being orders of magnitude faster per generated structure than either (Table 4). 17 D.2. Forward Simulation LIRHBackbone torsions MD FES Sample FES LIFE MAFM EHEV ESIC IVMABackbone torsions MD FES Sample FES FLRH VDRN WSAQ CSYR Figure 7. Additional backbone torsion angle distributions (orange from MD, blue from samples) and free energy surfaces along the top two TICA components for 10 randomly chosen test peptides. LIRH =0.40 LIFE =0.80 MAFM =0.68 EHEV =0.65 ESIC =0.54 IVMA =0.79 FLRH =0.64 VDRN =0.40 WSAQ =0.47 CSYR =0.60 Figure 8. Flux","Generative Modeling of Molecular Dynamics Trajectories Molecular dynamics (MD) is a powerful technique for studying microscopic phenomena, but its computational cost has driven significant interest in the development of deep learning-based surrogate models. We introduce generative modeling of molecular trajectories as a paradigm for learning flexible multi-task surrogate models of MD from data. By conditioning on appropriately chosen frames of the trajectory, we show such generative models can be adapted to diverse tasks such as forward simulation, transition path sampling, and trajectory upsampling. By alternatively conditioning on part of the molecular system and inpainting the rest, we also demonstrate the first steps towards dynamics-conditioned molecular design. We validate the full set of these capabilities on tetrapeptide simulations and show that our model can produce reasonable ensembles of protein monomers. Altogether, our work illustrates how generative modeling can unlock value from MD data towards diverse downstream tasks that are not straightforward to address with existing methods or even MD itself.","Generative Modeling of Molecular Dynamics Trajectories Molecular dynamics (MD) is a powerful technique for studying microscopic phenomena, but its computational cost has driven significant interest in the development of deep learning-based surrogate models. We introduce generative modeling of molecular trajectories as a paradigm for learning flexible multi-task surrogate models of MD from data. By conditioning on appropriately chosen frames of the trajectory, we show such generative models can be adapted to diverse tasks such as forward simulation, transition path sampling, and trajectory upsampling. By alternatively conditioning on part of the molecular system and inpainting the rest, we also demonstrate the first steps towards dynamics-conditioned molecular design. We validate the full set of these capabilities on tetrapeptide simulations and show that our model can produce reasonable ensembles of protein monomers. Altogether, our work illustrates how generative modeling can unlock value from MD data towards diverse downstream tasks that are not straightforward to address with existing methods or even MD itself. future work. Protein Simulation. To demonstrate the applicability of our method for larger systems such as proteins, we train a model to emulate explicit-solvent, all-atom simulations of proteins from the ATLAS dataset (Vander Meersche et al., 2024) conditioned on the first frame (i.e., forward simulation). We follow the same splits as Jing et al. (2024). Due to the much larger number of residues, we generate samples with 250 frames and 400 ps timestep, such that a single sample emulates the 100 ns ATLAS reference trajectory. The difficulty of running fully equilibrated trajectories for proteins prevents the 16 Table 4. Median results on test protein ensembles ( n= 82 ). Runtimes are reported per sample structure or frame. MDG ENAlphaFlow MSA sub. Pairwise RMSD r 0.48 0.48 0.22 Global RMSF r 0.50 0.60 0.29 Per-target RMSF r 0.71 0.85 0.55 Root mean W2dist. 2.69 2.61 3.62 MD PCA W2dist. 1.89 1.52 1.88 % PC-sim 0.5 10 44 21 Weak contacts J 0.51 0.62 0.40 Exposed residue J 0.29 0.41 0.27 Runtime (s) 0.2 70 4 MD Ours 02 Figure 6. MD vs generated ensembles for 6uof A, with C RMSFs plotted by residue index (Pearson r= 0.74). construction of Markov state models used in our main evaluations. Instead, we compare statistical properties of forward simulation ensembles following Jing et al. (2024). Our ensembles successfully emulate the ground-truth ensembles at a level of accuracy between AlphaFlow and MSA subsampling while being orders of magnitude faster per generated structure than either (Table 4). 17 D.2. Forward Simulation LIRHBackbone torsions MD FES Sample FES LIFE MAFM EHEV ESIC IVMABackbone torsions MD FES Sample FES FLRH VDRN WSAQ CSYR Figure 7. Additional backbone torsion angle distributions (orange from MD, blue from samples) and free energy surfaces along the top two TICA components for 10 randomly chosen test peptides. LIRH =0.40 LIFE =0.80 MAFM =0.68 EHEV =0.65 ESIC =0.54 IVMA =0.79 FLRH =0.64 VDRN =0.40 WSAQ =0.47 CSYR =0.60 Figure 8. Flux approaching the accuracy of replicate 100-ns simulations. To more stringently assess the ability to locate and populate modes in the joint distribution over state space, we build Markov State Models (MSMs) for each test peptide using the MD trajectory, extract the correspond- ing metastable states, and compare the ground-truth and emulated distributions over metastable states. Our model captures the relative ranking of states (Figure 2D). Dynamical content. We compute the dynamical properties of each tetrapeptide in terms of the decorrelation time ofeach torsion angle from the MD simulation and from our sampled trajectory. Intuitively, this assesses if our model can discriminate between slow- and fast-relaxing torsional barriers. The correlation between true and predicted re- laxation timescales is plotted in Figure 2F, showing excel- lent agreement for sidechain torsions. To assess coarser but higher-dimensional dynamical content, we compute the flux matrix between all pairs of distinct metastable states using ground-truth and sampled trajectories and find sub- stantial Spearman correlation between their entries (mean = 0.67 0.01; Figure 8). Sampling speed. Averaged across test peptides, our model samples 100 ns-equivalent trajectories in 60 GPU-seconds, compared to 3 GPU-hours for MD. To quantify the speedup more rigorously, we compute the decorrelation wall-clock times along the slowest independent component from TICA, capturing how quickly the simulation traverses the highest barriers in state space. These times are plotted in Figure 2E, showing that our model achieves an speedup of 10x 1000x over the MD simulation for 78 out of 100 peptides (the other 22 peptides did not fully decorrelate). 3.2. Interpolation In the interpolation ortransition path sampling setting, we train a model to sample 1 ns trajectories conditioned on the first and last frames. For evaluation, we identify the two most well-separated states (i.e., with the least flux between them) for e",0
ba854d94e289a97781bf8dc00557fd1dbea6c3a9,PLUTO: Pathology-Universal Transformer,"['Dinkar Juyal', 'Harshith Padigela', 'Chintan Shah', 'Daniel Shenker', 'Natalia Harguindeguy', 'Yi Liu', 'Blake Martin', 'Yibo Zhang', 'Michael Nercessian', 'Miles Markey', 'Isaac Finberg', 'Kelsey Luu', 'Daniel Borders', 'Syed Ashar Javed', 'Emma Krause', 'Raymond Biju', 'Aashish Sood', 'Allen Ma', 'Jackson Nyman', 'John Shamshoian', 'Guillaume Chhor', 'Darpan Sanghavi', 'Marc Thibault', 'Limin Yu', 'Fedaa Najdawi', 'Jennifer A. Hipp', 'Darren Fahy', 'Benjamin Glass', 'Eric Walk', 'John Abel', 'Harsha Vardhan pokkalla', 'Andrew H. Beck', 'Sean Grullon']",https://openreview.net/pdf/ba854d94e289a97781bf8dc00557fd1dbea6c3a9.pdf,"PLUTO: Pathology-Universal Transformer PLUTO (PathoLogy Universal TransfOrmer) - a performant, scalable, generalizable foundation model for pathology Pathology images provide a unique challenge for computer-vision-based analysis: a single pathology Whole Slide Image (WSI) is gigapixel-sized and often contains hundreds of thousands to millions of objects of interest across multiple resolutions. In this work, we propose PathoLogy Universal TransfOrmer (PLUTO): a light-weight pathology FM that is pre-trained on a diverse dataset of 195 million image tiles collected from multiple sites. We design task-specific adaptation heads that utilize PLUTO's output embeddings for tasks that span pathology scales ranging from subcellular to slide-scale, including instance segmentation, tile classification, and slide-level prediction. We find that PLUTO matches or outperforms existing task-specific baselines and pathology-specific foundation models, some of which use orders-of-magnitude larger datasets and model sizes when compared to PLUTO. Our findings present a path towards a universal embedding to power pathology image analysis, and motivate further exploration around pathology foundation models in terms of data diversity, architectural improvements, sample efficiency, and practical deployability in real-world applications.",ba854d94e289a97781bf8dc00557fd1dbea6c3a9.pdf,"Approach
We designed and built the PathoLogy-Universal Trans-
fOrmer , or PLUTO, a state-of-the-art pathology foundation
model that, inspired by the dwarf planet, is based on a novel
PLUTO: Pathology-Universal Transformer
light-weight ViT backbone that is pre-trained on a diverse
dataset from multiple sites and extracts meaningful repre-
sentations across the levels of the WSI pyramid outlined in
figure 1. The key features of PLUTO are outlined below:
1.Pre-training Dataset We compiled a large dataset
across a diverse spectrum of histology stains, scan-
ners, and biological objects across resolution scales
that include 200+ biologically-meaningful objects and
region types (which we term substances ) from more
than 50sources (Section 2.1).
2.Architecture We designed the PLUTO backbone to
generate informative feature representations at differ-
ent length scales from a compact ViT backbone. We
achieved this by implementing a self-supervised learn-
ing scheme that accommodates flexible patch sizes
from the FlexiViT scheme (Beyer et al., 2023), extend-
ing it to accommodate multiple magnifications during
training, and modifying the DINOv2 loss by adding
a Masked Autoencoder (MAE) (He et al., 2022) ob-
jective and a Fourier-loss-based term to modulate the
preservation of low- and high-frequency components
(Section 2.2).
3.Multi-scale Evaluation We evaluated the quality of
the resulting FM by constructing a suite of adaptation
heads to perform diverse, challenging tasks across the
levels of the WSI pyramid, and evaluated performance
across different biologically-relevant benchmarks (Sec-
tion 3).
4.Deployability. Performing a computational pathology
task may require embedding tens to hundreds of thou-
sands of WSI tiles to make a single prediction. To
enable this, we focused on developing a model that
was efficient (Section 3.3).
2. Methods
2.1. Pre-training Data Characteristics
The dataset used for self-supervised pre-training comprises
public and proprietary datasets, to","Conclusion
We present in this paper PLUTO: a competitive state-of-the-
art pathology Foundation Model based on a light-weight ViT.
PLUTO is designed to take advantage of the multi-scale na-
ture of WSIs and provide informative representations across
biological scales. We have quantified the performance of
PLUTO on a variety of adaptation tasks across biological
scales. Our work also demonstrates the importance of in-
corporation of biological priors in the construction of pre-
training datasets and the design of the model architecture for
large-scale self-supervised models. We hope that our efforts
with PLUTO further motivate building high-performing, de-
ployable FMs; drive FM adoption in pathology; and serve
real-world translational research and clinical applications.
References
Bandi, P., Geessink, O., Manson, Q., Dijk, M. V ., Balkenhol,
M., Hermsen, M., Bejnordi, B. E., Lee, B., Paeng, K.,
Zhong, A., Li, Q., Zanjani, F. G., Zinger, S., Fukuta, K.,
Komura, D., Ovtcharov, V ., Cheng, S., Zeng, S., Tha-
gaard, J., Dahl, A. B., Lin, H., Chen, H., Jacobsson, L.,
Hedlund, M., Cetin, M., Halici, E., Jackson, H., Chen,
R., Both, F., Franke, J., Kusters-Vandevelde, H., Vreuls,
W., Bult, P., van Ginneken, B., van der Laak, J., and
Litjens, G. From Detection of Individual Metastasesto Classification of Lymph Node Status at the Patient
Level: The CAMELYON17 Challenge. IEEE Trans-
actions on Medical Imaging , 38:550–560, 2019. doi:
10.1109/TMI.2018.2867350.
Beyer, L., Izmailov, P., Kolesnikov, A., Caron, M., Korn-
blith, S., Zhai, X., Minderer, M., Tschannen, M., Alabdul-
mohsin, I., and Pavetic, F. FlexiViT: One Model for All
Patch Sizes. In 2023 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pp. 14496–
14506, Los Alamitos, CA, USA, jun 2023. IEEE Com-
puter Society. doi: 10.1109/CVPR52729.2023.01393.
URL https://doi.ieeecomputersociety.
org/10.1109/CVPR52729.2023.01393 .
Caron, M., Touvron, H., Misra, I., J ´egou, H., Mairal, J.,
Bojanowski, P., a","Approach We designed and built the PathoLogy-Universal Trans- fOrmer , or PLUTO, a state-of-the-art pathology foundation model that, inspired by the dwarf planet, is based on a novel PLUTO: Pathology-Universal Transformer light-weight ViT backbone that is pre-trained on a diverse dataset from multiple sites and extracts meaningful repre- sentations across the levels of the WSI pyramid outlined in figure 1. The key features of PLUTO are outlined below: 1.Pre-training Dataset We compiled a large dataset across a diverse spectrum of histology stains, scan- ners, and biological objects across resolution scales that include 200+ biologically-meaningful objects and region types (which we term substances ) from more than 50sources (Section 2.1). 2.Architecture We designed the PLUTO backbone to generate informative feature representations at differ- ent length scales from a compact ViT backbone. We achieved this by implementing a self-supervised learn- ing scheme that accommodates flexible patch sizes from the FlexiViT scheme (Beyer et al., 2023), extend- ing it to accommodate multiple magnifications during training, and modifying the DINOv2 loss by adding a Masked Autoencoder (MAE) (He et al., 2022) ob- jective and a Fourier-loss-based term to modulate the preservation of low- and high-frequency components (Section 2.2). 3.Multi-scale Evaluation We evaluated the quality of the resulting FM by constructing a suite of adaptation heads to perform diverse, challenging tasks across the levels of the WSI pyramid, and evaluated performance across different biologically-relevant benchmarks (Sec- tion 3). 4.Deployability. Performing a computational pathology task may require embedding tens to hundreds of thou- sands of WSI tiles to make a single prediction. To enable this, we focused on developing a model that was efficient (Section 3.3). 2. Methods 2.1. Pre-training Data Characteristics The dataset used for self-supervised pre-training comprises public and proprietary datasets, to","Conclusion We present in this paper PLUTO: a competitive state-of-the- art pathology Foundation Model based on a light-weight ViT. PLUTO is designed to take advantage of the multi-scale na- ture of WSIs and provide informative representations across biological scales. We have quantified the performance of PLUTO on a variety of adaptation tasks across biological scales. Our work also demonstrates the importance of in- corporation of biological priors in the construction of pre- training datasets and the design of the model architecture for large-scale self-supervised models. We hope that our efforts with PLUTO further motivate building high-performing, de- ployable FMs; drive FM adoption in pathology; and serve real-world translational research and clinical applications.","PLUTO: Pathology-Universal Transformer PLUTO (PathoLogy Universal TransfOrmer) - a performant, scalable, generalizable foundation model for pathology Pathology images provide a unique challenge for computer-vision-based analysis: a single pathology Whole Slide Image (WSI) is gigapixel-sized and often contains hundreds of thousands to millions of objects of interest across multiple resolutions. In this work, we propose PathoLogy Universal TransfOrmer (PLUTO): a light-weight pathology FM that is pre-trained on a diverse dataset of 195 million image tiles collected from multiple sites. We design task-specific adaptation heads that utilize PLUTO's output embeddings for tasks that span pathology scales ranging from subcellular to slide-scale, including instance segmentation, tile classification, and slide-level prediction. We find that PLUTO matches or outperforms existing task-specific baselines and pathology-specific foundation models, some of which use orders-of-magnitude larger datasets and model sizes when compared to PLUTO. Our findings present a path towards a universal embedding to power pathology image analysis, and motivate further exploration around pathology foundation models in terms of data diversity, architectural improvements, sample efficiency, and practical deployability in real-world applications.","PLUTO: Pathology-Universal Transformer PLUTO (PathoLogy Universal TransfOrmer) - a performant, scalable, generalizable foundation model for pathology Pathology images provide a unique challenge for computer-vision-based analysis: a single pathology Whole Slide Image (WSI) is gigapixel-sized and often contains hundreds of thousands to millions of objects of interest across multiple resolutions. In this work, we propose PathoLogy Universal TransfOrmer (PLUTO): a light-weight pathology FM that is pre-trained on a diverse dataset of 195 million image tiles collected from multiple sites. We design task-specific adaptation heads that utilize PLUTO's output embeddings for tasks that span pathology scales ranging from subcellular to slide-scale, including instance segmentation, tile classification, and slide-level prediction. We find that PLUTO matches or outperforms existing task-specific baselines and pathology-specific foundation models, some of which use orders-of-magnitude larger datasets and model sizes when compared to PLUTO. Our findings present a path towards a universal embedding to power pathology image analysis, and motivate further exploration around pathology foundation models in terms of data diversity, architectural improvements, sample efficiency, and practical deployability in real-world applications. Conclusion We present in this paper PLUTO: a competitive state-of-the- art pathology Foundation Model based on a light-weight ViT. PLUTO is designed to take advantage of the multi-scale na- ture of WSIs and provide informative representations across biological scales. We have quantified the performance of PLUTO on a variety of adaptation tasks across biological scales. Our work also demonstrates the importance of in- corporation of biological priors in the construction of pre- training datasets and the design of the model architecture for large-scale self-supervised models. We hope that our efforts with PLUTO further motivate building high-performing, de- ployable FMs; drive FM adoption in pathology; and serve real-world translational research and clinical applications. Approach We designed and built the PathoLogy-Universal Trans- fOrmer , or PLUTO, a state-of-the-art pathology foundation model that, inspired by the dwarf planet, is based on a novel PLUTO: Pathology-Universal Transformer light-weight ViT backbone that is pre-trained on a diverse dataset from multiple sites and extracts meaningful repre- sentations across the levels of the WSI pyramid outlined in figure 1. The key features of PLUTO are outlined below: 1.Pre-training Dataset We compiled a large dataset across a diverse spectrum of histology stains, scan- ners, and biological objects across resolution scales that include 200+ biologically-meaningful objects and region types (which we term substances ) from more than 50sources (Section 2.1). 2.Architecture We designed the PLUTO backbone to generate informative feature representations at differ- ent length scales from a compact ViT backbone. We achieved this by implementing a self-supervised learn- ing scheme that accommodates flexible patch sizes from the FlexiViT scheme (Beyer et al., 2023), extend- ing it to accommodate multiple magnifications during training, and modifying the DINOv2 loss by adding a Masked Autoencoder (MAE) (He et al., 2022) ob- jective and a Fourier-loss-based term to modulate the preservation of low- and high-frequency components (Section 2.2). 3.Multi-scale Evaluation We evaluated the quality of the resulting FM by constructing a suite of adaptation heads to perform diverse, challenging tasks across the levels of the WSI pyramid, and evaluated performance across different biologically-relevant benchmarks (Sec- tion 3). 4.Deployability. Performing a computational pathology task may require embedding tens to hundreds of thou- sands of WSI tiles to make a single prediction. To enable this, we focused on developing a model that was efficient (Section 3.3). 2. Methods 2.1. Pre-training Data Characteristics The dataset used for self-supervised pre-training comprises public and proprietary datasets, to",0
8a755bddac38be9ab5d992e0f95823826096f6d9,RGFN: Synthesizable Molecular Generation Using GFlowNets,"['Michał Koziarski', 'Andrei Rekesh', 'Dmytro Shevchuk', 'Almer M. van der Sloot', 'Piotr Gaiński', 'Yoshua Bengio', 'Cheng-Hao Liu', 'Mike Tyers', 'Robert A. Batey']",https://openreview.net/pdf/8a755bddac38be9ab5d992e0f95823826096f6d9.pdf,"RGFN: Synthesizable Molecular Generation Using GFlowNets Molecular generation with GFlowNets in the chemical reaction space, ensuring synthesizability out-of-the-box In this paper, we propose an extension of the GFlowNet framework that operates directly in the space of chemical reactions, offering out-of-the-box synthesizability, while maintaining comparable quality of generated candidates. We demonstrate that with the proposed set of reactions and fragments, it is possible to obtain a search space of molecules orders of magnitude larger than existing screening libraries while offering low costs of synthesis. We also show that the approach scales to very large fragment libraries, further increasing the number of potential molecules. Our experiments showcase the effectiveness of the proposed approach across a range of oracle models.",8a755bddac38be9ab5d992e0f95823826096f6d9.pdf,"methodology, e.g., variational autoencoders (Jin et al., 2018;
Maziarka et al., 2020), reinforcement learning (Pedawi et al.,
2022; Korablyov et al., 2024) or diffusion models (Run-
cie & Mey, 2023). Recently, Generative Flow Networks
(GFlowNets) (Bengio et al., 2021; Nica et al., 2022; Roy
et al., 2023; Shen et al., 2023; V olokhova et al., 2024) have
emerged as a promising paradigm for molecular generation
due to their ability to sample diverse candidate molecules,
which is crucial in the drug discovery process. Tradition-
ally, GFlowNets operated on the graph representation level,
and candidate molecules were generated as a sequence of
actions in which either individual atoms or small molecular
fragments were combined to form a final molecule. While
using graph representations, as opposed to textual or 3D
representations, allows the enforcement of the validity of
the generated molecules, it doesn’t guarantee a valid route
by which to synthesize them. This work expands on the
GFlowNet framework by modifying the space of actions
to consist of choosing molecular fragments and executing
compatible chemical reactions/transformations, in turn guar-
anteeing both validity and synthesizability.
Synthesizability in generative models. One approach to
RGFN: Synthesizable Molecular Generation Using GFlowNets
ensuring the synthesizability of generated molecules is by
using a scoring function, either utilizing it as one of the op-
timization criteria (Korablyov et al., 2024), or as a postpro-
cessing step by which to filter generated molecules. Multiple
scoring approaches, both heuristic (Ertl & Schuffenhauer,
2009; Genheden et al., 2020) and ML-based (Liu et al.,
2022), exist in the literature. However, synthesizability esti-
mation is difficult in practice. It can fail to generalize out
of distribution in the case of ML models, may significantly
reduce the number of high-scoring candidates, and does not
necessarily account for the cost of synthesis. Because of
this, a prefer","Conclusions
In this paper, we present RGFN, an extension of the
GFlowNet framework that operates in the action space of
chemical reactions. We propose a curated set of high-yield
chemical reactions and low-cost molecular fragments that
can be used with the method. We demonstrate that even
with this small set of reactions and fragments, the proposed
approach produces a state space with a size orders of magni-
tude larger than typical screening libraries while providing
high synthesizability of generated compounds. We also
show that the size of the search space can be further in-
creased by including additional fragments and that the pro-
posed action embedding mechanism improves scalability to
very large fragment spaces.
We show that RGFN achieves roughly comparable average
rewards to state-of-the-art methods, and it outperforms an-
other approach operating directly in the space of chemical re-
RGFN: Synthesizable Molecular Generation Using GFlowNets
actions and, crucially, standard fragment-based GFlowNets.
At the same time, it significantly improves the synthesizabil-
ity of generated compounds when compared to a fragment-
based GFlowNet. Conducted analysis of ligands produced
across the set of diverse tasks demonstrates sufficient di-
versity of proposed chemical space to generalize to various
targets. While difficult to demonstrate experimentally, ease
of synthesis (due to the small stock of cheap fragments and
high-yield chemical reactions used) combined with reason-
ably high optimization quality offer a promising direction
for high-throughput screening applications.
Acknowledgements
This work was supported by funding from CQDM Fonds
d’Accélération des Collaborations en Santé (FACS) /
Acuité Québec and the National Research Council (NRC)
Canada, the Canadian Institutes for Health Research (CIHR),
Samsung and Microsoft. Computational resources were
provided by the Digital Research Alliance of Canada
(https://alliancecan.ca ) and Mila ( https://
mila.quebec ). Th","methodology, e.g., variational autoencoders (Jin et al., 2018; Maziarka et al., 2020), reinforcement learning (Pedawi et al., 2022; Korablyov et al., 2024) or diffusion models (Run- cie & Mey, 2023). Recently, Generative Flow Networks (GFlowNets) (Bengio et al., 2021; Nica et al., 2022; Roy et al., 2023; Shen et al., 2023; V olokhova et al., 2024) have emerged as a promising paradigm for molecular generation due to their ability to sample diverse candidate molecules, which is crucial in the drug discovery process. Tradition- ally, GFlowNets operated on the graph representation level, and candidate molecules were generated as a sequence of actions in which either individual atoms or small molecular fragments were combined to form a final molecule. While using graph representations, as opposed to textual or 3D representations, allows the enforcement of the validity of the generated molecules, it doesn t guarantee a valid route by which to synthesize them. This work expands on the GFlowNet framework by modifying the space of actions to consist of choosing molecular fragments and executing compatible chemical reactions/transformations, in turn guar- anteeing both validity and synthesizability. Synthesizability in generative models. One approach to RGFN: Synthesizable Molecular Generation Using GFlowNets ensuring the synthesizability of generated molecules is by using a scoring function, either utilizing it as one of the op- timization criteria (Korablyov et al., 2024), or as a postpro- cessing step by which to filter generated molecules. Multiple scoring approaches, both heuristic (Ertl & Schuffenhauer, 2009; Genheden et al., 2020) and ML-based (Liu et al., 2022), exist in the literature. However, synthesizability esti- mation is difficult in practice. It can fail to generalize out of distribution in the case of ML models, may significantly reduce the number of high-scoring candidates, and does not necessarily account for the cost of synthesis. Because of this, a prefer","Conclusions In this paper, we present RGFN, an extension of the GFlowNet framework that operates in the action space of chemical reactions. We propose a curated set of high-yield chemical reactions and low-cost molecular fragments that can be used with the method. We demonstrate that even with this small set of reactions and fragments, the proposed approach produces a state space with a size orders of magni- tude larger than typical screening libraries while providing high synthesizability of generated compounds. We also show that the size of the search space can be further in- creased by including additional fragments and that the pro- posed action embedding mechanism improves scalability to very large fragment spaces. We show that RGFN achieves roughly comparable average rewards to state-of-the-art methods, and it outperforms an- other approach operating directly in the space of chemical re- RGFN: Synthesizable Molecular Generation Using GFlowNets actions and, crucially, standard fragment-based GFlowNets. At the same time, it significantly improves the synthesizabil- ity of generated compounds when compared to a fragment- based GFlowNet. Conducted analysis of ligands produced across the set of diverse tasks demonstrates sufficient di- versity of proposed chemical space to generalize to various targets. While difficult to demonstrate experimentally, ease of synthesis (due to the small stock of cheap fragments and high-yield chemical reactions used) combined with reason- ably high optimization quality offer a promising direction for high-throughput screening applications.","RGFN: Synthesizable Molecular Generation Using GFlowNets Molecular generation with GFlowNets in the chemical reaction space, ensuring synthesizability out-of-the-box In this paper, we propose an extension of the GFlowNet framework that operates directly in the space of chemical reactions, offering out-of-the-box synthesizability, while maintaining comparable quality of generated candidates. We demonstrate that with the proposed set of reactions and fragments, it is possible to obtain a search space of molecules orders of magnitude larger than existing screening libraries while offering low costs of synthesis. We also show that the approach scales to very large fragment libraries, further increasing the number of potential molecules. Our experiments showcase the effectiveness of the proposed approach across a range of oracle models.","RGFN: Synthesizable Molecular Generation Using GFlowNets Molecular generation with GFlowNets in the chemical reaction space, ensuring synthesizability out-of-the-box In this paper, we propose an extension of the GFlowNet framework that operates directly in the space of chemical reactions, offering out-of-the-box synthesizability, while maintaining comparable quality of generated candidates. We demonstrate that with the proposed set of reactions and fragments, it is possible to obtain a search space of molecules orders of magnitude larger than existing screening libraries while offering low costs of synthesis. We also show that the approach scales to very large fragment libraries, further increasing the number of potential molecules. Our experiments showcase the effectiveness of the proposed approach across a range of oracle models. Conclusions In this paper, we present RGFN, an extension of the GFlowNet framework that operates in the action space of chemical reactions. We propose a curated set of high-yield chemical reactions and low-cost molecular fragments that can be used with the method. We demonstrate that even with this small set of reactions and fragments, the proposed approach produces a state space with a size orders of magni- tude larger than typical screening libraries while providing high synthesizability of generated compounds. We also show that the size of the search space can be further in- creased by including additional fragments and that the pro- posed action embedding mechanism improves scalability to very large fragment spaces. We show that RGFN achieves roughly comparable average rewards to state-of-the-art methods, and it outperforms an- other approach operating directly in the space of chemical re- RGFN: Synthesizable Molecular Generation Using GFlowNets actions and, crucially, standard fragment-based GFlowNets. At the same time, it significantly improves the synthesizabil- ity of generated compounds when compared to a fragment- based GFlowNet. Conducted analysis of ligands produced across the set of diverse tasks demonstrates sufficient di- versity of proposed chemical space to generalize to various targets. While difficult to demonstrate experimentally, ease of synthesis (due to the small stock of cheap fragments and high-yield chemical reactions used) combined with reason- ably high optimization quality offer a promising direction for high-throughput screening applications. methodology, e.g., variational autoencoders (Jin et al., 2018; Maziarka et al., 2020), reinforcement learning (Pedawi et al., 2022; Korablyov et al., 2024) or diffusion models (Run- cie & Mey, 2023). Recently, Generative Flow Networks (GFlowNets) (Bengio et al., 2021; Nica et al., 2022; Roy et al., 2023; Shen et al., 2023; V olokhova et al., 2024) have emerged as a promising paradigm for molecular generation due to their ability to sample diverse candidate molecules, which is crucial in the drug discovery process. Tradition- ally, GFlowNets operated on the graph representation level, and candidate molecules were generated as a sequence of actions in which either individual atoms or small molecular fragments were combined to form a final molecule. While using graph representations, as opposed to textual or 3D representations, allows the enforcement of the validity of the generated molecules, it doesn t guarantee a valid route by which to synthesize them. This work expands on the GFlowNet framework by modifying the space of actions to consist of choosing molecular fragments and executing compatible chemical reactions/transformations, in turn guar- anteeing both validity and synthesizability. Synthesizability in generative models. One approach to RGFN: Synthesizable Molecular Generation Using GFlowNets ensuring the synthesizability of generated molecules is by using a scoring function, either utilizing it as one of the op- timization criteria (Korablyov et al., 2024), or as a postpro- cessing step by which to filter generated molecules. Multiple scoring approaches, both heuristic (Ertl & Schuffenhauer, 2009; Genheden et al., 2020) and ML-based (Liu et al., 2022), exist in the literature. However, synthesizability esti- mation is difficult in practice. It can fail to generalize out of distribution in the case of ML models, may significantly reduce the number of high-scoring candidates, and does not necessarily account for the cost of synthesis. Because of this, a prefer",0
5a06827b76a02ccc57c496400516cfbabcac12a2,Reducing Uncertainty through Mutual Information in Structural and Systems Biology,"['Vincent Zaballa', 'Elliot E Hui']",https://openreview.net/pdf/5a06827b76a02ccc57c496400516cfbabcac12a2.pdf,"Reducing Uncertainty through Mutual Information in Structural and Systems Biology We describe a method that combines structural biology predictions with systems biology that helps support hypotheses in both areas. Systems biology models are useful models of complex biological systems that may require a large amount of experimental data to fit each model's parameters or to approximate a likelihood function. These models range from a few to thousands of parameters depending on the complexity of the biological system modeled, potentially making the task of fitting parameters to the model difficult - especially when new experimental data cannot be gathered. We demonstrate a method that uses structural biology predictions to augment systems biology models to improve systems biology models' predictions without having to gather more experimental data. Additionally, we show how systems biology models' predictions can help evaluate novel structural biology hypotheses, which may also be expensive or infeasible to validate.",5a06827b76a02ccc57c496400516cfbabcac12a2.pdf,"approach provides a novel method for proposing
and checking structural biology hypotheses by cross valida-
tion with a systems biology model, for example supporting
the structural hypothesis of a strong binding affinity in the
BMP4-BMPR1A-ACVR2A complex, and the structural ba-
sis as to why it might have such a strong affinity.
On structural & systems biology mutual information
We now formalize the use of mutual information in our
framework. The mutual information between two random
variables XandYcan be defined as
I(X;Y) =H(X)−H(X|Y), (2)
Reducing Uncertainty Through Mutual Information in Structural and Systems Biology
which is the change in entropy about one random variable
with knowledge of another. Per Theorem 2.6.5 of Cover
(1999) (Information Can’t Hurt), adding relevant condi-
tional information will reduce the entropy of the underlying
data distribution, such that H(X|Y)≤H(X). In our case,
letKeqandKstruct be latent random variables and Xbe
the random variable of the observed data point, treating the
median distance of simulated points x∼Xas a proxy for
the entropy. As we demonstrated, H(X|Keq, Kstruct )<
H(X|Keq); thus, we are able to increase information gained
I(X;Keq|Kstruct )> I(X;Keq). A natural extension of
information gain in biology is to drug discovery where mod-
els, such as DiffDock (Corso et al., 2022), can be included
as conditional information about the binding of a BMP lig-
and given a certain small molecule drug p(x|θ, Kstruct , β),
where p(β)is the probability of a given drug docking and
disrupting normal binding.
Future work A key concern about this method is the in-
troduction of errors from any component of the structure
prediction pipeline, from single-chain structure prediction
to binding affinity prediction. Replacing these steps with
probabilistic structural models such as AlphaFlow (Jing
et al., 2024), protein-protein docking methods, and binding
affinity predictions would provide a way to help reduce un-
certainty in structure pred","Future work A key concern about this method is the in-
troduction of errors from any component of the structure
prediction pipeline, from single-chain structure prediction
to binding affinity prediction. Replacing these steps with
probabilistic structural models such as AlphaFlow (Jing
et al., 2024), protein-protein docking methods, and binding
affinity predictions would provide a way to help reduce un-
certainty in structure prediction using systems biology data.
We leave this for future work.
Acknowledgements
We would like to thank Heidi Klumpe, Eric Bourgain, and
Pieter Derdeyn for helpful discussions. This research was
funded by the National Institute of General Medical Sci-
ences (NIGMS) of the National Institutes of Health (NIH)
under award number 1F31GM145188-01.
References
Abramson, J., Adler, J., Dunger, J., Evans, R., Green, T.,
Pritzel, A., Ronneberger, O., Willmore, L., Ballard, A. J.,
Bambrick, J., et al. Accurate structure prediction of
biomolecular interactions with alphafold 3. Nature , pp.
1–3, 2024.
Alarc ´on, C., Zaromytidou, A.-I., Xi, Q., Gao, S., Yu, J., Fuji-
sawa, S., Barlas, A., Miller, A. N., Manova-Todorova, K.,
Macias, M. J., et al. Nuclear cdks drive smad transcrip-
tional activation and turnover in bmp and tgf- βpathways.
Cell, 139(4):757–769, 2009.
Alon, U. An introduction to systems biology: design prin-
ciples of biological circuits . Chapman and Hall/CRC,
2019.
Antebi, Y . E., Linton, J. M., Klumpe, H., Bintu, B., Gong,
M., Su, C., McCardell, R., and Elowitz, M. B. Combina-torial signal perception in the bmp pathway. Cell, 170(6):
1184–1196, 2017.
Bach, D.-H., Park, H. J., and Lee, S. K. The dual role
of bone morphogenetic proteins in cancer. Molecular
Therapy-Oncolytics , 8:1–13, 2018.
Baek, M., DiMaio, F., Anishchenko, I., Dauparas, J.,
Ovchinnikov, S., Lee, G. R., Wang, J., Cong, Q., Kinch,
L. N., Schaeffer, R. D., et al. Accurate prediction of pro-
tein structures and interactions using a three-track neural
network. Science , 37","approach provides a novel method for proposing and checking structural biology hypotheses by cross valida- tion with a systems biology model, for example supporting the structural hypothesis of a strong binding affinity in the BMP4-BMPR1A-ACVR2A complex, and the structural ba- sis as to why it might have such a strong affinity. On structural & systems biology mutual information We now formalize the use of mutual information in our framework. The mutual information between two random variables XandYcan be defined as I(X;Y) =H(X) H(X|Y), (2) Reducing Uncertainty Through Mutual Information in Structural and Systems Biology which is the change in entropy about one random variable with knowledge of another. Per Theorem 2.6.5 of Cover (1999) (Information Can t Hurt), adding relevant condi- tional information will reduce the entropy of the underlying data distribution, such that H(X|Y) H(X). In our case, letKeqandKstruct be latent random variables and Xbe the random variable of the observed data point, treating the median distance of simulated points x Xas a proxy for the entropy. As we demonstrated, H(X|Keq, Kstruct )< H(X|Keq); thus, we are able to increase information gained I(X;Keq|Kstruct )> I(X;Keq). A natural extension of information gain in biology is to drug discovery where mod- els, such as DiffDock (Corso et al., 2022), can be included as conditional information about the binding of a BMP lig- and given a certain small molecule drug p(x| , Kstruct , ), where p( )is the probability of a given drug docking and disrupting normal binding. Future work A key concern about this method is the in- troduction of errors from any component of the structure prediction pipeline, from single-chain structure prediction to binding affinity prediction. Replacing these steps with probabilistic structural models such as AlphaFlow (Jing et al., 2024), protein-protein docking methods, and binding affinity predictions would provide a way to help reduce un- certainty in structure pred","Future work A key concern about this method is the in- troduction of errors from any component of the structure prediction pipeline, from single-chain structure prediction to binding affinity prediction. Replacing these steps with probabilistic structural models such as AlphaFlow (Jing et al., 2024), protein-protein docking methods, and binding affinity predictions would provide a way to help reduce un- certainty in structure prediction using systems biology data. We leave this for future work.","Reducing Uncertainty through Mutual Information in Structural and Systems Biology We describe a method that combines structural biology predictions with systems biology that helps support hypotheses in both areas. Systems biology models are useful models of complex biological systems that may require a large amount of experimental data to fit each model's parameters or to approximate a likelihood function. These models range from a few to thousands of parameters depending on the complexity of the biological system modeled, potentially making the task of fitting parameters to the model difficult - especially when new experimental data cannot be gathered. We demonstrate a method that uses structural biology predictions to augment systems biology models to improve systems biology models' predictions without having to gather more experimental data. Additionally, we show how systems biology models' predictions can help evaluate novel structural biology hypotheses, which may also be expensive or infeasible to validate.","Reducing Uncertainty through Mutual Information in Structural and Systems Biology We describe a method that combines structural biology predictions with systems biology that helps support hypotheses in both areas. Systems biology models are useful models of complex biological systems that may require a large amount of experimental data to fit each model's parameters or to approximate a likelihood function. These models range from a few to thousands of parameters depending on the complexity of the biological system modeled, potentially making the task of fitting parameters to the model difficult - especially when new experimental data cannot be gathered. We demonstrate a method that uses structural biology predictions to augment systems biology models to improve systems biology models' predictions without having to gather more experimental data. Additionally, we show how systems biology models' predictions can help evaluate novel structural biology hypotheses, which may also be expensive or infeasible to validate. Future work A key concern about this method is the in- troduction of errors from any component of the structure prediction pipeline, from single-chain structure prediction to binding affinity prediction. Replacing these steps with probabilistic structural models such as AlphaFlow (Jing et al., 2024), protein-protein docking methods, and binding affinity predictions would provide a way to help reduce un- certainty in structure prediction using systems biology data. We leave this for future work. approach provides a novel method for proposing and checking structural biology hypotheses by cross valida- tion with a systems biology model, for example supporting the structural hypothesis of a strong binding affinity in the BMP4-BMPR1A-ACVR2A complex, and the structural ba- sis as to why it might have such a strong affinity. On structural & systems biology mutual information We now formalize the use of mutual information in our framework. The mutual information between two random variables XandYcan be defined as I(X;Y) =H(X) H(X|Y), (2) Reducing Uncertainty Through Mutual Information in Structural and Systems Biology which is the change in entropy about one random variable with knowledge of another. Per Theorem 2.6.5 of Cover (1999) (Information Can t Hurt), adding relevant condi- tional information will reduce the entropy of the underlying data distribution, such that H(X|Y) H(X). In our case, letKeqandKstruct be latent random variables and Xbe the random variable of the observed data point, treating the median distance of simulated points x Xas a proxy for the entropy. As we demonstrated, H(X|Keq, Kstruct )< H(X|Keq); thus, we are able to increase information gained I(X;Keq|Kstruct )> I(X;Keq). A natural extension of information gain in biology is to drug discovery where mod- els, such as DiffDock (Corso et al., 2022), can be included as conditional information about the binding of a BMP lig- and given a certain small molecule drug p(x| , Kstruct , ), where p( )is the probability of a given drug docking and disrupting normal binding. Future work A key concern about this method is the in- troduction of errors from any component of the structure prediction pipeline, from single-chain structure prediction to binding affinity prediction. Replacing these steps with probabilistic structural models such as AlphaFlow (Jing et al., 2024), protein-protein docking methods, and binding affinity predictions would provide a way to help reduce un- certainty in structure pred",0
38d6ff840ad277c924bb5d6652e3269a8d01cc1a,Accelerating the inference of string generation-based chemical reaction models for industrial applications,"['Mikhail Andronov', 'Natalia Andronova', 'Michael Wand', 'Djork-Arné Clevert', 'Jürgen Schmidhuber']",https://openreview.net/pdf/38d6ff840ad277c924bb5d6652e3269a8d01cc1a.pdf,"Accelerating the inference of string generation-based chemical reaction models for industrial applications We accelerate the inference of the SMILES-to-SMILES transformer by three times combining speculative decoding and chemical insights Template-free SMILES-to-SMILES translation models for reaction prediction and single-step retrosynthesis are of interest for industrial applications in computer-aided synthesis planning systems due to their state-of-the-art accuracy. However, they suffer from slow inference speed. We present a method to accelerate inference in autoregressive SMILES generators through speculative decoding by copying query string subsequences into target strings in the right places. We apply our method to the molecular transformer implemented in Pytorch Lightning and achieve over 3X faster inference in reaction prediction and single-step retrosynthesis.",38d6ff840ad277c924bb5d6652e3269a8d01cc1a.pdf,"approached with a model like an encoder-decoder transformer.
The principle of template-based models is to use a set of
SMIRKS templates extracted from reaction data and a ma-
chine learning model for classification or ranking to select a
matching template for a query SMILES that will, upon ap-
plication, transform the query into the product SMILES (for
product prediction), or the SMILES of possible reactants
(for single-step retrosynthesis). In contrast, in template-
free models, the query transforms into the result without
resorting to SMIRKS templates, e.g., with a sequence of
predicted graph edits (Sacha et al., 2021; Bradshaw et al.,
2018) or through “translation” of the query SMILES into the
desired SMILES with a conditioned text generation model
(Schwaller et al., 2019; Tetko et al., 2020; Irwin et al., 2022).
While CASP systems leveraging template-based single-step
models proved to be effective (Genheden et al., 2020), there
is an interest in building CASP with template-free mod-
els instead, as they demonstrate state-of-the-art accuracy
in both single-step retrosynthesis and reaction product pre-
diction. Most accurate template-free models are currently
conditional autoregressive SMILES generators based on the
transformer architecture (Vaswani et al., 2017), which also
serves as the backbone for Large Language Models (LLM)
(Brown et al., 2020; Zhao et al., 2023). Unfortunately, the
1
Accelerating the inference of string generation-based chemical reaction models for industrial applications
high accuracy of autoregressive models like Chemformer
(Irwin et al., 2022) comes at the cost of a slow inference
speed (Torren-Peraire et al., 2024), which hinders their suc-
cessful adoption as part of industrial CASP systems. In our
work, we propose a method to accelerate inference from
SMILES-to-SMILES translation models based on specula-
tive decoding (Leviathan et al., 2023; Xia et al., 2023), a gen-
eral technique for LLM inference acceleration, combined
with insight","Conclusion
We combine speculative decoding and chemical insights
to accelerate inference in the molecular transformer, a
SMILES-to-SMILES translation model. Our method makes
processing the test set more than three times faster in both
single-step retrosynthesis on USPTO 50K and reaction prod-
4
Accelerating the inference of string generation-based chemical reaction models for industrial applications
uct prediction on USPTO MIT compared to the standard
decoding procedures. Our method aims at making state-
of-the-art template-free SMILES-generation-based models
such as the molecular transformer more suitable for indus-
trial applications such as computer-aided synthesis planning
systems.
Acknowledgements
This study was partially funded by the European Union’s
Horizon 2020 research and innovation program under the
Marie Skłodowska-Curie Innovative Training Network Eu-
ropean Industrial Doctorate grant agreement No. 956832
“Advanced machine learning for Innovative Drug Discov-
ery, and also by the Horizon Europe funding programme
under the Marie Skłodowska-Curie Actions Doctoral Net-
works grant agreement “Explainable AI for Molecules -
AiChemist” No. 101120466.
Conflict of interests
The authors have no conflicts of interests.
References
Bradshaw, J., Kusner, M. J., Paige, B., Segler, M. H., and
Hern ´andez-Lobato, J. M. A generative model for electron
paths. arXiv preprint arXiv:1805.10970 , 2018.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
Advances in Neural Information Processing Systems , 33:
1877–1901, 2020.
Cai, T., Li, Y ., Geng, Z., Peng, H., Lee, J. D., Chen, D.,
and Dao, T. Medusa: Simple LLM inference acceleration
framework with multiple decoding heads. arXiv preprint
arXiv:2401.10774 , 2024.
Gasteiger, J., Pf ¨ortner, M., Sitzmann, M., H ¨ollering, R.,
Sacher, O., Kostka, T., and Karg, N. Computer-assisted
synthesis and reaction pla","approached with a model like an encoder-decoder transformer. The principle of template-based models is to use a set of SMIRKS templates extracted from reaction data and a ma- chine learning model for classification or ranking to select a matching template for a query SMILES that will, upon ap- plication, transform the query into the product SMILES (for product prediction), or the SMILES of possible reactants (for single-step retrosynthesis). In contrast, in template- free models, the query transforms into the result without resorting to SMIRKS templates, e.g., with a sequence of predicted graph edits (Sacha et al., 2021; Bradshaw et al., 2018) or through translation of the query SMILES into the desired SMILES with a conditioned text generation model (Schwaller et al., 2019; Tetko et al., 2020; Irwin et al., 2022). While CASP systems leveraging template-based single-step models proved to be effective (Genheden et al., 2020), there is an interest in building CASP with template-free mod- els instead, as they demonstrate state-of-the-art accuracy in both single-step retrosynthesis and reaction product pre- diction. Most accurate template-free models are currently conditional autoregressive SMILES generators based on the transformer architecture (Vaswani et al., 2017), which also serves as the backbone for Large Language Models (LLM) (Brown et al., 2020; Zhao et al., 2023). Unfortunately, the 1 Accelerating the inference of string generation-based chemical reaction models for industrial applications high accuracy of autoregressive models like Chemformer (Irwin et al., 2022) comes at the cost of a slow inference speed (Torren-Peraire et al., 2024), which hinders their suc- cessful adoption as part of industrial CASP systems. In our work, we propose a method to accelerate inference from SMILES-to-SMILES translation models based on specula- tive decoding (Leviathan et al., 2023; Xia et al., 2023), a gen- eral technique for LLM inference acceleration, combined with insight","Conclusion We combine speculative decoding and chemical insights to accelerate inference in the molecular transformer, a SMILES-to-SMILES translation model. Our method makes processing the test set more than three times faster in both single-step retrosynthesis on USPTO 50K and reaction prod- 4 Accelerating the inference of string generation-based chemical reaction models for industrial applications uct prediction on USPTO MIT compared to the standard decoding procedures. Our method aims at making state- of-the-art template-free SMILES-generation-based models such as the molecular transformer more suitable for indus- trial applications such as computer-aided synthesis planning systems.","Accelerating the inference of string generation-based chemical reaction models for industrial applications We accelerate the inference of the SMILES-to-SMILES transformer by three times combining speculative decoding and chemical insights Template-free SMILES-to-SMILES translation models for reaction prediction and single-step retrosynthesis are of interest for industrial applications in computer-aided synthesis planning systems due to their state-of-the-art accuracy. However, they suffer from slow inference speed. We present a method to accelerate inference in autoregressive SMILES generators through speculative decoding by copying query string subsequences into target strings in the right places. We apply our method to the molecular transformer implemented in Pytorch Lightning and achieve over 3X faster inference in reaction prediction and single-step retrosynthesis.","Accelerating the inference of string generation-based chemical reaction models for industrial applications We accelerate the inference of the SMILES-to-SMILES transformer by three times combining speculative decoding and chemical insights Template-free SMILES-to-SMILES translation models for reaction prediction and single-step retrosynthesis are of interest for industrial applications in computer-aided synthesis planning systems due to their state-of-the-art accuracy. However, they suffer from slow inference speed. We present a method to accelerate inference in autoregressive SMILES generators through speculative decoding by copying query string subsequences into target strings in the right places. We apply our method to the molecular transformer implemented in Pytorch Lightning and achieve over 3X faster inference in reaction prediction and single-step retrosynthesis. Conclusion We combine speculative decoding and chemical insights to accelerate inference in the molecular transformer, a SMILES-to-SMILES translation model. Our method makes processing the test set more than three times faster in both single-step retrosynthesis on USPTO 50K and reaction prod- 4 Accelerating the inference of string generation-based chemical reaction models for industrial applications uct prediction on USPTO MIT compared to the standard decoding procedures. Our method aims at making state- of-the-art template-free SMILES-generation-based models such as the molecular transformer more suitable for indus- trial applications such as computer-aided synthesis planning systems. approached with a model like an encoder-decoder transformer. The principle of template-based models is to use a set of SMIRKS templates extracted from reaction data and a ma- chine learning model for classification or ranking to select a matching template for a query SMILES that will, upon ap- plication, transform the query into the product SMILES (for product prediction), or the SMILES of possible reactants (for single-step retrosynthesis). In contrast, in template- free models, the query transforms into the result without resorting to SMIRKS templates, e.g., with a sequence of predicted graph edits (Sacha et al., 2021; Bradshaw et al., 2018) or through translation of the query SMILES into the desired SMILES with a conditioned text generation model (Schwaller et al., 2019; Tetko et al., 2020; Irwin et al., 2022). While CASP systems leveraging template-based single-step models proved to be effective (Genheden et al., 2020), there is an interest in building CASP with template-free mod- els instead, as they demonstrate state-of-the-art accuracy in both single-step retrosynthesis and reaction product pre- diction. Most accurate template-free models are currently conditional autoregressive SMILES generators based on the transformer architecture (Vaswani et al., 2017), which also serves as the backbone for Large Language Models (LLM) (Brown et al., 2020; Zhao et al., 2023). Unfortunately, the 1 Accelerating the inference of string generation-based chemical reaction models for industrial applications high accuracy of autoregressive models like Chemformer (Irwin et al., 2022) comes at the cost of a slow inference speed (Torren-Peraire et al., 2024), which hinders their suc- cessful adoption as part of industrial CASP systems. In our work, we propose a method to accelerate inference from SMILES-to-SMILES translation models based on specula- tive decoding (Leviathan et al., 2023; Xia et al., 2023), a gen- eral technique for LLM inference acceleration, combined with insight",0
68ab8729b87a4a9823720791f34d3e14ec967242,Hyperspectral Unmixing for Raman Spectroscopy via Physics-Constrained Autoencoders,"['Dimitar Georgiev', 'Álvaro Fernández-Galiana', 'Simon Vilms Pedersen', 'Georgios Papadopoulos', 'Ruoxiao Xie', 'Molly M. Stevens', 'Mauricio Barahona']",https://openreview.net/pdf/68ab8729b87a4a9823720791f34d3e14ec967242.pdf,"Hyperspectral Unmixing for Raman Spectroscopy via Physics-Constrained Autoencoders We develop autoencoder models for hyperspectral unmixing of Raman spectroscopy data, which provide improved accuracy, robustness and efficiency compared to standard methods for unmixing. Raman spectroscopy is widely used across life and material sciences to characterize the chemical composition of samples in a nondestructive, label-free manner. Many applications entail the unmixing of signals from mixtures of molecular species to identify the individual components present and their proportions, yet conventional methods for chemometrics often struggle with complex mixture scenarios encountered in practice. Here, we develop autoencoder neural network models for hyperspectral unmixing of Raman spectroscopy data, which we systematically validate using synthetic and experimental benchmark datasets we created in-house. Our results demonstrate that autoencoders provide improved accuracy, robustness and efficiency compared to standard unmixing methods. We also showcase the applicability of our approach to complex biological settings by showing improved biochemical characterization of volumetric Raman imaging data from a human leukemia monocytic cell line.",68ab8729b87a4a9823720791f34d3e14ec967242.pdf,"methodology for hyperspectral unmixing in Raman spec-
troscopy, which we validated on a wide array of synthetic
and experimental datasets. Our results demonstrate that au-
toencoders are adept at handling diverse mixture scenarios
and exhibit robustness against data artifacts, offering an ef-
fective, versatile and efficient framework for RS unmixing.
Acknowledgements
D.G. and G.P. are supported by UK Research and Innovation
[UKRI Centre for Doctoral Training in AI for Healthcare
grant number EP/S023283/1]. A.F.G. acknowledges sup-
port from the Schmidt Science Fellows, in partnership with
the Rhodes Trust. S.V .P. acknowledges support from the
Independent Research Fund Denmark (0170-00011B). R.X.
and M.M.S. acknowledge support from the Engineering
and Physical Sciences Research Council (EP/P00114/1 and
EP/T020792/1). M.M.S. acknowledges support from the
Royal Academy of Engineering Chair in Emerging Tech-
nologies award (CiET2021 \94) and the Bio Innovation In-
stitute. M.B. acknowledges support from the Engineering
and Physical Sciences Research Council (EP/N014529/1,
funding the EPSRC Centre for Mathematics of Precision
Healthcare at Imperial, and EP/T027258/1). The authors
thank Dr Akemi Nogiwa Valdez for proofreading and data
management support. Figures were assembled in BioRen-
der.
References
Auner, G. W., Koya, S. K., Huang, C., Broadbent, B.,
Trexler, M., Auner, Z., Elias, A., Mehne, K. C., and
Brusatori, M. A. Applications of Raman spectroscopy in
cancer diagnosis. Cancer and Metastasis Reviews , 37(4):
691–717, 2018.
Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.
arXiv preprint arXiv:1607.06450 , 2016.
Balan, V ., Mihai, C.-T., Cojocaru, F.-D., Uritu, C.-M., Dodi,
G., Botezat, D., and Gardikiotis, I. Vibrational spec-
troscopy fingerprinting in medicine: from molecular to
clinical practice. Materials , 12(18):2884, 2019.
Bhatt, J. S. and Joshi, M. V . Deep learning in hyperspectral
unmixing: A review. In IGARSS 2020-2020 IEEE Inter-
natio","Conclusion
In summary, we have presented an autoencoder-based
methodology for hyperspectral unmixing in Raman spec-
troscopy, which we validated on a wide array of synthetic
and experimental datasets. Our results demonstrate that au-
toencoders are adept at handling diverse mixture scenarios
and exhibit robustness against data artifacts, offering an ef-
fective, versatile and efficient framework for RS unmixing.
Acknowledgements
D.G. and G.P. are supported by UK Research and Innovation
[UKRI Centre for Doctoral Training in AI for Healthcare
grant number EP/S023283/1]. A.F.G. acknowledges sup-
port from the Schmidt Science Fellows, in partnership with
the Rhodes Trust. S.V .P. acknowledges support from the
Independent Research Fund Denmark (0170-00011B). R.X.
and M.M.S. acknowledge support from the Engineering
and Physical Sciences Research Council (EP/P00114/1 and
EP/T020792/1). M.M.S. acknowledges support from the
Royal Academy of Engineering Chair in Emerging Tech-
nologies award (CiET2021 \94) and the Bio Innovation In-
stitute. M.B. acknowledges support from the Engineering
and Physical Sciences Research Council (EP/N014529/1,
funding the EPSRC Centre for Mathematics of Precision
Healthcare at Imperial, and EP/T027258/1). The authors
thank Dr Akemi Nogiwa Valdez for proofreading and data
management support. Figures were assembled in BioRen-
der.
References
Auner, G. W., Koya, S. K., Huang, C., Broadbent, B.,
Trexler, M., Auner, Z., Elias, A., Mehne, K. C., and
Brusatori, M. A. Applications of Raman spectroscopy in
cancer diagnosis. Cancer and Metastasis Reviews , 37(4):
691–717, 2018.
Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.
arXiv preprint arXiv:1607.06450 , 2016.
Balan, V ., Mihai, C.-T., Cojocaru, F.-D., Uritu, C.-M., Dodi,
G., Botezat, D., and Gardikiotis, I. Vibrational spec-
troscopy fingerprinting in medicine: from molecular to
clinical practice. Materials , 12(18):2884, 2019.
Bhatt, J. S. and Joshi, M. V . Deep learning in hyperspec","methodology for hyperspectral unmixing in Raman spec- troscopy, which we validated on a wide array of synthetic and experimental datasets. Our results demonstrate that au- toencoders are adept at handling diverse mixture scenarios and exhibit robustness against data artifacts, offering an ef- fective, versatile and efficient framework for RS unmixing.","Conclusion In summary, we have presented an autoencoder-based methodology for hyperspectral unmixing in Raman spec- troscopy, which we validated on a wide array of synthetic and experimental datasets. Our results demonstrate that au- toencoders are adept at handling diverse mixture scenarios and exhibit robustness against data artifacts, offering an ef- fective, versatile and efficient framework for RS unmixing.","Hyperspectral Unmixing for Raman Spectroscopy via Physics-Constrained Autoencoders We develop autoencoder models for hyperspectral unmixing of Raman spectroscopy data, which provide improved accuracy, robustness and efficiency compared to standard methods for unmixing. Raman spectroscopy is widely used across life and material sciences to characterize the chemical composition of samples in a nondestructive, label-free manner. Many applications entail the unmixing of signals from mixtures of molecular species to identify the individual components present and their proportions, yet conventional methods for chemometrics often struggle with complex mixture scenarios encountered in practice. Here, we develop autoencoder neural network models for hyperspectral unmixing of Raman spectroscopy data, which we systematically validate using synthetic and experimental benchmark datasets we created in-house. Our results demonstrate that autoencoders provide improved accuracy, robustness and efficiency compared to standard unmixing methods. We also showcase the applicability of our approach to complex biological settings by showing improved biochemical characterization of volumetric Raman imaging data from a human leukemia monocytic cell line.","Hyperspectral Unmixing for Raman Spectroscopy via Physics-Constrained Autoencoders We develop autoencoder models for hyperspectral unmixing of Raman spectroscopy data, which provide improved accuracy, robustness and efficiency compared to standard methods for unmixing. Raman spectroscopy is widely used across life and material sciences to characterize the chemical composition of samples in a nondestructive, label-free manner. Many applications entail the unmixing of signals from mixtures of molecular species to identify the individual components present and their proportions, yet conventional methods for chemometrics often struggle with complex mixture scenarios encountered in practice. Here, we develop autoencoder neural network models for hyperspectral unmixing of Raman spectroscopy data, which we systematically validate using synthetic and experimental benchmark datasets we created in-house. Our results demonstrate that autoencoders provide improved accuracy, robustness and efficiency compared to standard unmixing methods. We also showcase the applicability of our approach to complex biological settings by showing improved biochemical characterization of volumetric Raman imaging data from a human leukemia monocytic cell line. Conclusion In summary, we have presented an autoencoder-based methodology for hyperspectral unmixing in Raman spec- troscopy, which we validated on a wide array of synthetic and experimental datasets. Our results demonstrate that au- toencoders are adept at handling diverse mixture scenarios and exhibit robustness against data artifacts, offering an ef- fective, versatile and efficient framework for RS unmixing. methodology for hyperspectral unmixing in Raman spec- troscopy, which we validated on a wide array of synthetic and experimental datasets. Our results demonstrate that au- toencoders are adept at handling diverse mixture scenarios and exhibit robustness against data artifacts, offering an ef- fective, versatile and efficient framework for RS unmixing.",0
fc9573475550b12bd11247f63ffda756f1b3fb20,Enhancing Multi-Tip Artifact Detection in STM Images Using Fourier Transform and Vision Transformers,"['Tommaso Rodani', 'Alessio ansuini', 'Alberto Cazzaniga']",https://openreview.net/pdf/fc9573475550b12bd11247f63ffda756f1b3fb20.pdf,"Enhancing Multi-Tip Artifact Detection in STM Images Using Fourier Transform and Vision Transformers We enhance multi-tip artifact detection in STM images using FFT-based preprocessing and Vision Transformers We address the issue of multi-tip artifacts in Scanning Tunneling Microscopy (STM) images by applying the fast Fourier transform (FFT) as a feature engineering method. We fine-tune various neural network architectures using a synthetic dataset, including Vision Transformers (ViT). The FFT-based preprocessing significantly improves the performance of ViT models compared to using only the grayscale channel. Ablation experiments highlight the optimal conditions for synthetic dataset generation. Unlike traditional methods that are challenging to implement for large datasets and used offline, our method enables on-the-fly classification at scale. Our findings demonstrate the efficacy of combining the Fourier transform with deep learning for enhanced artifact detection in STM images, contributing to more accurate analysis in material science research.",fc9573475550b12bd11247f63ffda756f1b3fb20.pdf,"approaches are challenging to implement for general
use, especially with large datasets, and existing software
tools such as Gwyddion [19] are typically used offline after
data acquisition. In contrast, our method enables on-the-fly
classification at scale.
The Fourier transform, a powerful tool for extracting subtle
information, can reveal features such as perturbations that
are not noticeable in the spatial domain but become promi-
nent in the Fourier domain[20]. In this paper, we address
multi-tip artifacts in STM images by applying the Fourier
transform to decompose image content into constituent fre-
quencies. This transformation enhances the model’s ability
to identify and classify these artifacts accurately by making
key information more discernible in the frequency domain.
Enhancing Multi-Tip Artifact Detection in STM Images Using Fourier Transform and Vision Transformers
2. Method
We generated a synthetic dataset using experimental STM
images originally recorded using an Omicron Variable Tem-
perature STM (VT-STM) microscope. The dataset is com-
posed of 2080 grayscale images of 400x400 pixels, man-
ually labeled as either containing (82 images) or not con-
taining (1998 images) the multi-tip artifact. We’ll refer to
artifact-free images as clean during the rest of the paper.
To create a balanced synthetic dataset, we transformed a
subset of clean images into synthetic multi-tip images. This
was achieved by summing the clean image and N of its
translations, where N represents the number of tips. The hy-
perparameters used for the dataset generation included the
X and Y components of the translation vector set between 2
to 8 pixels, the intensity of the translation set between 50%
to 80% of the original signal, and a uniform number of tips
up to 12.
The images were then cropped to 224x224, a suitable size
for training. The Fourier Transform produces a complex-
valued output image, which can be displayed using either
the real and imaginary parts or the amplit","Conclusion
In this study, we address the challenge of multi-tip arti-
facts in STM images by leveraging the Fourier transform to
decompose image content into constituent frequencies.
Our experimental results demonstrated that Vision Trans-
former models showed considerable performance improve-
ments when using the proposed FFT-based preprocessing
method compared to using only the greyscale channel. Fur-
Enhancing Multi-Tip Artifact Detection in STM Images Using Fourier Transform and Vision Transformers
Table 1. Classification accuracies of different models with and
without our FFT-method on the test set.
MODEL ACCURACY FFT- BASED
RESNET18 52.43 ×
RESNET18 57.07√
RESNET50 58.84 ×
RESNET50 65.24√
VIT-B/32 78.96 ×
VIT-B/16 88.11 ×
VIT-B/16 97.25√
VIT-B/32 97.86√
Table 2. Classification accuracies of different models for each com-
ponent of our FFT-method on the test set.
MODEL IMAGE AMPLITUDE PHASE
RESNET18 52.43 60.06 53.04
RESNET50 58.84 62.50 51.21
VIT-B/16 88.11 97.56 66.15
VIT-B/32 78.96 97.86 68.90
Table 3. Classification accuracies of ViT-B/16 on the test set using
different range of possible tips on the synthetic dataset generation.
TIPS ACCURACY
4 75.30
8 75.61
12 84.76
16 71.34
Table 4. Classification accuracies on the test set using different
pixel ranges of translation vector on the synthetic dataset genera-
tion.
PIXEL RANGE ACCURACY
2-8 97.86
10-30 85.97
10-50 92.98
ther analysis revealed that the amplitude component was
the primary contributor to this improvement, underscoring
its importance in capturing the geometrical structure of fea-
tures in the images. Moreover, theoretical findings suggest
that Multi-Head Self-Attention (MSA) inherently performs
low-pass filtering on image signals, leading to rank collapse
and patch uniformity issues in deep Vision Transformers
[21]. This might explain why the FFT-based method out-
performs, as FFT-based preprocessing transforms image
data into the frequency domain, effectively highlighting and
preserving high-fre","approaches are challenging to implement for general use, especially with large datasets, and existing software tools such as Gwyddion [19] are typically used offline after data acquisition. In contrast, our method enables on-the-fly classification at scale. The Fourier transform, a powerful tool for extracting subtle information, can reveal features such as perturbations that are not noticeable in the spatial domain but become promi- nent in the Fourier domain[20]. In this paper, we address multi-tip artifacts in STM images by applying the Fourier transform to decompose image content into constituent fre- quencies. This transformation enhances the model s ability to identify and classify these artifacts accurately by making key information more discernible in the frequency domain. Enhancing Multi-Tip Artifact Detection in STM Images Using Fourier Transform and Vision Transformers 2. Method We generated a synthetic dataset using experimental STM images originally recorded using an Omicron Variable Tem- perature STM (VT-STM) microscope. The dataset is com- posed of 2080 grayscale images of 400x400 pixels, man- ually labeled as either containing (82 images) or not con- taining (1998 images) the multi-tip artifact. We ll refer to artifact-free images as clean during the rest of the paper. To create a balanced synthetic dataset, we transformed a subset of clean images into synthetic multi-tip images. This was achieved by summing the clean image and N of its translations, where N represents the number of tips. The hy- perparameters used for the dataset generation included the X and Y components of the translation vector set between 2 to 8 pixels, the intensity of the translation set between 50% to 80% of the original signal, and a uniform number of tips up to 12. The images were then cropped to 224x224, a suitable size for training. The Fourier Transform produces a complex- valued output image, which can be displayed using either the real and imaginary parts or the amplit","Conclusion In this study, we address the challenge of multi-tip arti- facts in STM images by leveraging the Fourier transform to decompose image content into constituent frequencies. Our experimental results demonstrated that Vision Trans- former models showed considerable performance improve- ments when using the proposed FFT-based preprocessing method compared to using only the greyscale channel. Fur- Enhancing Multi-Tip Artifact Detection in STM Images Using Fourier Transform and Vision Transformers Table 1. Classification accuracies of different models with and without our FFT-method on the test set. MODEL ACCURACY FFT- BASED RESNET18 52.43 RESNET18 57.07 RESNET50 58.84 RESNET50 65.24 VIT-B/32 78.96 VIT-B/16 88.11 VIT-B/16 97.25 VIT-B/32 97.86 Table 2. Classification accuracies of different models for each com- ponent of our FFT-method on the test set. MODEL IMAGE AMPLITUDE PHASE RESNET18 52.43 60.06 53.04 RESNET50 58.84 62.50 51.21 VIT-B/16 88.11 97.56 66.15 VIT-B/32 78.96 97.86 68.90 Table 3. Classification accuracies of ViT-B/16 on the test set using different range of possible tips on the synthetic dataset generation. TIPS ACCURACY 4 75.30 8 75.61 12 84.76 16 71.34 Table 4. Classification accuracies on the test set using different pixel ranges of translation vector on the synthetic dataset genera- tion. PIXEL RANGE ACCURACY 2-8 97.86 10-30 85.97 10-50 92.98 ther analysis revealed that the amplitude component was the primary contributor to this improvement, underscoring its importance in capturing the geometrical structure of fea- tures in the images. Moreover, theoretical findings suggest that Multi-Head Self-Attention (MSA) inherently performs low-pass filtering on image signals, leading to rank collapse and patch uniformity issues in deep Vision Transformers [21]. This might explain why the FFT-based method out- performs, as FFT-based preprocessing transforms image data into the frequency domain, effectively highlighting and preserving high-fre","Enhancing Multi-Tip Artifact Detection in STM Images Using Fourier Transform and Vision Transformers We enhance multi-tip artifact detection in STM images using FFT-based preprocessing and Vision Transformers We address the issue of multi-tip artifacts in Scanning Tunneling Microscopy (STM) images by applying the fast Fourier transform (FFT) as a feature engineering method. We fine-tune various neural network architectures using a synthetic dataset, including Vision Transformers (ViT). The FFT-based preprocessing significantly improves the performance of ViT models compared to using only the grayscale channel. Ablation experiments highlight the optimal conditions for synthetic dataset generation. Unlike traditional methods that are challenging to implement for large datasets and used offline, our method enables on-the-fly classification at scale. Our findings demonstrate the efficacy of combining the Fourier transform with deep learning for enhanced artifact detection in STM images, contributing to more accurate analysis in material science research.","Enhancing Multi-Tip Artifact Detection in STM Images Using Fourier Transform and Vision Transformers We enhance multi-tip artifact detection in STM images using FFT-based preprocessing and Vision Transformers We address the issue of multi-tip artifacts in Scanning Tunneling Microscopy (STM) images by applying the fast Fourier transform (FFT) as a feature engineering method. We fine-tune various neural network architectures using a synthetic dataset, including Vision Transformers (ViT). The FFT-based preprocessing significantly improves the performance of ViT models compared to using only the grayscale channel. Ablation experiments highlight the optimal conditions for synthetic dataset generation. Unlike traditional methods that are challenging to implement for large datasets and used offline, our method enables on-the-fly classification at scale. Our findings demonstrate the efficacy of combining the Fourier transform with deep learning for enhanced artifact detection in STM images, contributing to more accurate analysis in material science research. Conclusion In this study, we address the challenge of multi-tip arti- facts in STM images by leveraging the Fourier transform to decompose image content into constituent frequencies. Our experimental results demonstrated that Vision Trans- former models showed considerable performance improve- ments when using the proposed FFT-based preprocessing method compared to using only the greyscale channel. Fur- Enhancing Multi-Tip Artifact Detection in STM Images Using Fourier Transform and Vision Transformers Table 1. Classification accuracies of different models with and without our FFT-method on the test set. MODEL ACCURACY FFT- BASED RESNET18 52.43 RESNET18 57.07 RESNET50 58.84 RESNET50 65.24 VIT-B/32 78.96 VIT-B/16 88.11 VIT-B/16 97.25 VIT-B/32 97.86 Table 2. Classification accuracies of different models for each com- ponent of our FFT-method on the test set. MODEL IMAGE AMPLITUDE PHASE RESNET18 52.43 60.06 53.04 RESNET50 58.84 62.50 51.21 VIT-B/16 88.11 97.56 66.15 VIT-B/32 78.96 97.86 68.90 Table 3. Classification accuracies of ViT-B/16 on the test set using different range of possible tips on the synthetic dataset generation. TIPS ACCURACY 4 75.30 8 75.61 12 84.76 16 71.34 Table 4. Classification accuracies on the test set using different pixel ranges of translation vector on the synthetic dataset genera- tion. PIXEL RANGE ACCURACY 2-8 97.86 10-30 85.97 10-50 92.98 ther analysis revealed that the amplitude component was the primary contributor to this improvement, underscoring its importance in capturing the geometrical structure of fea- tures in the images. Moreover, theoretical findings suggest that Multi-Head Self-Attention (MSA) inherently performs low-pass filtering on image signals, leading to rank collapse and patch uniformity issues in deep Vision Transformers [21]. This might explain why the FFT-based method out- performs, as FFT-based preprocessing transforms image data into the frequency domain, effectively highlighting and preserving high-fre approaches are challenging to implement for general use, especially with large datasets, and existing software tools such as Gwyddion [19] are typically used offline after data acquisition. In contrast, our method enables on-the-fly classification at scale. The Fourier transform, a powerful tool for extracting subtle information, can reveal features such as perturbations that are not noticeable in the spatial domain but become promi- nent in the Fourier domain[20]. In this paper, we address multi-tip artifacts in STM images by applying the Fourier transform to decompose image content into constituent fre- quencies. This transformation enhances the model s ability to identify and classify these artifacts accurately by making key information more discernible in the frequency domain. Enhancing Multi-Tip Artifact Detection in STM Images Using Fourier Transform and Vision Transformers 2. Method We generated a synthetic dataset using experimental STM images originally recorded using an Omicron Variable Tem- perature STM (VT-STM) microscope. The dataset is com- posed of 2080 grayscale images of 400x400 pixels, man- ually labeled as either containing (82 images) or not con- taining (1998 images) the multi-tip artifact. We ll refer to artifact-free images as clean during the rest of the paper. To create a balanced synthetic dataset, we transformed a subset of clean images into synthetic multi-tip images. This was achieved by summing the clean image and N of its translations, where N represents the number of tips. The hy- perparameters used for the dataset generation included the X and Y components of the translation vector set between 2 to 8 pixels, the intensity of the translation set between 50% to 80% of the original signal, and a uniform number of tips up to 12. The images were then cropped to 224x224, a suitable size for training. The Fourier Transform produces a complex- valued output image, which can be displayed using either the real and imaginary parts or the amplit",0
882a126c0529a61a8d75874de2487adf45aa7fc4,Energy-Free Guidance of Geometric Diffusion Models for 3D Molecule Inverse Design,"['Sanjay Nagaraj', 'Jiaqi Han', 'Aksh Garg', 'Minkai Xu']",https://openreview.net/pdf/882a126c0529a61a8d75874de2487adf45aa7fc4.pdf,"Energy-Free Guidance of Geometric Diffusion Models for 3D Molecule Inverse Design Propose a novel energy-free guidance method for inverse molecular design, which achieves SOTA results on 4 out of 6 QM9 properties. Molecule inverse design is of critical significance in drug discovery which requires molecules to be generated based on certain chemical properties or structural compositions. Generative models, most popularly diffusion models, have shown great promise in performing inverse design through conditioning techniques and/or explicit energy guidance during sampling. In this work, we propose a novel guidance framework, Energy-Free Guidance for Geometric Diffusion Models, that effectively boosts the utility of molecule inverse design without any auxiliary energy head for guidance. The key innovation lies in the joint training strategy for the conditional and unconditional score models via random masking, which are then composed during sampling in an SE(3)-equivariant fashion, ensuring the critical physical symmetry of the geometric distribution. This feature alleviates practitioners from needing additional efforts in training energy prediction heads and avoids the adversarial gradient coming from them. We conduct experiments on a diverse range of inverse design tasks on QM9, showing that our approach achieves state-of-the-art on 4 out of 6 design targets without leveraging any external energy gradients.",882a126c0529a61a8d75874de2487adf45aa7fc4.pdf,"approach achieves state-of-the-art on 4
out of 6 design targets without leveraging any
external energy gradients. Code is available at
https://github.com/akshgarg7/cfgedm.
1. Introduction
Generative models, especially diffusion models (Sohl-
Dickstein et al., 2015; Song & Ermon, 2019; Ho et al.,
2020; Song et al., 2021), have made remarkable progress
*Equal contribution . Jiaqi and Minkai discussed Energy-free
Guidance and Jiaqi explored the idea; Independently, Aksh and
Sanjay studied classifier-free guidance as a Stanford CS236 course
project mentored by Minkai. All authors later collaborated to-
gether to wrap up individual results as this paper.1Department
of Computer Science, Stanford University, Stanford, CA 94305,
USA. Correspondence to: Minkai Xu <minkai@stanford.edu >.
Accepted at the 1st Machine Learning for Life and Material Sci-
ences Workshop at ICML 2024 . Copyright 2024 by the author(s).inde novo 3D molecular design, a fundamental component
in the blueprint of artificial intelligence-driven drug discov-
ery (Cheng et al., 2021). The core to the success of these
models, namely geometric diffusion models (Xu et al., 2022;
2023; Hoogeboom et al., 2022), lies in the preservation of
the physical symmetry in the marginal distribution of the
generated molecules. In particular, they feature an SE (3)-
equivariant diffusion process with the reverse kernels param-
eterized by SE (3)-equivariant graph neural networks, thus
ensuring that the molecular structures reachable via rota-
tions and translations in 3D share the same likelihood. This
consideration has been demonstrated as being effective in
promoting the sample quality of geometric diffusion models.
Among the tasks in molecule design, 3D molecular inverse
design (Bao et al., 2022) has attracted growing interest,
which requires generation of 3D molecules conditioning on
desired properties, e.g., polarizability or structural patterns.
However, tackling such a task is highly challenging, since
the inverse de","Future work
include extending EFG-GDM to more advanced geometric
diffusion models and applying it to more datasets and tasks,
e.g., target-aware molecule generation.References
Bao, F., Zhao, M., Hao, Z., Li, P., Li, C., and Zhu, J.
Equivariant energy-guided sde for inverse molecular de-
sign, 2022. URL https://arxiv.org/abs/2209.
15408 .
Cheng, Y ., Gong, Y ., Liu, Y ., Song, B., and Zou, Q.
Molecular design in drug discovery: a comprehensive
review of deep generative models. Briefings in Bioin-
formatics , 22(6), August 2021. ISSN 1477-4054. doi:
10.1093/bib/bbab344. URL http://dx.doi.org/
10.1093/bib/bbab344 .
Dhariwal, P. and Nichol, A. Diffusion models beat gans on
image synthesis, 2021. URL https://arxiv.org/
abs/2105.05233 .
Ho, J. and Salimans, T. Classifier-free diffusion guid-
ance, 2022. URL https://arxiv.org/abs/
2207.12598 .
Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba-
bilistic models. Advances in neural information process-
ing systems , 33:6840–6851, 2020.
Hoogeboom, E., Satorras, V . G., Vignac, C., and Welling, M.
Equivariant diffusion for molecule generation in 3d. In
International conference on machine learning , pp. 8867–
8887. PMLR, 2022.
K¨ohler, J., Klein, L., and No ´e, F. Equivariant flows: sam-
pling configurations for multi-body systems with sym-
metric energies. arXiv preprint arXiv:1910.00753 , 2019.
Lee, Y . J., Kahng, H., and Kim, S. B. Generative adver-
sarial networks for de novo molecular design. Molec-
ular Informatics , 40(10), July 2021. ISSN 1868-1751.
doi: 10.1002/minf.202100045. URL http://dx.doi.
org/10.1002/minf.202100045 .
Lim, J., Ryu, S., Kim, J. W., and Kim, W. Y . Molecular
generative model based on conditional variational autoen-
coder for de novo molecular design. Journal of Chem-
informatics , 10(1), July 2018. ISSN 1758-2946. doi:
10.1186/s13321-018-0286-7. URL http://dx.doi.
org/10.1186/s13321-018-0286-7 .
Luo, Y . and Ji, S. An autoregressive flow model for 3d
molecular geometry generation from scrat","approach achieves state-of-the-art on 4 out of 6 design targets without leveraging any external energy gradients. Code is available at https://github.com/akshgarg7/cfgedm. 1. Introduction Generative models, especially diffusion models (Sohl- Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021), have made remarkable progress *Equal contribution . Jiaqi and Minkai discussed Energy-free Guidance and Jiaqi explored the idea; Independently, Aksh and Sanjay studied classifier-free guidance as a Stanford CS236 course project mentored by Minkai. All authors later collaborated to- gether to wrap up individual results as this paper.1Department of Computer Science, Stanford University, Stanford, CA 94305, USA. Correspondence to: Minkai Xu <minkai@stanford.edu >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).inde novo 3D molecular design, a fundamental component in the blueprint of artificial intelligence-driven drug discov- ery (Cheng et al., 2021). The core to the success of these models, namely geometric diffusion models (Xu et al., 2022; 2023; Hoogeboom et al., 2022), lies in the preservation of the physical symmetry in the marginal distribution of the generated molecules. In particular, they feature an SE (3)- equivariant diffusion process with the reverse kernels param- eterized by SE (3)-equivariant graph neural networks, thus ensuring that the molecular structures reachable via rota- tions and translations in 3D share the same likelihood. This consideration has been demonstrated as being effective in promoting the sample quality of geometric diffusion models. Among the tasks in molecule design, 3D molecular inverse design (Bao et al., 2022) has attracted growing interest, which requires generation of 3D molecules conditioning on desired properties, e.g., polarizability or structural patterns. However, tackling such a task is highly challenging, since the inverse de","Future work include extending EFG-GDM to more advanced geometric diffusion models and applying it to more datasets and tasks, e.g., target-aware molecule generation.","Energy-Free Guidance of Geometric Diffusion Models for 3D Molecule Inverse Design Propose a novel energy-free guidance method for inverse molecular design, which achieves SOTA results on 4 out of 6 QM9 properties. Molecule inverse design is of critical significance in drug discovery which requires molecules to be generated based on certain chemical properties or structural compositions. Generative models, most popularly diffusion models, have shown great promise in performing inverse design through conditioning techniques and/or explicit energy guidance during sampling. In this work, we propose a novel guidance framework, Energy-Free Guidance for Geometric Diffusion Models, that effectively boosts the utility of molecule inverse design without any auxiliary energy head for guidance. The key innovation lies in the joint training strategy for the conditional and unconditional score models via random masking, which are then composed during sampling in an SE(3)-equivariant fashion, ensuring the critical physical symmetry of the geometric distribution. This feature alleviates practitioners from needing additional efforts in training energy prediction heads and avoids the adversarial gradient coming from them. We conduct experiments on a diverse range of inverse design tasks on QM9, showing that our approach achieves state-of-the-art on 4 out of 6 design targets without leveraging any external energy gradients.","Energy-Free Guidance of Geometric Diffusion Models for 3D Molecule Inverse Design Propose a novel energy-free guidance method for inverse molecular design, which achieves SOTA results on 4 out of 6 QM9 properties. Molecule inverse design is of critical significance in drug discovery which requires molecules to be generated based on certain chemical properties or structural compositions. Generative models, most popularly diffusion models, have shown great promise in performing inverse design through conditioning techniques and/or explicit energy guidance during sampling. In this work, we propose a novel guidance framework, Energy-Free Guidance for Geometric Diffusion Models, that effectively boosts the utility of molecule inverse design without any auxiliary energy head for guidance. The key innovation lies in the joint training strategy for the conditional and unconditional score models via random masking, which are then composed during sampling in an SE(3)-equivariant fashion, ensuring the critical physical symmetry of the geometric distribution. This feature alleviates practitioners from needing additional efforts in training energy prediction heads and avoids the adversarial gradient coming from them. We conduct experiments on a diverse range of inverse design tasks on QM9, showing that our approach achieves state-of-the-art on 4 out of 6 design targets without leveraging any external energy gradients. Future work include extending EFG-GDM to more advanced geometric diffusion models and applying it to more datasets and tasks, e.g., target-aware molecule generation. approach achieves state-of-the-art on 4 out of 6 design targets without leveraging any external energy gradients. Code is available at https://github.com/akshgarg7/cfgedm. 1. Introduction Generative models, especially diffusion models (Sohl- Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021), have made remarkable progress *Equal contribution . Jiaqi and Minkai discussed Energy-free Guidance and Jiaqi explored the idea; Independently, Aksh and Sanjay studied classifier-free guidance as a Stanford CS236 course project mentored by Minkai. All authors later collaborated to- gether to wrap up individual results as this paper.1Department of Computer Science, Stanford University, Stanford, CA 94305, USA. Correspondence to: Minkai Xu <minkai@stanford.edu >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).inde novo 3D molecular design, a fundamental component in the blueprint of artificial intelligence-driven drug discov- ery (Cheng et al., 2021). The core to the success of these models, namely geometric diffusion models (Xu et al., 2022; 2023; Hoogeboom et al., 2022), lies in the preservation of the physical symmetry in the marginal distribution of the generated molecules. In particular, they feature an SE (3)- equivariant diffusion process with the reverse kernels param- eterized by SE (3)-equivariant graph neural networks, thus ensuring that the molecular structures reachable via rota- tions and translations in 3D share the same likelihood. This consideration has been demonstrated as being effective in promoting the sample quality of geometric diffusion models. Among the tasks in molecule design, 3D molecular inverse design (Bao et al., 2022) has attracted growing interest, which requires generation of 3D molecules conditioning on desired properties, e.g., polarizability or structural patterns. However, tackling such a task is highly challenging, since the inverse de",0
00c3a11ae5cca4970e9dc093d22eb65943492602,Protein Language Models in Directed Evolution,"['Russell Maguire', 'Kotryna Bloznelyte', 'Fikayo Adepoju', 'Matthew Armean-Jones', 'Shafiat Dewan', 'Akash Gupta', 'Frances Patricia Jones', 'Preet Lalli', 'Anna Schooneveld', 'Sean Thompson', 'Ece Ebrahimi', 'Stella Fozzard', 'David Berman', 'Luca Rossoni', 'Will Addison', 'Ian Taylor']",https://openreview.net/pdf/00c3a11ae5cca4970e9dc093d22eb65943492602.pdf,"Protein Language Models in Directed Evolution The dominant paradigms for integrating machine-learning into protein engineering are de novo protein design and guided directed evolution. Guiding directed evolution requires a model of protein fitness, but most models are only evaluated in silico on datasets comprising few mutations. Due to the limited number of mutations in these datasets, it is unclear how well these models can guide directed evolution efforts. We demonstrate in vitro how zero-shot and few-shot protein language models of fitness can be used to guide two rounds of directed evolution with simulated annealing. Our few-shot simulated annealing approach recommended enzyme variants with 1.62 improved PET degradation over 72 h period, outperforming the top engineered variant from the literature, which was 1.40 fitter than wild-type. In the second round, 240 in vitro examples were used for training, 32 homologous sequences were used for evolutionary context and 176 variants were evaluated for improved PET degradation, achieving a hit-rate of 39 % of variants fitter than wild-type.",00c3a11ae5cca4970e9dc093d22eb65943492602.pdf,"approach recom-
mended enzyme variants with 1.62 × improved
PET degradation over 72 h period, outperform-
ing the top engineered variant from the literature,
which was 1.40 × fitter than wild-type. In the sec-
ond round, 240in vitro examples were used for
training, 32homologous sequences were used for
evolutionary context and 176variants were evalu-
ated for improved PET degradation, achieving a
hit-rate of 39 % of variants fitter than wild-type.
1. Introduction
Proteins are nature’s catalytic and functional materials;
macro-molecules increasingly programmed in the lab for so-
cietal benefit with applications in biomaterials, diagnostics
and biocatalysis.
Whilst strides have been made in de novo protein design in
recent years (Ferruz et al., 2022; Wang et al., 2022; Watson
et al., 2023), directed evolution comprising iterative mutage-
nesis and screening remains the primary method used for the
development of optimized protein variants (Arnold, 1998;
*Equal contribution1Cambridge Consultants, Cambridge,
United Kingdom2University of Cambridge, Cambridge, United
Kingdom. Correspondence to: Russell Maguire <rus-
sell.maguire@cambridgeconsultants.com >.
Accepted at the 1st Machine Learning for Life and Material Sci-
ences Workshop at ICML 2024 . Copyright 2024 by the author(s).Arnold & V olkov, 1999; Turner, 2009; Wang et al., 2021).
Protein sequence space is vast and epistatic, so directed evo-
lution experiments may require upwards of 10 000 screen-
ing samples to achieve desired protein function (Romero &
Arnold, 2009; Sarkisyan et al., 2016; Hartman & Tullman-
Ercek, 2019), which is a screening burden sufficient to dis-
suade adoption of protein engineering in smaller scale labs.
Machine-learning-guided directed evolution has emerged as
a paradigm for improving the sample efficiency of protein
engineering efforts (Yang et al., 2019; Biswas et al., 2021;
Saito et al., 2021; Wittmann et al., 2021), and comprises
twoin silico components: a model of protein fitness and a","Discussion
Site-saturation mutagenesis is a powerful tool for explor-
ing a small number of mutations in a focused region, but
machine-learning-guided mutagenesis can explore a largerTable 1. Comparison of PET degradation, fPET, of variants from
each experiment relative to wild-type LCC; hit-rate, which is the
proportion of variants fitter than controls; and Spearman rank cor-
relation, which measures repeatability across two bio-replicates.
Few-shot variants are optimized for fPET.
epPCR Zero-shot Few-shot
Fitness, mean 0.24 0.58 0.68
fPETmedian 0.03 0.71 0.72
max 1.24 1.20 1.62
Hit-rate, fPET>LCC 0.05 0.20 0.39
fPET>ICCM 0.00 0.00 0.10
Spearman, ρ 0.89 0.83 0.94
Controls, fPETLCC — — 1.00
fPETICCM — — 1.40
number of mutations across the scaffold and active site,
and identify more beneficial mutations. In Tournier et al.
(2020), ICCM was identified by recombination of four out
of five beneficial mutations around the active site identified
by site-saturation mutagenesis and an additional stabilizing
disulfide bond. However, with only five beneficial mutation
sites, there is little scope to improve ICCM further without
another round of mutagenesis. In contrast, our machine-
learning-guided approach identified a variant with 16 %
greater catalytic activity than ICCM exploring a similar
number of variants. The resulting library of beneficial muta-
tions across the scaffold and active site could be recombined
to further improve performance.
In vitro measurements are necessary to ensure alignment
between machine-learning-guided variants and desired phe-
notypes. The zero-shot variants measured in our assay had
a significantly higher hit-rate relative to LCC than epPCR,
but variants with performance greater than ICCM were only
identified after training few-shot models on those zero-shot
Protein Language Models in Directed Evolution
(a) Tournier et al. (2020)
 (b) Few-shot simulated annealing
P osition along the sequenceMutant amino acid
(c) Tournier et al. (2020)
P ositio","approach recom- mended enzyme variants with 1.62 improved PET degradation over 72 h period, outperform- ing the top engineered variant from the literature, which was 1.40 fitter than wild-type. In the sec- ond round, 240in vitro examples were used for training, 32homologous sequences were used for evolutionary context and 176variants were evalu- ated for improved PET degradation, achieving a hit-rate of 39 % of variants fitter than wild-type. 1. Introduction Proteins are nature s catalytic and functional materials; macro-molecules increasingly programmed in the lab for so- cietal benefit with applications in biomaterials, diagnostics and biocatalysis. Whilst strides have been made in de novo protein design in recent years (Ferruz et al., 2022; Wang et al., 2022; Watson et al., 2023), directed evolution comprising iterative mutage- nesis and screening remains the primary method used for the development of optimized protein variants (Arnold, 1998; *Equal contribution1Cambridge Consultants, Cambridge, United Kingdom2University of Cambridge, Cambridge, United Kingdom. Correspondence to: Russell Maguire <rus- sell.maguire@cambridgeconsultants.com >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).Arnold & V olkov, 1999; Turner, 2009; Wang et al., 2021). Protein sequence space is vast and epistatic, so directed evo- lution experiments may require upwards of 10 000 screen- ing samples to achieve desired protein function (Romero & Arnold, 2009; Sarkisyan et al., 2016; Hartman & Tullman- Ercek, 2019), which is a screening burden sufficient to dis- suade adoption of protein engineering in smaller scale labs. Machine-learning-guided directed evolution has emerged as a paradigm for improving the sample efficiency of protein engineering efforts (Yang et al., 2019; Biswas et al., 2021; Saito et al., 2021; Wittmann et al., 2021), and comprises twoin silico components: a model of protein fitness and a","Discussion Site-saturation mutagenesis is a powerful tool for explor- ing a small number of mutations in a focused region, but machine-learning-guided mutagenesis can explore a largerTable 1. Comparison of PET degradation, fPET, of variants from each experiment relative to wild-type LCC; hit-rate, which is the proportion of variants fitter than controls; and Spearman rank cor- relation, which measures repeatability across two bio-replicates. Few-shot variants are optimized for fPET. epPCR Zero-shot Few-shot Fitness, mean 0.24 0.58 0.68 fPETmedian 0.03 0.71 0.72 max 1.24 1.20 1.62 Hit-rate, fPET>LCC 0.05 0.20 0.39 fPET>ICCM 0.00 0.00 0.10 Spearman, 0.89 0.83 0.94 Controls, fPETLCC 1.00 fPETICCM 1.40 number of mutations across the scaffold and active site, and identify more beneficial mutations. In Tournier et al. (2020), ICCM was identified by recombination of four out of five beneficial mutations around the active site identified by site-saturation mutagenesis and an additional stabilizing disulfide bond. However, with only five beneficial mutation sites, there is little scope to improve ICCM further without another round of mutagenesis. In contrast, our machine- learning-guided approach identified a variant with 16 % greater catalytic activity than ICCM exploring a similar number of variants. The resulting library of beneficial muta- tions across the scaffold and active site could be recombined to further improve performance. In vitro measurements are necessary to ensure alignment between machine-learning-guided variants and desired phe- notypes. The zero-shot variants measured in our assay had a significantly higher hit-rate relative to LCC than epPCR, but variants with performance greater than ICCM were only identified after training few-shot models on those zero-shot Protein Language Models in Directed Evolution (a) Tournier et al. (2020) (b) Few-shot simulated annealing P osition along the sequenceMutant amino acid (c) Tournier et al. (2020) P ositio","Protein Language Models in Directed Evolution The dominant paradigms for integrating machine-learning into protein engineering are de novo protein design and guided directed evolution. Guiding directed evolution requires a model of protein fitness, but most models are only evaluated in silico on datasets comprising few mutations. Due to the limited number of mutations in these datasets, it is unclear how well these models can guide directed evolution efforts. We demonstrate in vitro how zero-shot and few-shot protein language models of fitness can be used to guide two rounds of directed evolution with simulated annealing. Our few-shot simulated annealing approach recommended enzyme variants with 1.62 improved PET degradation over 72 h period, outperforming the top engineered variant from the literature, which was 1.40 fitter than wild-type. In the second round, 240 in vitro examples were used for training, 32 homologous sequences were used for evolutionary context and 176 variants were evaluated for improved PET degradation, achieving a hit-rate of 39 % of variants fitter than wild-type.","Protein Language Models in Directed Evolution The dominant paradigms for integrating machine-learning into protein engineering are de novo protein design and guided directed evolution. Guiding directed evolution requires a model of protein fitness, but most models are only evaluated in silico on datasets comprising few mutations. Due to the limited number of mutations in these datasets, it is unclear how well these models can guide directed evolution efforts. We demonstrate in vitro how zero-shot and few-shot protein language models of fitness can be used to guide two rounds of directed evolution with simulated annealing. Our few-shot simulated annealing approach recommended enzyme variants with 1.62 improved PET degradation over 72 h period, outperforming the top engineered variant from the literature, which was 1.40 fitter than wild-type. In the second round, 240 in vitro examples were used for training, 32 homologous sequences were used for evolutionary context and 176 variants were evaluated for improved PET degradation, achieving a hit-rate of 39 % of variants fitter than wild-type. Discussion Site-saturation mutagenesis is a powerful tool for explor- ing a small number of mutations in a focused region, but machine-learning-guided mutagenesis can explore a largerTable 1. Comparison of PET degradation, fPET, of variants from each experiment relative to wild-type LCC; hit-rate, which is the proportion of variants fitter than controls; and Spearman rank cor- relation, which measures repeatability across two bio-replicates. Few-shot variants are optimized for fPET. epPCR Zero-shot Few-shot Fitness, mean 0.24 0.58 0.68 fPETmedian 0.03 0.71 0.72 max 1.24 1.20 1.62 Hit-rate, fPET>LCC 0.05 0.20 0.39 fPET>ICCM 0.00 0.00 0.10 Spearman, 0.89 0.83 0.94 Controls, fPETLCC 1.00 fPETICCM 1.40 number of mutations across the scaffold and active site, and identify more beneficial mutations. In Tournier et al. (2020), ICCM was identified by recombination of four out of five beneficial mutations around the active site identified by site-saturation mutagenesis and an additional stabilizing disulfide bond. However, with only five beneficial mutation sites, there is little scope to improve ICCM further without another round of mutagenesis. In contrast, our machine- learning-guided approach identified a variant with 16 % greater catalytic activity than ICCM exploring a similar number of variants. The resulting library of beneficial muta- tions across the scaffold and active site could be recombined to further improve performance. In vitro measurements are necessary to ensure alignment between machine-learning-guided variants and desired phe- notypes. The zero-shot variants measured in our assay had a significantly higher hit-rate relative to LCC than epPCR, but variants with performance greater than ICCM were only identified after training few-shot models on those zero-shot Protein Language Models in Directed Evolution (a) Tournier et al. (2020) (b) Few-shot simulated annealing P osition along the sequenceMutant amino acid (c) Tournier et al. (2020) P ositio approach recom- mended enzyme variants with 1.62 improved PET degradation over 72 h period, outperform- ing the top engineered variant from the literature, which was 1.40 fitter than wild-type. In the sec- ond round, 240in vitro examples were used for training, 32homologous sequences were used for evolutionary context and 176variants were evalu- ated for improved PET degradation, achieving a hit-rate of 39 % of variants fitter than wild-type. 1. Introduction Proteins are nature s catalytic and functional materials; macro-molecules increasingly programmed in the lab for so- cietal benefit with applications in biomaterials, diagnostics and biocatalysis. Whilst strides have been made in de novo protein design in recent years (Ferruz et al., 2022; Wang et al., 2022; Watson et al., 2023), directed evolution comprising iterative mutage- nesis and screening remains the primary method used for the development of optimized protein variants (Arnold, 1998; *Equal contribution1Cambridge Consultants, Cambridge, United Kingdom2University of Cambridge, Cambridge, United Kingdom. Correspondence to: Russell Maguire <rus- sell.maguire@cambridgeconsultants.com >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).Arnold & V olkov, 1999; Turner, 2009; Wang et al., 2021). Protein sequence space is vast and epistatic, so directed evo- lution experiments may require upwards of 10 000 screen- ing samples to achieve desired protein function (Romero & Arnold, 2009; Sarkisyan et al., 2016; Hartman & Tullman- Ercek, 2019), which is a screening burden sufficient to dis- suade adoption of protein engineering in smaller scale labs. Machine-learning-guided directed evolution has emerged as a paradigm for improving the sample efficiency of protein engineering efforts (Yang et al., 2019; Biswas et al., 2021; Saito et al., 2021; Wittmann et al., 2021), and comprises twoin silico components: a model of protein fitness and a",0
48be9473ce485049b45cddc7f2807c2e389e9efc,Structural activity prediction models recover known binding modes (Poster abstract),"['Michael Backenköhler', 'Joschka Groß', 'Paula Linh Kramer', 'Verena Wolf', 'Andrea Volkamer']",https://openreview.net/pdf/48be9473ce485049b45cddc7f2807c2e389e9efc.pdf,"Structural activity prediction models recover known binding modes (Poster abstract) Drug discovery can benefit from machine learning on 3D structural data of protein-ligand (PL) complexes, but a shortage of such data limits model training. For kinase targets, we generated a large in-silico dataset, kinodata-3D, using template docking. This dataset improved binding affinity predictions. Using an E(3)-invariant GNN model, we investigated learned protein-ligand interactions by removing spatial edges between protein and ligand atoms. Significant prediction changes in known binding regions confirmed the model's understanding of binding mechanisms. This approach aims to enhance explainable AI (XAI) methods, aiding the discovery of novel kinase binding mechanisms and improving model transparency.",48be9473ce485049b45cddc7f2807c2e389e9efc.pdf,"approaches and the analysis of their respective explanations
for the sake of guiding the exploration of potentially novel
kinase binding mechanisms. It allows users to understand
the processes that influence model predictions, enabling er-
ror detection and a more detailed understanding of a model’s
ability to make reasonable predictions. This transparency
may also support scientific discovery by elucidating com-
plex patterns and relationships within data.
References
Backenk ¨ohler, M., Groß, J., Wolf, V ., and V olkamer, A.
Guided docking as a data generation approach facilitates
structure-based machine learning on kinases. Journal of
Chemical Information and Modeling , 2024.
Ivanovs, M., Kadikis, R., and Ozols, K. Perturbation-based
methods for explaining deep neural networks: A survey.
Pattern Recognition Letters , 150:228–234, 2021.
Kanev, G. K., de Graaf, C., Westerman, B. A., de Esch,
I. J. P., and Kooistra, A. J. KLIFS: an overhaul after
the first 5 years of supporting kinase research. Nucleic
Acids Research , 49(D1):D562–D569, 10 2020. ISSN
0305-1048. doi: 10.1093/nar/gkaa895. URL https:
//doi.org/10.1093/nar/gkaa895 .
Structural activity prediction models recover known kinase binding modes
Reference complex
Masked-residue complexes
Query pretrained E(3)-invariant complex-GNN
kinodata-3D
Residue attributions
1
2
3
85
.....
in-silico structural data
Template Docking
Reference
E(3)-GNN
[ChEMBL pIC50]
pIC50
predictions
Mask residue (1)
Mask residue (2)
Residue (1)
Residue (3)
Residue (2)
E(3)-GNN
R 1
-masked input
      ...
Figure 1. Residue masking: (a) PL complexes are generated using template docking. An E(3)-invariant GNN is trained for binding affinity
prediction on the resulting dataset. (b) We occlude each pocket residue once by masking the complex graph. (c) The prediction on the
unmasked reference is compared to the masked prediction. (d) The resulting changes are analyzed with respect to their biochemical
interpretation and known binding mechanisms i",,"approaches and the analysis of their respective explanations for the sake of guiding the exploration of potentially novel kinase binding mechanisms. It allows users to understand the processes that influence model predictions, enabling er- ror detection and a more detailed understanding of a model s ability to make reasonable predictions. This transparency may also support scientific discovery by elucidating com- plex patterns and relationships within data.",,"Structural activity prediction models recover known binding modes (Poster abstract) Drug discovery can benefit from machine learning on 3D structural data of protein-ligand (PL) complexes, but a shortage of such data limits model training. For kinase targets, we generated a large in-silico dataset, kinodata-3D, using template docking. This dataset improved binding affinity predictions. Using an E(3)-invariant GNN model, we investigated learned protein-ligand interactions by removing spatial edges between protein and ligand atoms. Significant prediction changes in known binding regions confirmed the model's understanding of binding mechanisms. This approach aims to enhance explainable AI (XAI) methods, aiding the discovery of novel kinase binding mechanisms and improving model transparency.","Structural activity prediction models recover known binding modes (Poster abstract) Drug discovery can benefit from machine learning on 3D structural data of protein-ligand (PL) complexes, but a shortage of such data limits model training. For kinase targets, we generated a large in-silico dataset, kinodata-3D, using template docking. This dataset improved binding affinity predictions. Using an E(3)-invariant GNN model, we investigated learned protein-ligand interactions by removing spatial edges between protein and ligand atoms. Significant prediction changes in known binding regions confirmed the model's understanding of binding mechanisms. This approach aims to enhance explainable AI (XAI) methods, aiding the discovery of novel kinase binding mechanisms and improving model transparency.  approaches and the analysis of their respective explanations for the sake of guiding the exploration of potentially novel kinase binding mechanisms. It allows users to understand the processes that influence model predictions, enabling er- ror detection and a more detailed understanding of a model s ability to make reasonable predictions. This transparency may also support scientific discovery by elucidating com- plex patterns and relationships within data.",0
74800aa806387dab3daa4ee2a3cf83a49d331353,Generalizing Microscopy Image Labeling via Layer-Matching Adversarial Domain Adaptation,[],https://openreview.net/pdf/74800aa806387dab3daa4ee2a3cf83a49d331353.pdf,"Generalizing Microscopy Image Labeling via Layer-Matching Adversarial Domain Adaptation Image-to-image translation models are valuable tools that convert light microscopy images into in silico labeled biological immunofluorescence images, enabling non-invasive and label-free measurement of protein expression levels in live cells. Despite their potential, these models have not gained significant traction in the life sciences due to their low transferability and lack of robustness to common variances in microscopy images. Additionally, re-training a model for each new microscope setting is infeasible due to the high cost of data acquisition. In this work, we explore domain adaptation techniques to make image-to-image translation models more robust to common distribution shifts in microscopy images. Specifically, we propose Layer-Matching Adversarial Domain Adaptation (LM-ADDA), a general framework that leverages the information-rich latent spaces within the translation model to perform unsupervised domain adaptation. Through experiments on multiple domain shifts, we demonstrate that LM-ADDA enhances the robustness of image-to-image translation models without requiring additional paired or labeled data.",74800aa806387dab3daa4ee2a3cf83a49d331353.pdf,"Methodology
3.1. Problem definition
The goal of image translation is to label an image x∈X
with a corresponding output image y∈Y. In order to
do this, we train a model G:X→Y. In our case, X
is the set of phase-contrast microscopy images, while Y
represents immunofluorescence images which show marker
protein levels. Given that immunofluorescence intensity
represents the density of the protein, this can be seen as
labelling the pixels of the input image with protein level.
OurGis the UNet architecture (Ronneberger et al., 2015),
which includes convolutional downsampling to a bottleneck,
followed by upsampling to the original shape. In order to
preserve lower level information, skip connections in the
front half directly link to the back half as well.
However, in the real world, we may wish to work with a
new input distribution X′, which has the same dimension
asX: for example, bright-field rather than phase-contrast
microscopy. Without access to any paired images in the
new input domain, it is infeasible to train a new model from
scratch. The goal of domain adaptation is to train a model
G′:X′→Ybased on a previously trained G.
For the image translation domain adaptation problem, we
propose the following Layer-Matching Adversarial Discrim-
inative Domain Adaptation (LM-ADDA) algorithm. Sim-
ilar to the Adversarial Discrimative Domain Adaptation
(ADDA) algorithm (Tzeng et al., 2017), our goal is to match
the final output distribution of the original model with the
new model. The natural way to do this is to adversari-
ally match the final layer of the image-to-image translation
model. Our key contribution is to consider matching at
internal layers.
One way to intuitively think about how matching at an ear-
lier layer could be superior to matching the final objective
is in terms of the information latent space. In an ideal sce-
nario, an image-to-image translation model can be seen as a
two-step process: Extracting features from the image, and
Generalizing Microscopy Ima","Conclusion
We find that adversarial domain adaptation can successfully
adapt microscopy image translation methods across a vari-
ety of common real-world shifts, and across varying shift
magnitudes. We show that the layer at which matching oc-
curs has a significant impact on ability to undo shifts, and
that the optimal layer varies depending on the nature of the
domain shift.
Figure 4. Analysis of domain adaptation performance under vary-
ing levels of scaling. Top panel: Comparison of overexposure.
Bottom panels: Pearson correlation coefficients across scale en-
hancement levels (top graph) and by layer matched when scaled
150% (bottom graph).
We note some limitations of our work as well. Currently,
the unpaired matching still requires a large number of input
images from the target domain, which can be expensive in
a real-world setting. In addition, the framework assumes a
minor shift, given that we start with existing weights, and
attempt to use the same feature space. Finally, discovering
the optimal layer requires testing all layers, although this
can be done relatively inexpensively.
Further work can work explore combining information from
multiple layers, which has shown some promising results
(Barbato et al., 2021). We can also attempt to explain the
way performance varies through successive model layers
and why matching certain features best undoes the domain
shift, in order to theoretically find an optimal layer given a
certain model and shift.
Generalizing Microscopy Image Labeling via Layer-Matching Adversarial Domain Adaptation
References
Barbato, F., Toldo, M., Michieli, U., and Zanuttigh, P. Latent
space regularization for unsupervised domain adaptation
in semantic segmentation, 2021.
Bourou, A., Daupin, K., Dubreuil, V ., De Thonel, A.,
Mezger-Lallemand, V ., and Genovesio, A. Unpaired
image-to-image translation with limited data to reveal
subtle phenotypes. In 2023 IEEE 20th International Sym-
posium on Biomedical Imaging (ISBI) , pp. 1–5. IEEE,
202","Methodology 3.1. Problem definition The goal of image translation is to label an image x X with a corresponding output image y Y. In order to do this, we train a model G:X Y. In our case, X is the set of phase-contrast microscopy images, while Y represents immunofluorescence images which show marker protein levels. Given that immunofluorescence intensity represents the density of the protein, this can be seen as labelling the pixels of the input image with protein level. OurGis the UNet architecture (Ronneberger et al., 2015), which includes convolutional downsampling to a bottleneck, followed by upsampling to the original shape. In order to preserve lower level information, skip connections in the front half directly link to the back half as well. However, in the real world, we may wish to work with a new input distribution X , which has the same dimension asX: for example, bright-field rather than phase-contrast microscopy. Without access to any paired images in the new input domain, it is infeasible to train a new model from scratch. The goal of domain adaptation is to train a model G :X Ybased on a previously trained G. For the image translation domain adaptation problem, we propose the following Layer-Matching Adversarial Discrim- inative Domain Adaptation (LM-ADDA) algorithm. Sim- ilar to the Adversarial Discrimative Domain Adaptation (ADDA) algorithm (Tzeng et al., 2017), our goal is to match the final output distribution of the original model with the new model. The natural way to do this is to adversari- ally match the final layer of the image-to-image translation model. Our key contribution is to consider matching at internal layers. One way to intuitively think about how matching at an ear- lier layer could be superior to matching the final objective is in terms of the information latent space. In an ideal sce- nario, an image-to-image translation model can be seen as a two-step process: Extracting features from the image, and Generalizing Microscopy Ima","Conclusion We find that adversarial domain adaptation can successfully adapt microscopy image translation methods across a vari- ety of common real-world shifts, and across varying shift magnitudes. We show that the layer at which matching oc- curs has a significant impact on ability to undo shifts, and that the optimal layer varies depending on the nature of the domain shift. Figure 4. Analysis of domain adaptation performance under vary- ing levels of scaling. Top panel: Comparison of overexposure. Bottom panels: Pearson correlation coefficients across scale en- hancement levels (top graph) and by layer matched when scaled 150% (bottom graph). We note some limitations of our work as well. Currently, the unpaired matching still requires a large number of input images from the target domain, which can be expensive in a real-world setting. In addition, the framework assumes a minor shift, given that we start with existing weights, and attempt to use the same feature space. Finally, discovering the optimal layer requires testing all layers, although this can be done relatively inexpensively. Further work can work explore combining information from multiple layers, which has shown some promising results (Barbato et al., 2021). We can also attempt to explain the way performance varies through successive model layers and why matching certain features best undoes the domain shift, in order to theoretically find an optimal layer given a certain model and shift. Generalizing Microscopy Image Labeling via Layer-Matching Adversarial Domain Adaptation","Generalizing Microscopy Image Labeling via Layer-Matching Adversarial Domain Adaptation Image-to-image translation models are valuable tools that convert light microscopy images into in silico labeled biological immunofluorescence images, enabling non-invasive and label-free measurement of protein expression levels in live cells. Despite their potential, these models have not gained significant traction in the life sciences due to their low transferability and lack of robustness to common variances in microscopy images. Additionally, re-training a model for each new microscope setting is infeasible due to the high cost of data acquisition. In this work, we explore domain adaptation techniques to make image-to-image translation models more robust to common distribution shifts in microscopy images. Specifically, we propose Layer-Matching Adversarial Domain Adaptation (LM-ADDA), a general framework that leverages the information-rich latent spaces within the translation model to perform unsupervised domain adaptation. Through experiments on multiple domain shifts, we demonstrate that LM-ADDA enhances the robustness of image-to-image translation models without requiring additional paired or labeled data.","Generalizing Microscopy Image Labeling via Layer-Matching Adversarial Domain Adaptation Image-to-image translation models are valuable tools that convert light microscopy images into in silico labeled biological immunofluorescence images, enabling non-invasive and label-free measurement of protein expression levels in live cells. Despite their potential, these models have not gained significant traction in the life sciences due to their low transferability and lack of robustness to common variances in microscopy images. Additionally, re-training a model for each new microscope setting is infeasible due to the high cost of data acquisition. In this work, we explore domain adaptation techniques to make image-to-image translation models more robust to common distribution shifts in microscopy images. Specifically, we propose Layer-Matching Adversarial Domain Adaptation (LM-ADDA), a general framework that leverages the information-rich latent spaces within the translation model to perform unsupervised domain adaptation. Through experiments on multiple domain shifts, we demonstrate that LM-ADDA enhances the robustness of image-to-image translation models without requiring additional paired or labeled data. Conclusion We find that adversarial domain adaptation can successfully adapt microscopy image translation methods across a vari- ety of common real-world shifts, and across varying shift magnitudes. We show that the layer at which matching oc- curs has a significant impact on ability to undo shifts, and that the optimal layer varies depending on the nature of the domain shift. Figure 4. Analysis of domain adaptation performance under vary- ing levels of scaling. Top panel: Comparison of overexposure. Bottom panels: Pearson correlation coefficients across scale en- hancement levels (top graph) and by layer matched when scaled 150% (bottom graph). We note some limitations of our work as well. Currently, the unpaired matching still requires a large number of input images from the target domain, which can be expensive in a real-world setting. In addition, the framework assumes a minor shift, given that we start with existing weights, and attempt to use the same feature space. Finally, discovering the optimal layer requires testing all layers, although this can be done relatively inexpensively. Further work can work explore combining information from multiple layers, which has shown some promising results (Barbato et al., 2021). We can also attempt to explain the way performance varies through successive model layers and why matching certain features best undoes the domain shift, in order to theoretically find an optimal layer given a certain model and shift. Generalizing Microscopy Image Labeling via Layer-Matching Adversarial Domain Adaptation Methodology 3.1. Problem definition The goal of image translation is to label an image x X with a corresponding output image y Y. In order to do this, we train a model G:X Y. In our case, X is the set of phase-contrast microscopy images, while Y represents immunofluorescence images which show marker protein levels. Given that immunofluorescence intensity represents the density of the protein, this can be seen as labelling the pixels of the input image with protein level. OurGis the UNet architecture (Ronneberger et al., 2015), which includes convolutional downsampling to a bottleneck, followed by upsampling to the original shape. In order to preserve lower level information, skip connections in the front half directly link to the back half as well. However, in the real world, we may wish to work with a new input distribution X , which has the same dimension asX: for example, bright-field rather than phase-contrast microscopy. Without access to any paired images in the new input domain, it is infeasible to train a new model from scratch. The goal of domain adaptation is to train a model G :X Ybased on a previously trained G. For the image translation domain adaptation problem, we propose the following Layer-Matching Adversarial Discrim- inative Domain Adaptation (LM-ADDA) algorithm. Sim- ilar to the Adversarial Discrimative Domain Adaptation (ADDA) algorithm (Tzeng et al., 2017), our goal is to match the final output distribution of the original model with the new model. The natural way to do this is to adversari- ally match the final layer of the image-to-image translation model. Our key contribution is to consider matching at internal layers. One way to intuitively think about how matching at an ear- lier layer could be superior to matching the final objective is in terms of the information latent space. In an ideal sce- nario, an image-to-image translation model can be seen as a two-step process: Extracting features from the image, and Generalizing Microscopy Ima",0
cfdc1c63041a51cec19c92d002d30819204c0600,Improving Route Development Using Convergent Retrosynthesis Planning,"['Paula Torren-Peraire', 'Jonas Verhoeven', 'Dorota Herman', 'Hugo Ceulemans', 'Igor V. Tetko', 'Jörg K. Wegner']",https://openreview.net/pdf/cfdc1c63041a51cec19c92d002d30819204c0600.pdf,"Improving Route Development Using Convergent Retrosynthesis Planning Computer-aided synthesis planning approaches have allowed a greater exploration of potential synthesis routes, however, these methods are generally developed to produce linear routes from a singular product to a set of proposed building blocks and are not designed to leverage potential shared paths between targets. These convergent routes allow the simultaneous synthesis of compounds, reducing the time and cost of synthesis across compound libraries. We introduce a novel planning approach to develop convergent synthesis routes, which can search multiple products and intermediates simultaneously, enhancing the overall efficiency and practical applicability of retrosynthetic planning. We evaluate the multi-step synthesis planning approach using extracted convergent routes from Johnson \& Johnson Electronic Laboratory Notebooks (J\&J ELN) and publicly available datasets and observe that solvability is generally very high across those routes, being able to identify a convergent route for over 90\% of the test routes and showing an individual compound solvability of over 98\%.",cfdc1c63041a51cec19c92d002d30819204c0600.pdf,"approaches
have allowed a greater exploration of potential
synthesis routes, however, these methods are gen-
erally developed to produce linear routes from
a singular product to a set of proposed building
blocks and are not designed to leverage potential
shared paths between targets. These convergent
routes allow the simultaneous synthesis of com-
pounds, reducing the time and cost of synthesis
across compound libraries. We introduce a novel
planning approach to develop convergent synthe-
sis routes, which can search multiple products
and intermediates simultaneously, enhancing the
overall efficiency and practical applicability of
retrosynthetic planning. We evaluate the multi-
step synthesis planning approach using extracted
convergent routes from Johnson & Johnson Elec-
tronic Laboratory Notebooks (J&J ELN) and pub-
licly available datasets and observe that solvability
is generally very high across those routes, being
able to identify a convergent route for over 90%
of the test routes and showing an individual com-
pound solvability of over 98%.
1. Introduction
Compound synthesis is a crucial starting point in early-stage
drug discovery to validate hit compounds coming out of a
target screening exercise. Exploring the structure-activity
relationship (SAR) space involves the identification of a
synthesis path typically through a process known as ret-
rosynthesis. Retrosynthesis involves hypothetically break-
ing down a compound into progressive reactants until a set
1In-Silico Discovery, Janssen Research & Development,
Janssen Pharmaceutica N.V , Beerse, Belgium2Institute of
Structural Biology, Molecular Targets and Therapeutics Center,
Helmholtz Munich, Neuherberg, Germany3In-Silico Discovery,
Janssen Research & Development, Janssen Research & Develop-
ment LLC, Cambridge, US. Correspondence to: Paula Torren-
Peraire <ptorrenp@its.jnj.com >.
Accepted at the 1st Machine Learning for Life and Material Sci-
ences Workshop at ICML 2024 . Copyright 2024 by the author(s","Conclusion
Convergent routes, producing the synthesis of multiple tar-
get molecules from a shared synthetic path, are a central and
common part of medicinal chemistry. Here, we introduce a
multi-step synthesis planning approach to develop conver-
gent synthesis routes, which can search multiple products
and intermediates simultaneously, enhancing the overall effi-
ciency and practical applicability of retrosynthetic planning,
reducing the time and cost of synthesis across compound
libraries. We evaluate the multi-step synthesis planning ap-
proach using a novel dataset of extracted convergent routes
from industry-relevant and publicly available datasets, be-
ing able to identify a convergent route for over 90% of the
test routes and producing a synthesis route for over 98% of
compounds found within the compound libraries. Moreover,
the approach shows promising results with the proposed
routes being similar to the experimentally validated routes
in over a third of the compound libraries.
Acknowledgements
This study was partially funded by the European Union’s
Horizon 2020 research and innovation programme under
the Marie Skłodowska-Curie Actions grant agreement “Ad-
vanced machine learning for Innovative Drug Discovery
(AIDD)” No. 956832.
References
Brown, D. G. and Bostr ¨om, J. Where Do Recent Small
Molecule Clinical Development Candidates Come From?
Journal of Medicinal Chemistry , 61(21):9442–9468,
November 2018. ISSN 0022-2623. doi: 10.1021/acs.
jmedchem.8b00675. URL https://doi.org/10.
1021/acs.jmedchem.8b00675 .
Chen, B., Li, C., Dai, H., and Song, L. Retro*: Learning
Retrosynthetic Planning with Neural Guided A* Search.
InProceedings of the 37th International Conference on
Machine Learning , pp. 1608–1616. PMLR, November
2020. URL https://proceedings.mlr.press/
v119/chen20k.html . ISSN: 2640-3498.
Corey, E. J. The logic of chemical synthesis . , 1991. ISBN
5-88501-081-1.
Dandapani, S., Rosse, G., Southall, N., Salvino, J. M.,
and Thomas, C. J. Selecting, Acq","approaches have allowed a greater exploration of potential synthesis routes, however, these methods are gen- erally developed to produce linear routes from a singular product to a set of proposed building blocks and are not designed to leverage potential shared paths between targets. These convergent routes allow the simultaneous synthesis of com- pounds, reducing the time and cost of synthesis across compound libraries. We introduce a novel planning approach to develop convergent synthe- sis routes, which can search multiple products and intermediates simultaneously, enhancing the overall efficiency and practical applicability of retrosynthetic planning. We evaluate the multi- step synthesis planning approach using extracted convergent routes from Johnson & Johnson Elec- tronic Laboratory Notebooks (J&J ELN) and pub- licly available datasets and observe that solvability is generally very high across those routes, being able to identify a convergent route for over 90% of the test routes and showing an individual com- pound solvability of over 98%. 1. Introduction Compound synthesis is a crucial starting point in early-stage drug discovery to validate hit compounds coming out of a target screening exercise. Exploring the structure-activity relationship (SAR) space involves the identification of a synthesis path typically through a process known as ret- rosynthesis. Retrosynthesis involves hypothetically break- ing down a compound into progressive reactants until a set 1In-Silico Discovery, Janssen Research & Development, Janssen Pharmaceutica N.V , Beerse, Belgium2Institute of Structural Biology, Molecular Targets and Therapeutics Center, Helmholtz Munich, Neuherberg, Germany3In-Silico Discovery, Janssen Research & Development, Janssen Research & Develop- ment LLC, Cambridge, US. Correspondence to: Paula Torren- Peraire <ptorrenp@its.jnj.com >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s","Conclusion Convergent routes, producing the synthesis of multiple tar- get molecules from a shared synthetic path, are a central and common part of medicinal chemistry. Here, we introduce a multi-step synthesis planning approach to develop conver- gent synthesis routes, which can search multiple products and intermediates simultaneously, enhancing the overall effi- ciency and practical applicability of retrosynthetic planning, reducing the time and cost of synthesis across compound libraries. We evaluate the multi-step synthesis planning ap- proach using a novel dataset of extracted convergent routes from industry-relevant and publicly available datasets, be- ing able to identify a convergent route for over 90% of the test routes and producing a synthesis route for over 98% of compounds found within the compound libraries. Moreover, the approach shows promising results with the proposed routes being similar to the experimentally validated routes in over a third of the compound libraries.","Improving Route Development Using Convergent Retrosynthesis Planning Computer-aided synthesis planning approaches have allowed a greater exploration of potential synthesis routes, however, these methods are generally developed to produce linear routes from a singular product to a set of proposed building blocks and are not designed to leverage potential shared paths between targets. These convergent routes allow the simultaneous synthesis of compounds, reducing the time and cost of synthesis across compound libraries. We introduce a novel planning approach to develop convergent synthesis routes, which can search multiple products and intermediates simultaneously, enhancing the overall efficiency and practical applicability of retrosynthetic planning. We evaluate the multi-step synthesis planning approach using extracted convergent routes from Johnson \& Johnson Electronic Laboratory Notebooks (J\&J ELN) and publicly available datasets and observe that solvability is generally very high across those routes, being able to identify a convergent route for over 90\% of the test routes and showing an individual compound solvability of over 98\%.","Improving Route Development Using Convergent Retrosynthesis Planning Computer-aided synthesis planning approaches have allowed a greater exploration of potential synthesis routes, however, these methods are generally developed to produce linear routes from a singular product to a set of proposed building blocks and are not designed to leverage potential shared paths between targets. These convergent routes allow the simultaneous synthesis of compounds, reducing the time and cost of synthesis across compound libraries. We introduce a novel planning approach to develop convergent synthesis routes, which can search multiple products and intermediates simultaneously, enhancing the overall efficiency and practical applicability of retrosynthetic planning. We evaluate the multi-step synthesis planning approach using extracted convergent routes from Johnson \& Johnson Electronic Laboratory Notebooks (J\&J ELN) and publicly available datasets and observe that solvability is generally very high across those routes, being able to identify a convergent route for over 90\% of the test routes and showing an individual compound solvability of over 98\%. Conclusion Convergent routes, producing the synthesis of multiple tar- get molecules from a shared synthetic path, are a central and common part of medicinal chemistry. Here, we introduce a multi-step synthesis planning approach to develop conver- gent synthesis routes, which can search multiple products and intermediates simultaneously, enhancing the overall effi- ciency and practical applicability of retrosynthetic planning, reducing the time and cost of synthesis across compound libraries. We evaluate the multi-step synthesis planning ap- proach using a novel dataset of extracted convergent routes from industry-relevant and publicly available datasets, be- ing able to identify a convergent route for over 90% of the test routes and producing a synthesis route for over 98% of compounds found within the compound libraries. Moreover, the approach shows promising results with the proposed routes being similar to the experimentally validated routes in over a third of the compound libraries. approaches have allowed a greater exploration of potential synthesis routes, however, these methods are gen- erally developed to produce linear routes from a singular product to a set of proposed building blocks and are not designed to leverage potential shared paths between targets. These convergent routes allow the simultaneous synthesis of com- pounds, reducing the time and cost of synthesis across compound libraries. We introduce a novel planning approach to develop convergent synthe- sis routes, which can search multiple products and intermediates simultaneously, enhancing the overall efficiency and practical applicability of retrosynthetic planning. We evaluate the multi- step synthesis planning approach using extracted convergent routes from Johnson & Johnson Elec- tronic Laboratory Notebooks (J&J ELN) and pub- licly available datasets and observe that solvability is generally very high across those routes, being able to identify a convergent route for over 90% of the test routes and showing an individual com- pound solvability of over 98%. 1. Introduction Compound synthesis is a crucial starting point in early-stage drug discovery to validate hit compounds coming out of a target screening exercise. Exploring the structure-activity relationship (SAR) space involves the identification of a synthesis path typically through a process known as ret- rosynthesis. Retrosynthesis involves hypothetically break- ing down a compound into progressive reactants until a set 1In-Silico Discovery, Janssen Research & Development, Janssen Pharmaceutica N.V , Beerse, Belgium2Institute of Structural Biology, Molecular Targets and Therapeutics Center, Helmholtz Munich, Neuherberg, Germany3In-Silico Discovery, Janssen Research & Development, Janssen Research & Develop- ment LLC, Cambridge, US. Correspondence to: Paula Torren- Peraire <ptorrenp@its.jnj.com >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s",0
5a11cd20141665cda78dc4355c7ed31a3e0eef99,Combining Graph Attention and Recurrent Neural Networks in a Variational Autoencoder for Molecular Representation Learning and Drug Design,"['Alex T. Müller', 'Kenneth Atz', 'Michael Reutlinger', 'Nicolas Zorn']",https://openreview.net/pdf/5a11cd20141665cda78dc4355c7ed31a3e0eef99.pdf,"Combining Graph Attention and Recurrent Neural Networks in a Variational Autoencoder for Molecular Representation Learning and Drug Design We explore the fusion of GNNs with RNNs within a VAE framework for molecular representation learning, demonstrating competitive performance in QSAR benchmarks, high validity & drug-likeness of sampled molecules, and robust latent space interpolation. Finding a meaningful molecular representation that can be leveraged for a variety of tasks in chemical sciences and drug discovery is of wide interest, and new representation learning techniques are continuously being explored. Here, we investigate the fusion of graph attention neural networks with recurrent neural networks within a variational autoencoder framework for molecular representation learning. This combination leverages the strengths of both architectures to capture properties of molecular structures, enabling more effective encoding and flexible decoding processes. With the resulting representation, we observe competitive performance in quantitative structure-activity relationship (QSAR) benchmarks, a high validity and drug-likeness of randomly sampled molecules and robustness for linear latent space interpolation between two molecules. Our approach holds promise for facilitating downstream tasks such as clustering, QSAR, virtual screening and generative molecular design, all unified in one molecular representation.",5a11cd20141665cda78dc4355c7ed31a3e0eef99.pdf,"methodology and encoding
rules. Journal of chemical information and computer
sciences , 28(1):31–36, 1988.
Wen, N., Liu, G., Zhang, J., Zhang, R., Fu, Y ., and Han, X. A
fingerprints based molecular property prediction method
using the bert model. Journal of Cheminformatics , 14(1):
71, 2022.
Winter, R., Montanari, F., Steffen, A., Briem, H., No ´e, F.,
and Clevert, D.-A. Efficient multi-objective molecular
optimization in a continuous latent space. Chemical
science , 10(34):8016–8024, 2019.
Wu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J.,
Geniesse, C., Pappu, A. S., Leswing, K., and Pande,
V . Moleculenet: a benchmark for molecular machine
learning. Chemical science , 9(2):513–530, 2018.
Xiong, Z., Wang, D., Liu, X., Zhong, F., Wan, X., Li, X.,
Li, Z., Luo, X., Chen, K., Jiang, H., et al. Pushing the
boundaries of molecular representation for drug discovery
with the graph attention mechanism. Journal of medicinal
chemistry , 63(16):8749–8760, 2019.
Zhu, J., Xia, Y ., Wu, L., Xie, S., Qin, T., Zhou, W., Li,
H., and Liu, T.-Y . Unified 2d and 3d pre-training of
molecular representations. In Proceedings of the 28th
ACM SIGKDD conference on knowledge discovery and
data mining , pp. 2626–2636, 2022.
Combining Graph and Recurrent Neural Networks in a Variational Autoencoder for Molecular Representation Learning
A. Appendix
Table A.1. Value distribution comparing training data and 25’600 sampled molecules for selected calculated properties.
PROPERTY TRAINING DATA GIRAFFE SAMPLED
LOGP 3.37 ± 1.34 3.20 ± 1.85
MOLWEIGHT 365 ± 133 359 ± 113
FCSP3 0.42 ± 0.24 0.42 ± 0.21
NR. HBD 1.34 ± 1.23 1.36 ± 1.11
AROM . RINGS 1.96 ± 1.46 1.87 ± 1.12
QED 0.59 ± 0.23 0.59 ± 0.21
Figure A.1. Histograms of properties presented in Table A.1 of the training data (yellow) and randomly sampled 25’000 molecules (teal).
The y-axis describes the relative frequency.
Figure A.2. Example molecules decoded from randomly sampled points in G IRAFFE latent space.
Combining Graph and Recurrent Neural Net","Conclusion and Outlook
With GIRAFFE , we showed that it is possible to obtain a
smooth latent space representation by using a V AE with
GNN encoder and LSTM decoder. The obtained latent
Figure 5. Visualization of the latent space of 25’600 random
molecules from the training set embedded with GIRAFFE, using
a principal component analysis for dimensionality reduction
and selected scaled RDKit properties for coloring. The same
visualization reduced using t-distributed Stochastic Neighbor
Embedding (tSNE) is shown in Figure A.6.
space can be traversed or randomly sampled to recreate
SMILES-strings with high validity and similarity to the
training data, and is well performing for QSAR and drug
design tasks at the same time. Still, more work is needed
to find a molecular representation that works satisfactorily
well on predictive tasks important in drug discovery (Dias
et al., 2023). We will continue to train and evaluate our
GIRAFFE model using actual assay readouts of compounds
on biological targets, cells or from physicochemical end
points to see if this further improves the performance
of the learned representation, potentially also employing
contrastive learning. Finally, we are looking forward to
expanding this approach to property-, similarity-, docking-
or scaffold-constrained generation approaches with direct
impact on drug discovery projects.
Data and Code Availability
The code used to train the here presented models together
with the model weights of the GIRAFFE model is made
available in the supplementary information as well as on
https://github.com/alexarnimueller/giraffe. The dataset with
10M molecules from PubChem will be made available upon
request.
Acknowledgements
We would like to thank all reviewers who gave useful
comments and thereby helped to improve the manuscript.
We further thank Eugen Eirich and the Roche SMDA
network for their ideas, feedback and critical discussions.
Finally, we are indebted to the communities behind the
multiple open-source so","methodology and encoding rules. Journal of chemical information and computer sciences , 28(1):31 36, 1988. Wen, N., Liu, G., Zhang, J., Zhang, R., Fu, Y ., and Han, X. A fingerprints based molecular property prediction method using the bert model. Journal of Cheminformatics , 14(1): 71, 2022. Winter, R., Montanari, F., Steffen, A., Briem, H., No e, F., and Clevert, D.-A. Efficient multi-objective molecular optimization in a continuous latent space. Chemical science , 10(34):8016 8024, 2019. Wu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J., Geniesse, C., Pappu, A. S., Leswing, K., and Pande, V . Moleculenet: a benchmark for molecular machine learning. Chemical science , 9(2):513 530, 2018. Xiong, Z., Wang, D., Liu, X., Zhong, F., Wan, X., Li, X., Li, Z., Luo, X., Chen, K., Jiang, H., et al. Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism. Journal of medicinal chemistry , 63(16):8749 8760, 2019. Zhu, J., Xia, Y ., Wu, L., Xie, S., Qin, T., Zhou, W., Li, H., and Liu, T.-Y . Unified 2d and 3d pre-training of molecular representations. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining , pp. 2626 2636, 2022. Combining Graph and Recurrent Neural Networks in a Variational Autoencoder for Molecular Representation Learning A. Appendix Table A.1. Value distribution comparing training data and 25 600 sampled molecules for selected calculated properties. PROPERTY TRAINING DATA GIRAFFE SAMPLED LOGP 3.37 1.34 3.20 1.85 MOLWEIGHT 365 133 359 113 FCSP3 0.42 0.24 0.42 0.21 NR. HBD 1.34 1.23 1.36 1.11 AROM . RINGS 1.96 1.46 1.87 1.12 QED 0.59 0.23 0.59 0.21 Figure A.1. Histograms of properties presented in Table A.1 of the training data (yellow) and randomly sampled 25 000 molecules (teal). The y-axis describes the relative frequency. Figure A.2. Example molecules decoded from randomly sampled points in G IRAFFE latent space. Combining Graph and Recurrent Neural Net","Conclusion and Outlook With GIRAFFE , we showed that it is possible to obtain a smooth latent space representation by using a V AE with GNN encoder and LSTM decoder. The obtained latent Figure 5. Visualization of the latent space of 25 600 random molecules from the training set embedded with GIRAFFE, using a principal component analysis for dimensionality reduction and selected scaled RDKit properties for coloring. The same visualization reduced using t-distributed Stochastic Neighbor Embedding (tSNE) is shown in Figure A.6. space can be traversed or randomly sampled to recreate SMILES-strings with high validity and similarity to the training data, and is well performing for QSAR and drug design tasks at the same time. Still, more work is needed to find a molecular representation that works satisfactorily well on predictive tasks important in drug discovery (Dias et al., 2023). We will continue to train and evaluate our GIRAFFE model using actual assay readouts of compounds on biological targets, cells or from physicochemical end points to see if this further improves the performance of the learned representation, potentially also employing contrastive learning. Finally, we are looking forward to expanding this approach to property-, similarity-, docking- or scaffold-constrained generation approaches with direct impact on drug discovery projects. Data and Code Availability The code used to train the here presented models together with the model weights of the GIRAFFE model is made available in the supplementary information as well as on https://github.com/alexarnimueller/giraffe. The dataset with 10M molecules from PubChem will be made available upon request.","Combining Graph Attention and Recurrent Neural Networks in a Variational Autoencoder for Molecular Representation Learning and Drug Design We explore the fusion of GNNs with RNNs within a VAE framework for molecular representation learning, demonstrating competitive performance in QSAR benchmarks, high validity & drug-likeness of sampled molecules, and robust latent space interpolation. Finding a meaningful molecular representation that can be leveraged for a variety of tasks in chemical sciences and drug discovery is of wide interest, and new representation learning techniques are continuously being explored. Here, we investigate the fusion of graph attention neural networks with recurrent neural networks within a variational autoencoder framework for molecular representation learning. This combination leverages the strengths of both architectures to capture properties of molecular structures, enabling more effective encoding and flexible decoding processes. With the resulting representation, we observe competitive performance in quantitative structure-activity relationship (QSAR) benchmarks, a high validity and drug-likeness of randomly sampled molecules and robustness for linear latent space interpolation between two molecules. Our approach holds promise for facilitating downstream tasks such as clustering, QSAR, virtual screening and generative molecular design, all unified in one molecular representation.","Combining Graph Attention and Recurrent Neural Networks in a Variational Autoencoder for Molecular Representation Learning and Drug Design We explore the fusion of GNNs with RNNs within a VAE framework for molecular representation learning, demonstrating competitive performance in QSAR benchmarks, high validity & drug-likeness of sampled molecules, and robust latent space interpolation. Finding a meaningful molecular representation that can be leveraged for a variety of tasks in chemical sciences and drug discovery is of wide interest, and new representation learning techniques are continuously being explored. Here, we investigate the fusion of graph attention neural networks with recurrent neural networks within a variational autoencoder framework for molecular representation learning. This combination leverages the strengths of both architectures to capture properties of molecular structures, enabling more effective encoding and flexible decoding processes. With the resulting representation, we observe competitive performance in quantitative structure-activity relationship (QSAR) benchmarks, a high validity and drug-likeness of randomly sampled molecules and robustness for linear latent space interpolation between two molecules. Our approach holds promise for facilitating downstream tasks such as clustering, QSAR, virtual screening and generative molecular design, all unified in one molecular representation. Conclusion and Outlook With GIRAFFE , we showed that it is possible to obtain a smooth latent space representation by using a V AE with GNN encoder and LSTM decoder. The obtained latent Figure 5. Visualization of the latent space of 25 600 random molecules from the training set embedded with GIRAFFE, using a principal component analysis for dimensionality reduction and selected scaled RDKit properties for coloring. The same visualization reduced using t-distributed Stochastic Neighbor Embedding (tSNE) is shown in Figure A.6. space can be traversed or randomly sampled to recreate SMILES-strings with high validity and similarity to the training data, and is well performing for QSAR and drug design tasks at the same time. Still, more work is needed to find a molecular representation that works satisfactorily well on predictive tasks important in drug discovery (Dias et al., 2023). We will continue to train and evaluate our GIRAFFE model using actual assay readouts of compounds on biological targets, cells or from physicochemical end points to see if this further improves the performance of the learned representation, potentially also employing contrastive learning. Finally, we are looking forward to expanding this approach to property-, similarity-, docking- or scaffold-constrained generation approaches with direct impact on drug discovery projects. Data and Code Availability The code used to train the here presented models together with the model weights of the GIRAFFE model is made available in the supplementary information as well as on https://github.com/alexarnimueller/giraffe. The dataset with 10M molecules from PubChem will be made available upon request. methodology and encoding rules. Journal of chemical information and computer sciences , 28(1):31 36, 1988. Wen, N., Liu, G., Zhang, J., Zhang, R., Fu, Y ., and Han, X. A fingerprints based molecular property prediction method using the bert model. Journal of Cheminformatics , 14(1): 71, 2022. Winter, R., Montanari, F., Steffen, A., Briem, H., No e, F., and Clevert, D.-A. Efficient multi-objective molecular optimization in a continuous latent space. Chemical science , 10(34):8016 8024, 2019. Wu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J., Geniesse, C., Pappu, A. S., Leswing, K., and Pande, V . Moleculenet: a benchmark for molecular machine learning. Chemical science , 9(2):513 530, 2018. Xiong, Z., Wang, D., Liu, X., Zhong, F., Wan, X., Li, X., Li, Z., Luo, X., Chen, K., Jiang, H., et al. Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism. Journal of medicinal chemistry , 63(16):8749 8760, 2019. Zhu, J., Xia, Y ., Wu, L., Xie, S., Qin, T., Zhou, W., Li, H., and Liu, T.-Y . Unified 2d and 3d pre-training of molecular representations. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining , pp. 2626 2636, 2022. Combining Graph and Recurrent Neural Networks in a Variational Autoencoder for Molecular Representation Learning A. Appendix Table A.1. Value distribution comparing training data and 25 600 sampled molecules for selected calculated properties. PROPERTY TRAINING DATA GIRAFFE SAMPLED LOGP 3.37 1.34 3.20 1.85 MOLWEIGHT 365 133 359 113 FCSP3 0.42 0.24 0.42 0.21 NR. HBD 1.34 1.23 1.36 1.11 AROM . RINGS 1.96 1.46 1.87 1.12 QED 0.59 0.23 0.59 0.21 Figure A.1. Histograms of properties presented in Table A.1 of the training data (yellow) and randomly sampled 25 000 molecules (teal). The y-axis describes the relative frequency. Figure A.2. Example molecules decoded from randomly sampled points in G IRAFFE latent space. Combining Graph and Recurrent Neural Net",0
c196965f12b8e599abc7f2ccf18b9cd1a8ae492a,PLINDER: The protein-ligand interactions dataset and evaluation resource,"['Janani Durairaj', 'Yusuf Adeshina', 'Zhonglin Cao', 'Xuejin Zhang', 'Vladas Oleinikovas', 'Thomas Duignan', 'Zachary McClure', 'Xavier Robin', 'Emanuele Rossi', 'Guoqing Zhou', 'Srimukh Prasad Veccham', 'Clemens Isert', 'Yuxing Peng', 'Prabindh Sundareson', 'Mehmet Akdel', 'Gabriele Corso', 'Hannes Stark', 'Zachary Wayne Carpenter', 'Michael M. Bronstein', 'Emine Kucukbenli', 'Torsten Schwede', 'Luca Naef']",https://openreview.net/pdf/c196965f12b8e599abc7f2ccf18b9cd1a8ae492a.pdf,"PLINDER: The protein-ligand interactions dataset and evaluation resource Protein-ligand interactions (PLI) are foundational to small molecule drug design. With computational methods striving towards experimental accuracy, there is a critical demand for a well-curated and diverse PLI dataset. Existing datasets are often limited in size and diversity, and commonly used evaluation sets suffer from training information leakage, hindering the realistic assessment of method generalization capabilities. To address these shortcomings, we present PLINDER, the largest and most annotated dataset to date, comprising 449,383 PLI systems, each with over 500 annotations, similarity metrics at protein, pocket, interaction and ligand levels, and paired unbound (apo) and predicted structures. We propose an approach to generate training and evaluation splits that minimizes task-specific leakage and maximizes test set quality, and compare the resulting performance of DiffDock when re-trained with different kinds of splits.",c196965f12b8e599abc7f2ccf18b9cd1a8ae492a.pdf,"approach to generate training and
evaluation splits that minimizes task-specific leak-
age and maximizes test set quality, and compare
the resulting performance of DiffDock when re-
trained with different kinds of splits.
1. Introduction
The protein-ligand field has seen a surge in the application
of deep learning-based prediction methods, notably in tasks
such as rigid body docking (St¨ark et al., 2022; Lu et al.,
2022; Corso et al., 2023) where the pose of a ligand within
a given rigid protein structure is predicted, flexible pocket
docking (Plainer et al., 2023; Qiao et al., 2024) which al-
lows side-chain movements of pocket residues, co-folding
(Qiao et al., 2024; Krishna et al., 2024; Abramson et al.,
*Equal contribution1Biozentrum, University of Basel,
Basel, Switzerland2SIB: Swiss Institute of Bioinformat-
ics, Basel, Switzerland3VantAI, New York, USA4NVIDIA
5MIT CSAIL6Oxford University. Correspondence to:
Janani Durairaj <janani.durairaj@unibas.ch >, Yusuf Adeshina
<yusuf@vant.ai >.
Accepted at the 1st Machine Learning for Life and Material Sci-
ences Workshop at ICML 2024 . Copyright 2024 by the author(s).2024) where both the protein conformation and ligand pose
are predicted at once, pocket-conditioned ligand genera-
tion (Schneuing et al., 2023) where novel ligand molecules
are generated within a given protein structure and pocket,
ligand-conditioned protein engineering (Dauparas et al.,
2023) where, conversely, novel protein sequences are de-
signed to selectively bind a ligand, and molecular scaffold-
ing(Chawdhury et al., 2021) where ligands are modified to
enhance their affinity to a protein or pocket.
These methods hold promise in accelerating drug discovery
and protein engineering by facilitating the accurate pre-
diction of ligand binding poses within protein structures.
However, their effectiveness relies heavily on the datasets
used for training and evaluation, where several key consider-
ations must be addressed: 1. Training set diversity to mo","Conclusion
We present PLINDER a large, comprehensive and automated
dataset resource for protein-ligand interactions. We demon-
strate the value in scalable similarity measures between
protein-ligand complexes and a splitting algorithm that pri-
oritises test set quality and low information leakage. By
retraining DiffDock on various splits, we show that our
methods and results provide a solid foundation for dataset
generation, as well as measuring and addressing both quality
and dataset leakage in a quantitative and tunable manner.
7. Acknowledgements
This work has received funding from the LIGATE public-
private consortium supported by the European High-
Performance Computing Joint Undertaking (JU) under grant
agreement No 956137, SIB Swiss Institute of Bioinformat-
ics (https://www.sib.swiss/) and the Biozentrum of the Uni-
versity of Basel (https://www.biozentrum.unibas.ch/). The
JU receives support from the European Union’s Horizon
2020 research and innovation programme and Italy, Sweden,
Austria, the Czech Republic, Switzerland.
PLINDER : The protein-ligand interactions dataset and resource
References
Argo Workflow (v3.5.8). https://github.com/
argoproj .
NVIDIA BioNeMo (v1.4). https://www.nvidia.
com/en-us/clara/bionemo .
Kubernetes (v1.30). https://kubernetes.io/ .
Metaflow (v2.11.15). https://docs.metaflow.
org/ .
Rdkit: Open-source cheminformatics. https://www.
rdkit.org . Accessed: 2024-05-17.
Abramson, J., Adler, J., and etal. Accurate struc-
ture prediction of biomolecular interactions with
alphafold3. Nature , 2024. doi: 10.1038/
s41586-024-07487-w. URL https://www.nature.
com/articles/s41586-024-07487-w .
Adasme, M. F., Linnemann, K. L., Bolz, S. N., Kaiser, F.,
Salentin, S., Haupt, V . J., and Schroeder, M. PLIP 2021:
expanding the scope of the protein–ligand interaction
profiler to DNA and RNA. Nucleic Acids Research , 49
(W1):W530–W534, 05 2021. ISSN 0305-1048. doi:
10.1093/nar/gkab294. URL https://doi.org/10.
1093/nar/gkab294 .
Angriman, E., van der","approach to generate training and evaluation splits that minimizes task-specific leak- age and maximizes test set quality, and compare the resulting performance of DiffDock when re- trained with different kinds of splits. 1. Introduction The protein-ligand field has seen a surge in the application of deep learning-based prediction methods, notably in tasks such as rigid body docking (St ark et al., 2022; Lu et al., 2022; Corso et al., 2023) where the pose of a ligand within a given rigid protein structure is predicted, flexible pocket docking (Plainer et al., 2023; Qiao et al., 2024) which al- lows side-chain movements of pocket residues, co-folding (Qiao et al., 2024; Krishna et al., 2024; Abramson et al., *Equal contribution1Biozentrum, University of Basel, Basel, Switzerland2SIB: Swiss Institute of Bioinformat- ics, Basel, Switzerland3VantAI, New York, USA4NVIDIA 5MIT CSAIL6Oxford University. Correspondence to: Janani Durairaj <janani.durairaj@unibas.ch >, Yusuf Adeshina <yusuf@vant.ai >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).2024) where both the protein conformation and ligand pose are predicted at once, pocket-conditioned ligand genera- tion (Schneuing et al., 2023) where novel ligand molecules are generated within a given protein structure and pocket, ligand-conditioned protein engineering (Dauparas et al., 2023) where, conversely, novel protein sequences are de- signed to selectively bind a ligand, and molecular scaffold- ing(Chawdhury et al., 2021) where ligands are modified to enhance their affinity to a protein or pocket. These methods hold promise in accelerating drug discovery and protein engineering by facilitating the accurate pre- diction of ligand binding poses within protein structures. However, their effectiveness relies heavily on the datasets used for training and evaluation, where several key consider- ations must be addressed: 1. Training set diversity to mo","Conclusion We present PLINDER a large, comprehensive and automated dataset resource for protein-ligand interactions. We demon- strate the value in scalable similarity measures between protein-ligand complexes and a splitting algorithm that pri- oritises test set quality and low information leakage. By retraining DiffDock on various splits, we show that our methods and results provide a solid foundation for dataset generation, as well as measuring and addressing both quality and dataset leakage in a quantitative and tunable manner. 7.","PLINDER: The protein-ligand interactions dataset and evaluation resource Protein-ligand interactions (PLI) are foundational to small molecule drug design. With computational methods striving towards experimental accuracy, there is a critical demand for a well-curated and diverse PLI dataset. Existing datasets are often limited in size and diversity, and commonly used evaluation sets suffer from training information leakage, hindering the realistic assessment of method generalization capabilities. To address these shortcomings, we present PLINDER, the largest and most annotated dataset to date, comprising 449,383 PLI systems, each with over 500 annotations, similarity metrics at protein, pocket, interaction and ligand levels, and paired unbound (apo) and predicted structures. We propose an approach to generate training and evaluation splits that minimizes task-specific leakage and maximizes test set quality, and compare the resulting performance of DiffDock when re-trained with different kinds of splits.","PLINDER: The protein-ligand interactions dataset and evaluation resource Protein-ligand interactions (PLI) are foundational to small molecule drug design. With computational methods striving towards experimental accuracy, there is a critical demand for a well-curated and diverse PLI dataset. Existing datasets are often limited in size and diversity, and commonly used evaluation sets suffer from training information leakage, hindering the realistic assessment of method generalization capabilities. To address these shortcomings, we present PLINDER, the largest and most annotated dataset to date, comprising 449,383 PLI systems, each with over 500 annotations, similarity metrics at protein, pocket, interaction and ligand levels, and paired unbound (apo) and predicted structures. We propose an approach to generate training and evaluation splits that minimizes task-specific leakage and maximizes test set quality, and compare the resulting performance of DiffDock when re-trained with different kinds of splits. Conclusion We present PLINDER a large, comprehensive and automated dataset resource for protein-ligand interactions. We demon- strate the value in scalable similarity measures between protein-ligand complexes and a splitting algorithm that pri- oritises test set quality and low information leakage. By retraining DiffDock on various splits, we show that our methods and results provide a solid foundation for dataset generation, as well as measuring and addressing both quality and dataset leakage in a quantitative and tunable manner. 7. approach to generate training and evaluation splits that minimizes task-specific leak- age and maximizes test set quality, and compare the resulting performance of DiffDock when re- trained with different kinds of splits. 1. Introduction The protein-ligand field has seen a surge in the application of deep learning-based prediction methods, notably in tasks such as rigid body docking (St ark et al., 2022; Lu et al., 2022; Corso et al., 2023) where the pose of a ligand within a given rigid protein structure is predicted, flexible pocket docking (Plainer et al., 2023; Qiao et al., 2024) which al- lows side-chain movements of pocket residues, co-folding (Qiao et al., 2024; Krishna et al., 2024; Abramson et al., *Equal contribution1Biozentrum, University of Basel, Basel, Switzerland2SIB: Swiss Institute of Bioinformat- ics, Basel, Switzerland3VantAI, New York, USA4NVIDIA 5MIT CSAIL6Oxford University. Correspondence to: Janani Durairaj <janani.durairaj@unibas.ch >, Yusuf Adeshina <yusuf@vant.ai >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).2024) where both the protein conformation and ligand pose are predicted at once, pocket-conditioned ligand genera- tion (Schneuing et al., 2023) where novel ligand molecules are generated within a given protein structure and pocket, ligand-conditioned protein engineering (Dauparas et al., 2023) where, conversely, novel protein sequences are de- signed to selectively bind a ligand, and molecular scaffold- ing(Chawdhury et al., 2021) where ligands are modified to enhance their affinity to a protein or pocket. These methods hold promise in accelerating drug discovery and protein engineering by facilitating the accurate pre- diction of ligand binding poses within protein structures. However, their effectiveness relies heavily on the datasets used for training and evaluation, where several key consider- ations must be addressed: 1. Training set diversity to mo",0
6c50293803a507ee2e09b4353f2586c99c64def1,Analysis of Atom-level pretraining with QM data for Graph Neural Networks Molecular property models,"['Jose Arjona-Medina', 'Ramil Nugmanov']",https://openreview.net/pdf/6c50293803a507ee2e09b4353f2586c99c64def1.pdf,"Analysis of Atom-level pretraining with QM data for Graph Neural Networks Molecular property models This paper shows how atom-level pretraining with quantum mechanics data enhances the generalization and robustness of molecular representations in QSAR models. Despite the rapid and significant advancements in deep learning for Quantitative Structure-Activity Relationship (QSAR) models, the challenge of learning robust molecular representations that effectively generalize in real-world scenarios to novel compounds remains an elusive and unresolved task. This study examines how atom-level pretraining with quantum mechanics (QM) data can mitigate violations of assumptions regarding the distributional similarity between training and test data and therefore improve performance and generalization in downstream tasks. In the public dataset Therapeutics Data Commons (TDC), we show how pretraining on atom-level QM improves performance overall and makes the activation of the features distributes more Gaussian-like which results in a representation that is more robust to distribution shifts. To the best of our knowledge, this is the first time that hidden state molecular representations are analyzed to compare the effects of molecule-level and atom-level pretraining on QM data.",6c50293803a507ee2e09b4353f2586c99c64def1.pdf,"methodology
aligns with techniques used in graph convolutional networks
(GCNs), where virtual nodes connected to every node of
the graph can serve a comparable purpose (Cai et al., 2023).
Building on this concept, we propose a novel extension tai-
lored for multitask learning applications. In our approach,
we employ the same encoder and output head to estimate
multiple properties, but differentiate the tasks by using dis-
tinct virtual nodes for each. This allows for task-specific
processing while maintaining a unified architecture, enhanc-
ing the model’s efficiency, adaptability, and scalability in
handling diverse learning tasks.
Quantitative Assessment of learned features. To ana-
lyze the impact of pretraining methods on the molecular
Analysis of Atom-level pretraining with QM data for Graph Neural Networks Molecular property models
02000ScratchDim 1 Dim 2 Dim 3 Dim 4 Dim 5 Dim 6 Dim 7 Dim 8 Dim 9 Dim 10
020004000HL
Pretrained
020004000Atom-level
Pretrained
020004000ScratchDim 11 Dim 12 Dim 13 Dim 14 Dim 15 Dim 16 Dim 17 Dim 18 Dim 19 Dim 20
020004000HL
Pretrained
2.5
0.02.5020004000Atom-level
Pretrained
2.5
0.02.5 0 5 5
 0 5
 0 5 0 5 2.5
0.02.5 2.5
0.0 2.5 0 5 5
 0 5Distribution of Activations for 20 Features
Figure 2. Distribution of first 20 features from the first layer of the Graphormer network for three different training approaches —scratch,
HOMO-LUMO pretrained and atom-level pretrained— across test split of lipophilicity dataset .
representation captured by the model, we conducted a com-
prehensive analysis of learned feature distributions. We run
the models on the three different data splits (training, valida-
tion, and test), and extract the distribution of features after
the first layer of the Graphormer architecture.
For each compound in the dataset, we aggregated the node
representations after the first Graphormer layer, resulting in
a cumulative dataset-wide node representation. This aggre-
gation facilitated the derivation of an empirical distri","Conclusions
In this study, we have demonstrated that pretraining of graph-
based neural networks with atom-level quantum mechanics
(QM) data significantly enhances performance on down-
stream tasks related to ADMET properties within the TDC
dataset, as illustrated in Table 1. Furthermore, we showed
the change in the distributions of activations of the internal
model’s features due to specific pretraining. After atom-
level pretraining with QM data, these distributions become
more Gaussian-like, which is known to be conducive to bet-
ter learning dynamics and thus improved performance (see
Figure 2). Moreover, our findings indicate that pretrained
models exhibit smaller distribution shifts from training to
testing datasets, further supporting the efficacy of QM data
Analysis of Atom-level pretraining with QM data for Graph Neural Networks Molecular property models
pretraining in enhancing model robustness (see Figure 2).
To our knowledge, this is the first study that elucidates how
atom-level pretraining can optimize molecular representa-
tions by analyzing the model’s internal representation and
robustness to distribution shifts.
References
Bajorath, J., Peltason, L., Wawer, M., Guha, R., Lajiness,
M. S., and Drie, J. H. V . Navigating structure-activity
landscapes. Drug Discov Today , 14(13-14):698–705,
2009. doi: 10.1016/j.drudis.2009.04.003.
Baptista, D., Correia, J., Pereira, B., and Rocha, M. Eval-
uating molecular representations in machine learning
models for drug response prediction and interpretabil-
ity. J Integr Bioinform , 19(3):20220006, 2022. doi:
10.1515/jib-2022-0006.
Bishop, C. M. Pattern Recognition and Machine Learning .
Springer, 2006.
Cai, C., Hy, T. S., Yu, R., and Wang, Y . On the connection
between mpnn and graph transformer. In Proceedings of
the 40th International Conference on Machine Learning ,
pp. 138. JMLR.org, 2023.
Cao, Z., Sciabola, S., and Wang, Y . Large-scale pretrain-
ing improves sample efficiency of active learning based
molecu","methodology aligns with techniques used in graph convolutional networks (GCNs), where virtual nodes connected to every node of the graph can serve a comparable purpose (Cai et al., 2023). Building on this concept, we propose a novel extension tai- lored for multitask learning applications. In our approach, we employ the same encoder and output head to estimate multiple properties, but differentiate the tasks by using dis- tinct virtual nodes for each. This allows for task-specific processing while maintaining a unified architecture, enhanc- ing the model s efficiency, adaptability, and scalability in handling diverse learning tasks. Quantitative Assessment of learned features. To ana- lyze the impact of pretraining methods on the molecular Analysis of Atom-level pretraining with QM data for Graph Neural Networks Molecular property models 02000ScratchDim 1 Dim 2 Dim 3 Dim 4 Dim 5 Dim 6 Dim 7 Dim 8 Dim 9 Dim 10 020004000HL Pretrained 020004000Atom-level Pretrained 020004000ScratchDim 11 Dim 12 Dim 13 Dim 14 Dim 15 Dim 16 Dim 17 Dim 18 Dim 19 Dim 20 020004000HL Pretrained 2.5 0.02.5020004000Atom-level Pretrained 2.5 0.02.5 0 5 5 0 5 0 5 0 5 2.5 0.02.5 2.5 0.0 2.5 0 5 5 0 5Distribution of Activations for 20 Features Figure 2. Distribution of first 20 features from the first layer of the Graphormer network for three different training approaches scratch, HOMO-LUMO pretrained and atom-level pretrained across test split of lipophilicity dataset . representation captured by the model, we conducted a com- prehensive analysis of learned feature distributions. We run the models on the three different data splits (training, valida- tion, and test), and extract the distribution of features after the first layer of the Graphormer architecture. For each compound in the dataset, we aggregated the node representations after the first Graphormer layer, resulting in a cumulative dataset-wide node representation. This aggre- gation facilitated the derivation of an empirical distri","Conclusions In this study, we have demonstrated that pretraining of graph- based neural networks with atom-level quantum mechanics (QM) data significantly enhances performance on down- stream tasks related to ADMET properties within the TDC dataset, as illustrated in Table 1. Furthermore, we showed the change in the distributions of activations of the internal model s features due to specific pretraining. After atom- level pretraining with QM data, these distributions become more Gaussian-like, which is known to be conducive to bet- ter learning dynamics and thus improved performance (see Figure 2). Moreover, our findings indicate that pretrained models exhibit smaller distribution shifts from training to testing datasets, further supporting the efficacy of QM data Analysis of Atom-level pretraining with QM data for Graph Neural Networks Molecular property models pretraining in enhancing model robustness (see Figure 2). To our knowledge, this is the first study that elucidates how atom-level pretraining can optimize molecular representa- tions by analyzing the model s internal representation and robustness to distribution shifts.","Analysis of Atom-level pretraining with QM data for Graph Neural Networks Molecular property models This paper shows how atom-level pretraining with quantum mechanics data enhances the generalization and robustness of molecular representations in QSAR models. Despite the rapid and significant advancements in deep learning for Quantitative Structure-Activity Relationship (QSAR) models, the challenge of learning robust molecular representations that effectively generalize in real-world scenarios to novel compounds remains an elusive and unresolved task. This study examines how atom-level pretraining with quantum mechanics (QM) data can mitigate violations of assumptions regarding the distributional similarity between training and test data and therefore improve performance and generalization in downstream tasks. In the public dataset Therapeutics Data Commons (TDC), we show how pretraining on atom-level QM improves performance overall and makes the activation of the features distributes more Gaussian-like which results in a representation that is more robust to distribution shifts. To the best of our knowledge, this is the first time that hidden state molecular representations are analyzed to compare the effects of molecule-level and atom-level pretraining on QM data.","Analysis of Atom-level pretraining with QM data for Graph Neural Networks Molecular property models This paper shows how atom-level pretraining with quantum mechanics data enhances the generalization and robustness of molecular representations in QSAR models. Despite the rapid and significant advancements in deep learning for Quantitative Structure-Activity Relationship (QSAR) models, the challenge of learning robust molecular representations that effectively generalize in real-world scenarios to novel compounds remains an elusive and unresolved task. This study examines how atom-level pretraining with quantum mechanics (QM) data can mitigate violations of assumptions regarding the distributional similarity between training and test data and therefore improve performance and generalization in downstream tasks. In the public dataset Therapeutics Data Commons (TDC), we show how pretraining on atom-level QM improves performance overall and makes the activation of the features distributes more Gaussian-like which results in a representation that is more robust to distribution shifts. To the best of our knowledge, this is the first time that hidden state molecular representations are analyzed to compare the effects of molecule-level and atom-level pretraining on QM data. Conclusions In this study, we have demonstrated that pretraining of graph- based neural networks with atom-level quantum mechanics (QM) data significantly enhances performance on down- stream tasks related to ADMET properties within the TDC dataset, as illustrated in Table 1. Furthermore, we showed the change in the distributions of activations of the internal model s features due to specific pretraining. After atom- level pretraining with QM data, these distributions become more Gaussian-like, which is known to be conducive to bet- ter learning dynamics and thus improved performance (see Figure 2). Moreover, our findings indicate that pretrained models exhibit smaller distribution shifts from training to testing datasets, further supporting the efficacy of QM data Analysis of Atom-level pretraining with QM data for Graph Neural Networks Molecular property models pretraining in enhancing model robustness (see Figure 2). To our knowledge, this is the first study that elucidates how atom-level pretraining can optimize molecular representa- tions by analyzing the model s internal representation and robustness to distribution shifts. methodology aligns with techniques used in graph convolutional networks (GCNs), where virtual nodes connected to every node of the graph can serve a comparable purpose (Cai et al., 2023). Building on this concept, we propose a novel extension tai- lored for multitask learning applications. In our approach, we employ the same encoder and output head to estimate multiple properties, but differentiate the tasks by using dis- tinct virtual nodes for each. This allows for task-specific processing while maintaining a unified architecture, enhanc- ing the model s efficiency, adaptability, and scalability in handling diverse learning tasks. Quantitative Assessment of learned features. To ana- lyze the impact of pretraining methods on the molecular Analysis of Atom-level pretraining with QM data for Graph Neural Networks Molecular property models 02000ScratchDim 1 Dim 2 Dim 3 Dim 4 Dim 5 Dim 6 Dim 7 Dim 8 Dim 9 Dim 10 020004000HL Pretrained 020004000Atom-level Pretrained 020004000ScratchDim 11 Dim 12 Dim 13 Dim 14 Dim 15 Dim 16 Dim 17 Dim 18 Dim 19 Dim 20 020004000HL Pretrained 2.5 0.02.5020004000Atom-level Pretrained 2.5 0.02.5 0 5 5 0 5 0 5 0 5 2.5 0.02.5 2.5 0.0 2.5 0 5 5 0 5Distribution of Activations for 20 Features Figure 2. Distribution of first 20 features from the first layer of the Graphormer network for three different training approaches scratch, HOMO-LUMO pretrained and atom-level pretrained across test split of lipophilicity dataset . representation captured by the model, we conducted a com- prehensive analysis of learned feature distributions. We run the models on the three different data splits (training, valida- tion, and test), and extract the distribution of features after the first layer of the Graphormer architecture. For each compound in the dataset, we aggregated the node representations after the first Graphormer layer, resulting in a cumulative dataset-wide node representation. This aggre- gation facilitated the derivation of an empirical distri",0
d0ddd6249b228279e1f0c84c2f985497ad67f317,Improving Molecular Modeling with Geometric GNNs: an Empirical Study,"['Ali Ramlaoui', 'Théo Saulus', 'Basile Terver', 'Victor Schmidt', 'David Rolnick', 'Fragkiskos D. Malliaros', 'Alexandre AGM Duval']",https://openreview.net/pdf/d0ddd6249b228279e1f0c84c2f985497ad67f317.pdf,"Improving Molecular Modeling with Geometric GNNs: an Empirical Study An empirical study on Geometric Graph Neural Networks for 3D atomic systems focusing on the impact of different canonicalization methods, graph creation strategies, and auxiliary tasks Rapid advancements in machine learning (ML) are transforming materials science by significantly speeding up material property calculations. However, the proliferation of ML approaches has made it challenging for scientists to keep up with the most promising techniques. This paper presents an empirical study on Geometric Graph Neural Networks for 3D atomic systems, focusing on the impact of different (1) canonicalization methods, (2) graph creation strategies, and (3) auxiliary tasks, on performance, scalability and symmetry enforcement. Our findings and insights aim to guide researchers in selecting optimal modeling components for molecular modeling tasks.",d0ddd6249b228279e1f0c84c2f985497ad67f317.pdf,"approaches has made
it challenging for scientists to keep up with the
most promising techniques. This paper presents
an empirical study on Geometric Graph Neu-
ral Networks for 3D atomic systems, focusing
on the impact on performance, scalability, and
symmetry enforcement of different (1) canoni-
calization methods, (2) graph creation strategies,
and (3) auxiliary tasks. Our findings and in-
sights aim to guide researchers in selecting op-
timal modeling components for molecular mod-
eling tasks. Our code is available at https:
//github.com/RolnickLab/ocp .
1. Introduction
The field of computational materials science has witnessed
an increasing interest in recent years, with the explosion of
machine learning approaches to model material properties at
the quantum scale. This is possible thanks to the release of
large-scale datasets such as OC20 (Chanussot et al., 2020)
and QM7-X (Hoja et al., 2021), which contain millions of
molecular structures along with various properties (forces,
energy, band gap) computed using quantum mechanical
simulations involving Density Functional Theory (DFT)
(Kohn et al., 1996).
ML models have been trained to approximate DFT, thus
constituting an even faster proxy to the Schr ¨odinger equa-
tion, modeling atomic interactions and systems’ behavior.
*Equal contribution1Universit ´e Paris-Saclay, Centrale-
Sup´elec2´Ecole Normale Sup ´erieure Paris-Saclay3´Ecole Poly-
technique, IP Paris4Universit ´e de Montr ´eal5Mila - Que-
bec AI Institute6Entalpic7McGill University8Inria Saclay.
Correspondence to: Ali Ramlaoui <ali.ramlaoui@student-
cs.fr>, Th´eo Saulus <theo.saulus@student-cs.fr >, Basile Terver
<basile.terver@polytechnique.edu >.
Accepted at the 1st Machine Learning for Life and Material Sci-
ences Workshop at ICML 2024 . Copyright 2024 by the author(s).They enable the calculation of material properties in seconds
instead of hours or days, offering the potential to accelerate
scientific discovery via high-throughput screening of novel","Future work could focus on refining these techniques,
exploring their applications across a wider spectrum of
datasets, and developing new methods to combine the
strengths of various approaches.
Acknowledgement
The authors thank S ´ekou-Oumar Kaba, Derek Lim, Joshua
David Robinson, and Hannah Lawrence for their insight-
ful comments and discussions, as well as the anonymous
reviewers for their suggestions and feedback. Supported
in part by ANR (French National Research Agency) under
the JCJC project GraphIA (ANR-20-CE23-0009-01) and
the Canada CIFAR AI Chairs program. This research was
enabled in part by computing resources provided by Mila
(mila.quebec) and material support from NVIDIA Corpora-
tion in the form of computational resources.
References
Abramson, J., Adler, J., Dunger, J., Evans, R., Green, T.,
Pritzel, A., Ronneberger, O., Willmore, L., Ballard, A. J.,
Bambrick, J., et al. Accurate structure prediction of
biomolecular interactions with alphafold 3. Nature , pp.
1–3, 2024.
Batatia, I., Benner, P., Chiang, Y ., Elena, A. M., Kov ´acs,
D. P., Riebesell, J., Advincula, X. R., Asta, M.,
Baldwin, W. J., Bernstein, N., et al. A foundation
model for atomistic materials chemistry. arXiv preprint
arXiv:2401.00096 , 2023a.
Batatia, I., Kov ´acs, D. P., Simm, G. N. C., Ortner, C., and
Cs´anyi, G. Mace: Higher order equivariant message
passing neural networks for fast and accurate force fields,
2023b.
Chanussot, L., Das, A., Goyal, S., Lavril, T., Shuaibi, M.,
Rivi`ere, M., Tran, K., Heras-Domingo, J., Ho, C., Hu, W.,
Palizhati, A., Sriram, A., Wood, B., Yoon, J., Parikh, D.,
Zitnick, C. L., and Ulissi, Z. W. The open catalyst 2020
(oc20) dataset and community challenges. ACS Catalysis ,
2020. doi: 10.1021/acscatal.0c04525.Chen, D., Lin, Y ., Li, W., Li, P., Zhou, J., and Sun, X.
Measuring and relieving the over-smoothing problem for
graph neural networks from the topological view, 2019.
Deng, B., Zhong, P., Jun, K., Riebesell, J., Han, K., Bar-
tel, C. J., and Ce","approaches has made it challenging for scientists to keep up with the most promising techniques. This paper presents an empirical study on Geometric Graph Neu- ral Networks for 3D atomic systems, focusing on the impact on performance, scalability, and symmetry enforcement of different (1) canoni- calization methods, (2) graph creation strategies, and (3) auxiliary tasks. Our findings and in- sights aim to guide researchers in selecting op- timal modeling components for molecular mod- eling tasks. Our code is available at https: //github.com/RolnickLab/ocp . 1. Introduction The field of computational materials science has witnessed an increasing interest in recent years, with the explosion of machine learning approaches to model material properties at the quantum scale. This is possible thanks to the release of large-scale datasets such as OC20 (Chanussot et al., 2020) and QM7-X (Hoja et al., 2021), which contain millions of molecular structures along with various properties (forces, energy, band gap) computed using quantum mechanical simulations involving Density Functional Theory (DFT) (Kohn et al., 1996). ML models have been trained to approximate DFT, thus constituting an even faster proxy to the Schr odinger equa- tion, modeling atomic interactions and systems behavior. *Equal contribution1Universit e Paris-Saclay, Centrale- Sup elec2 Ecole Normale Sup erieure Paris-Saclay3 Ecole Poly- technique, IP Paris4Universit e de Montr eal5Mila - Que- bec AI Institute6Entalpic7McGill University8Inria Saclay. Correspondence to: Ali Ramlaoui <ali.ramlaoui@student- cs.fr>, Th eo Saulus <theo.saulus@student-cs.fr >, Basile Terver <basile.terver@polytechnique.edu >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).They enable the calculation of material properties in seconds instead of hours or days, offering the potential to accelerate scientific discovery via high-throughput screening of novel","Future work could focus on refining these techniques, exploring their applications across a wider spectrum of datasets, and developing new methods to combine the strengths of various approaches.","Improving Molecular Modeling with Geometric GNNs: an Empirical Study An empirical study on Geometric Graph Neural Networks for 3D atomic systems focusing on the impact of different canonicalization methods, graph creation strategies, and auxiliary tasks Rapid advancements in machine learning (ML) are transforming materials science by significantly speeding up material property calculations. However, the proliferation of ML approaches has made it challenging for scientists to keep up with the most promising techniques. This paper presents an empirical study on Geometric Graph Neural Networks for 3D atomic systems, focusing on the impact of different (1) canonicalization methods, (2) graph creation strategies, and (3) auxiliary tasks, on performance, scalability and symmetry enforcement. Our findings and insights aim to guide researchers in selecting optimal modeling components for molecular modeling tasks.","Improving Molecular Modeling with Geometric GNNs: an Empirical Study An empirical study on Geometric Graph Neural Networks for 3D atomic systems focusing on the impact of different canonicalization methods, graph creation strategies, and auxiliary tasks Rapid advancements in machine learning (ML) are transforming materials science by significantly speeding up material property calculations. However, the proliferation of ML approaches has made it challenging for scientists to keep up with the most promising techniques. This paper presents an empirical study on Geometric Graph Neural Networks for 3D atomic systems, focusing on the impact of different (1) canonicalization methods, (2) graph creation strategies, and (3) auxiliary tasks, on performance, scalability and symmetry enforcement. Our findings and insights aim to guide researchers in selecting optimal modeling components for molecular modeling tasks. Future work could focus on refining these techniques, exploring their applications across a wider spectrum of datasets, and developing new methods to combine the strengths of various approaches. approaches has made it challenging for scientists to keep up with the most promising techniques. This paper presents an empirical study on Geometric Graph Neu- ral Networks for 3D atomic systems, focusing on the impact on performance, scalability, and symmetry enforcement of different (1) canoni- calization methods, (2) graph creation strategies, and (3) auxiliary tasks. Our findings and in- sights aim to guide researchers in selecting op- timal modeling components for molecular mod- eling tasks. Our code is available at https: //github.com/RolnickLab/ocp . 1. Introduction The field of computational materials science has witnessed an increasing interest in recent years, with the explosion of machine learning approaches to model material properties at the quantum scale. This is possible thanks to the release of large-scale datasets such as OC20 (Chanussot et al., 2020) and QM7-X (Hoja et al., 2021), which contain millions of molecular structures along with various properties (forces, energy, band gap) computed using quantum mechanical simulations involving Density Functional Theory (DFT) (Kohn et al., 1996). ML models have been trained to approximate DFT, thus constituting an even faster proxy to the Schr odinger equa- tion, modeling atomic interactions and systems behavior. *Equal contribution1Universit e Paris-Saclay, Centrale- Sup elec2 Ecole Normale Sup erieure Paris-Saclay3 Ecole Poly- technique, IP Paris4Universit e de Montr eal5Mila - Que- bec AI Institute6Entalpic7McGill University8Inria Saclay. Correspondence to: Ali Ramlaoui <ali.ramlaoui@student- cs.fr>, Th eo Saulus <theo.saulus@student-cs.fr >, Basile Terver <basile.terver@polytechnique.edu >. Accepted at the 1st Machine Learning for Life and Material Sci- ences Workshop at ICML 2024 . Copyright 2024 by the author(s).They enable the calculation of material properties in seconds instead of hours or days, offering the potential to accelerate scientific discovery via high-throughput screening of novel",0
