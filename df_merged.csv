paper_id,title,authors,url,text,filename,methodology,future_work,methodology_clean,future_work_clean,combined,text_clean,cluster,method_future_text,is_similar
f3aad42fb7a93a0a80417c4bf1486c090f4651a0,PutnamBench: A Multilingual Competition-Mathematics Benchmark for Formal Theorem-Proving,"['George Tsoukalas', 'Jasper Lee', 'John Jennings', 'Jimmy Xin', 'Michelle Ding', 'Michael Jennings', 'Amitayush Thakur', 'Swarat Chaudhuri']",https://openreview.net/pdf/f3aad42fb7a93a0a80417c4bf1486c090f4651a0.pdf,"PutnamBench: A Multilingual Competition-Mathematics Benchmark for Formal Theorem-Proving We present PutnamBench, a benchmark of 1337 formalizations of Putnam competition problems in Lean 4, Isabelle, and Coq. We present PutnamBench, a new multilingual evaluation benchmark for formal theorem-proving. PutnamBench consists of formalizations of problems sourced from the William Lowell Putnam Mathematical Competition, the premier undergraduate-level mathematics competition in North America. All the problem statements come with formalizations in Lean 4 and Isabelle; a substantial subset have Coq formalizations as well. PutnamBench consists of 1337 hand-written formalizations across the three proof assistants, and aims to benchmark the next generation of theorem-proving algorithms for competition mathematics. Proving the theorems requires significant problem-solving ability and proficiency in a broad range of topics taught in undergraduate mathematics courses. We evaluate several established neural and symbolic theorem provers using PutnamBench. These approaches can only solve a handful of the problems, establishing our benchmark as a difficult open challenge for research on formal theorem-proving. PutnamBench is available at https://github.com/trishullab/PUTNAM.",f3aad42fb7a93a0a80417c4bf1486c090f4651a0.pdf,"methodology which
has seen significant application for theorem-proving in
MINIF2F. We run DSP with pass@10, using temperature
T= 0.1and GPT-4 as the underlying language model. Our
evaluation yields two successful proofs: one of Putnam 2001
A1, a problem involving magmas (sets with a binary opera-
tion), and one of Putnam 1995 A1, a problem involving a
closed-under-multiplication subset of the reals. In particular,
Putnam 1995 A1 cannot be solved by Sledgehammer alone.Putnam 2001 A1. Consider a set Sand a binary op-
eration ⋆, i.e., for each a, b∈S,a ⋆ b∈S. Assume
(a⋆b)⋆a=bfor all a, b∈S. Prove that a⋆(b⋆a) =b
for all a, b∈S.
theorem putnam_2001_a1:
fixes op :: "" 'a⇒'a⇒'a""
assumes hop : "" ∀a b :: 'a.
op (op a b) a = b""
shows "" ∀a b :: 'a. op a (op b a) = b""
proof -
{
fix a b :: 'a
have ""op (op a (op b a)) a = op b a"" using
hop by simp
then have ""op a (op b a) = b"" using hop by
metis
}
then show ?thesis by simp
qed
Figure 3: A formalization of Putnam 2001 A1 in Isabelle
and the corresponding proof discovered by our evaluation
with DSP. Sledgehammer alone can also produce a success-
ful proof to this theorem.
The generated proof is included in Figure 3.
We run a baseline using Sledgehammer, a powerful automa-
tion tool in Isabelle which makes calls to external SMT
solvers to prove a given goal. With a set timeout of t= 120
seconds, we run Sledgehammer on each Isabelle formaliza-
tion. The result of this evaluation is 3 successfully proven
problems: Putnam 1971 B1, 2001 A1, and 2012 A2. No-
tably, all of these problems are statements about sets with
binary operations. We include the statements of 1971 B1
and 2012 A2 in Figure 20.
Coq. We run GPT-4 with a Coq-based prompt on our Coq
formalizations using the same configuration as in Lean and
Isabelle. The result of the experiment is 1 solved problem,
namely Putnam 1988 B1, which was also solved in Lean 4.
The proof, which we include in Figure 13, generally follows
5
PUTNAM BENCH : A Multilingual Competition-Mathematics Be","Conclusion
We presented PUTNAM BENCH , a benchmark for neural
theorem-proving consisting of formalizations of Putnam
competition problems. A distinctive feature of PUTNAM -
BENCH is that it spans a broad range of undergraduate-level
mathematical topics, including algebra, analysis, and num-
ber theory. Another unique benefit is that it includes prob-
lems in Lean 4, Isabelle, and Coq, the three most popular
formal proof frameworks.
As our experiments show, PUTNAM BENCH is a challeng-
ing benchmark: all current theorem-proving approaches
fail to solve more than a handful of its problems. We be-
lieve that these failures have two root causes: (i) While
current theorem-provers can effectively stitch together stan-
dard proof steps well-represented in the training corpus,
they often fail at synthesizing new lemmas and orchestrat-
ing these lemmas into complex proofs. (ii) Current methods
often fail to leverage the deep knowledge available in math-
ematics repositories. Developing a new generation of neural
theorem-provers in which these weaknesses are at least
partly addressed is an exciting direction of future research.
7
PUTNAM BENCH : A Multilingual Competition-Mathematics Benchmark for Formal Theorem-Proving
7. Impact Statement
This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal
consequences of our work, none which we feel must be
specifically highlighted here.
References
Archive of Formal Proofs — isa-afp.org. https://www.
isa-afp.org/ . [Accessed 25-05-2024].
Alexanderson, G., Klosinski, L., and Larson, L. The William
Lowell Putnam Mathematical Competition: Problems
and Solutions, 1965-1984 . MAA problem books se-
ries. Mathematical Association of America, 1985. ISBN
9780883854419. URL https://books.google.com/
books?id=mv0oAQAAMAAJ .
Azerbayev, Z., Piotrowski, B., Schoelkopf, H., Ayers, E. W.,
Radev, D., and Avigad, J. Proofnet: Autoformalizing and
formally proving undergraduate-level mathematics, 202","methodology which has seen significant application for theorem-proving in MINIF2F. We run DSP with pass@10, using temperature T= 0.1and GPT-4 as the underlying language model. Our evaluation yields two successful Figure 3: A formalization of Putnam 2001 A1 in Isabelle and the corresponding proof discovered by our evaluation with DSP. Sledgehammer alone can also produce a success- ful proof to this theorem. The generated proof is included in Figure 3. We run a baseline using Sledgehammer, a powerful automa- tion tool in Isabelle which makes calls to external SMT solvers to prove a given goal. With a set timeout of t= 120 seconds, we run Sledgehammer on each Isabelle formaliza- tion. The result of this evaluation is 3 successfully proven problems: Putnam 1971 B1, 2001 A1, and 2012 A2. No- tably, all of these problems are statements about sets with binary operations. We include the statements of 1971 B1 and 2012 A2 in Figure 20. Coq. We run GPT-4 with a Coq-based prompt on our Coq formalizations using the same configuration as in Lean and Isabelle. The result of the experiment is 1 solved problem, namely Putnam 1988 B1, which was also solved in Lean 4. The proof, which we include in Figure 13, generally follows 5 PUTNAM BENCH : A Multilingual Competition-Mathematics Be","Conclusion We presented PUTNAM BENCH , a benchmark for neural theorem-proving consisting of formalizations of Putnam competition problems. A distinctive feature of PUTNAM - BENCH is that it spans a broad range of undergraduate-level mathematical topics, including algebra, analysis, and num- ber theory. Another unique benefit is that it includes prob- lems in Lean 4, Isabelle, and Coq, the three most popular formal proof frameworks. As our experiments show, PUTNAM BENCH is a challeng- ing benchmark: all current theorem-proving approaches fail to solve more than a handful of its problems. We be- lieve that these failures have two root causes: (i) While current theorem-provers can effectively stitch together stan- dard proof steps well-represented in the training corpus, they often fail at synthesizing new lemmas and orchestrat- ing these lemmas into complex proofs. (ii) Current methods often fail to leverage the deep knowledge available in math- ematics repositories. Developing a new generation of neural theorem-provers in which these weaknesses are at least partly addressed is an exciting direction of future research. 7 PUTNAM BENCH : A Multilingual Competition-Mathematics Benchmark for Formal Theorem-Proving 7. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.","PutnamBench: A Multilingual Competition-Mathematics Benchmark for Formal Theorem-Proving We present PutnamBench, a benchmark of 1337 formalizations of Putnam competition problems in Lean 4, Isabelle, and Coq. We present PutnamBench, a new multilingual evaluation benchmark for formal theorem-proving. PutnamBench consists of formalizations of problems sourced from the William Lowell Putnam Mathematical Competition, the premier undergraduate-level mathematics competition in North America. All the problem statements come with formalizations in Lean 4 and Isabelle; a substantial subset have Coq formalizations as well. PutnamBench consists of 1337 hand-written formalizations across the three proof assistants, and aims to benchmark the next generation of theorem-proving algorithms for competition mathematics. Proving the theorems requires significant problem-solving ability and proficiency in a broad range of topics taught in undergraduate mathematics courses. We evaluate several established neural and symbolic theorem provers using PutnamBench. These approaches can only solve a handful of the problems, establishing our benchmark as a difficult open challenge for research on formal theorem-proving. PutnamBench is available at https://github.com/trishullab/PUTNAM. Conclusion We presented PUTNAM BENCH , a benchmark for neural theorem-proving consisting of formalizations of Putnam competition problems. A distinctive feature of PUTNAM - BENCH is that it spans a broad range of undergraduate-level mathematical topics, including algebra, analysis, and num- ber theory. Another unique benefit is that it includes prob- lems in Lean 4, Isabelle, and Coq, the three most popular formal proof frameworks. As our experiments show, PUTNAM BENCH is a challeng- ing benchmark: all current theorem-proving approaches fail to solve more than a handful of its problems. We be- lieve that these failures have two root causes: (i) While current theorem-provers can effectively stitch together stan- dard proof steps well-represented in the training corpus, they often fail at synthesizing new lemmas and orchestrat- ing these lemmas into complex proofs. (ii) Current methods often fail to leverage the deep knowledge available in math- ematics repositories. Developing a new generation of neural theorem-provers in which these weaknesses are at least partly addressed is an exciting direction of future research. 7 PUTNAM BENCH : A Multilingual Competition-Mathematics Benchmark for Formal Theorem-Proving 7. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. methodology which has seen significant application for theorem-proving in MINIF2F. We run DSP with pass@10, using temperature T= 0.1and GPT-4 as the underlying language model. Our evaluation yields two successful Figure 3: A formalization of Putnam 2001 A1 in Isabelle and the corresponding proof discovered by our evaluation with DSP. Sledgehammer alone can also produce a success- ful proof to this theorem. The generated proof is included in Figure 3. We run a baseline using Sledgehammer, a powerful automa- tion tool in Isabelle which makes calls to external SMT solvers to prove a given goal. With a set timeout of t= 120 seconds, we run Sledgehammer on each Isabelle formaliza- tion. The result of this evaluation is 3 successfully proven problems: Putnam 1971 B1, 2001 A1, and 2012 A2. No- tably, all of these problems are statements about sets with binary operations. We include the statements of 1971 B1 and 2012 A2 in Figure 20. Coq. We run GPT-4 with a Coq-based prompt on our Coq formalizations using the same configuration as in Lean and Isabelle. The result of the experiment is 1 solved problem, namely Putnam 1988 B1, which was also solved in Lean 4. The proof, which we include in Figure 13, generally follows 5 PUTNAM BENCH : A Multilingual Competition-Mathematics Be","PutnamBench: A Multilingual Competition-Mathematics Benchmark for Formal Theorem-Proving We present PutnamBench, a benchmark of 1337 formalizations of Putnam competition problems in Lean 4, Isabelle, and Coq. We present PutnamBench, a new multilingual evaluation benchmark for formal theorem-proving. PutnamBench consists of formalizations of problems sourced from the William Lowell Putnam Mathematical Competition, the premier undergraduate-level mathematics competition in North America. All the problem statements come with formalizations in Lean 4 and Isabelle; a substantial subset have Coq formalizations as well. PutnamBench consists of 1337 hand-written formalizations across the three proof assistants, and aims to benchmark the next generation of theorem-proving algorithms for competition mathematics. Proving the theorems requires significant problem-solving ability and proficiency in a broad range of topics taught in undergraduate mathematics courses. We evaluate several established neural and symbolic theorem provers using PutnamBench. These approaches can only solve a handful of the problems, establishing our benchmark as a difficult open challenge for research on formal theorem-proving. PutnamBench is available at https://github.com/trishullab/PUTNAM.",0,"methodology which
has seen significant application for theorem-proving in
MINIF2F. We run DSP with pass@10, using temperature
T= 0.1and GPT-4 as the underlying language model. Our
evaluation yields two successful proofs: one of Putnam 2001
A1, a problem involving magmas (sets with a binary opera-
tion), and one of Putnam 1995 A1, a problem involving a
closed-under-multiplication subset of the reals. In particular,
Putnam 1995 A1 cannot be solved by Sledgehammer alone.Putnam 2001 A1. Consider a set Sand a binary op-
eration ⋆, i.e., for each a, b∈S,a ⋆ b∈S. Assume
(a⋆b)⋆a=bfor all a, b∈S. Prove that a⋆(b⋆a) =b
for all a, b∈S.
theorem putnam_2001_a1:
fixes op :: "" 'a⇒'a⇒'a""
assumes hop : "" ∀a b :: 'a.
op (op a b) a = b""
shows "" ∀a b :: 'a. op a (op b a) = b""
proof -
{
fix a b :: 'a
have ""op (op a (op b a)) a = op b a"" using
hop by simp
then have ""op a (op b a) = b"" using hop by
metis
}
then show ?thesis by simp
qed
Figure 3: A formalization of Putnam 2001 A1 in Isabelle
and the corresponding proof discovered by our evaluation
with DSP. Sledgehammer alone can also produce a success-
ful proof to this theorem.
The generated proof is included in Figure 3.
We run a baseline using Sledgehammer, a powerful automa-
tion tool in Isabelle which makes calls to external SMT
solvers to prove a given goal. With a set timeout of t= 120
seconds, we run Sledgehammer on each Isabelle formaliza-
tion. The result of this evaluation is 3 successfully proven
problems: Putnam 1971 B1, 2001 A1, and 2012 A2. No-
tably, all of these problems are statements about sets with
binary operations. We include the statements of 1971 B1
and 2012 A2 in Figure 20.
Coq. We run GPT-4 with a Coq-based prompt on our Coq
formalizations using the same configuration as in Lean and
Isabelle. The result of the experiment is 1 solved problem,
namely Putnam 1988 B1, which was also solved in Lean 4.
The proof, which we include in Figure 13, generally follows
5
PUTNAM BENCH : A Multilingual Competition-Mathematics Be Conclusion
We presented PUTNAM BENCH , a benchmark for neural
theorem-proving consisting of formalizations of Putnam
competition problems. A distinctive feature of PUTNAM -
BENCH is that it spans a broad range of undergraduate-level
mathematical topics, including algebra, analysis, and num-
ber theory. Another unique benefit is that it includes prob-
lems in Lean 4, Isabelle, and Coq, the three most popular
formal proof frameworks.
As our experiments show, PUTNAM BENCH is a challeng-
ing benchmark: all current theorem-proving approaches
fail to solve more than a handful of its problems. We be-
lieve that these failures have two root causes: (i) While
current theorem-provers can effectively stitch together stan-
dard proof steps well-represented in the training corpus,
they often fail at synthesizing new lemmas and orchestrat-
ing these lemmas into complex proofs. (ii) Current methods
often fail to leverage the deep knowledge available in math-
ematics repositories. Developing a new generation of neural
theorem-provers in which these weaknesses are at least
partly addressed is an exciting direction of future research.
7
PUTNAM BENCH : A Multilingual Competition-Mathematics Benchmark for Formal Theorem-Proving
7. Impact Statement
This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal
consequences of our work, none which we feel must be
specifically highlighted here.
References
Archive of Formal Proofs — isa-afp.org. https://www.
isa-afp.org/ . [Accessed 25-05-2024].
Alexanderson, G., Klosinski, L., and Larson, L. The William
Lowell Putnam Mathematical Competition: Problems
and Solutions, 1965-1984 . MAA problem books se-
ries. Mathematical Association of America, 1985. ISBN
9780883854419. URL https://books.google.com/
books?id=mv0oAQAAMAAJ .
Azerbayev, Z., Piotrowski, B., Schoelkopf, H., Ayers, E. W.,
Radev, D., and Avigad, J. Proofnet: Autoformalizing and
formally proving undergraduate-level mathematics, 202",True
aa31d8eb1de7fb2f55359bbe2174056de902fe7d,Learning to Reason by Failing: Offline RL on Sub-optimal Rollouts Scales Synthetic Data by 8x,"['Amrith Setlur', 'Saurabh Garg', 'Xinyang Geng', 'Naman Garg', 'Virginia Smith', 'Aviral Kumar']",https://openreview.net/pdf/aa31d8eb1de7fb2f55359bbe2174056de902fe7d.pdf,"Learning to Reason by Failing: Offline RL on Sub-optimal Rollouts Scales Synthetic Data by 8x Training on model-generated synthetic data is a promising approach for finetuning LLMs, but it remains unclear when it helps or hurts. In this paper, we investigate this for reasoning problems via an empirical study, followed by a theoretical formalization. First, we find that while the typical approach of finetuning a model on synthetic correct or *positive* problem-solution pairs generated by capable models offers modest performance gains, sampling more correct solutions from the finetuned learner **doubles** the sample efficiency of synthetic data. At the same time, training on model-generated positives can amplify spurious correlations, resulting in flat or even inverse scaling trends as the amount of data increases. Surprisingly, we find that several of these issues can be addressed if we also utilize *negative* responses, \ie model-generated responses that are deemed incorrect via final answer checking. Crucially, these negatives must be constructed such that the training can appropriately recover the utility or credit of each intermediate step in the negative response. With this \emph{per-step} scheme, we are able to attain consistent gains over only positive data, attaining performance similar to amplifying the amount of synthetic data by **8x**. We show that training on per-step negatives can help to unlearn spurious correlations in the positive data, and is equivalent to advantage-weighted reinforcement learning (RL), implying that it inherits benefits of RL over imitating positive data alone.",aa31d8eb1de7fb2f55359bbe2174056de902fe7d.pdf,"approach for finetuning LLMs, but it
remains unclear when it helps or hurts. In this
paper, we investigate this for reasoning problems
via an empirical study, followed by a theoretical
formalization. First, we find that while the typical
approach of finetuning a model on synthetic cor-
rect or positive problem-solution pairs generated
by capable models offers modest performance
gains, sampling more correct solutions from the
finetuned learner doubles the sample efficiency
of synthetic data. At the same time, training on
model-generated positives can amplify spurious
correlations, resulting in flat or even inverse scal-
ing trends as the amount of data increases. Sur-
prisingly, we find that several of these issues can
be addressed if we also utilize negative responses,
i.e., model-generated responses that are deemed
incorrect via final answer checking. Crucially,
these negatives must be constructed such that the
training can appropriately recover the utility or
credit of each intermediate step in the negative
response. With this per-step scheme, we are able
to attain consistent gains over only positive data,
attaining performance similar to amplifying the
amount of synthetic data by 8×. We show that
training on per-step negatives can help to unlearn
spurious correlations in the positive data, and is
equivalent to advantage-weighted reinforcement
learning (RL), implying that it inherits benefits of
RL over imitating positive data alone.
1. Introduction
Training large language models (LLMs) relies on the ability
to train on large amounts of high-quality data. It is pre-
dicted that we will run out of high-quality internet data by
1Anonymous Institution, Anonymous City, Anonymous Region,
Anonymous Country. Correspondence to: Anonymous Author
<anon.email@domain.com>.
Preliminary work. Under review by the International Conference
on Machine Learning (ICML). Do not distribute.2026 (Villalobos et al., 2022; Liu et al., 2024), necessitat-
ing training on model-generated dat","future work should focus on.
Broader impact. Our work focuses purely on understanding the role of synthetic data in improving reasoning capabilities of
LLMs. While excessive use of synthetic data can have unintended side effects upon deployment (e.g., fitting onto spurious
correlations as we illustrate in Section 4) and advanced reasoning capabilities may have the potential to affect economy,
human life, and society in both good and bad ways, we believe that these societal impacts are not unique or special to our
work when compared to other works studying similar problems. In fact, with capabilities in foundation models improving
day-by-day, future research and policy decisions would only benefit from more conceptual models to understand how
algorithms operate and how data affects performance, which our work attempts to study.
C. Proof of Theorem 5.1
We first restate the theorem statement and then provide a proof for this below. Our main goal in this theorem is to show that
training with per-step DPO is equivalent to running advantage-weighted RL shown in the theoretical result.
Theorem C.1 (Equivalence of advantage-weighted RL and DPO with per-step pairs) .The optimal policy from Equation 1
withD±
πsftgiven by (x,[y1∶i,+yi+1],[y1∶i,−yi+1])where the positive and negative traces share prefix y1∶i∼πsft, and
−yi+1∼πsft(⋅∣x,y1∶i),+yi+1∼σ(A˜π(x,y1∶i;⋅)−A˜π(x,y1∶i;−yi+1)), is identical to the optima of the advantage-
weighted RL objective:
max
πEx∼psyn(x),y∼πsft(⋅∣x)[L
∑
i=1logπ(yi∣x,y0∶i−1)⋅exp(A˜π(x,y0∶i−1,yi)/β)]. (5)
Proof. To prove this statement, we make the following observation: DPO (Rafailov et al., 2023) is equivalent to optimizing
a KL-divergence penalized expected reward objective in an induced Bradly-Terry model of preferences defined by the
reward function. That is, for any reward function r(x,y)over contexts x∼µand responses y, the optimal solution to the
following RL objective:
max
πEx∼µ,y∼π(⋅∣x)[r(x,y)]−βDKL(π(⋅∣x)∣∣πsft(⋅∣x)), (6)
is given by the followi","approach for finetuning LLMs, but it remains unclear when it helps or hurts. In this paper, we investigate this for reasoning problems via an empirical study, followed by a theoretical formalization. First, we find that while the typical approach of finetuning a model on synthetic cor- rect or positive problem-solution pairs generated by capable models offers modest performance gains, sampling more correct solutions from the finetuned learner doubles the sample efficiency of synthetic data. At the same time, training on model-generated positives can amplify spurious correlations, resulting in flat or even inverse scal- ing trends as the amount of data increases. Sur- prisingly, we find that several of these issues can be addressed if we also utilize negative responses, i.e., model-generated responses that are deemed incorrect via final answer checking. Crucially, these negatives must be constructed such that the training can appropriately recover the utility or credit of each intermediate step in the negative response. With this per-step scheme, we are able to attain consistent gains over only positive data, attaining performance similar to amplifying the amount of synthetic data by 8 . We show that training on per-step negatives can help to unlearn spurious correlations in the positive data, and is equivalent to advantage-weighted reinforcement learning (RL), implying that it inherits benefits of RL over imitating positive data alone. 1. Introduction Training large language models (LLMs) relies on the ability to train on large amounts of high-quality data. It is pre- dicted that we will run out of high-quality internet data by 1Anonymous Institution, Anonymous City, Anonymous Region, Anonymous Country. Correspondence to: Anonymous Author <anon.email@domain.com>. Preliminary work. Under review by the International Conference on Machine Learning (ICML). Do not distribute.2026 (Villalobos et al., 2022; Liu et al., 2024), necessitat- ing training on model-generated dat","future work should focus on. Broader impact. Our work focuses purely on understanding the role of synthetic data in improving reasoning capabilities of LLMs. While excessive use of synthetic data can have unintended side effects upon deployment (e.g., fitting onto spurious correlations as we illustrate in Section 4) and advanced reasoning capabilities may have the potential to affect economy, human life, and society in both good and bad ways, we believe that these societal impacts are not unique or special to our work when compared to other works studying similar problems. In fact, with capabilities in foundation models improving day-by-day, future research and policy decisions would only benefit from more conceptual models to understand how algorithms operate and how data affects performance, which our work attempts to study. C. Proof of Theorem 5.1 We first restate the theorem statement and then provide a proof for this below. Our main goal in this theorem is to show that training with per-step DPO is equivalent to running advantage-weighted RL shown in the theoretical result. Theorem C.1 (Equivalence of advantage-weighted RL and DPO with per-step pairs) .The optimal policy from Equation 1 withD sftgiven by (x,[y1 i,+yi+1],[y1 i, yi+1])where the positive and negative traces share prefix y1 i sft, and yi+1 sft( x,y1 i),+yi+1 (A (x,y1 i; ) A (x,y1 i; yi+1)), is identical to the optima of the advantage- weighted RL objective: max Ex psyn(x),y sft( x)[L i=1log (yi x,y0 i 1) exp(A (x,y0 i 1,yi)/ )]. (5) Proof. To prove this statement, we make the following observation: DPO (Rafailov et al., 2023) is equivalent to optimizing a KL-divergence penalized expected reward objective in an induced Bradly-Terry model of preferences defined by the reward function. That is, for any reward function r(x,y)over contexts x and responses y, the optimal solution to the following RL objective: max Ex ,y ( x)[r(x,y)] DKL( ( x) sft( x)), (6) is given by the followi","Learning to Reason by Failing: Offline RL on Sub-optimal Rollouts Scales Synthetic Data by 8x Training on model-generated synthetic data is a promising approach for finetuning LLMs, but it remains unclear when it helps or hurts. In this paper, we investigate this for reasoning problems via an empirical study, followed by a theoretical formalization. First, we find that while the typical approach of finetuning a model on synthetic correct or *positive* problem-solution pairs generated by capable models offers modest performance gains, sampling more correct solutions from the finetuned learner **doubles** the sample efficiency of synthetic data. At the same time, training on model-generated positives can amplify spurious correlations, resulting in flat or even inverse scaling trends as the amount of data increases. Surprisingly, we find that several of these issues can be addressed if we also utilize *negative* responses, \ie model-generated responses that are deemed incorrect via final answer checking. Crucially, these negatives must be constructed such that the training can appropriately recover the utility or credit of each intermediate step in the negative response. With this \emph{per-step} scheme, we are able to attain consistent gains over only positive data, attaining performance similar to amplifying the amount of synthetic data by **8x**. We show that training on per-step negatives can help to unlearn spurious correlations in the positive data, and is equivalent to advantage-weighted reinforcement learning (RL), implying that it inherits benefits of RL over imitating positive data alone. future work should focus on. Broader impact. Our work focuses purely on understanding the role of synthetic data in improving reasoning capabilities of LLMs. While excessive use of synthetic data can have unintended side effects upon deployment (e.g., fitting onto spurious correlations as we illustrate in Section 4) and advanced reasoning capabilities may have the potential to affect economy, human life, and society in both good and bad ways, we believe that these societal impacts are not unique or special to our work when compared to other works studying similar problems. In fact, with capabilities in foundation models improving day-by-day, future research and policy decisions would only benefit from more conceptual models to understand how algorithms operate and how data affects performance, which our work attempts to study. C. Proof of Theorem 5.1 We first restate the theorem statement and then provide a proof for this below. Our main goal in this theorem is to show that training with per-step DPO is equivalent to running advantage-weighted RL shown in the theoretical result. Theorem C.1 (Equivalence of advantage-weighted RL and DPO with per-step pairs) .The optimal policy from Equation 1 withD sftgiven by (x,[y1 i,+yi+1],[y1 i, yi+1])where the positive and negative traces share prefix y1 i sft, and yi+1 sft( x,y1 i),+yi+1 (A (x,y1 i; ) A (x,y1 i; yi+1)), is identical to the optima of the advantage- weighted RL objective: max Ex psyn(x),y sft( x)[L i=1log (yi x,y0 i 1) exp(A (x,y0 i 1,yi)/ )]. (5) Proof. To prove this statement, we make the following observation: DPO (Rafailov et al., 2023) is equivalent to optimizing a KL-divergence penalized expected reward objective in an induced Bradly-Terry model of preferences defined by the reward function. That is, for any reward function r(x,y)over contexts x and responses y, the optimal solution to the following RL objective: max Ex ,y ( x)[r(x,y)] DKL( ( x) sft( x)), (6) is given by the followi approach for finetuning LLMs, but it remains unclear when it helps or hurts. In this paper, we investigate this for reasoning problems via an empirical study, followed by a theoretical formalization. First, we find that while the typical approach of finetuning a model on synthetic cor- rect or positive problem-solution pairs generated by capable models offers modest performance gains, sampling more correct solutions from the finetuned learner doubles the sample efficiency of synthetic data. At the same time, training on model-generated positives can amplify spurious correlations, resulting in flat or even inverse scal- ing trends as the amount of data increases. Sur- prisingly, we find that several of these issues can be addressed if we also utilize negative responses, i.e., model-generated responses that are deemed incorrect via final answer checking. Crucially, these negatives must be constructed such that the training can appropriately recover the utility or credit of each intermediate step in the negative response. With this per-step scheme, we are able to attain consistent gains over only positive data, attaining performance similar to amplifying the amount of synthetic data by 8 . We show that training on per-step negatives can help to unlearn spurious correlations in the positive data, and is equivalent to advantage-weighted reinforcement learning (RL), implying that it inherits benefits of RL over imitating positive data alone. 1. Introduction Training large language models (LLMs) relies on the ability to train on large amounts of high-quality data. It is pre- dicted that we will run out of high-quality internet data by 1Anonymous Institution, Anonymous City, Anonymous Region, Anonymous Country. Correspondence to: Anonymous Author <anon.email@domain.com>. Preliminary work. Under review by the International Conference on Machine Learning (ICML). Do not distribute.2026 (Villalobos et al., 2022; Liu et al., 2024), necessitat- ing training on model-generated dat","Learning to Reason by Failing: Offline RL on Sub-optimal Rollouts Scales Synthetic Data by 8x Training on model-generated synthetic data is a promising approach for finetuning LLMs, but it remains unclear when it helps or hurts. In this paper, we investigate this for reasoning problems via an empirical study, followed by a theoretical formalization. First, we find that while the typical approach of finetuning a model on synthetic correct or *positive* problem-solution pairs generated by capable models offers modest performance gains, sampling more correct solutions from the finetuned learner **doubles** the sample efficiency of synthetic data. At the same time, training on model-generated positives can amplify spurious correlations, resulting in flat or even inverse scaling trends as the amount of data increases. Surprisingly, we find that several of these issues can be addressed if we also utilize *negative* responses, \ie model-generated responses that are deemed incorrect via final answer checking. Crucially, these negatives must be constructed such that the training can appropriately recover the utility or credit of each intermediate step in the negative response. With this \emph{per-step} scheme, we are able to attain consistent gains over only positive data, attaining performance similar to amplifying the amount of synthetic data by **8x**. We show that training on per-step negatives can help to unlearn spurious correlations in the positive data, and is equivalent to advantage-weighted reinforcement learning (RL), implying that it inherits benefits of RL over imitating positive data alone.",0,"approach for finetuning LLMs, but it
remains unclear when it helps or hurts. In this
paper, we investigate this for reasoning problems
via an empirical study, followed by a theoretical
formalization. First, we find that while the typical
approach of finetuning a model on synthetic cor-
rect or positive problem-solution pairs generated
by capable models offers modest performance
gains, sampling more correct solutions from the
finetuned learner doubles the sample efficiency
of synthetic data. At the same time, training on
model-generated positives can amplify spurious
correlations, resulting in flat or even inverse scal-
ing trends as the amount of data increases. Sur-
prisingly, we find that several of these issues can
be addressed if we also utilize negative responses,
i.e., model-generated responses that are deemed
incorrect via final answer checking. Crucially,
these negatives must be constructed such that the
training can appropriately recover the utility or
credit of each intermediate step in the negative
response. With this per-step scheme, we are able
to attain consistent gains over only positive data,
attaining performance similar to amplifying the
amount of synthetic data by 8×. We show that
training on per-step negatives can help to unlearn
spurious correlations in the positive data, and is
equivalent to advantage-weighted reinforcement
learning (RL), implying that it inherits benefits of
RL over imitating positive data alone.
1. Introduction
Training large language models (LLMs) relies on the ability
to train on large amounts of high-quality data. It is pre-
dicted that we will run out of high-quality internet data by
1Anonymous Institution, Anonymous City, Anonymous Region,
Anonymous Country. Correspondence to: Anonymous Author
<anon.email@domain.com>.
Preliminary work. Under review by the International Conference
on Machine Learning (ICML). Do not distribute.2026 (Villalobos et al., 2022; Liu et al., 2024), necessitat-
ing training on model-generated dat future work should focus on.
Broader impact. Our work focuses purely on understanding the role of synthetic data in improving reasoning capabilities of
LLMs. While excessive use of synthetic data can have unintended side effects upon deployment (e.g., fitting onto spurious
correlations as we illustrate in Section 4) and advanced reasoning capabilities may have the potential to affect economy,
human life, and society in both good and bad ways, we believe that these societal impacts are not unique or special to our
work when compared to other works studying similar problems. In fact, with capabilities in foundation models improving
day-by-day, future research and policy decisions would only benefit from more conceptual models to understand how
algorithms operate and how data affects performance, which our work attempts to study.
C. Proof of Theorem 5.1
We first restate the theorem statement and then provide a proof for this below. Our main goal in this theorem is to show that
training with per-step DPO is equivalent to running advantage-weighted RL shown in the theoretical result.
Theorem C.1 (Equivalence of advantage-weighted RL and DPO with per-step pairs) .The optimal policy from Equation 1
withD±
πsftgiven by (x,[y1∶i,+yi+1],[y1∶i,−yi+1])where the positive and negative traces share prefix y1∶i∼πsft, and
−yi+1∼πsft(⋅∣x,y1∶i),+yi+1∼σ(A˜π(x,y1∶i;⋅)−A˜π(x,y1∶i;−yi+1)), is identical to the optima of the advantage-
weighted RL objective:
max
πEx∼psyn(x),y∼πsft(⋅∣x)[L
∑
i=1logπ(yi∣x,y0∶i−1)⋅exp(A˜π(x,y0∶i−1,yi)/β)]. (5)
Proof. To prove this statement, we make the following observation: DPO (Rafailov et al., 2023) is equivalent to optimizing
a KL-divergence penalized expected reward objective in an induced Bradly-Terry model of preferences defined by the
reward function. That is, for any reward function r(x,y)over contexts x∼µand responses y, the optimal solution to the
following RL objective:
max
πEx∼µ,y∼π(⋅∣x)[r(x,y)]−βDKL(π(⋅∣x)∣∣πsft(⋅∣x)), (6)
is given by the followi",False
7bc6e168a9fe100095a13b5221a503d8d173f3ab,Lean4trace: Data augmentation for neural theorem proving in Lean,"['Vasilii Nesterov', 'Yermek Kapushev', 'Mikhail Burtsev']",https://openreview.net/pdf/7bc6e168a9fe100095a13b5221a503d8d173f3ab.pdf,"Lean4trace: Data augmentation for neural theorem proving in Lean Integrating large language models as proof assistants with theorem provers has shown great promise. However, one of the major challenges in this field is the scarcity of training data. To address this, we release a new open-source tool, *Lean4trace*, for training data extraction from Lean 4 sources. Unlike previous approaches, *Lean4trace* is deeply integrated into the Lean elaborator, allowing us to modify proofs on-the-fly. Leveraging this feature, we propose two methods of data augmentation in Lean: (1) decomposing composite proof steps into multiple simpler steps; (2) testing existing proof automation tactics at each proof state and collecting the successful ones. Models trained on this augmented data are capable of proving 58.0% of theorems from a hold-out subset of Mathlib and 35.6% of the test subset of the MiniF2F benchmark.",7bc6e168a9fe100095a13b5221a503d8d173f3ab.pdf,"approaches,
Lean4trace is deeply integrated into the Lean elab-
orator, allowing us to modify proofs on-the-fly.
Leveraging this feature, we propose two methods
of data augmentation in Lean: (1) decomposing
composite proof steps into multiple simpler steps;
(2) testing existing proof automation tactics at
each proof state and collecting the successful ones.
Models trained on this augmented data are capa-
ble of proving 58.0% of theorems from a hold-out
subset of Mathlib and 35.6% of the test subset of
the MiniF2F benchmark.
1. Introduction
One of the advantages of mathematics over other sciences
is that the correctness of its results can, in principle, be
verified mechanically. This is particularly desirable because
the standard peer review process inevitably sometimes re-
sults in invalid proofs. However, in practice, formalizing
mathematics is a labor-intensive and time-consuming task.
The standard approach involves using interactive theorem
proving (ITP) systems, among which it is worth mentioning
Lean (de Moura et al., 2015), Isabelle (Nipkow et al., 2002),
Coq (Barras et al., 1997), and Metamath (Megill & Wheeler,
2019). The process of theorem proving in such a system is
similar to programming in an IDE: a user interacts with the
system via commands in a formal language, and the system
provides feedback on whether the proof is successful.
In recent years, significant efforts have been made to sim-
plify the formalization process. The most developed li-
1Moscow Institute for Physics and Technology2Yandex
3London Institute for Mathematical Sciences. Correspondence
to: Vasilii Nesterov <vas.nesterov63@google.com >.
AI for MATH Workshop at ICML 2024 . Copyright 2024 by the
author(s)braries of formalized mathematics now contain more than
100,000 theorems. One example is Mathlib (mathlib Com-
munity, 2020), a user-maintained mathematical library for
the Lean theorem prover, which covers a wide range of
mathematical fields.
From another perspective, formal theorem pro","Conclusion
In this paper, we present Lean4trace , a novel tool for data
extraction and augmentation tailored for training neural the-
orem provers in Lean. Our experimental results demonstrate
that models trained using our dataset achieve a 9% higher
performance on the MiniF2F benchmark compared to Re-
Prover (Yang et al., 2023), when trained and evaluated under
identical conditions. While proposed augmentations pro-
vides improvements on Mathlib dataset, they may degrade
the model when evaluated on a dataset from different distri-
bution. Nevertheless, our tool allows gathering a more com-
plete set of proof states in canonical setup and significantly
reduces computational resource requirements compared to
LeanDojo (Yang et al., 2023), making it feasible to run on a
modern PC. We believe that these advancements will lower
the barrier to entry in this field, fostering more accessible
and widespread research in neural theorem proving.
6. Acknowledgements
This work was supported by a grant for research centers in
the field of artificial intelligence, provided by the Analytical
Center for the Government of the Russian Federation in
accordance with the subsidy agreement (agreement identi-
fier 000000D730324P540002) and the agreement with the
Moscow Institute of Physics and Technology dated Novem-
ber 1, 2021 No. 70-2021-00138.
References
Azerbayev, Z., Schoelkopf, H., Paster, K., Dos Santos, M.,
McAleer, S., Jiang, A. Q., Deng, J., Biderman, S., and
Welleck, S. Llemma: An open language model for math-
ematics. arXiv preprint arXiv:2310.06786 , 2023.theorem mathd_numbertheory_135
(n A B C : Nat)
(h0: n = 3ˆ17 + 3ˆ10)
(h1: 11 | (n + 1))
(h2: [A,B,C].Pairwise ( · ̸=·))
(h3: {A,B,C} ⊆Finset.Icc 0 9)
(h4: Odd A ∧Odd C)
(h5:¬3 | B)
(h6: Nat.digits 10 n = [B,A,B,C,C,A,C,B,A]) :
100 *A + 10 *B + C = 129 := by
aesop -- general-purpose automatic tactic
theorem mathd_numbertheory_229 :
(5ˆ30) % 7 = 1 := by
decide -- tactic that proves some ""decidable"" goals
-- here it just compute","approaches, Lean4trace is deeply integrated into the Lean elab- orator, allowing us to modify proofs on-the-fly. Leveraging this feature, we propose two methods of data augmentation in Lean: (1) decomposing composite proof steps into multiple simpler steps; (2) testing existing proof automation tactics at each proof state and collecting the successful ones. Models trained on this augmented data are capa- ble of proving 58.0% of theorems from a hold-out subset of Mathlib and 35.6% of the test subset of the MiniF2F benchmark. 1. Introduction One of the advantages of mathematics over other sciences is that the correctness of its results can, in principle, be verified mechanically. This is particularly desirable because the standard peer review process inevitably sometimes re- sults in invalid proofs. However, in practice, formalizing mathematics is a labor-intensive and time-consuming task. The standard approach involves using interactive theorem proving (ITP) systems, among which it is worth mentioning Lean (de Moura et al., 2015), Isabelle (Nipkow et al., 2002), Coq (Barras et al., 1997), and Metamath (Megill & Wheeler, 2019). The process of theorem proving in such a system is similar to programming in an IDE: a user interacts with the system via commands in a formal language, and the system provides feedback on whether the proof is successful. In recent years, significant efforts have been made to sim- plify the formalization process. The most developed li- 1Moscow Institute for Physics and Technology2Yandex 3London Institute for Mathematical Sciences. Correspondence to: Vasilii Nesterov <vas.nesterov63@google.com >. AI for MATH Workshop at ICML 2024 . Copyright 2024 by the author(s)braries of formalized mathematics now contain more than 100,000 theorems. One example is Mathlib (mathlib Com- munity, 2020), a user-maintained mathematical library for the Lean theorem prover, which covers a wide range of mathematical fields. From another perspective, formal theorem pro","Conclusion In this paper, we present Lean4trace , a novel tool for data extraction and augmentation tailored for training neural the- orem provers in Lean. Our experimental results demonstrate that models trained using our dataset achieve a 9% higher performance on the MiniF2F benchmark compared to Re- Prover (Yang et al., 2023), when trained and evaluated under identical conditions. While proposed augmentations pro- vides improvements on Mathlib dataset, they may degrade the model when evaluated on a dataset from different distri- bution. Nevertheless, our tool allows gathering a more com- plete set of proof states in canonical setup and significantly reduces computational resource requirements compared to LeanDojo (Yang et al., 2023), making it feasible to run on a modern PC. We believe that these advancements will lower the barrier to entry in this field, fostering more accessible and widespread research in neural theorem proving. 6.","Lean4trace: Data augmentation for neural theorem proving in Lean Integrating large language models as proof assistants with theorem provers has shown great promise. However, one of the major challenges in this field is the scarcity of training data. To address this, we release a new open-source tool, *Lean4trace*, for training data extraction from Lean 4 sources. Unlike previous approaches, *Lean4trace* is deeply integrated into the Lean elaborator, allowing us to modify proofs on-the-fly. Leveraging this feature, we propose two methods of data augmentation in Lean: (1) decomposing composite proof steps into multiple simpler steps; (2) testing existing proof automation tactics at each proof state and collecting the successful ones. Models trained on this augmented data are capable of proving 58.0% of theorems from a hold-out subset of Mathlib and 35.6% of the test subset of the MiniF2F benchmark. Conclusion In this paper, we present Lean4trace , a novel tool for data extraction and augmentation tailored for training neural the- orem provers in Lean. Our experimental results demonstrate that models trained using our dataset achieve a 9% higher performance on the MiniF2F benchmark compared to Re- Prover (Yang et al., 2023), when trained and evaluated under identical conditions. While proposed augmentations pro- vides improvements on Mathlib dataset, they may degrade the model when evaluated on a dataset from different distri- bution. Nevertheless, our tool allows gathering a more com- plete set of proof states in canonical setup and significantly reduces computational resource requirements compared to LeanDojo (Yang et al., 2023), making it feasible to run on a modern PC. We believe that these advancements will lower the barrier to entry in this field, fostering more accessible and widespread research in neural theorem proving. 6. approaches, Lean4trace is deeply integrated into the Lean elab- orator, allowing us to modify proofs on-the-fly. Leveraging this feature, we propose two methods of data augmentation in Lean: (1) decomposing composite proof steps into multiple simpler steps; (2) testing existing proof automation tactics at each proof state and collecting the successful ones. Models trained on this augmented data are capa- ble of proving 58.0% of theorems from a hold-out subset of Mathlib and 35.6% of the test subset of the MiniF2F benchmark. 1. Introduction One of the advantages of mathematics over other sciences is that the correctness of its results can, in principle, be verified mechanically. This is particularly desirable because the standard peer review process inevitably sometimes re- sults in invalid proofs. However, in practice, formalizing mathematics is a labor-intensive and time-consuming task. The standard approach involves using interactive theorem proving (ITP) systems, among which it is worth mentioning Lean (de Moura et al., 2015), Isabelle (Nipkow et al., 2002), Coq (Barras et al., 1997), and Metamath (Megill & Wheeler, 2019). The process of theorem proving in such a system is similar to programming in an IDE: a user interacts with the system via commands in a formal language, and the system provides feedback on whether the proof is successful. In recent years, significant efforts have been made to sim- plify the formalization process. The most developed li- 1Moscow Institute for Physics and Technology2Yandex 3London Institute for Mathematical Sciences. Correspondence to: Vasilii Nesterov <vas.nesterov63@google.com >. AI for MATH Workshop at ICML 2024 . Copyright 2024 by the author(s)braries of formalized mathematics now contain more than 100,000 theorems. One example is Mathlib (mathlib Com- munity, 2020), a user-maintained mathematical library for the Lean theorem prover, which covers a wide range of mathematical fields. From another perspective, formal theorem pro","Lean4trace: Data augmentation for neural theorem proving in Lean Integrating large language models as proof assistants with theorem provers has shown great promise. However, one of the major challenges in this field is the scarcity of training data. To address this, we release a new open-source tool, *Lean4trace*, for training data extraction from Lean 4 sources. Unlike previous approaches, *Lean4trace* is deeply integrated into the Lean elaborator, allowing us to modify proofs on-the-fly. Leveraging this feature, we propose two methods of data augmentation in Lean: (1) decomposing composite proof steps into multiple simpler steps; (2) testing existing proof automation tactics at each proof state and collecting the successful ones. Models trained on this augmented data are capable of proving 58.0% of theorems from a hold-out subset of Mathlib and 35.6% of the test subset of the MiniF2F benchmark.",0,"approaches,
Lean4trace is deeply integrated into the Lean elab-
orator, allowing us to modify proofs on-the-fly.
Leveraging this feature, we propose two methods
of data augmentation in Lean: (1) decomposing
composite proof steps into multiple simpler steps;
(2) testing existing proof automation tactics at
each proof state and collecting the successful ones.
Models trained on this augmented data are capa-
ble of proving 58.0% of theorems from a hold-out
subset of Mathlib and 35.6% of the test subset of
the MiniF2F benchmark.
1. Introduction
One of the advantages of mathematics over other sciences
is that the correctness of its results can, in principle, be
verified mechanically. This is particularly desirable because
the standard peer review process inevitably sometimes re-
sults in invalid proofs. However, in practice, formalizing
mathematics is a labor-intensive and time-consuming task.
The standard approach involves using interactive theorem
proving (ITP) systems, among which it is worth mentioning
Lean (de Moura et al., 2015), Isabelle (Nipkow et al., 2002),
Coq (Barras et al., 1997), and Metamath (Megill & Wheeler,
2019). The process of theorem proving in such a system is
similar to programming in an IDE: a user interacts with the
system via commands in a formal language, and the system
provides feedback on whether the proof is successful.
In recent years, significant efforts have been made to sim-
plify the formalization process. The most developed li-
1Moscow Institute for Physics and Technology2Yandex
3London Institute for Mathematical Sciences. Correspondence
to: Vasilii Nesterov <vas.nesterov63@google.com >.
AI for MATH Workshop at ICML 2024 . Copyright 2024 by the
author(s)braries of formalized mathematics now contain more than
100,000 theorems. One example is Mathlib (mathlib Com-
munity, 2020), a user-maintained mathematical library for
the Lean theorem prover, which covers a wide range of
mathematical fields.
From another perspective, formal theorem pro Conclusion
In this paper, we present Lean4trace , a novel tool for data
extraction and augmentation tailored for training neural the-
orem provers in Lean. Our experimental results demonstrate
that models trained using our dataset achieve a 9% higher
performance on the MiniF2F benchmark compared to Re-
Prover (Yang et al., 2023), when trained and evaluated under
identical conditions. While proposed augmentations pro-
vides improvements on Mathlib dataset, they may degrade
the model when evaluated on a dataset from different distri-
bution. Nevertheless, our tool allows gathering a more com-
plete set of proof states in canonical setup and significantly
reduces computational resource requirements compared to
LeanDojo (Yang et al., 2023), making it feasible to run on a
modern PC. We believe that these advancements will lower
the barrier to entry in this field, fostering more accessible
and widespread research in neural theorem proving.
6. Acknowledgements
This work was supported by a grant for research centers in
the field of artificial intelligence, provided by the Analytical
Center for the Government of the Russian Federation in
accordance with the subsidy agreement (agreement identi-
fier 000000D730324P540002) and the agreement with the
Moscow Institute of Physics and Technology dated Novem-
ber 1, 2021 No. 70-2021-00138.
References
Azerbayev, Z., Schoelkopf, H., Paster, K., Dos Santos, M.,
McAleer, S., Jiang, A. Q., Deng, J., Biderman, S., and
Welleck, S. Llemma: An open language model for math-
ematics. arXiv preprint arXiv:2310.06786 , 2023.theorem mathd_numbertheory_135
(n A B C : Nat)
(h0: n = 3ˆ17 + 3ˆ10)
(h1: 11 | (n + 1))
(h2: [A,B,C].Pairwise ( · ̸=·))
(h3: {A,B,C} ⊆Finset.Icc 0 9)
(h4: Odd A ∧Odd C)
(h5:¬3 | B)
(h6: Nat.digits 10 n = [B,A,B,C,C,A,C,B,A]) :
100 *A + 10 *B + C = 129 := by
aesop -- general-purpose automatic tactic
theorem mathd_numbertheory_229 :
(5ˆ30) % 7 = 1 := by
decide -- tactic that proves some ""decidable"" goals
-- here it just compute",True
70ff41de083363020856e5381537edb482990dd0,Efficient Linear System Solver with Transformers,"['Max Vladymyrov', 'Johannes von Oswald', 'Nolan Andrew Miller', 'Mark Sandler']",https://openreview.net/pdf/70ff41de083363020856e5381537edb482990dd0.pdf,"Efficient Linear System Solver with Transformers A novel, efficient transformer-based approach to solving small linear systems. This paper investigates the potential of linear Transformers as solvers for systems of linear equations. We propose a novel approach where the Transformer encodes each equation as a separate token, allowing the model to process the system in a permutation-invariant manner. To enhance generalizability and reduce the parameter count, we introduce a block-wise re-parameterization technique for the attention weight matrices. This technique decouples the problem dimension from the model's parameter count, enabling the Transformer to effectively handle systems of varying sizes. Our experiments demonstrate the Transformer's competitive performance compared to established classical methods such as Conjugate Gradient, especially for systems with smaller sizes. We further explore the model's ability to extrapolate to larger systems, providing evidence for its potential as a versatile and efficient solver for linear equations.",70ff41de083363020856e5381537edb482990dd0.pdf,"approach where the
Transformer encodes each equation as a separate
token, allowing the model to process the system
in a permutation-invariant manner. To enhance
generalizability and reduce the parameter count,
we introduce a block-wise re-parameterization
technique for the attention weight matrices. This
technique decouples the problem dimension from
the model’s parameter count, enabling the Trans-
former to effectively handle systems of varying
sizes. Our experiments demonstrate the Trans-
former’s competitive performance compared to
established classical methods such as Conjugate
Gradient, especially for systems with smaller
sizes. We further explore the model’s ability to
extrapolate to larger systems, providing evidence
for its potential as a versatile and efﬁcient solver
for linear equations.
1. Introduction
Solving linear systems of equations is a fundamental prob-
lem in numerous ﬁelds, including scientiﬁc computing, ma-
chine learning, and engineering. While traditional methods
like Gaussian elimination and iterative solvers like Conju-
gate Gradient (Hestenes et al., 1952) are widely used, ex-
ploring alternative approaches holds the potential for more
efﬁcient and versatile solutions.
Transformers, originally developed for natural language pro-
cessing tasks (Vaswani et al., 2017), have demonstrated a
remarkable ability to capture complex relationships within
sequential data. This ability extends beyond natural lan-
guage processing, as evidenced by their successful applica-
tion in solving a variety of problems, ranging from noisy
linear regression and classiﬁcation (Garg et al., 2022) to the
1Google Research. Correspondence to: Max Vladymyrov
<mxv@google.com >.
The ﬁrst AI for MATH Workshop at the 41st International Confer-
ence on Machine Learning , Vienna, Austria. Copyright 2024 by
the author(s).traveling salesmen problem (Yang et al., 2023) and other
domains (Mirchandani et al., 2023). Transformers have
shown promise in various scientiﬁc computing ta","Conclusions
This paper explored the novel application of linear Trans-
formers for efﬁciently solving small systems of linear equa-
tions with symmetric and positive deﬁnite coefﬁcient ma-
trices. We demonstrated that by encoding each equation
as a distinct token and implementing a block-wise re-
parameterization technique, Transformers could achieve
accuracy comparable to 6-8 iterations of Conjugate Gradi-
ent, while being faster for small problem sizes. Our model
exhibited the ability to generalize beyond its training data,
effectively handling systems of varying sizes and unseen
condition numbers.
This research opens up exciting possibilities for utilizing
Transformers in numerical tasks. Further investigation into
architectural choices, training strategies, and the boundaries
of generalization could lead to the development of even
more efﬁcient and adaptable solvers. This work reinforces
the notion that Transformers, initially designed for natural
language processing, hold remarkable potential as powerful
tools across various scientiﬁc computing domains.
While our approach shows promise, it has several limitations.
Currently, we only handle positive deﬁnite symmetric matri-
ces, which restricts the applicability of our method. Future
work should explore extending this approach to general ma-
trices, including non-symmetric and indeﬁnite cases. Addi-
tionally, our method’s performance on very large systems or
highly ill-conditioned matrices needs further investigation.
4
Efﬁcient Linear System Solver with Transformers
Scaling the approach to handle sparse matrices efﬁciently is
another important direction for future research. Integrating
our Transformer-based solver with classical methods, po-
tentially as a preconditioner or in a hybrid algorithm, could
leverage the strengths of both approaches and is an exciting
area for further study.
References
Ahn, K., Cheng, X., Daneshmand, H., and Sra, S.
Transformers learn to implement preconditioned gra-
dient descent fo","approach where the Transformer encodes each equation as a separate token, allowing the model to process the system in a permutation-invariant manner. To enhance generalizability and reduce the parameter count, we introduce a block-wise re-parameterization technique for the attention weight matrices. This technique decouples the problem dimension from the model s parameter count, enabling the Trans- former to effectively handle systems of varying sizes. Our experiments demonstrate the Trans- former s competitive performance compared to established classical methods such as Conjugate Gradient, especially for systems with smaller sizes. We further explore the model s ability to extrapolate to larger systems, providing evidence for its potential as a versatile and ef cient solver for linear equations. 1. Introduction Solving linear systems of equations is a fundamental prob- lem in numerous elds, including scienti c computing, ma- chine learning, and engineering. While traditional methods like Gaussian elimination and iterative solvers like Conju- gate Gradient (Hestenes et al., 1952) are widely used, ex- ploring alternative approaches holds the potential for more ef cient and versatile solutions. Transformers, originally developed for natural language pro- cessing tasks (Vaswani et al., 2017), have demonstrated a remarkable ability to capture complex relationships within sequential data. This ability extends beyond natural lan- guage processing, as evidenced by their successful applica- tion in solving a variety of problems, ranging from noisy linear regression and classi cation (Garg et al., 2022) to the 1Google Research. Correspondence to: Max Vladymyrov <mxv@google.com >. The rst AI for MATH Workshop at the 41st International Confer- ence on Machine Learning , Vienna, Austria. Copyright 2024 by the author(s).traveling salesmen problem (Yang et al., 2023) and other domains (Mirchandani et al., 2023). Transformers have shown promise in various scienti c computing ta","Conclusions This paper explored the novel application of linear Trans- formers for ef ciently solving small systems of linear equa- tions with symmetric and positive de nite coef cient ma- trices. We demonstrated that by encoding each equation as a distinct token and implementing a block-wise re- parameterization technique, Transformers could achieve accuracy comparable to 6-8 iterations of Conjugate Gradi- ent, while being faster for small problem sizes. Our model exhibited the ability to generalize beyond its training data, effectively handling systems of varying sizes and unseen condition numbers. This research opens up exciting possibilities for utilizing Transformers in numerical tasks. Further investigation into architectural choices, training strategies, and the boundaries of generalization could lead to the development of even more ef cient and adaptable solvers. This work reinforces the notion that Transformers, initially designed for natural language processing, hold remarkable potential as powerful tools across various scienti c computing domains. While our approach shows promise, it has several limitations. Currently, we only handle positive de nite symmetric matri- ces, which restricts the applicability of our method. Future work should explore extending this approach to general ma- trices, including non-symmetric and inde nite cases. Addi- tionally, our method s performance on very large systems or highly ill-conditioned matrices needs further investigation. 4 Ef cient Linear System Solver with Transformers Scaling the approach to handle sparse matrices ef ciently is another important direction for future research. Integrating our Transformer-based solver with classical methods, po- tentially as a preconditioner or in a hybrid algorithm, could leverage the strengths of both approaches and is an exciting area for further study.","Efficient Linear System Solver with Transformers A novel, efficient transformer-based approach to solving small linear systems. This paper investigates the potential of linear Transformers as solvers for systems of linear equations. We propose a novel approach where the Transformer encodes each equation as a separate token, allowing the model to process the system in a permutation-invariant manner. To enhance generalizability and reduce the parameter count, we introduce a block-wise re-parameterization technique for the attention weight matrices. This technique decouples the problem dimension from the model's parameter count, enabling the Transformer to effectively handle systems of varying sizes. Our experiments demonstrate the Transformer's competitive performance compared to established classical methods such as Conjugate Gradient, especially for systems with smaller sizes. We further explore the model's ability to extrapolate to larger systems, providing evidence for its potential as a versatile and efficient solver for linear equations. Conclusions This paper explored the novel application of linear Trans- formers for ef ciently solving small systems of linear equa- tions with symmetric and positive de nite coef cient ma- trices. We demonstrated that by encoding each equation as a distinct token and implementing a block-wise re- parameterization technique, Transformers could achieve accuracy comparable to 6-8 iterations of Conjugate Gradi- ent, while being faster for small problem sizes. Our model exhibited the ability to generalize beyond its training data, effectively handling systems of varying sizes and unseen condition numbers. This research opens up exciting possibilities for utilizing Transformers in numerical tasks. Further investigation into architectural choices, training strategies, and the boundaries of generalization could lead to the development of even more ef cient and adaptable solvers. This work reinforces the notion that Transformers, initially designed for natural language processing, hold remarkable potential as powerful tools across various scienti c computing domains. While our approach shows promise, it has several limitations. Currently, we only handle positive de nite symmetric matri- ces, which restricts the applicability of our method. Future work should explore extending this approach to general ma- trices, including non-symmetric and inde nite cases. Addi- tionally, our method s performance on very large systems or highly ill-conditioned matrices needs further investigation. 4 Ef cient Linear System Solver with Transformers Scaling the approach to handle sparse matrices ef ciently is another important direction for future research. Integrating our Transformer-based solver with classical methods, po- tentially as a preconditioner or in a hybrid algorithm, could leverage the strengths of both approaches and is an exciting area for further study. approach where the Transformer encodes each equation as a separate token, allowing the model to process the system in a permutation-invariant manner. To enhance generalizability and reduce the parameter count, we introduce a block-wise re-parameterization technique for the attention weight matrices. This technique decouples the problem dimension from the model s parameter count, enabling the Trans- former to effectively handle systems of varying sizes. Our experiments demonstrate the Trans- former s competitive performance compared to established classical methods such as Conjugate Gradient, especially for systems with smaller sizes. We further explore the model s ability to extrapolate to larger systems, providing evidence for its potential as a versatile and ef cient solver for linear equations. 1. Introduction Solving linear systems of equations is a fundamental prob- lem in numerous elds, including scienti c computing, ma- chine learning, and engineering. While traditional methods like Gaussian elimination and iterative solvers like Conju- gate Gradient (Hestenes et al., 1952) are widely used, ex- ploring alternative approaches holds the potential for more ef cient and versatile solutions. Transformers, originally developed for natural language pro- cessing tasks (Vaswani et al., 2017), have demonstrated a remarkable ability to capture complex relationships within sequential data. This ability extends beyond natural lan- guage processing, as evidenced by their successful applica- tion in solving a variety of problems, ranging from noisy linear regression and classi cation (Garg et al., 2022) to the 1Google Research. Correspondence to: Max Vladymyrov <mxv@google.com >. The rst AI for MATH Workshop at the 41st International Confer- ence on Machine Learning , Vienna, Austria. Copyright 2024 by the author(s).traveling salesmen problem (Yang et al., 2023) and other domains (Mirchandani et al., 2023). Transformers have shown promise in various scienti c computing ta","Efficient Linear System Solver with Transformers A novel, efficient transformer-based approach to solving small linear systems. This paper investigates the potential of linear Transformers as solvers for systems of linear equations. We propose a novel approach where the Transformer encodes each equation as a separate token, allowing the model to process the system in a permutation-invariant manner. To enhance generalizability and reduce the parameter count, we introduce a block-wise re-parameterization technique for the attention weight matrices. This technique decouples the problem dimension from the model's parameter count, enabling the Transformer to effectively handle systems of varying sizes. Our experiments demonstrate the Transformer's competitive performance compared to established classical methods such as Conjugate Gradient, especially for systems with smaller sizes. We further explore the model's ability to extrapolate to larger systems, providing evidence for its potential as a versatile and efficient solver for linear equations.",0,"approach where the
Transformer encodes each equation as a separate
token, allowing the model to process the system
in a permutation-invariant manner. To enhance
generalizability and reduce the parameter count,
we introduce a block-wise re-parameterization
technique for the attention weight matrices. This
technique decouples the problem dimension from
the model’s parameter count, enabling the Trans-
former to effectively handle systems of varying
sizes. Our experiments demonstrate the Trans-
former’s competitive performance compared to
established classical methods such as Conjugate
Gradient, especially for systems with smaller
sizes. We further explore the model’s ability to
extrapolate to larger systems, providing evidence
for its potential as a versatile and efﬁcient solver
for linear equations.
1. Introduction
Solving linear systems of equations is a fundamental prob-
lem in numerous ﬁelds, including scientiﬁc computing, ma-
chine learning, and engineering. While traditional methods
like Gaussian elimination and iterative solvers like Conju-
gate Gradient (Hestenes et al., 1952) are widely used, ex-
ploring alternative approaches holds the potential for more
efﬁcient and versatile solutions.
Transformers, originally developed for natural language pro-
cessing tasks (Vaswani et al., 2017), have demonstrated a
remarkable ability to capture complex relationships within
sequential data. This ability extends beyond natural lan-
guage processing, as evidenced by their successful applica-
tion in solving a variety of problems, ranging from noisy
linear regression and classiﬁcation (Garg et al., 2022) to the
1Google Research. Correspondence to: Max Vladymyrov
<mxv@google.com >.
The ﬁrst AI for MATH Workshop at the 41st International Confer-
ence on Machine Learning , Vienna, Austria. Copyright 2024 by
the author(s).traveling salesmen problem (Yang et al., 2023) and other
domains (Mirchandani et al., 2023). Transformers have
shown promise in various scientiﬁc computing ta Conclusions
This paper explored the novel application of linear Trans-
formers for efﬁciently solving small systems of linear equa-
tions with symmetric and positive deﬁnite coefﬁcient ma-
trices. We demonstrated that by encoding each equation
as a distinct token and implementing a block-wise re-
parameterization technique, Transformers could achieve
accuracy comparable to 6-8 iterations of Conjugate Gradi-
ent, while being faster for small problem sizes. Our model
exhibited the ability to generalize beyond its training data,
effectively handling systems of varying sizes and unseen
condition numbers.
This research opens up exciting possibilities for utilizing
Transformers in numerical tasks. Further investigation into
architectural choices, training strategies, and the boundaries
of generalization could lead to the development of even
more efﬁcient and adaptable solvers. This work reinforces
the notion that Transformers, initially designed for natural
language processing, hold remarkable potential as powerful
tools across various scientiﬁc computing domains.
While our approach shows promise, it has several limitations.
Currently, we only handle positive deﬁnite symmetric matri-
ces, which restricts the applicability of our method. Future
work should explore extending this approach to general ma-
trices, including non-symmetric and indeﬁnite cases. Addi-
tionally, our method’s performance on very large systems or
highly ill-conditioned matrices needs further investigation.
4
Efﬁcient Linear System Solver with Transformers
Scaling the approach to handle sparse matrices efﬁciently is
another important direction for future research. Integrating
our Transformer-based solver with classical methods, po-
tentially as a preconditioner or in a hybrid algorithm, could
leverage the strengths of both approaches and is an exciting
area for further study.
References
Ahn, K., Cheng, X., Daneshmand, H., and Sra, S.
Transformers learn to implement preconditioned gra-
dient descent fo",False
0afed850bcc759f25fe19106cf23af0f53a7d57d,Large Language Models Can Self-Correct with Minimal Effort,"['Zhenyu Wu', 'Qingkai Zeng', 'Zhihan Zhang', 'Zhaoxuan Tan', 'Chao Shen', 'Meng Jiang']",https://openreview.net/pdf/0afed850bcc759f25fe19106cf23af0f53a7d57d.pdf,"Large Language Models Can Self-Correct with Minimal Effort Unleash inherent capabilities of LLMs to detect and rectify incorrect responses without external feedback. Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple yet effective verification method can unleash inherent capabilities of the LLMs. That is to mask a key condition in the question, add the current response to construct a verification question, and predict the condition to verify the response. The condition can be an entity in an open-domain question or a numeric value in a math question, which requires minimal effort (via prompting) to identify. We propose an iterative verify-then-correct framework to progressively identify and correct (probably) false responses, named ProCo. We conduct experiments on three reasoning tasks. On average, ProCo, with GPT-3.5-Turbo as the backend LLM, yields exact match on four open-domain question answering datasets, accuracy on three arithmetic reasoning datasets, and accuracy on a commonsense reasoning dataset, compared to Self-Correct.",0afed850bcc759f25fe19106cf23af0f53a7d57d.pdf,"approaches have
employed scalar reward functions as an alternative. For in-
stance, Rainer (Liu et al., 2022) used reinforcement learning
to generate contextual relevant knowledge in response to
queries. Self-Correction (Welleck et al., 2023) trained a cor-
rector to iteratively correct imperfect outputs. Other sources,
such as compilers (Chen et al., 2024) or search engines (Yu
et al., 2023b) can provide domain-specific feedback.
Recent research used LLMs to generate feedback. Self-
Correct (Kim et al., 2023) and Self-Refine (Madaan et al.,
2023) utilized LLMs to verify and refine their initial out-
puts. However, Huang et al. questioned the intrinsic self-
correcting capability of LLMs, indicating that without ex-
ternal feedback, LLMs struggle to correct their previous
responses. To unleash inherent capabilities of LLMs to
detect and rectify incorrect responses without external feed-
back, we introduce substitute verification . By providing
natural language feedback based on verification results, we
can steer LLMs away from incorrect answers, thus enhanc-
ing their performance in various reasoning tasks.
Verify Correctness of LLM Output Several studies
trained or fine-tuned language models to check the correct-
Numeric ValueKeith has 20 books. Jason has 21 books. How many 
books do they have together?Arithmetic Question
EntityWhen is the last time the minnesota  vikings  have 
been in the playoffs?Open -domain Question
ConceptWhat could happen to a paper  if you leave it outside 
even if it does not move?Commonsense QuestionFigure 2: Key conditions in complex reasoning tasks play
a crucial role in the problem-solving process. These condi-
tions can take various forms: a numeric value in arithmetic
questions, an entity in open-domain questions, or a concept
in commonsense questions.
ness of answers. Cobbe et al. fine-tuned GPT-3 as a verifier
to judge the correctness of solutions. Li et al. fine-tuned
DeBERTa-v3-large (He et al., 2021) to predict the proba-
bility","conclusions or judgements
(Huang & Chang, 2023). People have been exploiting and
improving the reasoning ability of large language mod-
els (LLMs). Wei et al. proposed chain-of-thought (CoT)
prompting and yielded promising results on several reason-
ing tasks, such as arithmetic reasoning (Kojima et al., 2022;
Zhou et al., 2023), commonsense reasoning (Wei et al.,
2022; Zhang et al., 2023; Wang et al., 2023b), and open-
domain question answering (Wang et al., 2023a), using only
1School of Cyber Science and Engineering, Xi’an Jiaotong Uni-
versity, China2Department of Computer Science and Engineering,
University of Notre Dame, IN, USA. Correspondence to: Chao
Shen<chaoshen@xjtu.edu.cn >.
The first AI for MATH Workshop at the 41stInternational Confer-
ence on Machine Learning , Vienna, Austria. Copyright 2024 by
the author(s).Method NQ CSQA AQuA
CoT 40.3 72 .9 51 .3
Self-Correct 40.1 65.9 48.7
PROCO(Ours) 48.0 75.5 65.2
Table 1: Performance comparison of different prompting
methods using GPT-3.5-Turbo as backend LLM.
a few or no reasoning exemplars. CoT guides LLMs to
generate intermediate reasoning steps instead of generating
the final answer directly, which helps the LLMs simulate
the human-like reasoning process.
Although CoT enables LLMs to handle some complex rea-
soning examples, it remains vulnerable to the negative im-
pact of individual errors in each step. Specifically, even a
minor error in one step can alter the trajectory of the en-
tire reasoning process, ultimately leading to an incorrect
conclusion. To address this issue, Dhuliawala et al.; Kim
et al. have explored the verification and correction on the
responses. For example, as shown in Figure 1 a, for a given
question and its initial LLM-generated answer, Self-Correct
(Kim et al., 2023) first instructs the LLM to criticize its
generated answer using the hint: “ Review previous answer
and find mistakes ”. Then, Self-Correct instructs the LLM to
refine initial answers based on the critique.
However, r","approaches have employed scalar reward functions as an alternative. For in- stance, Rainer (Liu et al., 2022) used reinforcement learning to generate contextual relevant knowledge in response to queries. Self-Correction (Welleck et al., 2023) trained a cor- rector to iteratively correct imperfect outputs. Other sources, such as compilers (Chen et al., 2024) or search engines (Yu et al., 2023b) can provide domain-specific feedback. Recent research used LLMs to generate feedback. Self- Correct (Kim et al., 2023) and Self-Refine (Madaan et al., 2023) utilized LLMs to verify and refine their initial out- puts. However, Huang et al. questioned the intrinsic self- correcting capability of LLMs, indicating that without ex- ternal feedback, LLMs struggle to correct their previous responses. To unleash inherent capabilities of LLMs to detect and rectify incorrect responses without external feed- back, we introduce substitute verification . By providing natural language feedback based on verification results, we can steer LLMs away from incorrect answers, thus enhanc- ing their performance in various reasoning tasks. Verify Correctness of LLM Output Several studies trained or fine-tuned language models to check the correct- Numeric ValueKeith has 20 books. Jason has 21 books. How many books do they have together?Arithmetic Question EntityWhen is the last time the minnesota vikings have been in the playoffs?Open -domain Question ConceptWhat could happen to a paper if you leave it outside even if it does not move?Commonsense QuestionFigure 2: Key conditions in complex reasoning tasks play a crucial role in the problem-solving process. These condi- tions can take various forms: a numeric value in arithmetic questions, an entity in open-domain questions, or a concept in commonsense questions. ness of answers. Cobbe et al. fine-tuned GPT-3 as a verifier to judge the correctness of solutions. Li et al. fine-tuned DeBERTa-v3-large (He et al., 2021) to predict the proba- bility","conclusions or judgements (Huang & Chang, 2023). People have been exploiting and improving the reasoning ability of large language mod- els (LLMs). Wei et al. proposed chain-of-thought (CoT) prompting and yielded promising results on several reason- ing tasks, such as arithmetic reasoning (Kojima et al., 2022; Zhou et al., 2023), commonsense reasoning (Wei et al., 2022; Zhang et al., 2023; Wang et al., 2023b), and open- domain question answering (Wang et al., 2023a), using only 1School of Cyber Science and Engineering, Xi an Jiaotong Uni- versity, China2Department of Computer Science and Engineering, University of Notre Dame, IN, USA. Correspondence to: Chao Shen<chaoshen@xjtu.edu.cn >. The first AI for MATH Workshop at the 41stInternational Confer- ence on Machine Learning , Vienna, Austria. Copyright 2024 by the author(s).Method NQ CSQA AQuA CoT 40.3 72 .9 51 .3 Self-Correct 40.1 65.9 48.7 PROCO(Ours) 48.0 75.5 65.2 Table 1: Performance comparison of different prompting methods using GPT-3.5-Turbo as backend LLM. a few or no reasoning exemplars. CoT guides LLMs to generate intermediate reasoning steps instead of generating the final answer directly, which helps the LLMs simulate the human-like reasoning process. Although CoT enables LLMs to handle some complex rea- soning examples, it remains vulnerable to the negative im- pact of individual errors in each step. Specifically, even a minor error in one step can alter the trajectory of the en- tire reasoning process, ultimately leading to an incorrect conclusion. To address this issue, Dhuliawala et al.; Kim et al. have explored the verification and correction on the responses. For example, as shown in Figure 1 a, for a given question and its initial LLM-generated answer, Self-Correct (Kim et al., 2023) first instructs the LLM to criticize its generated answer using the hint: Review previous answer and find mistakes . Then, Self-Correct instructs the LLM to refine initial answers based on the critique. However, r","Large Language Models Can Self-Correct with Minimal Effort Unleash inherent capabilities of LLMs to detect and rectify incorrect responses without external feedback. Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple yet effective verification method can unleash inherent capabilities of the LLMs. That is to mask a key condition in the question, add the current response to construct a verification question, and predict the condition to verify the response. The condition can be an entity in an open-domain question or a numeric value in a math question, which requires minimal effort (via prompting) to identify. We propose an iterative verify-then-correct framework to progressively identify and correct (probably) false responses, named ProCo. We conduct experiments on three reasoning tasks. On average, ProCo, with GPT-3.5-Turbo as the backend LLM, yields exact match on four open-domain question answering datasets, accuracy on three arithmetic reasoning datasets, and accuracy on a commonsense reasoning dataset, compared to Self-Correct. conclusions or judgements (Huang & Chang, 2023). People have been exploiting and improving the reasoning ability of large language mod- els (LLMs). Wei et al. proposed chain-of-thought (CoT) prompting and yielded promising results on several reason- ing tasks, such as arithmetic reasoning (Kojima et al., 2022; Zhou et al., 2023), commonsense reasoning (Wei et al., 2022; Zhang et al., 2023; Wang et al., 2023b), and open- domain question answering (Wang et al., 2023a), using only 1School of Cyber Science and Engineering, Xi an Jiaotong Uni- versity, China2Department of Computer Science and Engineering, University of Notre Dame, IN, USA. Correspondence to: Chao Shen<chaoshen@xjtu.edu.cn >. The first AI for MATH Workshop at the 41stInternational Confer- ence on Machine Learning , Vienna, Austria. Copyright 2024 by the author(s).Method NQ CSQA AQuA CoT 40.3 72 .9 51 .3 Self-Correct 40.1 65.9 48.7 PROCO(Ours) 48.0 75.5 65.2 Table 1: Performance comparison of different prompting methods using GPT-3.5-Turbo as backend LLM. a few or no reasoning exemplars. CoT guides LLMs to generate intermediate reasoning steps instead of generating the final answer directly, which helps the LLMs simulate the human-like reasoning process. Although CoT enables LLMs to handle some complex rea- soning examples, it remains vulnerable to the negative im- pact of individual errors in each step. Specifically, even a minor error in one step can alter the trajectory of the en- tire reasoning process, ultimately leading to an incorrect conclusion. To address this issue, Dhuliawala et al.; Kim et al. have explored the verification and correction on the responses. For example, as shown in Figure 1 a, for a given question and its initial LLM-generated answer, Self-Correct (Kim et al., 2023) first instructs the LLM to criticize its generated answer using the hint: Review previous answer and find mistakes . Then, Self-Correct instructs the LLM to refine initial answers based on the critique. However, r approaches have employed scalar reward functions as an alternative. For in- stance, Rainer (Liu et al., 2022) used reinforcement learning to generate contextual relevant knowledge in response to queries. Self-Correction (Welleck et al., 2023) trained a cor- rector to iteratively correct imperfect outputs. Other sources, such as compilers (Chen et al., 2024) or search engines (Yu et al., 2023b) can provide domain-specific feedback. Recent research used LLMs to generate feedback. Self- Correct (Kim et al., 2023) and Self-Refine (Madaan et al., 2023) utilized LLMs to verify and refine their initial out- puts. However, Huang et al. questioned the intrinsic self- correcting capability of LLMs, indicating that without ex- ternal feedback, LLMs struggle to correct their previous responses. To unleash inherent capabilities of LLMs to detect and rectify incorrect responses without external feed- back, we introduce substitute verification . By providing natural language feedback based on verification results, we can steer LLMs away from incorrect answers, thus enhanc- ing their performance in various reasoning tasks. Verify Correctness of LLM Output Several studies trained or fine-tuned language models to check the correct- Numeric ValueKeith has 20 books. Jason has 21 books. How many books do they have together?Arithmetic Question EntityWhen is the last time the minnesota vikings have been in the playoffs?Open -domain Question ConceptWhat could happen to a paper if you leave it outside even if it does not move?Commonsense QuestionFigure 2: Key conditions in complex reasoning tasks play a crucial role in the problem-solving process. These condi- tions can take various forms: a numeric value in arithmetic questions, an entity in open-domain questions, or a concept in commonsense questions. ness of answers. Cobbe et al. fine-tuned GPT-3 as a verifier to judge the correctness of solutions. Li et al. fine-tuned DeBERTa-v3-large (He et al., 2021) to predict the proba- bility","Large Language Models Can Self-Correct with Minimal Effort Unleash inherent capabilities of LLMs to detect and rectify incorrect responses without external feedback. Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple yet effective verification method can unleash inherent capabilities of the LLMs. That is to mask a key condition in the question, add the current response to construct a verification question, and predict the condition to verify the response. The condition can be an entity in an open-domain question or a numeric value in a math question, which requires minimal effort (via prompting) to identify. We propose an iterative verify-then-correct framework to progressively identify and correct (probably) false responses, named ProCo. We conduct experiments on three reasoning tasks. On average, ProCo, with GPT-3.5-Turbo as the backend LLM, yields exact match on four open-domain question answering datasets, accuracy on three arithmetic reasoning datasets, and accuracy on a commonsense reasoning dataset, compared to Self-Correct.",0,"approaches have
employed scalar reward functions as an alternative. For in-
stance, Rainer (Liu et al., 2022) used reinforcement learning
to generate contextual relevant knowledge in response to
queries. Self-Correction (Welleck et al., 2023) trained a cor-
rector to iteratively correct imperfect outputs. Other sources,
such as compilers (Chen et al., 2024) or search engines (Yu
et al., 2023b) can provide domain-specific feedback.
Recent research used LLMs to generate feedback. Self-
Correct (Kim et al., 2023) and Self-Refine (Madaan et al.,
2023) utilized LLMs to verify and refine their initial out-
puts. However, Huang et al. questioned the intrinsic self-
correcting capability of LLMs, indicating that without ex-
ternal feedback, LLMs struggle to correct their previous
responses. To unleash inherent capabilities of LLMs to
detect and rectify incorrect responses without external feed-
back, we introduce substitute verification . By providing
natural language feedback based on verification results, we
can steer LLMs away from incorrect answers, thus enhanc-
ing their performance in various reasoning tasks.
Verify Correctness of LLM Output Several studies
trained or fine-tuned language models to check the correct-
Numeric ValueKeith has 20 books. Jason has 21 books. How many 
books do they have together?Arithmetic Question
EntityWhen is the last time the minnesota  vikings  have 
been in the playoffs?Open -domain Question
ConceptWhat could happen to a paper  if you leave it outside 
even if it does not move?Commonsense QuestionFigure 2: Key conditions in complex reasoning tasks play
a crucial role in the problem-solving process. These condi-
tions can take various forms: a numeric value in arithmetic
questions, an entity in open-domain questions, or a concept
in commonsense questions.
ness of answers. Cobbe et al. fine-tuned GPT-3 as a verifier
to judge the correctness of solutions. Li et al. fine-tuned
DeBERTa-v3-large (He et al., 2021) to predict the proba-
bility conclusions or judgements
(Huang & Chang, 2023). People have been exploiting and
improving the reasoning ability of large language mod-
els (LLMs). Wei et al. proposed chain-of-thought (CoT)
prompting and yielded promising results on several reason-
ing tasks, such as arithmetic reasoning (Kojima et al., 2022;
Zhou et al., 2023), commonsense reasoning (Wei et al.,
2022; Zhang et al., 2023; Wang et al., 2023b), and open-
domain question answering (Wang et al., 2023a), using only
1School of Cyber Science and Engineering, Xi’an Jiaotong Uni-
versity, China2Department of Computer Science and Engineering,
University of Notre Dame, IN, USA. Correspondence to: Chao
Shen<chaoshen@xjtu.edu.cn >.
The first AI for MATH Workshop at the 41stInternational Confer-
ence on Machine Learning , Vienna, Austria. Copyright 2024 by
the author(s).Method NQ CSQA AQuA
CoT 40.3 72 .9 51 .3
Self-Correct 40.1 65.9 48.7
PROCO(Ours) 48.0 75.5 65.2
Table 1: Performance comparison of different prompting
methods using GPT-3.5-Turbo as backend LLM.
a few or no reasoning exemplars. CoT guides LLMs to
generate intermediate reasoning steps instead of generating
the final answer directly, which helps the LLMs simulate
the human-like reasoning process.
Although CoT enables LLMs to handle some complex rea-
soning examples, it remains vulnerable to the negative im-
pact of individual errors in each step. Specifically, even a
minor error in one step can alter the trajectory of the en-
tire reasoning process, ultimately leading to an incorrect
conclusion. To address this issue, Dhuliawala et al.; Kim
et al. have explored the verification and correction on the
responses. For example, as shown in Figure 1 a, for a given
question and its initial LLM-generated answer, Self-Correct
(Kim et al., 2023) first instructs the LLM to criticize its
generated answer using the hint: “ Review previous answer
and find mistakes ”. Then, Self-Correct instructs the LLM to
refine initial answers based on the critique.
However, r",False
8b3bf32d71f7ec91ebdcfd798094623345755e15,Teaching Large Language Models to Reason with Reinforcement Learning,"['Alexander Havrilla', 'Yuqing Du', 'Sharath Chandra Raparthy', 'Christoforos Nalmpantis', 'Jane Dwivedi-Yu', 'Eric Hambro', 'Sainbayar Sukhbaatar', 'Roberta Raileanu']",https://openreview.net/pdf/8b3bf32d71f7ec91ebdcfd798094623345755e15.pdf,"Teaching Large Language Models to Reason with Reinforcement Learning We compare the performance of multiple RL algorithms across multiple setups for improving LLM reasoning Reinforcement Learning from Human Feedback (\textbf{RLHF}) has emerged as a dominant approach for aligning LLM outputs with human preferences. Inspired by the success of RLHF, we study the performance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization (\textbf{PPO}), Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model. We additionally start from multiple initializations with and without supervised fine-tuning (\textbf{SFT}) data. Overall, we find models fine-tuned with Expert Iteration to consistently achieve the highest task accuracy with PPO and RCRL close behind. Surprisingly, the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of samples to converge from a pretrained checkpoint. We investigate why this is the case, concluding that during RL training models fail to explore significantly beyond solutions already produced by SFT models. Additionally, we discuss a trade off between maj@1 and pass@96 metric performance during SFT training and how conversely RL training improves both simultaneously. We then conclude by discussing the implications of our findings for RLHF and the future role of RL in LLM fine-tuning.",8b3bf32d71f7ec91ebdcfd798094623345755e15.pdf,"approach for
aligning LLM outputs with human preferences.
Inspired by the success of RLHF, we study the per-
formance of multiple algorithms that learn from
feedback (Expert Iteration, Proximal Policy Op-
timization ( PPO ), Return-Conditioned RL) on
improving LLM reasoning capabilities. We in-
vestigate both sparse and dense rewards provided
to the LLM both heuristically and via a learned
reward model. We additionally start from mul-
tiple initializations with and without supervised
fine-tuning ( SFT) data. Overall, we find models
fine-tuned with Expert Iteration to consistently
achieve the highest task accuracy with PPO and
RCRL close behind. Surprisingly, the sample
complexity of Expert Iteration is similar to that
of PPO, requiring at most on the order of 106
samples to converge from a pretrained checkpoint.
We investigate why this is the case, concluding
that during RL training models fail to explore sig-
nificantly beyond solutions already produced by
SFT models. Additionally, we discuss a trade
off between maj@1 and pass@96 metric perfor-
mance during SFT training and how conversely
RL training improves both simultaneously. We
then conclude by discussing the implications of
our findings for RLHF and the future role of RL
in LLM fine-tuning.
1. Introduction
The reasoning abilities of large language models ( LLMs )
are rapidly improving as measured by their performance on
numerous math, science and code benchmarks (Cobbe et al.,
2021; Hendrycks et al., 2021b; Sawada et al., 2023; Liang
*Equal contribution1Meta2Georgia Tech3StabilityAI
4Anthropic. Correspondence to: Alex Havrilla
<ahavrilla3@gatech.edu >.
The first AI for MATH Workshop at the 41st International Confer-
ence on Machine Learning, Vienna, Austria. Copyright 2024 by
the author(s)et al., 2022; Srivastava et al., 2022; Rein et al., 2023; Mialon
et al., 2023; Chollet, 2019; Mishra et al., 2022; Hendrycks
et al., 2021a; Austin et al., 2021; Patel et al., 2021; Gao et al.,
2021). Simultaneously, Reinforc","future work.
Figure 16. Scores of synthetically backwards generated (Q, A )
pairs. Note: the score refers to the percentage of times the forward
student model MQ→Arecovers the intended final answer.
Question ”A school of 100 musicians goes on a skiing
trip. 40% are beginners, 30% are interme-
diate, and 50% are advanced. How many
people went on the skiing trip?”
Answer ”There are 100 * 0.4 = 40 beginner skiiers.
There are 100 * 0.3 = 30 intermediate ski-
iers. There are 100 * 0.5 = 50 advanced
skiiers. Therefore there are 40 + 30 + 50 =
120 skiiers total.”
F. RCRL Step-label Generating Process
Another natural candidate which could be used to iden-
tify mistakes at each step is a Process Based Reward
Model (PRM) (Lightman et al., 2023). A PRM es-
timates the probability of correctness of a step Si,
p(Sicorrect |Q, S 1, S2, ..., S i)independently of its im-
pact on the final answer. However, this would be expen-
sive, requiring collecting human annotated samples. Instead,
we propose to approximate the optimal value function V∗
of the reasoning task. V∗corresponds to the value func-
tion of the optimal policy which is able to successfully
solve the reasoning task from any logically valid interme-
diate state Sj. Such an optimal value function would have
V∗(Q, S 1, ..., S i) = 1 for a solution prefix with no mis-
takes, and V∗(Q, S 1, ..., S i) = 0 if the prefix already con-
tains a mistake which will result in an incorrect final an-
swer. Note however, V∗does not exactly correspond to
a PRM. This is because a partial solution S1, ..., S iwith
a mistake at step j̸=iand valid terminal step Siwill
haveV∗(Q, S 1, ..., S i) = 0 andPRM (Q, S 1, ..., S i) = 1 .
To make this distinction clear, we call models we train to
18
RL for LLM Reasoning
Figure 17. Comparison of reward curves for PPO architecture abla-
tions. Using both gradient stopping and a larger value head works
best.
directly approximate V∗stepwise ORMs or SORMs .
G. PPO Architecture Ablations
H. CommonsenseQA Benc","approach for aligning LLM outputs with human preferences. Inspired by the success of RLHF, we study the per- formance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Op- timization ( PPO ), Return-Conditioned RL) on improving LLM reasoning capabilities. We in- vestigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model. We additionally start from mul- tiple initializations with and without supervised fine-tuning ( SFT) data. Overall, we find models fine-tuned with Expert Iteration to consistently achieve the highest task accuracy with PPO and RCRL close behind. Surprisingly, the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of 106 samples to converge from a pretrained checkpoint. We investigate why this is the case, concluding that during RL training models fail to explore sig- nificantly beyond solutions already produced by SFT models. Additionally, we discuss a trade off between maj@1 and pass@96 metric perfor- mance during SFT training and how conversely RL training improves both simultaneously. We then conclude by discussing the implications of our findings for RLHF and the future role of RL in LLM fine-tuning. 1. Introduction The reasoning abilities of large language models ( LLMs ) are rapidly improving as measured by their performance on numerous math, science and code benchmarks (Cobbe et al., 2021; Hendrycks et al., 2021b; Sawada et al., 2023; Liang *Equal contribution1Meta2Georgia Tech3StabilityAI 4Anthropic. Correspondence to: Alex Havrilla <ahavrilla3@gatech.edu >. The first AI for MATH Workshop at the 41st International Confer- ence on Machine Learning, Vienna, Austria. Copyright 2024 by the author(s)et al., 2022; Srivastava et al., 2022; Rein et al., 2023; Mialon et al., 2023; Chollet, 2019; Mishra et al., 2022; Hendrycks et al., 2021a; Austin et al., 2021; Patel et al., 2021; Gao et al., 2021). Simultaneously, Reinforc","future work. Figure 16. Scores of synthetically backwards generated (Q, A ) pairs. Note: the score refers to the percentage of times the forward student model MQ Arecovers the intended final answer. Question A school of 100 musicians goes on a skiing trip. 40% are beginners, 30% are interme- diate, and 50% are advanced. How many people went on the skiing trip? Answer There are 100 * 0.4 = 40 beginner skiiers. There are 100 * 0.3 = 30 intermediate ski- iers. There are 100 * 0.5 = 50 advanced skiiers. Therefore there are 40 + 30 + 50 = 120 skiiers total. F. RCRL Step-label Generating Process Another natural candidate which could be used to iden- tify mistakes at each step is a Process Based Reward Model (PRM) (Lightman et al., 2023). A PRM es- timates the probability of correctness of a step Si, p(Sicorrect |Q, S 1, S2, ..., S i)independently of its im- pact on the final answer. However, this would be expen- sive, requiring collecting human annotated samples. Instead, we propose to approximate the optimal value function V of the reasoning task. V corresponds to the value func- tion of the optimal policy which is able to successfully solve the reasoning task from any logically valid interme- diate state Sj. Such an optimal value function would have V (Q, S 1, ..., S i) = 1 for a solution prefix with no mis- takes, and V (Q, S 1, ..., S i) = 0 if the prefix already con- tains a mistake which will result in an incorrect final an- swer. Note however, V does not exactly correspond to a PRM. This is because a partial solution S1, ..., S iwith a mistake at step j =iand valid terminal step Siwill haveV (Q, S 1, ..., S i) = 0 andPRM (Q, S 1, ..., S i) = 1 . To make this distinction clear, we call models we train to 18 RL for LLM Reasoning Figure 17. Comparison of reward curves for PPO architecture abla- tions. Using both gradient stopping and a larger value head works best. directly approximate V stepwise ORMs or SORMs . G. PPO Architecture Ablations H. CommonsenseQA Benc","Teaching Large Language Models to Reason with Reinforcement Learning We compare the performance of multiple RL algorithms across multiple setups for improving LLM reasoning Reinforcement Learning from Human Feedback (\textbf{RLHF}) has emerged as a dominant approach for aligning LLM outputs with human preferences. Inspired by the success of RLHF, we study the performance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization (\textbf{PPO}), Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model. We additionally start from multiple initializations with and without supervised fine-tuning (\textbf{SFT}) data. Overall, we find models fine-tuned with Expert Iteration to consistently achieve the highest task accuracy with PPO and RCRL close behind. Surprisingly, the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of samples to converge from a pretrained checkpoint. We investigate why this is the case, concluding that during RL training models fail to explore significantly beyond solutions already produced by SFT models. Additionally, we discuss a trade off between maj@1 and pass@96 metric performance during SFT training and how conversely RL training improves both simultaneously. We then conclude by discussing the implications of our findings for RLHF and the future role of RL in LLM fine-tuning. future work. Figure 16. Scores of synthetically backwards generated (Q, A ) pairs. Note: the score refers to the percentage of times the forward student model MQ Arecovers the intended final answer. Question A school of 100 musicians goes on a skiing trip. 40% are beginners, 30% are interme- diate, and 50% are advanced. How many people went on the skiing trip? Answer There are 100 * 0.4 = 40 beginner skiiers. There are 100 * 0.3 = 30 intermediate ski- iers. There are 100 * 0.5 = 50 advanced skiiers. Therefore there are 40 + 30 + 50 = 120 skiiers total. F. RCRL Step-label Generating Process Another natural candidate which could be used to iden- tify mistakes at each step is a Process Based Reward Model (PRM) (Lightman et al., 2023). A PRM es- timates the probability of correctness of a step Si, p(Sicorrect |Q, S 1, S2, ..., S i)independently of its im- pact on the final answer. However, this would be expen- sive, requiring collecting human annotated samples. Instead, we propose to approximate the optimal value function V of the reasoning task. V corresponds to the value func- tion of the optimal policy which is able to successfully solve the reasoning task from any logically valid interme- diate state Sj. Such an optimal value function would have V (Q, S 1, ..., S i) = 1 for a solution prefix with no mis- takes, and V (Q, S 1, ..., S i) = 0 if the prefix already con- tains a mistake which will result in an incorrect final an- swer. Note however, V does not exactly correspond to a PRM. This is because a partial solution S1, ..., S iwith a mistake at step j =iand valid terminal step Siwill haveV (Q, S 1, ..., S i) = 0 andPRM (Q, S 1, ..., S i) = 1 . To make this distinction clear, we call models we train to 18 RL for LLM Reasoning Figure 17. Comparison of reward curves for PPO architecture abla- tions. Using both gradient stopping and a larger value head works best. directly approximate V stepwise ORMs or SORMs . G. PPO Architecture Ablations H. CommonsenseQA Benc approach for aligning LLM outputs with human preferences. Inspired by the success of RLHF, we study the per- formance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Op- timization ( PPO ), Return-Conditioned RL) on improving LLM reasoning capabilities. We in- vestigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model. We additionally start from mul- tiple initializations with and without supervised fine-tuning ( SFT) data. Overall, we find models fine-tuned with Expert Iteration to consistently achieve the highest task accuracy with PPO and RCRL close behind. Surprisingly, the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of 106 samples to converge from a pretrained checkpoint. We investigate why this is the case, concluding that during RL training models fail to explore sig- nificantly beyond solutions already produced by SFT models. Additionally, we discuss a trade off between maj@1 and pass@96 metric perfor- mance during SFT training and how conversely RL training improves both simultaneously. We then conclude by discussing the implications of our findings for RLHF and the future role of RL in LLM fine-tuning. 1. Introduction The reasoning abilities of large language models ( LLMs ) are rapidly improving as measured by their performance on numerous math, science and code benchmarks (Cobbe et al., 2021; Hendrycks et al., 2021b; Sawada et al., 2023; Liang *Equal contribution1Meta2Georgia Tech3StabilityAI 4Anthropic. Correspondence to: Alex Havrilla <ahavrilla3@gatech.edu >. The first AI for MATH Workshop at the 41st International Confer- ence on Machine Learning, Vienna, Austria. Copyright 2024 by the author(s)et al., 2022; Srivastava et al., 2022; Rein et al., 2023; Mialon et al., 2023; Chollet, 2019; Mishra et al., 2022; Hendrycks et al., 2021a; Austin et al., 2021; Patel et al., 2021; Gao et al., 2021). Simultaneously, Reinforc","Teaching Large Language Models to Reason with Reinforcement Learning We compare the performance of multiple RL algorithms across multiple setups for improving LLM reasoning Reinforcement Learning from Human Feedback (\textbf{RLHF}) has emerged as a dominant approach for aligning LLM outputs with human preferences. Inspired by the success of RLHF, we study the performance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization (\textbf{PPO}), Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model. We additionally start from multiple initializations with and without supervised fine-tuning (\textbf{SFT}) data. Overall, we find models fine-tuned with Expert Iteration to consistently achieve the highest task accuracy with PPO and RCRL close behind. Surprisingly, the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of samples to converge from a pretrained checkpoint. We investigate why this is the case, concluding that during RL training models fail to explore significantly beyond solutions already produced by SFT models. Additionally, we discuss a trade off between maj@1 and pass@96 metric performance during SFT training and how conversely RL training improves both simultaneously. We then conclude by discussing the implications of our findings for RLHF and the future role of RL in LLM fine-tuning.",0,"approach for
aligning LLM outputs with human preferences.
Inspired by the success of RLHF, we study the per-
formance of multiple algorithms that learn from
feedback (Expert Iteration, Proximal Policy Op-
timization ( PPO ), Return-Conditioned RL) on
improving LLM reasoning capabilities. We in-
vestigate both sparse and dense rewards provided
to the LLM both heuristically and via a learned
reward model. We additionally start from mul-
tiple initializations with and without supervised
fine-tuning ( SFT) data. Overall, we find models
fine-tuned with Expert Iteration to consistently
achieve the highest task accuracy with PPO and
RCRL close behind. Surprisingly, the sample
complexity of Expert Iteration is similar to that
of PPO, requiring at most on the order of 106
samples to converge from a pretrained checkpoint.
We investigate why this is the case, concluding
that during RL training models fail to explore sig-
nificantly beyond solutions already produced by
SFT models. Additionally, we discuss a trade
off between maj@1 and pass@96 metric perfor-
mance during SFT training and how conversely
RL training improves both simultaneously. We
then conclude by discussing the implications of
our findings for RLHF and the future role of RL
in LLM fine-tuning.
1. Introduction
The reasoning abilities of large language models ( LLMs )
are rapidly improving as measured by their performance on
numerous math, science and code benchmarks (Cobbe et al.,
2021; Hendrycks et al., 2021b; Sawada et al., 2023; Liang
*Equal contribution1Meta2Georgia Tech3StabilityAI
4Anthropic. Correspondence to: Alex Havrilla
<ahavrilla3@gatech.edu >.
The first AI for MATH Workshop at the 41st International Confer-
ence on Machine Learning, Vienna, Austria. Copyright 2024 by
the author(s)et al., 2022; Srivastava et al., 2022; Rein et al., 2023; Mialon
et al., 2023; Chollet, 2019; Mishra et al., 2022; Hendrycks
et al., 2021a; Austin et al., 2021; Patel et al., 2021; Gao et al.,
2021). Simultaneously, Reinforc future work.
Figure 16. Scores of synthetically backwards generated (Q, A )
pairs. Note: the score refers to the percentage of times the forward
student model MQ→Arecovers the intended final answer.
Question ”A school of 100 musicians goes on a skiing
trip. 40% are beginners, 30% are interme-
diate, and 50% are advanced. How many
people went on the skiing trip?”
Answer ”There are 100 * 0.4 = 40 beginner skiiers.
There are 100 * 0.3 = 30 intermediate ski-
iers. There are 100 * 0.5 = 50 advanced
skiiers. Therefore there are 40 + 30 + 50 =
120 skiiers total.”
F. RCRL Step-label Generating Process
Another natural candidate which could be used to iden-
tify mistakes at each step is a Process Based Reward
Model (PRM) (Lightman et al., 2023). A PRM es-
timates the probability of correctness of a step Si,
p(Sicorrect |Q, S 1, S2, ..., S i)independently of its im-
pact on the final answer. However, this would be expen-
sive, requiring collecting human annotated samples. Instead,
we propose to approximate the optimal value function V∗
of the reasoning task. V∗corresponds to the value func-
tion of the optimal policy which is able to successfully
solve the reasoning task from any logically valid interme-
diate state Sj. Such an optimal value function would have
V∗(Q, S 1, ..., S i) = 1 for a solution prefix with no mis-
takes, and V∗(Q, S 1, ..., S i) = 0 if the prefix already con-
tains a mistake which will result in an incorrect final an-
swer. Note however, V∗does not exactly correspond to
a PRM. This is because a partial solution S1, ..., S iwith
a mistake at step j̸=iand valid terminal step Siwill
haveV∗(Q, S 1, ..., S i) = 0 andPRM (Q, S 1, ..., S i) = 1 .
To make this distinction clear, we call models we train to
18
RL for LLM Reasoning
Figure 17. Comparison of reward curves for PPO architecture abla-
tions. Using both gradient stopping and a larger value head works
best.
directly approximate V∗stepwise ORMs or SORMs .
G. PPO Architecture Ablations
H. CommonsenseQA Benc",False
1843f03c96256409a143dfdb23c8318c0813895d,AI for an inverse problem: Physical model solving quantum gravity,"['Koji Hashimoto', 'Koshiro Matsuo', 'Masaki Murata', 'Gakuto Ogiwara', 'Daichi Takeda']",https://openreview.net/pdf/1843f03c96256409a143dfdb23c8318c0813895d.pdf,"AI for an inverse problem: Physical model solving quantum gravity Mathematical inverse problems of determining a governing differential equation for given solution data remain a fundamental challenge. To find a working example of AI for math, we provide a concrete example using a physical setup of a quantum gravity problem. We present a novel sparse Neural Network (NN) model which is interpretable, to solve the inverse problem: the AdS/CFT correspondence. According to the conjectured correspondence, a special condensed matter system on a ring is equivalent to a gravity system on a bulk disk. The inverse problem is to reconstruct the higher-dimensional gravity metric from the data of the condensed matter system. We use the response functions of a condensed matter system as our data, and by supervised machine learning, we successfully train the neural network which is equivalent to a scalar field equation on an emergent geometry of the bulk spacetime. The developed method may work as a ground for generic bulk reconstruction, i.e. a solution to the inverse problem of the AdS/CFT correspondence. From a technical perspective, to achieve better numerical control, our neural network model incorporates a novel layer that implements the Runge-Kutta method.",1843f03c96256409a143dfdb23c8318c0813895d.pdf,"approach this question, we set up a con-
crete physical model concerning the long-standing problem
of quantum gravity. The most promising formulation of
quantum gravity is to use the AdS/CFT correspondence
(Maldacena, 1999), which is a conjecture that two physical
quantum theories, conformal field theory (CFT) and gravity
theory, are equivalent to each other. Unfortunately, there
has been no proof of the conjecture, but there are a lot of
working examples. The principal concern against this con-
jecture is that for a given CFT data, there is no constructive
way to find a background geometry in the dual equivalent
gravity theory. This is precisely the inverse problem which
we stated above: CFT provides a boundary data, and the
issue is to find a differential equation in a curved geometry
(which is unknown) whose solution is consistent with the
boundary data.
A possible solution to this inverse problem (widely known
as “bulk reconstruction program” in the field of theoretical
particle physics) can be a ground-breaking path to a proof
of the conjecture, and furthermore, a working example for
AI for math of inverse problems.
The best and the simplest setup for solving this inverse prob-
lem of the AdS/CFT correspondence is with a low-enough
spacetime dimensions and the simplest matter content in
the bulk spacetime, thus we follow the situation provided in
1
AI for an inverse problem: Physical model solving quantum gravity
(Hashimoto et al., 2023) where the CFT (material quantum
theory) lives on a one-dimensional circle while the corre-
sponding gravity background is a disk which is rotationally
symmetric. We follow the method developed in (Hashimoto
et al., 2018a) which replaces the bulk differential equation
with a sparse neural network securing the interpretability
by regarding the weights as a metric on the curved space-
time.1With this setup, if we find what kind of differential
equations are emergent and what are not, it would be a great
step for proving the AdS/CF","discussions. The work of K. H., K. M., M. M. and
G. O. was supported in part by JSPS KAKENHI Grant
Nos. JP22H01217, JP22H05111, and JP22H05115. The
work of D. T. was supported in part by Grant-in-Aid for
JSPS Fellows No. 22KJ1944.
References
Akutagawa, T., Hashimoto, K., and Sumimoto, T. Deep
learning and ads/qcd. Physical Review D , 102(2):026020,
2020.
Cai, T., Merz, G. W., Charton, F., Nolte, N., Wilhelm,
M., Cranmer, K., and Dixon, L. J. Transforming the
bootstrap: Using transformers to compute scattering am-
plitudes in planar n= 4 super yang-mills theory. arXiv
preprint arXiv:2405.06107 , 2024.
Chakraborty, S. Boundary Terms of the Einstein–Hilbert
Action , pp. 43–59. Springer International Publishing,
Cham, 2017.
Charton, F. Linear algebra with transformers. arXiv preprint
arXiv:2112.01898 , 2021.
Chen, R. T., Rubanova, Y ., Bettencourt, J., and Duvenaud,
D. K. Neural ordinary differential equations. Advances
in neural information processing systems , 31, 2018.
Cybenko, G. Approximation by superpositions of a sig-
moidal function. Mathematics of control, signals and
systems , 2(4):303–314, 1989.
Hashimoto, K. Ads/cft correspondence as a deep boltzmann
machine. Physical Review D , 99(10):106017, 2019.
Hashimoto, K., Sugishita, S., Tanaka, A., and Tomiya, A.
Deep learning and the ads/cft correspondence. Physical
Review D , 98(4):046019, 2018a.
Hashimoto, K., Sugishita, S., Tanaka, A., and Tomiya, A.
Deep learning and holographic qcd. Physical Review D ,
98(10):106014, 2018b.
Hashimoto, K., Hu, H.-Y ., and You, Y .-Z. Neural ordinary
differential equation and holographic quantum chromo-
dynamics. Machine Learning: Science and Technology ,
2(3):035011, 2021.
9
AI for an inverse problem: Physical model solving quantum gravity
Hashimoto, K., Takeda, D., Tanaka, K., and Yonezawa,
S. Spacetime-emergent ring toward tabletop quantum
gravity experiments. Physical Review Research , 5(2):
023168, 2023.
Hashimoto, K., Hirono, Y ., and Sannai, A. Unification of
symmetries i","approach this question, we set up a con- crete physical model concerning the long-standing problem of quantum gravity. The most promising formulation of quantum gravity is to use the AdS/CFT correspondence (Maldacena, 1999), which is a conjecture that two physical quantum theories, conformal field theory (CFT) and gravity theory, are equivalent to each other. Unfortunately, there has been no proof of the conjecture, but there are a lot of working examples. The principal concern against this con- jecture is that for a given CFT data, there is no constructive way to find a background geometry in the dual equivalent gravity theory. This is precisely the inverse problem which we stated above: CFT provides a boundary data, and the issue is to find a differential equation in a curved geometry (which is unknown) whose solution is consistent with the boundary data. A possible solution to this inverse problem (widely known as bulk reconstruction program in the field of theoretical particle physics) can be a ground-breaking path to a proof of the conjecture, and furthermore, a working example for AI for math of inverse problems. The best and the simplest setup for solving this inverse prob- lem of the AdS/CFT correspondence is with a low-enough spacetime dimensions and the simplest matter content in the bulk spacetime, thus we follow the situation provided in 1 AI for an inverse problem: Physical model solving quantum gravity (Hashimoto et al., 2023) where the CFT (material quantum theory) lives on a one-dimensional circle while the corre- sponding gravity background is a disk which is rotationally symmetric. We follow the method developed in (Hashimoto et al., 2018a) which replaces the bulk differential equation with a sparse neural network securing the interpretability by regarding the weights as a metric on the curved space- time.1With this setup, if we find what kind of differential equations are emergent and what are not, it would be a great step for proving the AdS/CF","discussions. The work of K. H., K. M., M. M. and G. O. was supported in part by JSPS KAKENHI Grant Nos. JP22H01217, JP22H05111, and JP22H05115. The work of D. T. was supported in part by Grant-in-Aid for JSPS Fellows No. 22KJ1944.","AI for an inverse problem: Physical model solving quantum gravity Mathematical inverse problems of determining a governing differential equation for given solution data remain a fundamental challenge. To find a working example of AI for math, we provide a concrete example using a physical setup of a quantum gravity problem. We present a novel sparse Neural Network (NN) model which is interpretable, to solve the inverse problem: the AdS/CFT correspondence. According to the conjectured correspondence, a special condensed matter system on a ring is equivalent to a gravity system on a bulk disk. The inverse problem is to reconstruct the higher-dimensional gravity metric from the data of the condensed matter system. We use the response functions of a condensed matter system as our data, and by supervised machine learning, we successfully train the neural network which is equivalent to a scalar field equation on an emergent geometry of the bulk spacetime. The developed method may work as a ground for generic bulk reconstruction, i.e. a solution to the inverse problem of the AdS/CFT correspondence. From a technical perspective, to achieve better numerical control, our neural network model incorporates a novel layer that implements the Runge-Kutta method. discussions. The work of K. H., K. M., M. M. and G. O. was supported in part by JSPS KAKENHI Grant Nos. JP22H01217, JP22H05111, and JP22H05115. The work of D. T. was supported in part by Grant-in-Aid for JSPS Fellows No. 22KJ1944. approach this question, we set up a con- crete physical model concerning the long-standing problem of quantum gravity. The most promising formulation of quantum gravity is to use the AdS/CFT correspondence (Maldacena, 1999), which is a conjecture that two physical quantum theories, conformal field theory (CFT) and gravity theory, are equivalent to each other. Unfortunately, there has been no proof of the conjecture, but there are a lot of working examples. The principal concern against this con- jecture is that for a given CFT data, there is no constructive way to find a background geometry in the dual equivalent gravity theory. This is precisely the inverse problem which we stated above: CFT provides a boundary data, and the issue is to find a differential equation in a curved geometry (which is unknown) whose solution is consistent with the boundary data. A possible solution to this inverse problem (widely known as bulk reconstruction program in the field of theoretical particle physics) can be a ground-breaking path to a proof of the conjecture, and furthermore, a working example for AI for math of inverse problems. The best and the simplest setup for solving this inverse prob- lem of the AdS/CFT correspondence is with a low-enough spacetime dimensions and the simplest matter content in the bulk spacetime, thus we follow the situation provided in 1 AI for an inverse problem: Physical model solving quantum gravity (Hashimoto et al., 2023) where the CFT (material quantum theory) lives on a one-dimensional circle while the corre- sponding gravity background is a disk which is rotationally symmetric. We follow the method developed in (Hashimoto et al., 2018a) which replaces the bulk differential equation with a sparse neural network securing the interpretability by regarding the weights as a metric on the curved space- time.1With this setup, if we find what kind of differential equations are emergent and what are not, it would be a great step for proving the AdS/CF","AI for an inverse problem: Physical model solving quantum gravity Mathematical inverse problems of determining a governing differential equation for given solution data remain a fundamental challenge. To find a working example of AI for math, we provide a concrete example using a physical setup of a quantum gravity problem. We present a novel sparse Neural Network (NN) model which is interpretable, to solve the inverse problem: the AdS/CFT correspondence. According to the conjectured correspondence, a special condensed matter system on a ring is equivalent to a gravity system on a bulk disk. The inverse problem is to reconstruct the higher-dimensional gravity metric from the data of the condensed matter system. We use the response functions of a condensed matter system as our data, and by supervised machine learning, we successfully train the neural network which is equivalent to a scalar field equation on an emergent geometry of the bulk spacetime. The developed method may work as a ground for generic bulk reconstruction, i.e. a solution to the inverse problem of the AdS/CFT correspondence. From a technical perspective, to achieve better numerical control, our neural network model incorporates a novel layer that implements the Runge-Kutta method.",1,"approach this question, we set up a con-
crete physical model concerning the long-standing problem
of quantum gravity. The most promising formulation of
quantum gravity is to use the AdS/CFT correspondence
(Maldacena, 1999), which is a conjecture that two physical
quantum theories, conformal field theory (CFT) and gravity
theory, are equivalent to each other. Unfortunately, there
has been no proof of the conjecture, but there are a lot of
working examples. The principal concern against this con-
jecture is that for a given CFT data, there is no constructive
way to find a background geometry in the dual equivalent
gravity theory. This is precisely the inverse problem which
we stated above: CFT provides a boundary data, and the
issue is to find a differential equation in a curved geometry
(which is unknown) whose solution is consistent with the
boundary data.
A possible solution to this inverse problem (widely known
as “bulk reconstruction program” in the field of theoretical
particle physics) can be a ground-breaking path to a proof
of the conjecture, and furthermore, a working example for
AI for math of inverse problems.
The best and the simplest setup for solving this inverse prob-
lem of the AdS/CFT correspondence is with a low-enough
spacetime dimensions and the simplest matter content in
the bulk spacetime, thus we follow the situation provided in
1
AI for an inverse problem: Physical model solving quantum gravity
(Hashimoto et al., 2023) where the CFT (material quantum
theory) lives on a one-dimensional circle while the corre-
sponding gravity background is a disk which is rotationally
symmetric. We follow the method developed in (Hashimoto
et al., 2018a) which replaces the bulk differential equation
with a sparse neural network securing the interpretability
by regarding the weights as a metric on the curved space-
time.1With this setup, if we find what kind of differential
equations are emergent and what are not, it would be a great
step for proving the AdS/CF discussions. The work of K. H., K. M., M. M. and
G. O. was supported in part by JSPS KAKENHI Grant
Nos. JP22H01217, JP22H05111, and JP22H05115. The
work of D. T. was supported in part by Grant-in-Aid for
JSPS Fellows No. 22KJ1944.
References
Akutagawa, T., Hashimoto, K., and Sumimoto, T. Deep
learning and ads/qcd. Physical Review D , 102(2):026020,
2020.
Cai, T., Merz, G. W., Charton, F., Nolte, N., Wilhelm,
M., Cranmer, K., and Dixon, L. J. Transforming the
bootstrap: Using transformers to compute scattering am-
plitudes in planar n= 4 super yang-mills theory. arXiv
preprint arXiv:2405.06107 , 2024.
Chakraborty, S. Boundary Terms of the Einstein–Hilbert
Action , pp. 43–59. Springer International Publishing,
Cham, 2017.
Charton, F. Linear algebra with transformers. arXiv preprint
arXiv:2112.01898 , 2021.
Chen, R. T., Rubanova, Y ., Bettencourt, J., and Duvenaud,
D. K. Neural ordinary differential equations. Advances
in neural information processing systems , 31, 2018.
Cybenko, G. Approximation by superpositions of a sig-
moidal function. Mathematics of control, signals and
systems , 2(4):303–314, 1989.
Hashimoto, K. Ads/cft correspondence as a deep boltzmann
machine. Physical Review D , 99(10):106017, 2019.
Hashimoto, K., Sugishita, S., Tanaka, A., and Tomiya, A.
Deep learning and the ads/cft correspondence. Physical
Review D , 98(4):046019, 2018a.
Hashimoto, K., Sugishita, S., Tanaka, A., and Tomiya, A.
Deep learning and holographic qcd. Physical Review D ,
98(10):106014, 2018b.
Hashimoto, K., Hu, H.-Y ., and You, Y .-Z. Neural ordinary
differential equation and holographic quantum chromo-
dynamics. Machine Learning: Science and Technology ,
2(3):035011, 2021.
9
AI for an inverse problem: Physical model solving quantum gravity
Hashimoto, K., Takeda, D., Tanaka, K., and Yonezawa,
S. Spacetime-emergent ring toward tabletop quantum
gravity experiments. Physical Review Research , 5(2):
023168, 2023.
Hashimoto, K., Hirono, Y ., and Sannai, A. Unification of
symmetries i",True
92d4eaa2df1188934d60f18bffc337a55e590e99,Specify What? A Case-Study using GPT-4 and Formal Methods For Specification Synthesis,"['George Granberry', 'Wolfgang Ahrendt', 'Moa Johansson']",https://openreview.net/pdf/92d4eaa2df1188934d60f18bffc337a55e590e99.pdf,"Specify What? A Case-Study using GPT-4 and Formal Methods For Specification Synthesis We study how including the output of formal methods tools in LLM prompts can affect specification synthesis. Formal specifications are supposed to unambigu- ously describe the behaviour of (parts of) pro- grams and are usually provided as extra annota- tions of the program code. The intention is both to document the code and to be able to automati- cally check compliance of programs using formal methods tools. Writing good specifications can however be both difficult and time-consuming for the programmer. In this case-study, we investigate how GPT-4 can help with the task. We propose a neuro-symbolic integration, by which we aug- ment the LLM prompts with outputs from two formal methods tools in the Frama-C ecosystem (Pathcrawler and EVA), and produce C program annotations in the specifications language ACSL. We demonstrate how this impacts the quality of annotations: information about input/output ex- amples from Pathcrawler produce more context- aware annotations, while the inclusion of EVA reports yields annotations more attuned to run- time errors.",92d4eaa2df1188934d60f18bffc337a55e590e99.pdf,"methodology
does not attempt to benchmark the generated specifications
against a predefined gold standard, nor does it aim to de-
termine the optimal approach to creating specifications. In-
stead, our focus is on identifying the behaviors and patterns
that emerge from incorporating symbolic analysis outputs
into the specification generation prompts. This approach
allows us to better understand the dynamics at play and what
kinds of output to expect given a particular prompt.
We propose a primarily qualitative evaluation from two
different angles:
•Types of annotations : In addition to counting the dif-
ferent types of annotations produced per prompt, we
use a human-in-the-loop qualitative analysis to inter-
pret the specification and identify trends depending on
which prompt was used, to assess how the different
symbolic tool outputs influence the results of the LLM.
For this we use the programs in the pathcrawler set.
•Implementation vs. Intent : We specifically exam-
ine programs in the mutated setto study how errors
introduced into the program affect the resultant specifi-
cations. This analysis explores how errors, symbolic
analyses, intended program functionality, and actual
implementation interact.
5.1. Types of Annotations
To give an overview, Figure 1 displays the number anno-
tations generated for each annotation type for the three
promtps. For all three cases, the most common annota-
tions are unsurprisingly the requires andensures statements,
which are used to define pre- and post-conditions of func-
tions, followed by assigns statements and loop invariants .
5.1.1. B ASELINE PROMPT
Many of the annotations produced with the baseline prompt
were rather simplistic. While not necessarily incorrect or
completely useless, these specifications tended to focus on
surface-level details of the programs, overlooking deeper,
more substantive aspects. This can be seen in Appendix D
3
Specification Generation for C programs
Figure 1. Annotation-type counts for each p","conclusions from.
While open source models such as Llama-3 have recently
gained traction, the setup and fine tuning of such a model
was out scope for this project, and remain as further work.
We prompt GPT-4 with a C program, instructions for how
to generate ACSL annotations (in a step-by-step manner).
We also include a few examples of valid annotations in
the prompts (see Appendix A). We also experiment with
prompts which in addition contain outputs from the EV A
and Pathcrawler tools (see Appendix B and C).
2
Specification Generation for C programs
3. C-program Test Suits
For our study, we have chosen to utilize the 55 programs
from the closed-source test suite1of Pathcrawler which we
will refer to as the pathcrawler set. This suite includes a
variety of program types, balancing well-known algorithms
like Binary Search with more niche programs such as a
Soup Heater controller. It also contains small, specially
crafted programs designed to test specific capabilities of
Pathcrawler, adding another layer of diversity to our tests.
Additionally, pathcrawler tests includes files that provide
preconditions to Pathcrawler when it creates test inputs.
This was convenient as it saved us from having to provide
sensible test inputs for every program that we wanted to use
Pathcrawler with. Using a closed-source test suite also has
the advantage that at least some of the programs and their
annotations are less likley to have appeared in the training
data for GPT-4. This test suit helps us test to what extent
accurate annotations can be produced for correct programs.
To also investigate if our approach can help with buggy
programs, we created a second suite of programs titled
mutated set. This comprises 8 of ”correct” programs with
handcrafted mutations simulating typos, designed to explore
a range of programs across two key dimensions: clarity of
intent and complexity. To thoroughly study the interactions
between these dimensions, this set includes various types of
programs: s","methodology does not attempt to benchmark the generated specifications against a predefined gold standard, nor does it aim to de- termine the optimal approach to creating specifications. In- stead, our focus is on identifying the behaviors and patterns that emerge from incorporating symbolic analysis outputs into the specification generation prompts. This approach allows us to better understand the dynamics at play and what kinds of output to expect given a particular prompt. We propose a primarily qualitative evaluation from two different angles: Types of annotations : In addition to counting the dif- ferent types of annotations produced per prompt, we use a human-in-the-loop qualitative analysis to inter- pret the specification and identify trends depending on which prompt was used, to assess how the different symbolic tool outputs influence the results of the LLM. For this we use the programs in the pathcrawler set. Implementation vs. Intent : We specifically exam- ine programs in the mutated setto study how errors introduced into the program affect the resultant specifi- cations. This analysis explores how errors, symbolic analyses, intended program functionality, and actual implementation interact. 5.1. Types of Annotations To give an overview, Figure 1 displays the number anno- tations generated for each annotation type for the three promtps. For all three cases, the most common annota- tions are unsurprisingly the requires andensures statements, which are used to define pre- and post-conditions of func- tions, followed by assigns statements and loop invariants . 5.1.1. B ASELINE PROMPT Many of the annotations produced with the baseline prompt were rather simplistic. While not necessarily incorrect or completely useless, these specifications tended to focus on surface-level details of the programs, overlooking deeper, more substantive aspects. This can be seen in Appendix D 3 Specification Generation for C programs Figure 1. Annotation-type counts for each p","conclusions from. While open source models such as Llama-3 have recently gained traction, the setup and fine tuning of such a model was out scope for this project, and remain as further work. We prompt GPT-4 with a C program, instructions for how to generate ACSL annotations (in a step-by-step manner). We also include a few examples of valid annotations in the prompts (see Appendix A). We also experiment with prompts which in addition contain outputs from the EV A and Pathcrawler tools (see Appendix B and C). 2 Specification Generation for C programs 3. C-program Test Suits For our study, we have chosen to utilize the 55 programs from the closed-source test suite1of Pathcrawler which we will refer to as the pathcrawler set. This suite includes a variety of program types, balancing well-known algorithms like Binary Search with more niche programs such as a Soup Heater controller. It also contains small, specially crafted programs designed to test specific capabilities of Pathcrawler, adding another layer of diversity to our tests. Additionally, pathcrawler tests includes files that provide preconditions to Pathcrawler when it creates test inputs. This was convenient as it saved us from having to provide sensible test inputs for every program that we wanted to use Pathcrawler with. Using a closed-source test suite also has the advantage that at least some of the programs and their annotations are less likley to have appeared in the training data for GPT-4. This test suit helps us test to what extent accurate annotations can be produced for correct programs. To also investigate if our approach can help with buggy programs, we created a second suite of programs titled mutated set. This comprises 8 of correct programs with handcrafted mutations simulating typos, designed to explore a range of programs across two key dimensions: clarity of intent and complexity. To thoroughly study the interactions between these dimensions, this set includes various types of programs: s","Specify What? A Case-Study using GPT-4 and Formal Methods For Specification Synthesis We study how including the output of formal methods tools in LLM prompts can affect specification synthesis. Formal specifications are supposed to unambigu- ously describe the behaviour of (parts of) pro- grams and are usually provided as extra annota- tions of the program code. The intention is both to document the code and to be able to automati- cally check compliance of programs using formal methods tools. Writing good specifications can however be both difficult and time-consuming for the programmer. In this case-study, we investigate how GPT-4 can help with the task. We propose a neuro-symbolic integration, by which we aug- ment the LLM prompts with outputs from two formal methods tools in the Frama-C ecosystem (Pathcrawler and EVA), and produce C program annotations in the specifications language ACSL. We demonstrate how this impacts the quality of annotations: information about input/output ex- amples from Pathcrawler produce more context- aware annotations, while the inclusion of EVA reports yields annotations more attuned to run- time errors. conclusions from. While open source models such as Llama-3 have recently gained traction, the setup and fine tuning of such a model was out scope for this project, and remain as further work. We prompt GPT-4 with a C program, instructions for how to generate ACSL annotations (in a step-by-step manner). We also include a few examples of valid annotations in the prompts (see Appendix A). We also experiment with prompts which in addition contain outputs from the EV A and Pathcrawler tools (see Appendix B and C). 2 Specification Generation for C programs 3. C-program Test Suits For our study, we have chosen to utilize the 55 programs from the closed-source test suite1of Pathcrawler which we will refer to as the pathcrawler set. This suite includes a variety of program types, balancing well-known algorithms like Binary Search with more niche programs such as a Soup Heater controller. It also contains small, specially crafted programs designed to test specific capabilities of Pathcrawler, adding another layer of diversity to our tests. Additionally, pathcrawler tests includes files that provide preconditions to Pathcrawler when it creates test inputs. This was convenient as it saved us from having to provide sensible test inputs for every program that we wanted to use Pathcrawler with. Using a closed-source test suite also has the advantage that at least some of the programs and their annotations are less likley to have appeared in the training data for GPT-4. This test suit helps us test to what extent accurate annotations can be produced for correct programs. To also investigate if our approach can help with buggy programs, we created a second suite of programs titled mutated set. This comprises 8 of correct programs with handcrafted mutations simulating typos, designed to explore a range of programs across two key dimensions: clarity of intent and complexity. To thoroughly study the interactions between these dimensions, this set includes various types of programs: s methodology does not attempt to benchmark the generated specifications against a predefined gold standard, nor does it aim to de- termine the optimal approach to creating specifications. In- stead, our focus is on identifying the behaviors and patterns that emerge from incorporating symbolic analysis outputs into the specification generation prompts. This approach allows us to better understand the dynamics at play and what kinds of output to expect given a particular prompt. We propose a primarily qualitative evaluation from two different angles: Types of annotations : In addition to counting the dif- ferent types of annotations produced per prompt, we use a human-in-the-loop qualitative analysis to inter- pret the specification and identify trends depending on which prompt was used, to assess how the different symbolic tool outputs influence the results of the LLM. For this we use the programs in the pathcrawler set. Implementation vs. Intent : We specifically exam- ine programs in the mutated setto study how errors introduced into the program affect the resultant specifi- cations. This analysis explores how errors, symbolic analyses, intended program functionality, and actual implementation interact. 5.1. Types of Annotations To give an overview, Figure 1 displays the number anno- tations generated for each annotation type for the three promtps. For all three cases, the most common annota- tions are unsurprisingly the requires andensures statements, which are used to define pre- and post-conditions of func- tions, followed by assigns statements and loop invariants . 5.1.1. B ASELINE PROMPT Many of the annotations produced with the baseline prompt were rather simplistic. While not necessarily incorrect or completely useless, these specifications tended to focus on surface-level details of the programs, overlooking deeper, more substantive aspects. This can be seen in Appendix D 3 Specification Generation for C programs Figure 1. Annotation-type counts for each p","Specify What? A Case-Study using GPT-4 and Formal Methods For Specification Synthesis We study how including the output of formal methods tools in LLM prompts can affect specification synthesis. Formal specifications are supposed to unambigu- ously describe the behaviour of (parts of) pro- grams and are usually provided as extra annota- tions of the program code. The intention is both to document the code and to be able to automati- cally check compliance of programs using formal methods tools. Writing good specifications can however be both difficult and time-consuming for the programmer. In this case-study, we investigate how GPT-4 can help with the task. We propose a neuro-symbolic integration, by which we aug- ment the LLM prompts with outputs from two formal methods tools in the Frama-C ecosystem (Pathcrawler and EVA), and produce C program annotations in the specifications language ACSL. We demonstrate how this impacts the quality of annotations: information about input/output ex- amples from Pathcrawler produce more context- aware annotations, while the inclusion of EVA reports yields annotations more attuned to run- time errors.",0,"methodology
does not attempt to benchmark the generated specifications
against a predefined gold standard, nor does it aim to de-
termine the optimal approach to creating specifications. In-
stead, our focus is on identifying the behaviors and patterns
that emerge from incorporating symbolic analysis outputs
into the specification generation prompts. This approach
allows us to better understand the dynamics at play and what
kinds of output to expect given a particular prompt.
We propose a primarily qualitative evaluation from two
different angles:
•Types of annotations : In addition to counting the dif-
ferent types of annotations produced per prompt, we
use a human-in-the-loop qualitative analysis to inter-
pret the specification and identify trends depending on
which prompt was used, to assess how the different
symbolic tool outputs influence the results of the LLM.
For this we use the programs in the pathcrawler set.
•Implementation vs. Intent : We specifically exam-
ine programs in the mutated setto study how errors
introduced into the program affect the resultant specifi-
cations. This analysis explores how errors, symbolic
analyses, intended program functionality, and actual
implementation interact.
5.1. Types of Annotations
To give an overview, Figure 1 displays the number anno-
tations generated for each annotation type for the three
promtps. For all three cases, the most common annota-
tions are unsurprisingly the requires andensures statements,
which are used to define pre- and post-conditions of func-
tions, followed by assigns statements and loop invariants .
5.1.1. B ASELINE PROMPT
Many of the annotations produced with the baseline prompt
were rather simplistic. While not necessarily incorrect or
completely useless, these specifications tended to focus on
surface-level details of the programs, overlooking deeper,
more substantive aspects. This can be seen in Appendix D
3
Specification Generation for C programs
Figure 1. Annotation-type counts for each p conclusions from.
While open source models such as Llama-3 have recently
gained traction, the setup and fine tuning of such a model
was out scope for this project, and remain as further work.
We prompt GPT-4 with a C program, instructions for how
to generate ACSL annotations (in a step-by-step manner).
We also include a few examples of valid annotations in
the prompts (see Appendix A). We also experiment with
prompts which in addition contain outputs from the EV A
and Pathcrawler tools (see Appendix B and C).
2
Specification Generation for C programs
3. C-program Test Suits
For our study, we have chosen to utilize the 55 programs
from the closed-source test suite1of Pathcrawler which we
will refer to as the pathcrawler set. This suite includes a
variety of program types, balancing well-known algorithms
like Binary Search with more niche programs such as a
Soup Heater controller. It also contains small, specially
crafted programs designed to test specific capabilities of
Pathcrawler, adding another layer of diversity to our tests.
Additionally, pathcrawler tests includes files that provide
preconditions to Pathcrawler when it creates test inputs.
This was convenient as it saved us from having to provide
sensible test inputs for every program that we wanted to use
Pathcrawler with. Using a closed-source test suite also has
the advantage that at least some of the programs and their
annotations are less likley to have appeared in the training
data for GPT-4. This test suit helps us test to what extent
accurate annotations can be produced for correct programs.
To also investigate if our approach can help with buggy
programs, we created a second suite of programs titled
mutated set. This comprises 8 of ”correct” programs with
handcrafted mutations simulating typos, designed to explore
a range of programs across two key dimensions: clarity of
intent and complexity. To thoroughly study the interactions
between these dimensions, this set includes various types of
programs: s",False
46caee17a170a1a8ce500d42f3be4659ebc8f34b,DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation,"['Xueqing Wu', 'Rui Zheng', 'Jingzhen Sha', 'Te-Lin Wu', 'Hanyu Zhou', 'Tang Mohan', 'Kai-Wei Chang', 'Nanyun Peng', 'Haoran Huang']",https://openreview.net/pdf/46caee17a170a1a8ce500d42f3be4659ebc8f34b.pdf,"DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation We introduce DACO, a dataset for data analysis, containing (1) 440 databases of tabular data, (2) 2k automatically generated query-answer pairs that can serve as weak supervision for model training, and (3) a manually refined test set for evaluation. Data analysis is a crucial analytical process to generate in-depth studies and conclusive insights to comprehensively answer a given user query for tabular data. In this work, we aim to propose new resources and benchmarks to inspire future research on this crucial yet challenging and under-explored task. However, collecting data analysis annotations curated by experts can be prohibitively expensive. We propose to automatically generate high-quality answer annotations leveraging the code-generation capabilities of LLMs with a multi-turn prompting technique. We construct the **DACO dataset**, containing (1) 440 databases (of tabular data) collected from real-world scenarios, (2) ~2k query-answer pairs that can serve as weak supervision for model training, and (3) a concentrated but high-quality test set with human refined annotations that serves as our main evaluation benchmark. We train a 6B supervised fine-tuning (SFT) model on DACO dataset, and find that the SFT model learns reasonable data analysis capabilities. To further align the models with human preference, we use reinforcement learning to encourage generating analysis perceived by human as helpful, and design a set of dense rewards to propagate the sparse human preference reward to intermediate code generation steps. Our DACO-RL algorithm is evaluated by human annotators to produce more helpful answers than SFT model in 57.72% cases, validating the effectiveness of our proposed algorithm.",46caee17a170a1a8ce500d42f3be4659ebc8f34b.pdf,"approach
(Ouyang et al., 2022; Wu et al., 2023; Zheng et al., 2023b).
Given two analyses generated by two different systems, the
annotator (either human or simulated by ChatGPT) selects
the more helpful one based on our defined criteria. The
winning rate of each system is reported as helpfulness score.
To obtain a comparable set of numbers for all models, we
report the winning rate of each model against TestAand
TestHannotations. The upper bound for this score would
be 50, as a score of 50 indicates that the model generations
are perceived as helpful as annotations.
3. D ACO-RL
While DACO contains mostly algorithmic machine gen-
erated analyses, the machine generations without human
refinement cannot well align with human preferences (of
“good” analyses). Our human refinement process shows
that only 47.4% bullet points are “good” points perfectly
addressing user queries; the majority of 52.2% are evalu-
ated as “borderline” points that only partially aligns with
Table 2: Distribution of DACO queries. We display the top 15
verbs and their top 3 direct noun objectives, demonstrating the
diversity of D ACO queries.
human expectations; and the remaining 0.4% are considered
as “bad”.
We are therefore interested in investigating whether aligning
human preferences via an RLHF fashion could lead to better
machine generated analyses. We thus propose the DACO-RL
algorithm, which is illustrated in the left half of Figure 4.
Our end goal is to optimize the helpfulness of the analyzed
points, which is modelled with an answer RM Ra. In ad-
dition to this sparse reward signal, we use a heuristically
defined contribution RM Rcto reward each intermediate
step, which is further regularized with a regularization RM
Rrto prevent reward hacking. In the following sections,
we first explain the three reward models sequentially, and
eventually explain our whole RLHF pipeline.
Notations. We train a language model that interacts
with the python interpreter in a conversational man-
ner. Forma","Conclusion
In this work, we propose a novel and challenging data anal-
ysis task, which involves decomposing user query into mul-
tiple perspectives, grounding each perspective to the input
data and performing logical and mathematical reasoning.
To support this task, we build the DACO dataset containing
large-scale annotations automatically generated by GPT-4
and a small but high-quality test set with human curated
annotations. We employ LLM enhanced with code gener-
ation to this task and evaluate three models on our dataset:
zero-shot ChatGPT, zero-shot GPT-4 and a 6B SFT model.
While GPT-4 consistently performs the best, SFT achieves
reasonably good helpfulness with much less computation.
On top of the SFT model, we further proposed our DACO-
RL algorithm that significantly boosts the human evaluated
helpfulness.
References
Austin, J., Odena, A., Nye, M. I., Bosma, M., Michalewski,
H., Dohan, D., Jiang, E., Cai, C. J., Terry, M., Le, Q. V .,
and Sutton, C. Program synthesis with large language
models. CoRR , abs/2108.07732, 2021. URL https:
//arxiv.org/abs/2108.07732 .
Bai, Y ., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-
Sarma, N., Drain, D., Fort, S., Ganguli, D., Henighan,
8
DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation
T., Joseph, N., Kadavath, S., Kernion, J., Conerly, T.,
Showk, S. E., Elhage, N., Hatfield-Dodds, Z., Hernandez,
D., Hume, T., Johnston, S., Kravec, S., Lovitt, L., Nanda,
N., Olsson, C., Amodei, D., Brown, T. B., Clark, J., Mc-
Candlish, S., Olah, C., Mann, B., and Kaplan, J. Train-
ing a helpful and harmless assistant with reinforcement
learning from human feedback. CoRR , abs/2204.05862,
2022a. doi: 10.48550/ARXIV .2204.05862. URL https:
//doi.org/10.48550/arXiv.2204.05862 .
Bai, Y ., Kadavath, S., Kundu, S., Askell, A., Kernion, J.,
Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKin-
non, C., Chen, C., Olsson, C., Olah, C., Hernandez, D.,
Drain, D., Ganguli, D., Li, D., Tran-Johnson, E., Per","approach (Ouyang et al., 2022; Wu et al., 2023; Zheng et al., 2023b). Given two analyses generated by two different systems, the annotator (either human or simulated by ChatGPT) selects the more helpful one based on our defined criteria. The winning rate of each system is reported as helpfulness score. To obtain a comparable set of numbers for all models, we report the winning rate of each model against TestAand TestHannotations. The upper bound for this score would be 50, as a score of 50 indicates that the model generations are perceived as helpful as annotations. 3. D ACO-RL While DACO contains mostly algorithmic machine gen- erated analyses, the machine generations without human refinement cannot well align with human preferences (of good analyses). Our human refinement process shows that only 47.4% bullet points are good points perfectly addressing user queries; the majority of 52.2% are evalu- ated as borderline points that only partially aligns with Table 2: Distribution of DACO queries. We display the top 15 verbs and their top 3 direct noun objectives, demonstrating the diversity of D ACO queries. human expectations; and the remaining 0.4% are considered as bad . We are therefore interested in investigating whether aligning human preferences via an RLHF fashion could lead to better machine generated analyses. We thus propose the DACO-RL algorithm, which is illustrated in the left half of Figure 4. Our end goal is to optimize the helpfulness of the analyzed points, which is modelled with an answer RM Ra. In ad- dition to this sparse reward signal, we use a heuristically defined contribution RM Rcto reward each intermediate step, which is further regularized with a regularization RM Rrto prevent reward hacking. In the following sections, we first explain the three reward models sequentially, and eventually explain our whole RLHF pipeline. Notations. We train a language model that interacts with the python interpreter in a conversational man- ner. Forma","Conclusion In this work, we propose a novel and challenging data anal- ysis task, which involves decomposing user query into mul- tiple perspectives, grounding each perspective to the input data and performing logical and mathematical reasoning. To support this task, we build the DACO dataset containing large-scale annotations automatically generated by GPT-4 and a small but high-quality test set with human curated annotations. We employ LLM enhanced with code gener- ation to this task and evaluate three models on our dataset: zero-shot ChatGPT, zero-shot GPT-4 and a 6B SFT model. While GPT-4 consistently performs the best, SFT achieves reasonably good helpfulness with much less computation. On top of the SFT model, we further proposed our DACO- RL algorithm that significantly boosts the human evaluated helpfulness.","DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation We introduce DACO, a dataset for data analysis, containing (1) 440 databases of tabular data, (2) 2k automatically generated query-answer pairs that can serve as weak supervision for model training, and (3) a manually refined test set for evaluation. Data analysis is a crucial analytical process to generate in-depth studies and conclusive insights to comprehensively answer a given user query for tabular data. In this work, we aim to propose new resources and benchmarks to inspire future research on this crucial yet challenging and under-explored task. However, collecting data analysis annotations curated by experts can be prohibitively expensive. We propose to automatically generate high-quality answer annotations leveraging the code-generation capabilities of LLMs with a multi-turn prompting technique. We construct the **DACO dataset**, containing (1) 440 databases (of tabular data) collected from real-world scenarios, (2) ~2k query-answer pairs that can serve as weak supervision for model training, and (3) a concentrated but high-quality test set with human refined annotations that serves as our main evaluation benchmark. We train a 6B supervised fine-tuning (SFT) model on DACO dataset, and find that the SFT model learns reasonable data analysis capabilities. To further align the models with human preference, we use reinforcement learning to encourage generating analysis perceived by human as helpful, and design a set of dense rewards to propagate the sparse human preference reward to intermediate code generation steps. Our DACO-RL algorithm is evaluated by human annotators to produce more helpful answers than SFT model in 57.72% cases, validating the effectiveness of our proposed algorithm. Conclusion In this work, we propose a novel and challenging data anal- ysis task, which involves decomposing user query into mul- tiple perspectives, grounding each perspective to the input data and performing logical and mathematical reasoning. To support this task, we build the DACO dataset containing large-scale annotations automatically generated by GPT-4 and a small but high-quality test set with human curated annotations. We employ LLM enhanced with code gener- ation to this task and evaluate three models on our dataset: zero-shot ChatGPT, zero-shot GPT-4 and a 6B SFT model. While GPT-4 consistently performs the best, SFT achieves reasonably good helpfulness with much less computation. On top of the SFT model, we further proposed our DACO- RL algorithm that significantly boosts the human evaluated helpfulness. approach (Ouyang et al., 2022; Wu et al., 2023; Zheng et al., 2023b). Given two analyses generated by two different systems, the annotator (either human or simulated by ChatGPT) selects the more helpful one based on our defined criteria. The winning rate of each system is reported as helpfulness score. To obtain a comparable set of numbers for all models, we report the winning rate of each model against TestAand TestHannotations. The upper bound for this score would be 50, as a score of 50 indicates that the model generations are perceived as helpful as annotations. 3. D ACO-RL While DACO contains mostly algorithmic machine gen- erated analyses, the machine generations without human refinement cannot well align with human preferences (of good analyses). Our human refinement process shows that only 47.4% bullet points are good points perfectly addressing user queries; the majority of 52.2% are evalu- ated as borderline points that only partially aligns with Table 2: Distribution of DACO queries. We display the top 15 verbs and their top 3 direct noun objectives, demonstrating the diversity of D ACO queries. human expectations; and the remaining 0.4% are considered as bad . We are therefore interested in investigating whether aligning human preferences via an RLHF fashion could lead to better machine generated analyses. We thus propose the DACO-RL algorithm, which is illustrated in the left half of Figure 4. Our end goal is to optimize the helpfulness of the analyzed points, which is modelled with an answer RM Ra. In ad- dition to this sparse reward signal, we use a heuristically defined contribution RM Rcto reward each intermediate step, which is further regularized with a regularization RM Rrto prevent reward hacking. In the following sections, we first explain the three reward models sequentially, and eventually explain our whole RLHF pipeline. Notations. We train a language model that interacts with the python interpreter in a conversational man- ner. Forma","DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation We introduce DACO, a dataset for data analysis, containing (1) 440 databases of tabular data, (2) 2k automatically generated query-answer pairs that can serve as weak supervision for model training, and (3) a manually refined test set for evaluation. Data analysis is a crucial analytical process to generate in-depth studies and conclusive insights to comprehensively answer a given user query for tabular data. In this work, we aim to propose new resources and benchmarks to inspire future research on this crucial yet challenging and under-explored task. However, collecting data analysis annotations curated by experts can be prohibitively expensive. We propose to automatically generate high-quality answer annotations leveraging the code-generation capabilities of LLMs with a multi-turn prompting technique. We construct the **DACO dataset**, containing (1) 440 databases (of tabular data) collected from real-world scenarios, (2) ~2k query-answer pairs that can serve as weak supervision for model training, and (3) a concentrated but high-quality test set with human refined annotations that serves as our main evaluation benchmark. We train a 6B supervised fine-tuning (SFT) model on DACO dataset, and find that the SFT model learns reasonable data analysis capabilities. To further align the models with human preference, we use reinforcement learning to encourage generating analysis perceived by human as helpful, and design a set of dense rewards to propagate the sparse human preference reward to intermediate code generation steps. Our DACO-RL algorithm is evaluated by human annotators to produce more helpful answers than SFT model in 57.72% cases, validating the effectiveness of our proposed algorithm.",0,"approach
(Ouyang et al., 2022; Wu et al., 2023; Zheng et al., 2023b).
Given two analyses generated by two different systems, the
annotator (either human or simulated by ChatGPT) selects
the more helpful one based on our defined criteria. The
winning rate of each system is reported as helpfulness score.
To obtain a comparable set of numbers for all models, we
report the winning rate of each model against TestAand
TestHannotations. The upper bound for this score would
be 50, as a score of 50 indicates that the model generations
are perceived as helpful as annotations.
3. D ACO-RL
While DACO contains mostly algorithmic machine gen-
erated analyses, the machine generations without human
refinement cannot well align with human preferences (of
“good” analyses). Our human refinement process shows
that only 47.4% bullet points are “good” points perfectly
addressing user queries; the majority of 52.2% are evalu-
ated as “borderline” points that only partially aligns with
Table 2: Distribution of DACO queries. We display the top 15
verbs and their top 3 direct noun objectives, demonstrating the
diversity of D ACO queries.
human expectations; and the remaining 0.4% are considered
as “bad”.
We are therefore interested in investigating whether aligning
human preferences via an RLHF fashion could lead to better
machine generated analyses. We thus propose the DACO-RL
algorithm, which is illustrated in the left half of Figure 4.
Our end goal is to optimize the helpfulness of the analyzed
points, which is modelled with an answer RM Ra. In ad-
dition to this sparse reward signal, we use a heuristically
defined contribution RM Rcto reward each intermediate
step, which is further regularized with a regularization RM
Rrto prevent reward hacking. In the following sections,
we first explain the three reward models sequentially, and
eventually explain our whole RLHF pipeline.
Notations. We train a language model that interacts
with the python interpreter in a conversational man-
ner. Forma Conclusion
In this work, we propose a novel and challenging data anal-
ysis task, which involves decomposing user query into mul-
tiple perspectives, grounding each perspective to the input
data and performing logical and mathematical reasoning.
To support this task, we build the DACO dataset containing
large-scale annotations automatically generated by GPT-4
and a small but high-quality test set with human curated
annotations. We employ LLM enhanced with code gener-
ation to this task and evaluate three models on our dataset:
zero-shot ChatGPT, zero-shot GPT-4 and a 6B SFT model.
While GPT-4 consistently performs the best, SFT achieves
reasonably good helpfulness with much less computation.
On top of the SFT model, we further proposed our DACO-
RL algorithm that significantly boosts the human evaluated
helpfulness.
References
Austin, J., Odena, A., Nye, M. I., Bosma, M., Michalewski,
H., Dohan, D., Jiang, E., Cai, C. J., Terry, M., Le, Q. V .,
and Sutton, C. Program synthesis with large language
models. CoRR , abs/2108.07732, 2021. URL https:
//arxiv.org/abs/2108.07732 .
Bai, Y ., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-
Sarma, N., Drain, D., Fort, S., Ganguli, D., Henighan,
8
DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation
T., Joseph, N., Kadavath, S., Kernion, J., Conerly, T.,
Showk, S. E., Elhage, N., Hatfield-Dodds, Z., Hernandez,
D., Hume, T., Johnston, S., Kravec, S., Lovitt, L., Nanda,
N., Olsson, C., Amodei, D., Brown, T. B., Clark, J., Mc-
Candlish, S., Olah, C., Mann, B., and Kaplan, J. Train-
ing a helpful and harmless assistant with reinforcement
learning from human feedback. CoRR , abs/2204.05862,
2022a. doi: 10.48550/ARXIV .2204.05862. URL https:
//doi.org/10.48550/arXiv.2204.05862 .
Bai, Y ., Kadavath, S., Kundu, S., Askell, A., Kernion, J.,
Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKin-
non, C., Chen, C., Olsson, C., Olah, C., Hernandez, D.,
Drain, D., Ganguli, D., Li, D., Tran-Johnson, E., Per",False
b5ceba0148c6dcdfa3dd15ada8c3e29bc88ec2c8,Distilling LLMs’ Decomposition Abilities into Compact Language Models,"['Denis Tarasov', 'Kumar Shridhar']",https://openreview.net/pdf/b5ceba0148c6dcdfa3dd15ada8c3e29bc88ec2c8.pdf,"Distilling LLMs Decomposition Abilities into Compact Language Models In this work we develop AI-generated benchmark for the distillation of LLMs' decomposition abilities into smaller models and provide multiple baselines Large Language Models (LLMs) have demonstrated proficiency in their reasoning abilities, yet their large size presents scalability challenges and limits any further customization. In contrast, compact models offer customized training but often fall short in solving complex reasoning tasks. This study focuses on distilling the LLMs' decomposition skills into compact models using offline reinforcement learning. We leverage the advancements in the LLM`s capabilities to provide feedback and generate a specialized task-specific dataset for training compact models. The development of an AI-generated dataset and the establishment of baselines constitute the primary contributions of our work, underscoring the potential of compact models in replicating complex problem-solving skills.",b5ceba0148c6dcdfa3dd15ada8c3e29bc88ec2c8.pdf,"methodology inspired
by Shridhar et al. (2022), we use the BLEU score (Papineni
et al., 2002) calculated between the generated questions and
those produced by ChatGPT. Given that the primary goal
of BC is to replicate the original policy’s behavior, BLEU
serves as a suitable metric, indicating the similarity between
two texts. Our empirical observation show that BLEU cor-
relates with the final performance, making it a reasonable
choice for model evaluation in the context of BC. For all
subsequent approaches, the best BC model serves as the
initialization for the LM.
Filtered Behavioral Cloning. Filtered BC (Chen et al.,
2021) introduces a modification of BC by considering only a
fraction of the best trajectories in the dataset. This approach
proves particularly effective when a substantial number of
high-quality examples are at disposal. In the context of our
task, we exclusively retain samples corresponding to sub-
question sets that result in the correct solution. The model
selection process remains consistent with the standard BC
approach.
Implicit Language Q-Learning. Implicit Language Q-
Learning (ILQL) (Snell et al., 2022) represents an adaptation
of the offline RL approach known as IQL (Kostrikov et al.,
2021) to NLP tasks. The core idea behind ILQL involves
training additional Value (V) and Q-function heads with IQLobjectives. These additional functions are then employed to
reweight the original LM outputs using the advantage value,
which is the difference between V and Q values.
The selection of ILQL is motivated by the effectiveness of
IQL as one of the strongest offline RL approaches in diverse
domains (Tarasov et al., 2022). Given the limited adaptation
of offline RL approaches to NLP problems, ILQL emerges
as the state-of-the-art choice. Given that IQL optimizes for
rewards, which may not inherently correlate with the dataset
policy, selecting the best model becomes challenging. In
the absence of a clear best model selection criterion, we
have tried to","Future Work.
Our work serves as a foundational exploration, opening
avenues for various future directions.
Development of Offline RL Approaches: A pivotal area
for future exploration involves advancing offline RL or other
suitable methodologies for distilling reasoning abilities from
static datasets. This extension could contribute to more
effective utilization of language models in reasoning tasks.
Creation of a Larger Benchmark: Expanding our method-
ology, future work could focus on generating a more ex-
tensive benchmark as it requires only the access to ground
truth-answers in the datset which usualy holds. This bench-
mark might incorporate a diverse set of reasoning datasets,
such as MATH (Hendrycks et al., 2021) or AQuA (Ling
et al., 2017), providing a broader assessment of reasoning
capabilities.
Concentration on Sub-Question Answering: Delving
deeper into the sub-question answering aspect of the rea-
soning process presents a promising direction. While our
dataset includes ChatGPT responses for sub-questions, their
scoring and utilization remain unexplored. Future studies
could investigate this component to enhance understanding
and performance.
Utilization of Open-Source Models: Exploring the appli-
cation of open-source models, such as LLaMA, for sub-
question generation emerges as a cost-effective alterna-
tive. Accessible without financial constraints, these models
present an opportunity for researchers to delve into sub-
question generation without monetary limitations. We were
not able to run such kind of experiments ourselves due to
the computational limitations.
7. Conclusion
This work introduces a novel AI-generated benchmark tai-
lored for evaluating sub-questioning in reasoning tasks. We
employ diverse offline learning approaches, varying model
sizes for baselines, and assess the performance using dif-
ferent LLMs. Our experiments aim to shed light on the
challenges and potential avenues for enhancing reasoning
capabilities.
The outcomes reveal","methodology inspired by Shridhar et al. (2022), we use the BLEU score (Papineni et al., 2002) calculated between the generated questions and those produced by ChatGPT. Given that the primary goal of BC is to replicate the original policy s behavior, BLEU serves as a suitable metric, indicating the similarity between two texts. Our empirical observation show that BLEU cor- relates with the final performance, making it a reasonable choice for model evaluation in the context of BC. For all subsequent approaches, the best BC model serves as the initialization for the LM. Filtered Behavioral Cloning. Filtered BC (Chen et al., 2021) introduces a modification of BC by considering only a fraction of the best trajectories in the dataset. This approach proves particularly effective when a substantial number of high-quality examples are at disposal. In the context of our task, we exclusively retain samples corresponding to sub- question sets that result in the correct solution. The model selection process remains consistent with the standard BC approach. Implicit Language Q-Learning. Implicit Language Q- Learning (ILQL) (Snell et al., 2022) represents an adaptation of the offline RL approach known as IQL (Kostrikov et al., 2021) to NLP tasks. The core idea behind ILQL involves training additional Value (V) and Q-function heads with IQLobjectives. These additional functions are then employed to reweight the original LM outputs using the advantage value, which is the difference between V and Q values. The selection of ILQL is motivated by the effectiveness of IQL as one of the strongest offline RL approaches in diverse domains (Tarasov et al., 2022). Given the limited adaptation of offline RL approaches to NLP problems, ILQL emerges as the state-of-the-art choice. Given that IQL optimizes for rewards, which may not inherently correlate with the dataset policy, selecting the best model becomes challenging. In the absence of a clear best model selection criterion, we have tried to","Future Work. Our work serves as a foundational exploration, opening avenues for various future directions. Development of Offline RL Approaches: A pivotal area for future exploration involves advancing offline RL or other suitable methodologies for distilling reasoning abilities from static datasets. This extension could contribute to more effective utilization of language models in reasoning tasks. Creation of a Larger Benchmark: Expanding our method- ology, future work could focus on generating a more ex- tensive benchmark as it requires only the access to ground truth-answers in the datset which usualy holds. This bench- mark might incorporate a diverse set of reasoning datasets, such as MATH (Hendrycks et al., 2021) or AQuA (Ling et al., 2017), providing a broader assessment of reasoning capabilities. Concentration on Sub-Question Answering: Delving deeper into the sub-question answering aspect of the rea- soning process presents a promising direction. While our dataset includes ChatGPT responses for sub-questions, their scoring and utilization remain unexplored. Future studies could investigate this component to enhance understanding and performance. Utilization of Open-Source Models: Exploring the appli- cation of open-source models, such as LLaMA, for sub- question generation emerges as a cost-effective alterna- tive. Accessible without financial constraints, these models present an opportunity for researchers to delve into sub- question generation without monetary limitations. We were not able to run such kind of experiments ourselves due to the computational limitations. 7. Conclusion This work introduces a novel AI-generated benchmark tai- lored for evaluating sub-questioning in reasoning tasks. We employ diverse offline learning approaches, varying model sizes for baselines, and assess the performance using dif- ferent LLMs. Our experiments aim to shed light on the challenges and potential avenues for enhancing reasoning capabilities. The outcomes reveal","Distilling LLMs Decomposition Abilities into Compact Language Models In this work we develop AI-generated benchmark for the distillation of LLMs' decomposition abilities into smaller models and provide multiple baselines Large Language Models (LLMs) have demonstrated proficiency in their reasoning abilities, yet their large size presents scalability challenges and limits any further customization. In contrast, compact models offer customized training but often fall short in solving complex reasoning tasks. This study focuses on distilling the LLMs' decomposition skills into compact models using offline reinforcement learning. We leverage the advancements in the LLM`s capabilities to provide feedback and generate a specialized task-specific dataset for training compact models. The development of an AI-generated dataset and the establishment of baselines constitute the primary contributions of our work, underscoring the potential of compact models in replicating complex problem-solving skills. Future Work. Our work serves as a foundational exploration, opening avenues for various future directions. Development of Offline RL Approaches: A pivotal area for future exploration involves advancing offline RL or other suitable methodologies for distilling reasoning abilities from static datasets. This extension could contribute to more effective utilization of language models in reasoning tasks. Creation of a Larger Benchmark: Expanding our method- ology, future work could focus on generating a more ex- tensive benchmark as it requires only the access to ground truth-answers in the datset which usualy holds. This bench- mark might incorporate a diverse set of reasoning datasets, such as MATH (Hendrycks et al., 2021) or AQuA (Ling et al., 2017), providing a broader assessment of reasoning capabilities. Concentration on Sub-Question Answering: Delving deeper into the sub-question answering aspect of the rea- soning process presents a promising direction. While our dataset includes ChatGPT responses for sub-questions, their scoring and utilization remain unexplored. Future studies could investigate this component to enhance understanding and performance. Utilization of Open-Source Models: Exploring the appli- cation of open-source models, such as LLaMA, for sub- question generation emerges as a cost-effective alterna- tive. Accessible without financial constraints, these models present an opportunity for researchers to delve into sub- question generation without monetary limitations. We were not able to run such kind of experiments ourselves due to the computational limitations. 7. Conclusion This work introduces a novel AI-generated benchmark tai- lored for evaluating sub-questioning in reasoning tasks. We employ diverse offline learning approaches, varying model sizes for baselines, and assess the performance using dif- ferent LLMs. Our experiments aim to shed light on the challenges and potential avenues for enhancing reasoning capabilities. The outcomes reveal methodology inspired by Shridhar et al. (2022), we use the BLEU score (Papineni et al., 2002) calculated between the generated questions and those produced by ChatGPT. Given that the primary goal of BC is to replicate the original policy s behavior, BLEU serves as a suitable metric, indicating the similarity between two texts. Our empirical observation show that BLEU cor- relates with the final performance, making it a reasonable choice for model evaluation in the context of BC. For all subsequent approaches, the best BC model serves as the initialization for the LM. Filtered Behavioral Cloning. Filtered BC (Chen et al., 2021) introduces a modification of BC by considering only a fraction of the best trajectories in the dataset. This approach proves particularly effective when a substantial number of high-quality examples are at disposal. In the context of our task, we exclusively retain samples corresponding to sub- question sets that result in the correct solution. The model selection process remains consistent with the standard BC approach. Implicit Language Q-Learning. Implicit Language Q- Learning (ILQL) (Snell et al., 2022) represents an adaptation of the offline RL approach known as IQL (Kostrikov et al., 2021) to NLP tasks. The core idea behind ILQL involves training additional Value (V) and Q-function heads with IQLobjectives. These additional functions are then employed to reweight the original LM outputs using the advantage value, which is the difference between V and Q values. The selection of ILQL is motivated by the effectiveness of IQL as one of the strongest offline RL approaches in diverse domains (Tarasov et al., 2022). Given the limited adaptation of offline RL approaches to NLP problems, ILQL emerges as the state-of-the-art choice. Given that IQL optimizes for rewards, which may not inherently correlate with the dataset policy, selecting the best model becomes challenging. In the absence of a clear best model selection criterion, we have tried to","Distilling LLMs Decomposition Abilities into Compact Language Models In this work we develop AI-generated benchmark for the distillation of LLMs' decomposition abilities into smaller models and provide multiple baselines Large Language Models (LLMs) have demonstrated proficiency in their reasoning abilities, yet their large size presents scalability challenges and limits any further customization. In contrast, compact models offer customized training but often fall short in solving complex reasoning tasks. This study focuses on distilling the LLMs' decomposition skills into compact models using offline reinforcement learning. We leverage the advancements in the LLM`s capabilities to provide feedback and generate a specialized task-specific dataset for training compact models. The development of an AI-generated dataset and the establishment of baselines constitute the primary contributions of our work, underscoring the potential of compact models in replicating complex problem-solving skills.",0,"methodology inspired
by Shridhar et al. (2022), we use the BLEU score (Papineni
et al., 2002) calculated between the generated questions and
those produced by ChatGPT. Given that the primary goal
of BC is to replicate the original policy’s behavior, BLEU
serves as a suitable metric, indicating the similarity between
two texts. Our empirical observation show that BLEU cor-
relates with the final performance, making it a reasonable
choice for model evaluation in the context of BC. For all
subsequent approaches, the best BC model serves as the
initialization for the LM.
Filtered Behavioral Cloning. Filtered BC (Chen et al.,
2021) introduces a modification of BC by considering only a
fraction of the best trajectories in the dataset. This approach
proves particularly effective when a substantial number of
high-quality examples are at disposal. In the context of our
task, we exclusively retain samples corresponding to sub-
question sets that result in the correct solution. The model
selection process remains consistent with the standard BC
approach.
Implicit Language Q-Learning. Implicit Language Q-
Learning (ILQL) (Snell et al., 2022) represents an adaptation
of the offline RL approach known as IQL (Kostrikov et al.,
2021) to NLP tasks. The core idea behind ILQL involves
training additional Value (V) and Q-function heads with IQLobjectives. These additional functions are then employed to
reweight the original LM outputs using the advantage value,
which is the difference between V and Q values.
The selection of ILQL is motivated by the effectiveness of
IQL as one of the strongest offline RL approaches in diverse
domains (Tarasov et al., 2022). Given the limited adaptation
of offline RL approaches to NLP problems, ILQL emerges
as the state-of-the-art choice. Given that IQL optimizes for
rewards, which may not inherently correlate with the dataset
policy, selecting the best model becomes challenging. In
the absence of a clear best model selection criterion, we
have tried to Future Work.
Our work serves as a foundational exploration, opening
avenues for various future directions.
Development of Offline RL Approaches: A pivotal area
for future exploration involves advancing offline RL or other
suitable methodologies for distilling reasoning abilities from
static datasets. This extension could contribute to more
effective utilization of language models in reasoning tasks.
Creation of a Larger Benchmark: Expanding our method-
ology, future work could focus on generating a more ex-
tensive benchmark as it requires only the access to ground
truth-answers in the datset which usualy holds. This bench-
mark might incorporate a diverse set of reasoning datasets,
such as MATH (Hendrycks et al., 2021) or AQuA (Ling
et al., 2017), providing a broader assessment of reasoning
capabilities.
Concentration on Sub-Question Answering: Delving
deeper into the sub-question answering aspect of the rea-
soning process presents a promising direction. While our
dataset includes ChatGPT responses for sub-questions, their
scoring and utilization remain unexplored. Future studies
could investigate this component to enhance understanding
and performance.
Utilization of Open-Source Models: Exploring the appli-
cation of open-source models, such as LLaMA, for sub-
question generation emerges as a cost-effective alterna-
tive. Accessible without financial constraints, these models
present an opportunity for researchers to delve into sub-
question generation without monetary limitations. We were
not able to run such kind of experiments ourselves due to
the computational limitations.
7. Conclusion
This work introduces a novel AI-generated benchmark tai-
lored for evaluating sub-questioning in reasoning tasks. We
employ diverse offline learning approaches, varying model
sizes for baselines, and assess the performance using dif-
ferent LLMs. Our experiments aim to shed light on the
challenges and potential avenues for enhancing reasoning
capabilities.
The outcomes reveal",False
e6106df113d9ae7f3ed2d023ebc8c8f9d8c3a3f5,Progressive-Hint Prompting Improves Reasoning in Large Language Models,"['Chuanyang Zheng', 'Zhengying Liu', 'Enze Xie', 'Zhenguo Li', 'Yu Li']",https://openreview.net/pdf/e6106df113d9ae7f3ed2d023ebc8c8f9d8c3a3f5.pdf,"Progressive-Hint Prompting Improves Reasoning in Large Language Models We propose a new prompting strategy, Progressive-Hint Promoting, that can be easily combined with Chain-Of-Thought and Self-Consistency to improve performance. The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that en- hance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic mul- tiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experi- ments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text- davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sam- ple paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% 91.9%), GSM8K (92% 95.5%), AQuA (76.4% 79.9%) and MATH (50.3% 53.9%).",e6106df113d9ae7f3ed2d023ebc8c8f9d8c3a3f5.pdf,"approaches
to promote intermediate reasoning steps (Wei et al., 2022;
Zhou et al., 2023; Fu et al., 2023). Other works in this area,
such as Least-to-Most (Zhou et al., 2023) and Complex
CoT (Fu et al., 2023), have also explored this direction. An-
other area of research is self-consistency-related approaches.
In comparison to CoT-related work that focuses on design-
ing better prompts, self-consistency proposes to sample mul-
tiple answers from the LLMs and arrive at the correct answer
through a majority vote (Fu et al., 2023). This approach
is further improved upon by complex-based selection (Fu
et al., 2023). CoT-related and self-consistency-related works
can be seamlessly combined without any conflict.
Prior research has not explored the potential of leveraging
the outputs of LLM to refine reasoning paths iteratively.
It stands to reason that similar to human cognition, LLM
could benefit from reevaluating and adjusting its generated
reasoning paths in order to correct errors and enhance over-
all performance. In this paper, we propose a new method
named Progressive-Hint Prompting (PHP) that involves se-
quentially interacting with LLM to approach the correct an-
swer gradually. The method operates as follows: (1) given
a question, we ask the LLM to provide a Base Answer; (2)
we combine the question and answer to re-ask the LLM and
obtain the Subsequent Answer; (3) we repeat the operation
in (2) until the answer is stable and does not change over
the last two answers. PHP follows a human-like thought
process where previous answers are leveraged as hints to
arrive at the correct answer after re-evaluating the question.
Figure 1 illustrates the proposed PHP framework. We use
the base prompt to obtain the initial base answer, and then
employ the PHP prompt for subsequent questions. If the cur-
rent answer matches the previous answer, it is more likely to
be correct, and we terminate the LLM inquiry. With Com-
plex CoT and GPT-4, after adding PHP, the performance
ach","Conclusion
This paper introduces a novel approach named Progressive-
Hint Prompting (PHP) for interacting with LLMs, which
offers multiple advantages: 1) PHP achieves substantial per-
formance improvements on math reasoning tasks, leading
to state-of-the-art results on several reasoning benchmarks;
2) with more powerful models and prompts, PHP can better
and consistently benefit the LLMs; 3) PHP can be easily
combined with CoT and self-consistency to further improve
performance.
To better enhance the progressive-hint prompting approach,
future research endeavors can focus on improving the de-
sign of handcrafted hints in the question phase and prompt
sentences in the answer part. Additionally, novel hints that
aid the LLMs to reconsider the questions can be identified
and extracted beside the answer.
8
Progressive-Hint Prompting Improves Reasoning in Large Language Models
References
Asai, A., Hashimoto, K., Hajishirzi, H., Socher, R., and
Xiong, C. Learning to retrieve reasoning paths over
wikipedia graph for question answering. In International
Conference on Learning Representations , 2020. 2
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:
1877–1901, 2020. 2, 4
Chen, J., Lin, S.-t., and Durrett, G. Multi-hop ques-
tion answering via reasoning chains. arXiv preprint
arXiv:1910.02610 , 2019. 3
Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program
of thoughts prompting: Disentangling computation from
reasoning for numerical reasoning tasks. arXiv preprint
arXiv:2211.12588 , 2022. 7
Chowdhary, K. and Chowdhary, K. Natural language pro-
cessing. Fundamentals of artificial intelligence , pp. 603–
649, 2020. 1
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,
Gehrmann, S., et al. Palm: Scaling language modeling
with pathways. arXiv pr","approaches to promote intermediate reasoning steps (Wei et al., 2022; Zhou et al., 2023; Fu et al., 2023). Other works in this area, such as Least-to-Most (Zhou et al., 2023) and Complex CoT (Fu et al., 2023), have also explored this direction. An- other area of research is self-consistency-related approaches. In comparison to CoT-related work that focuses on design- ing better prompts, self-consistency proposes to sample mul- tiple answers from the LLMs and arrive at the correct answer through a majority vote (Fu et al., 2023). This approach is further improved upon by complex-based selection (Fu et al., 2023). CoT-related and self-consistency-related works can be seamlessly combined without any conflict. Prior research has not explored the potential of leveraging the outputs of LLM to refine reasoning paths iteratively. It stands to reason that similar to human cognition, LLM could benefit from reevaluating and adjusting its generated reasoning paths in order to correct errors and enhance over- all performance. In this paper, we propose a new method named Progressive-Hint Prompting (PHP) that involves se- quentially interacting with LLM to approach the correct an- swer gradually. The method operates as follows: (1) given a question, we ask the LLM to provide a Base Answer; (2) we combine the question and answer to re-ask the LLM and obtain the Subsequent Answer; (3) we repeat the operation in (2) until the answer is stable and does not change over the last two answers. PHP follows a human-like thought process where previous answers are leveraged as hints to arrive at the correct answer after re-evaluating the question. Figure 1 illustrates the proposed PHP framework. We use the base prompt to obtain the initial base answer, and then employ the PHP prompt for subsequent questions. If the cur- rent answer matches the previous answer, it is more likely to be correct, and we terminate the LLM inquiry. With Com- plex CoT and GPT-4, after adding PHP, the performance ach","Conclusion This paper introduces a novel approach named Progressive- Hint Prompting (PHP) for interacting with LLMs, which offers multiple advantages: 1) PHP achieves substantial per- formance improvements on math reasoning tasks, leading to state-of-the-art results on several reasoning benchmarks; 2) with more powerful models and prompts, PHP can better and consistently benefit the LLMs; 3) PHP can be easily combined with CoT and self-consistency to further improve performance. To better enhance the progressive-hint prompting approach, future research endeavors can focus on improving the de- sign of handcrafted hints in the question phase and prompt sentences in the answer part. Additionally, novel hints that aid the LLMs to reconsider the questions can be identified and extracted beside the answer. 8 Progressive-Hint Prompting Improves Reasoning in Large Language Models","Progressive-Hint Prompting Improves Reasoning in Large Language Models We propose a new prompting strategy, Progressive-Hint Promoting, that can be easily combined with Chain-Of-Thought and Self-Consistency to improve performance. The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that en- hance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic mul- tiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experi- ments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text- davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sam- ple paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% 91.9%), GSM8K (92% 95.5%), AQuA (76.4% 79.9%) and MATH (50.3% 53.9%). Conclusion This paper introduces a novel approach named Progressive- Hint Prompting (PHP) for interacting with LLMs, which offers multiple advantages: 1) PHP achieves substantial per- formance improvements on math reasoning tasks, leading to state-of-the-art results on several reasoning benchmarks; 2) with more powerful models and prompts, PHP can better and consistently benefit the LLMs; 3) PHP can be easily combined with CoT and self-consistency to further improve performance. To better enhance the progressive-hint prompting approach, future research endeavors can focus on improving the de- sign of handcrafted hints in the question phase and prompt sentences in the answer part. Additionally, novel hints that aid the LLMs to reconsider the questions can be identified and extracted beside the answer. 8 Progressive-Hint Prompting Improves Reasoning in Large Language Models approaches to promote intermediate reasoning steps (Wei et al., 2022; Zhou et al., 2023; Fu et al., 2023). Other works in this area, such as Least-to-Most (Zhou et al., 2023) and Complex CoT (Fu et al., 2023), have also explored this direction. An- other area of research is self-consistency-related approaches. In comparison to CoT-related work that focuses on design- ing better prompts, self-consistency proposes to sample mul- tiple answers from the LLMs and arrive at the correct answer through a majority vote (Fu et al., 2023). This approach is further improved upon by complex-based selection (Fu et al., 2023). CoT-related and self-consistency-related works can be seamlessly combined without any conflict. Prior research has not explored the potential of leveraging the outputs of LLM to refine reasoning paths iteratively. It stands to reason that similar to human cognition, LLM could benefit from reevaluating and adjusting its generated reasoning paths in order to correct errors and enhance over- all performance. In this paper, we propose a new method named Progressive-Hint Prompting (PHP) that involves se- quentially interacting with LLM to approach the correct an- swer gradually. The method operates as follows: (1) given a question, we ask the LLM to provide a Base Answer; (2) we combine the question and answer to re-ask the LLM and obtain the Subsequent Answer; (3) we repeat the operation in (2) until the answer is stable and does not change over the last two answers. PHP follows a human-like thought process where previous answers are leveraged as hints to arrive at the correct answer after re-evaluating the question. Figure 1 illustrates the proposed PHP framework. We use the base prompt to obtain the initial base answer, and then employ the PHP prompt for subsequent questions. If the cur- rent answer matches the previous answer, it is more likely to be correct, and we terminate the LLM inquiry. With Com- plex CoT and GPT-4, after adding PHP, the performance ach","Progressive-Hint Prompting Improves Reasoning in Large Language Models We propose a new prompting strategy, Progressive-Hint Promoting, that can be easily combined with Chain-Of-Thought and Self-Consistency to improve performance. The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that en- hance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic mul- tiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experi- ments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text- davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sam- ple paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% 91.9%), GSM8K (92% 95.5%), AQuA (76.4% 79.9%) and MATH (50.3% 53.9%).",0,"approaches
to promote intermediate reasoning steps (Wei et al., 2022;
Zhou et al., 2023; Fu et al., 2023). Other works in this area,
such as Least-to-Most (Zhou et al., 2023) and Complex
CoT (Fu et al., 2023), have also explored this direction. An-
other area of research is self-consistency-related approaches.
In comparison to CoT-related work that focuses on design-
ing better prompts, self-consistency proposes to sample mul-
tiple answers from the LLMs and arrive at the correct answer
through a majority vote (Fu et al., 2023). This approach
is further improved upon by complex-based selection (Fu
et al., 2023). CoT-related and self-consistency-related works
can be seamlessly combined without any conflict.
Prior research has not explored the potential of leveraging
the outputs of LLM to refine reasoning paths iteratively.
It stands to reason that similar to human cognition, LLM
could benefit from reevaluating and adjusting its generated
reasoning paths in order to correct errors and enhance over-
all performance. In this paper, we propose a new method
named Progressive-Hint Prompting (PHP) that involves se-
quentially interacting with LLM to approach the correct an-
swer gradually. The method operates as follows: (1) given
a question, we ask the LLM to provide a Base Answer; (2)
we combine the question and answer to re-ask the LLM and
obtain the Subsequent Answer; (3) we repeat the operation
in (2) until the answer is stable and does not change over
the last two answers. PHP follows a human-like thought
process where previous answers are leveraged as hints to
arrive at the correct answer after re-evaluating the question.
Figure 1 illustrates the proposed PHP framework. We use
the base prompt to obtain the initial base answer, and then
employ the PHP prompt for subsequent questions. If the cur-
rent answer matches the previous answer, it is more likely to
be correct, and we terminate the LLM inquiry. With Com-
plex CoT and GPT-4, after adding PHP, the performance
ach Conclusion
This paper introduces a novel approach named Progressive-
Hint Prompting (PHP) for interacting with LLMs, which
offers multiple advantages: 1) PHP achieves substantial per-
formance improvements on math reasoning tasks, leading
to state-of-the-art results on several reasoning benchmarks;
2) with more powerful models and prompts, PHP can better
and consistently benefit the LLMs; 3) PHP can be easily
combined with CoT and self-consistency to further improve
performance.
To better enhance the progressive-hint prompting approach,
future research endeavors can focus on improving the de-
sign of handcrafted hints in the question phase and prompt
sentences in the answer part. Additionally, novel hints that
aid the LLMs to reconsider the questions can be identified
and extracted beside the answer.
8
Progressive-Hint Prompting Improves Reasoning in Large Language Models
References
Asai, A., Hashimoto, K., Hajishirzi, H., Socher, R., and
Xiong, C. Learning to retrieve reasoning paths over
wikipedia graph for question answering. In International
Conference on Learning Representations , 2020. 2
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:
1877–1901, 2020. 2, 4
Chen, J., Lin, S.-t., and Durrett, G. Multi-hop ques-
tion answering via reasoning chains. arXiv preprint
arXiv:1910.02610 , 2019. 3
Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program
of thoughts prompting: Disentangling computation from
reasoning for numerical reasoning tasks. arXiv preprint
arXiv:2211.12588 , 2022. 7
Chowdhary, K. and Chowdhary, K. Natural language pro-
cessing. Fundamentals of artificial intelligence , pp. 603–
649, 2020. 1
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,
Gehrmann, S., et al. Palm: Scaling language modeling
with pathways. arXiv pr",False
2e116b08e4c0d273b422594f60a3c16662b3d034,VerityMath: Advancing Mathematical Reasoning by Self-Verification Through Unit Consistency,"['Vernon Toh Yan Han', 'Ratish Puduppully', 'Nancy F. Chen']",https://openreview.net/pdf/2e116b08e4c0d273b422594f60a3c16662b3d034.pdf,"VerityMath: Advancing Mathematical Reasoning by Self-Verification Through Unit Consistency Study evaluates 7B LLaMA, Code Llama and Mistral on math word problems, introduces ""Unit Consistency Programs"" for multi-unit challenges, and reports initial results with the enhanced ""VerityMath"" model. Large Language Models (LLMs), combined with program-based solving techniques, are increasingly demonstrating proficiency in mathematical reasoning. For example, closed-source models such as OpenAI GPT-4 and Claude show excellent results in solving math word problems. However, progress in math word problem-solving for open-source LLMs is limited, and the challenges these models face are not well-studied. In this paper, we study the performance of strong open-source LLMs, including Llama 2 (7B), Code Llama (7B), and Mistral (7B) on math word problems using program-based solving techniques. Specifically, we analyze the outputs of these models when applied to math word problems and identify a category of problems that pose a significant challenge, particularly those involving quantities spanning multiple units. To address this issue, we propose a systematic approach by defining the units for each quantity and ensuring the consistency of these units during mathematical operations. We developed Unit Consistency Programs (UCPs), an annotated dataset of math word problems, each paired with programs containing unit specifications and unit verification routines. We fine-tuned Llama 2 (7B), Code Llama (7B), and Mistral (7B) models with UCPs to produce their VerityMath variants. Our findings indicate that our approach, which incorporates unit consistency, currently slightly underperforms compared to an approach that does not. To understand the reasons behind this, we conducted an in-depth error analysis and suggested options for future improvements.",2e116b08e4c0d273b422594f60a3c16662b3d034.pdf,"Methodology
3.1. Unit Consistency Programs
Unit consistency checks are essential safeguards, helping
to identify and prevent errors from inconsistent units in
mathematical equations. In contrast to PAL/PoT approaches
that directly generate programs to solve math word prob-
lems, our method enhances these programs by integrating
specialized Counter objects. These objects are respon-
sible for tracking variable units and ensuring the correct
handling of operations with differing units. Additionally,
we incorporate assert statements after each equation, as
illustrated in Figure 1 (bottom). These assert statements
verify unit consistency within equations, triggering an error
if unit mismatches are detected.
Consider the example in Figure 1 (bottom), illustrating a
multiplication operation between shirts count (mea-
sured in ‘shirts’) and cost pershirt (measured in ‘dol-
lars per shirt’). In this operation, the units of ‘shirts’ fromPositive Predicted Negative Predicted
Actual Positive 37 16
Actual Negative 9 38
Precision Recall Accuracy
80.4% 69.8% 75.0%
Table 3: Small human evaluation compared on GPT-3.5
Turbo classification on 100 randomly sampled test examples
from GSM8K. Human annotations were done by the first
author.
shirts count and ‘per shirt’ from cost pershirt
naturally cancel each other out, resulting in a unit of ‘dol-
lars’. An assert statement is used to verify this expected
cancellation of units. In our notation, the exponent of a unit
in the numerator is represented as +1, and in the denominator
as -1. Therefore, in this multiplication, the positive exponent
of ‘shirts’ in shirts count cancels with the negative ex-
ponent of ‘per shirt’ in cost pershirt , aligning the
product’s right-hand side (RHS) with the expected left-hand
side (LHS) unit of total cost before discount ,
confirming it is in ‘dollars’. The example also illustrates
a unitless quantity, specifically a percentage. In this case,
there won’t be any units specified in the Counter initial-
iz","Future Work
In this study, we analyzed open-source Large Language
Models (LLMs) and pinpointed their struggle with math
problems involving multiple units, highlighting a key im-
provement area. We introduced Unit Consistency Programs(UCPs) as a novel method to address LLMs’ reasoning and
verification abilities, especially in complex math problems.
We identified some limitations in our current approach. Fu-
ture work will focus on advancing unit check methodologies
in UCPs to address these limitations.
Limitations
Recent creations of synthetic datasets for math problem-
solving often rely on prompting large language models
(LLMs), such as GPT-4. However, this approach can be
costly, and on a large scale, the expenses escalate. Our
dataset creation incurred a total cost of approximately $350
USD. Due to budget constraints, we couldn’t sample multi-
ple reasoning paths per question, as presented in Wang et al.
(2023), limiting the potential for increased annotations.
Impact Statement
This paper presents work whose goal is to advance the field
of math problem-solving using LLMs. However, it is also
crucial to be aware of the potential risks associated with
VerityMath. Due to the current challenges of VerityMath,
the units initialized by Counter andassert statements
may not always be accurate. Consequently, it is strongly rec-
ommended to exercise caution when relying on VerityMath
outputs for any use.
Acknowledgements
This research is supported by the Ministry of Education,
Singapore, under its Science of Learning Grant (award ID:
MOESOL2021-0006). Any opinions, findings and con-
clusions or recommendations expressed in this material
are those of the author(s) and do not reflect the views of
the Ministry of Education, Singapore. The computational
work for this article was partially performed on resources
of the National Supercomputing Centre (NSCC), Singapore
(https://www.nscc.sg ).
References
Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D.,
Passos, A., Shake","Methodology 3.1. Unit Consistency Programs Unit consistency checks are essential safeguards, helping to identify and prevent errors from inconsistent units in mathematical equations. In contrast to PAL/PoT approaches that directly generate programs to solve math word prob- lems, our method enhances these programs by integrating specialized Counter objects. These objects are respon- sible for tracking variable units and ensuring the correct handling of operations with differing units. Additionally, we incorporate assert statements after each equation, as illustrated in Figure 1 (bottom). These assert statements verify unit consistency within equations, triggering an error if unit mismatches are detected. Consider the example in Figure 1 (bottom), illustrating a multiplication operation between shirts count (mea- sured in shirts ) and cost pershirt (measured in dol- lars per shirt ). In this operation, the units of shirts fromPositive Predicted Negative Predicted Actual Positive 37 16 Actual Negative 9 38 Precision Recall Accuracy 80.4% 69.8% 75.0% Table 3: Small human evaluation compared on GPT-3.5 Turbo classification on 100 randomly sampled test examples from GSM8K. Human annotations were done by the first author. shirts count and per shirt from cost pershirt naturally cancel each other out, resulting in a unit of dol- lars . An assert statement is used to verify this expected cancellation of units. In our notation, the exponent of a unit in the numerator is represented as +1, and in the denominator as -1. Therefore, in this multiplication, the positive exponent of shirts in shirts count cancels with the negative ex- ponent of per shirt in cost pershirt , aligning the product s right-hand side (RHS) with the expected left-hand side (LHS) unit of total cost before discount , confirming it is in dollars . The example also illustrates a unitless quantity, specifically a percentage. In this case, there won t be any units specified in the Counter initial- iz","Future Work In this study, we analyzed open-source Large Language Models (LLMs) and pinpointed their struggle with math problems involving multiple units, highlighting a key im- provement area. We introduced Unit Consistency Programs(UCPs) as a novel method to address LLMs reasoning and verification abilities, especially in complex math problems. We identified some limitations in our current approach. Fu- ture work will focus on advancing unit check methodologies in UCPs to address these limitations. Limitations Recent creations of synthetic datasets for math problem- solving often rely on prompting large language models (LLMs), such as GPT-4. However, this approach can be costly, and on a large scale, the expenses escalate. Our dataset creation incurred a total cost of approximately $350 USD. Due to budget constraints, we couldn t sample multi- ple reasoning paths per question, as presented in Wang et al. (2023), limiting the potential for increased annotations. Impact Statement This paper presents work whose goal is to advance the field of math problem-solving using LLMs. However, it is also crucial to be aware of the potential risks associated with VerityMath. Due to the current challenges of VerityMath, the units initialized by Counter andassert statements may not always be accurate. Consequently, it is strongly rec- ommended to exercise caution when relying on VerityMath outputs for any use.","VerityMath: Advancing Mathematical Reasoning by Self-Verification Through Unit Consistency Study evaluates 7B LLaMA, Code Llama and Mistral on math word problems, introduces ""Unit Consistency Programs"" for multi-unit challenges, and reports initial results with the enhanced ""VerityMath"" model. Large Language Models (LLMs), combined with program-based solving techniques, are increasingly demonstrating proficiency in mathematical reasoning. For example, closed-source models such as OpenAI GPT-4 and Claude show excellent results in solving math word problems. However, progress in math word problem-solving for open-source LLMs is limited, and the challenges these models face are not well-studied. In this paper, we study the performance of strong open-source LLMs, including Llama 2 (7B), Code Llama (7B), and Mistral (7B) on math word problems using program-based solving techniques. Specifically, we analyze the outputs of these models when applied to math word problems and identify a category of problems that pose a significant challenge, particularly those involving quantities spanning multiple units. To address this issue, we propose a systematic approach by defining the units for each quantity and ensuring the consistency of these units during mathematical operations. We developed Unit Consistency Programs (UCPs), an annotated dataset of math word problems, each paired with programs containing unit specifications and unit verification routines. We fine-tuned Llama 2 (7B), Code Llama (7B), and Mistral (7B) models with UCPs to produce their VerityMath variants. Our findings indicate that our approach, which incorporates unit consistency, currently slightly underperforms compared to an approach that does not. To understand the reasons behind this, we conducted an in-depth error analysis and suggested options for future improvements. Future Work In this study, we analyzed open-source Large Language Models (LLMs) and pinpointed their struggle with math problems involving multiple units, highlighting a key im- provement area. We introduced Unit Consistency Programs(UCPs) as a novel method to address LLMs reasoning and verification abilities, especially in complex math problems. We identified some limitations in our current approach. Fu- ture work will focus on advancing unit check methodologies in UCPs to address these limitations. Limitations Recent creations of synthetic datasets for math problem- solving often rely on prompting large language models (LLMs), such as GPT-4. However, this approach can be costly, and on a large scale, the expenses escalate. Our dataset creation incurred a total cost of approximately $350 USD. Due to budget constraints, we couldn t sample multi- ple reasoning paths per question, as presented in Wang et al. (2023), limiting the potential for increased annotations. Impact Statement This paper presents work whose goal is to advance the field of math problem-solving using LLMs. However, it is also crucial to be aware of the potential risks associated with VerityMath. Due to the current challenges of VerityMath, the units initialized by Counter andassert statements may not always be accurate. Consequently, it is strongly rec- ommended to exercise caution when relying on VerityMath outputs for any use. Methodology 3.1. Unit Consistency Programs Unit consistency checks are essential safeguards, helping to identify and prevent errors from inconsistent units in mathematical equations. In contrast to PAL/PoT approaches that directly generate programs to solve math word prob- lems, our method enhances these programs by integrating specialized Counter objects. These objects are respon- sible for tracking variable units and ensuring the correct handling of operations with differing units. Additionally, we incorporate assert statements after each equation, as illustrated in Figure 1 (bottom). These assert statements verify unit consistency within equations, triggering an error if unit mismatches are detected. Consider the example in Figure 1 (bottom), illustrating a multiplication operation between shirts count (mea- sured in shirts ) and cost pershirt (measured in dol- lars per shirt ). In this operation, the units of shirts fromPositive Predicted Negative Predicted Actual Positive 37 16 Actual Negative 9 38 Precision Recall Accuracy 80.4% 69.8% 75.0% Table 3: Small human evaluation compared on GPT-3.5 Turbo classification on 100 randomly sampled test examples from GSM8K. Human annotations were done by the first author. shirts count and per shirt from cost pershirt naturally cancel each other out, resulting in a unit of dol- lars . An assert statement is used to verify this expected cancellation of units. In our notation, the exponent of a unit in the numerator is represented as +1, and in the denominator as -1. Therefore, in this multiplication, the positive exponent of shirts in shirts count cancels with the negative ex- ponent of per shirt in cost pershirt , aligning the product s right-hand side (RHS) with the expected left-hand side (LHS) unit of total cost before discount , confirming it is in dollars . The example also illustrates a unitless quantity, specifically a percentage. In this case, there won t be any units specified in the Counter initial- iz","VerityMath: Advancing Mathematical Reasoning by Self-Verification Through Unit Consistency Study evaluates 7B LLaMA, Code Llama and Mistral on math word problems, introduces ""Unit Consistency Programs"" for multi-unit challenges, and reports initial results with the enhanced ""VerityMath"" model. Large Language Models (LLMs), combined with program-based solving techniques, are increasingly demonstrating proficiency in mathematical reasoning. For example, closed-source models such as OpenAI GPT-4 and Claude show excellent results in solving math word problems. However, progress in math word problem-solving for open-source LLMs is limited, and the challenges these models face are not well-studied. In this paper, we study the performance of strong open-source LLMs, including Llama 2 (7B), Code Llama (7B), and Mistral (7B) on math word problems using program-based solving techniques. Specifically, we analyze the outputs of these models when applied to math word problems and identify a category of problems that pose a significant challenge, particularly those involving quantities spanning multiple units. To address this issue, we propose a systematic approach by defining the units for each quantity and ensuring the consistency of these units during mathematical operations. We developed Unit Consistency Programs (UCPs), an annotated dataset of math word problems, each paired with programs containing unit specifications and unit verification routines. We fine-tuned Llama 2 (7B), Code Llama (7B), and Mistral (7B) models with UCPs to produce their VerityMath variants. Our findings indicate that our approach, which incorporates unit consistency, currently slightly underperforms compared to an approach that does not. To understand the reasons behind this, we conducted an in-depth error analysis and suggested options for future improvements.",0,"Methodology
3.1. Unit Consistency Programs
Unit consistency checks are essential safeguards, helping
to identify and prevent errors from inconsistent units in
mathematical equations. In contrast to PAL/PoT approaches
that directly generate programs to solve math word prob-
lems, our method enhances these programs by integrating
specialized Counter objects. These objects are respon-
sible for tracking variable units and ensuring the correct
handling of operations with differing units. Additionally,
we incorporate assert statements after each equation, as
illustrated in Figure 1 (bottom). These assert statements
verify unit consistency within equations, triggering an error
if unit mismatches are detected.
Consider the example in Figure 1 (bottom), illustrating a
multiplication operation between shirts count (mea-
sured in ‘shirts’) and cost pershirt (measured in ‘dol-
lars per shirt’). In this operation, the units of ‘shirts’ fromPositive Predicted Negative Predicted
Actual Positive 37 16
Actual Negative 9 38
Precision Recall Accuracy
80.4% 69.8% 75.0%
Table 3: Small human evaluation compared on GPT-3.5
Turbo classification on 100 randomly sampled test examples
from GSM8K. Human annotations were done by the first
author.
shirts count and ‘per shirt’ from cost pershirt
naturally cancel each other out, resulting in a unit of ‘dol-
lars’. An assert statement is used to verify this expected
cancellation of units. In our notation, the exponent of a unit
in the numerator is represented as +1, and in the denominator
as -1. Therefore, in this multiplication, the positive exponent
of ‘shirts’ in shirts count cancels with the negative ex-
ponent of ‘per shirt’ in cost pershirt , aligning the
product’s right-hand side (RHS) with the expected left-hand
side (LHS) unit of total cost before discount ,
confirming it is in ‘dollars’. The example also illustrates
a unitless quantity, specifically a percentage. In this case,
there won’t be any units specified in the Counter initial-
iz Future Work
In this study, we analyzed open-source Large Language
Models (LLMs) and pinpointed their struggle with math
problems involving multiple units, highlighting a key im-
provement area. We introduced Unit Consistency Programs(UCPs) as a novel method to address LLMs’ reasoning and
verification abilities, especially in complex math problems.
We identified some limitations in our current approach. Fu-
ture work will focus on advancing unit check methodologies
in UCPs to address these limitations.
Limitations
Recent creations of synthetic datasets for math problem-
solving often rely on prompting large language models
(LLMs), such as GPT-4. However, this approach can be
costly, and on a large scale, the expenses escalate. Our
dataset creation incurred a total cost of approximately $350
USD. Due to budget constraints, we couldn’t sample multi-
ple reasoning paths per question, as presented in Wang et al.
(2023), limiting the potential for increased annotations.
Impact Statement
This paper presents work whose goal is to advance the field
of math problem-solving using LLMs. However, it is also
crucial to be aware of the potential risks associated with
VerityMath. Due to the current challenges of VerityMath,
the units initialized by Counter andassert statements
may not always be accurate. Consequently, it is strongly rec-
ommended to exercise caution when relying on VerityMath
outputs for any use.
Acknowledgements
This research is supported by the Ministry of Education,
Singapore, under its Science of Learning Grant (award ID:
MOESOL2021-0006). Any opinions, findings and con-
clusions or recommendations expressed in this material
are those of the author(s) and do not reflect the views of
the Ministry of Education, Singapore. The computational
work for this article was partially performed on resources
of the National Supercomputing Centre (NSCC), Singapore
(https://www.nscc.sg ).
References
Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D.,
Passos, A., Shake",False
e71382c2b2bdd9fa5eaeae740459879c5613a036,Smart Vision-Language Reasoners,"['Denisa Roberts', 'Lucas Roberts']",https://openreview.net/pdf/e71382c2b2bdd9fa5eaeae740459879c5613a036.pdf,"Smart Vision-Language Reasoners Deep learning innovations led to improvement in reasoning ability of vision-language models. In this article, we investigate vision-language models (VLM) as reasoners. The ability to form abstractions underlies mathematical reasoning, problem-solving, and other Math AI tasks. Several formalisms have been given to these underlying abstractions and skills utilized by humans and intelligent systems for reasoning. Furthermore, human reasoning is inherently multimodal, and as such, we focus our investigations on multimodal AI. In this article, we employ the abstractions given in the SMART task (Simple Multimodal Algorithmic Reasoning Task) introduced in \cite{cherian2022deep} as meta-reasoning and problem-solving skills along eight axes: math, counting, path, measure, logic, spatial, and pattern. We investigate the ability of vision-language models to reason along these axes and seek avenues of improvement. Including composite representations with vision-language cross-attention enabled learning multimodal representations adaptively from fused frozen pretrained backbones for better visual grounding. Furthermore, proper hyperparameter and other training choices led to strong improvements (up to gain in accuracy) on the SMART task, further underscoring the power of deep multimodal learning. The smartest VLM, which includes a novel QF multimodal layer, improves upon the best previous baselines in every one of the eight fundamental reasoning skills. End-to-end code is available at \href{https://github.com/D-Roberts/smarter}{github.com/D-Roberts/smarter}.",e71382c2b2bdd9fa5eaeae740459879c5613a036.pdf,"Methodology
We formalize the problem as supervised learning with classi-
fication loss. For each image-question instance, we predict
the probability of one of five answer options. When the
4
Smart Vision-Language Reasoners
Table 3. QF Ablations. Validation Accuracy per Skill Class (counting, math, logic, path) per Architectural, Optimization and Hyperparam-
eter Choices. The fused vision encoder is DinoV2+SigLIP and the text encoder is SigLIP. From CometML multimodalAI.
CHOICES COUNTING MATH LOGIC PATH
SMARTEST VLM 33.8 8.5 26.2 20.1
1 MHA HEADS 29.7 8.2 23.1 19.4
3 MHA HEADS 34.2 8.6 25.8 19.9
4 MHA HEADS 32.9 8.7 25.6 20.1
8 MHA HEADS 33.1 8.4 26.9 20.3
QF I NTERMEDIATE SIZE 128 33.1 8.6 25.6 19.8
QF I NTERMEDIATE SIZE 512 33.4 8.1 27.3 19.9
QF I NTERMEDIATE SIZE 768 33.4 8.7 25.1 19.3
QF I NTERMEDIATE RELU 32.8 8.7 26.7 19.8
QF I NTERMEDIATE SILU 33.2 8.7 26.5 19.6
COMPOSITE :NOQF LAYER 32.8 8.5 23.8 19.7
COMPOSITE : QF ONLY 32.2 8.0 26.3 18.8
COMPOSITE : QF AND VISION ONLY 33.7 8.7 24.8 20.4
COMPOSITE : QF AND LANGUAGE ONLY 33.6 8.9 24.9 19.4
NO RESIDUAL CONNECTION IN QF INTERMEDIATE 32 8.4 26.4 19
DROPOUT 0INQF LAYER 33 8.2 26.9 19.6
DROPOUT 0.1 INQF LAYER 30.3 8.1 25.5 19
Table 4. QF Layer Ablations. Validation Accuracy per Skill Class (algebra, measure, spatial, pattern) per Architectural, Optimization and
Hyperparameter Choices. The fused vision encoder is DinoV2+SigLIP and the text encoder is SigLIP. From CometML multimodalAI.
CHOICES ALGEBRA MEASURE SPATIAL PATTERN
SMARTEST VLM 11.2 10.4 26.8 27
1 MHA HEADS 10.5 10.8 23.2 22.7
3 MHA HEADS 11.1 11.3 26.8 27.0
4 MHA HEADS 11.2 10.6 27.8 26.6
8 MHA HEADS 11.6 10.4 27.9 25.8
QF I NTERMEDIATE SIZE 128 11.1 10.2 26.9 27.1
QF I NTERMEDIATE SIZE 512 11.3 10.4 27.8 26.4
QF I NTERMEDIATE SIZE 768 11.3 11.5 27 26.7
QF I NTERMEDIATE RELU 10.8 9.9 28.1 25.5
QF I NTERMEDIATE SILU 10.9 9.5 27.4 26
COMPOSITE :NOQF 11.5 10.3 25.6 25.4
COMPOSITE : QF ONLY 11.3 10.7 27.4 25.7
COMPOSITE : QF AND VISION ONLY 11.3 11.5 27.3 27.","future works. In a related vein, (Wu &
Xie, 2023) and (Tong et al., 2024), proffer the reasoning ca-pabilities of multimodal large language models and explores
their visual representation learning abilities.
2.1. Benchmark, Dataset, and Challenges
So how can we help (deep) artificial neural networks rea-
son better? In (Cherian et al., 2022) experiments show
that the visual signal is very important in solving complex
multi-reasoning skill puzzles and, despite being very large,
language-only models lag behind visual language models in
terms of performance. Conversely, in (Zhang et al., 2024b)
the conclusion appears to be that large multimodal models
cannot truly understand the visual diagrams for mathemati-
cal reasoning, along the line of weak visual grounding and
poor attention to visual detail in (Tong et al., 2024) and
(Wu & Xie, 2023) for large multimodal models for math,
question answering, and other reasoning tasks. The Simple
Multimodal Algorithmic Reasoning Task (SMART) intro-
duced in (Cherian et al., 2022) contains puzzles that measure
intelligence across eight different reasoning skill classes:
counting, math, logic, path, measure, logic, and pattern.
Problems include an image and a text question and are for-
mulated as multiple choice. We can see a few examples of
problems in Figure 2. Baseline models trained in (Cherian
et al., 2022) struggle to solve this task, especially when
employing transformers. In the past, specialized neural net-
works such as (Mikuła et al., 2023) have been developed to
solve specific reasoning tasks, specifically premise selection
in automated theorem proving. In this article, we investigate
how we can craft and train deep neural networks which em-
ploy several types of deep learning blocks and multimodal
inputs from deep frozen transformers to reason better across
the eight meta reasoning axes in the SMART task.
The SMART reasoning task and baselines . A set of vision-
language models are trained as benchmarks in (Cherian
et","Methodology We formalize the problem as supervised learning with classi- fication loss. For each image-question instance, we predict the probability of one of five answer options. When the 4 Smart Vision-Language Reasoners Table 3. QF Ablations. Validation Accuracy per Skill Class (counting, math, logic, path) per Architectural, Optimization and Hyperparam- eter Choices. The fused vision encoder is DinoV2+SigLIP and the text encoder is SigLIP. From CometML multimodalAI. CHOICES COUNTING MATH LOGIC PATH SMARTEST VLM 33.8 8.5 26.2 20.1 1 MHA HEADS 29.7 8.2 23.1 19.4 3 MHA HEADS 34.2 8.6 25.8 19.9 4 MHA HEADS 32.9 8.7 25.6 20.1 8 MHA HEADS 33.1 8.4 26.9 20.3 QF I NTERMEDIATE SIZE 128 33.1 8.6 25.6 19.8 QF I NTERMEDIATE SIZE 512 33.4 8.1 27.3 19.9 QF I NTERMEDIATE SIZE 768 33.4 8.7 25.1 19.3 QF I NTERMEDIATE RELU 32.8 8.7 26.7 19.8 QF I NTERMEDIATE SILU 33.2 8.7 26.5 19.6 COMPOSITE :NOQF LAYER 32.8 8.5 23.8 19.7 COMPOSITE : QF ONLY 32.2 8.0 26.3 18.8 COMPOSITE : QF AND VISION ONLY 33.7 8.7 24.8 20.4 COMPOSITE : QF AND LANGUAGE ONLY 33.6 8.9 24.9 19.4 NO RESIDUAL CONNECTION IN QF INTERMEDIATE 32 8.4 26.4 19 DROPOUT 0INQF LAYER 33 8.2 26.9 19.6 DROPOUT 0.1 INQF LAYER 30.3 8.1 25.5 19 Table 4. QF Layer Ablations. Validation Accuracy per Skill Class (algebra, measure, spatial, pattern) per Architectural, Optimization and Hyperparameter Choices. The fused vision encoder is DinoV2+SigLIP and the text encoder is SigLIP. From CometML multimodalAI. CHOICES ALGEBRA MEASURE SPATIAL PATTERN SMARTEST VLM 11.2 10.4 26.8 27 1 MHA HEADS 10.5 10.8 23.2 22.7 3 MHA HEADS 11.1 11.3 26.8 27.0 4 MHA HEADS 11.2 10.6 27.8 26.6 8 MHA HEADS 11.6 10.4 27.9 25.8 QF I NTERMEDIATE SIZE 128 11.1 10.2 26.9 27.1 QF I NTERMEDIATE SIZE 512 11.3 10.4 27.8 26.4 QF I NTERMEDIATE SIZE 768 11.3 11.5 27 26.7 QF I NTERMEDIATE RELU 10.8 9.9 28.1 25.5 QF I NTERMEDIATE SILU 10.9 9.5 27.4 26 COMPOSITE :NOQF 11.5 10.3 25.6 25.4 COMPOSITE : QF ONLY 11.3 10.7 27.4 25.7 COMPOSITE : QF AND VISION ONLY 11.3 11.5 27.3 27.","future works. In a related vein, (Wu & Xie, 2023) and (Tong et al., 2024), proffer the reasoning ca-pabilities of multimodal large language models and explores their visual representation learning abilities. 2.1. Benchmark, Dataset, and Challenges So how can we help (deep) artificial neural networks rea- son better? In (Cherian et al., 2022) experiments show that the visual signal is very important in solving complex multi-reasoning skill puzzles and, despite being very large, language-only models lag behind visual language models in terms of performance. Conversely, in (Zhang et al., 2024b) the conclusion appears to be that large multimodal models cannot truly understand the visual diagrams for mathemati- cal reasoning, along the line of weak visual grounding and poor attention to visual detail in (Tong et al., 2024) and (Wu & Xie, 2023) for large multimodal models for math, question answering, and other reasoning tasks. The Simple Multimodal Algorithmic Reasoning Task (SMART) intro- duced in (Cherian et al., 2022) contains puzzles that measure intelligence across eight different reasoning skill classes: counting, math, logic, path, measure, logic, and pattern. Problems include an image and a text question and are for- mulated as multiple choice. We can see a few examples of problems in Figure 2. Baseline models trained in (Cherian et al., 2022) struggle to solve this task, especially when employing transformers. In the past, specialized neural net- works such as (Miku a et al., 2023) have been developed to solve specific reasoning tasks, specifically premise selection in automated theorem proving. In this article, we investigate how we can craft and train deep neural networks which em- ploy several types of deep learning blocks and multimodal inputs from deep frozen transformers to reason better across the eight meta reasoning axes in the SMART task. The SMART reasoning task and baselines . A set of vision- language models are trained as benchmarks in (Cherian et","Smart Vision-Language Reasoners Deep learning innovations led to improvement in reasoning ability of vision-language models. In this article, we investigate vision-language models (VLM) as reasoners. The ability to form abstractions underlies mathematical reasoning, problem-solving, and other Math AI tasks. Several formalisms have been given to these underlying abstractions and skills utilized by humans and intelligent systems for reasoning. Furthermore, human reasoning is inherently multimodal, and as such, we focus our investigations on multimodal AI. In this article, we employ the abstractions given in the SMART task (Simple Multimodal Algorithmic Reasoning Task) introduced in \cite{cherian2022deep} as meta-reasoning and problem-solving skills along eight axes: math, counting, path, measure, logic, spatial, and pattern. We investigate the ability of vision-language models to reason along these axes and seek avenues of improvement. Including composite representations with vision-language cross-attention enabled learning multimodal representations adaptively from fused frozen pretrained backbones for better visual grounding. Furthermore, proper hyperparameter and other training choices led to strong improvements (up to gain in accuracy) on the SMART task, further underscoring the power of deep multimodal learning. The smartest VLM, which includes a novel QF multimodal layer, improves upon the best previous baselines in every one of the eight fundamental reasoning skills. End-to-end code is available at \href{https://github.com/D-Roberts/smarter}{github.com/D-Roberts/smarter}. future works. In a related vein, (Wu & Xie, 2023) and (Tong et al., 2024), proffer the reasoning ca-pabilities of multimodal large language models and explores their visual representation learning abilities. 2.1. Benchmark, Dataset, and Challenges So how can we help (deep) artificial neural networks rea- son better? In (Cherian et al., 2022) experiments show that the visual signal is very important in solving complex multi-reasoning skill puzzles and, despite being very large, language-only models lag behind visual language models in terms of performance. Conversely, in (Zhang et al., 2024b) the conclusion appears to be that large multimodal models cannot truly understand the visual diagrams for mathemati- cal reasoning, along the line of weak visual grounding and poor attention to visual detail in (Tong et al., 2024) and (Wu & Xie, 2023) for large multimodal models for math, question answering, and other reasoning tasks. The Simple Multimodal Algorithmic Reasoning Task (SMART) intro- duced in (Cherian et al., 2022) contains puzzles that measure intelligence across eight different reasoning skill classes: counting, math, logic, path, measure, logic, and pattern. Problems include an image and a text question and are for- mulated as multiple choice. We can see a few examples of problems in Figure 2. Baseline models trained in (Cherian et al., 2022) struggle to solve this task, especially when employing transformers. In the past, specialized neural net- works such as (Miku a et al., 2023) have been developed to solve specific reasoning tasks, specifically premise selection in automated theorem proving. In this article, we investigate how we can craft and train deep neural networks which em- ploy several types of deep learning blocks and multimodal inputs from deep frozen transformers to reason better across the eight meta reasoning axes in the SMART task. The SMART reasoning task and baselines . A set of vision- language models are trained as benchmarks in (Cherian et Methodology We formalize the problem as supervised learning with classi- fication loss. For each image-question instance, we predict the probability of one of five answer options. When the 4 Smart Vision-Language Reasoners Table 3. QF Ablations. Validation Accuracy per Skill Class (counting, math, logic, path) per Architectural, Optimization and Hyperparam- eter Choices. The fused vision encoder is DinoV2+SigLIP and the text encoder is SigLIP. From CometML multimodalAI. CHOICES COUNTING MATH LOGIC PATH SMARTEST VLM 33.8 8.5 26.2 20.1 1 MHA HEADS 29.7 8.2 23.1 19.4 3 MHA HEADS 34.2 8.6 25.8 19.9 4 MHA HEADS 32.9 8.7 25.6 20.1 8 MHA HEADS 33.1 8.4 26.9 20.3 QF I NTERMEDIATE SIZE 128 33.1 8.6 25.6 19.8 QF I NTERMEDIATE SIZE 512 33.4 8.1 27.3 19.9 QF I NTERMEDIATE SIZE 768 33.4 8.7 25.1 19.3 QF I NTERMEDIATE RELU 32.8 8.7 26.7 19.8 QF I NTERMEDIATE SILU 33.2 8.7 26.5 19.6 COMPOSITE :NOQF LAYER 32.8 8.5 23.8 19.7 COMPOSITE : QF ONLY 32.2 8.0 26.3 18.8 COMPOSITE : QF AND VISION ONLY 33.7 8.7 24.8 20.4 COMPOSITE : QF AND LANGUAGE ONLY 33.6 8.9 24.9 19.4 NO RESIDUAL CONNECTION IN QF INTERMEDIATE 32 8.4 26.4 19 DROPOUT 0INQF LAYER 33 8.2 26.9 19.6 DROPOUT 0.1 INQF LAYER 30.3 8.1 25.5 19 Table 4. QF Layer Ablations. Validation Accuracy per Skill Class (algebra, measure, spatial, pattern) per Architectural, Optimization and Hyperparameter Choices. The fused vision encoder is DinoV2+SigLIP and the text encoder is SigLIP. From CometML multimodalAI. CHOICES ALGEBRA MEASURE SPATIAL PATTERN SMARTEST VLM 11.2 10.4 26.8 27 1 MHA HEADS 10.5 10.8 23.2 22.7 3 MHA HEADS 11.1 11.3 26.8 27.0 4 MHA HEADS 11.2 10.6 27.8 26.6 8 MHA HEADS 11.6 10.4 27.9 25.8 QF I NTERMEDIATE SIZE 128 11.1 10.2 26.9 27.1 QF I NTERMEDIATE SIZE 512 11.3 10.4 27.8 26.4 QF I NTERMEDIATE SIZE 768 11.3 11.5 27 26.7 QF I NTERMEDIATE RELU 10.8 9.9 28.1 25.5 QF I NTERMEDIATE SILU 10.9 9.5 27.4 26 COMPOSITE :NOQF 11.5 10.3 25.6 25.4 COMPOSITE : QF ONLY 11.3 10.7 27.4 25.7 COMPOSITE : QF AND VISION ONLY 11.3 11.5 27.3 27.","Smart Vision-Language Reasoners Deep learning innovations led to improvement in reasoning ability of vision-language models. In this article, we investigate vision-language models (VLM) as reasoners. The ability to form abstractions underlies mathematical reasoning, problem-solving, and other Math AI tasks. Several formalisms have been given to these underlying abstractions and skills utilized by humans and intelligent systems for reasoning. Furthermore, human reasoning is inherently multimodal, and as such, we focus our investigations on multimodal AI. In this article, we employ the abstractions given in the SMART task (Simple Multimodal Algorithmic Reasoning Task) introduced in \cite{cherian2022deep} as meta-reasoning and problem-solving skills along eight axes: math, counting, path, measure, logic, spatial, and pattern. We investigate the ability of vision-language models to reason along these axes and seek avenues of improvement. Including composite representations with vision-language cross-attention enabled learning multimodal representations adaptively from fused frozen pretrained backbones for better visual grounding. Furthermore, proper hyperparameter and other training choices led to strong improvements (up to gain in accuracy) on the SMART task, further underscoring the power of deep multimodal learning. The smartest VLM, which includes a novel QF multimodal layer, improves upon the best previous baselines in every one of the eight fundamental reasoning skills. End-to-end code is available at \href{https://github.com/D-Roberts/smarter}{github.com/D-Roberts/smarter}.",0,"Methodology
We formalize the problem as supervised learning with classi-
fication loss. For each image-question instance, we predict
the probability of one of five answer options. When the
4
Smart Vision-Language Reasoners
Table 3. QF Ablations. Validation Accuracy per Skill Class (counting, math, logic, path) per Architectural, Optimization and Hyperparam-
eter Choices. The fused vision encoder is DinoV2+SigLIP and the text encoder is SigLIP. From CometML multimodalAI.
CHOICES COUNTING MATH LOGIC PATH
SMARTEST VLM 33.8 8.5 26.2 20.1
1 MHA HEADS 29.7 8.2 23.1 19.4
3 MHA HEADS 34.2 8.6 25.8 19.9
4 MHA HEADS 32.9 8.7 25.6 20.1
8 MHA HEADS 33.1 8.4 26.9 20.3
QF I NTERMEDIATE SIZE 128 33.1 8.6 25.6 19.8
QF I NTERMEDIATE SIZE 512 33.4 8.1 27.3 19.9
QF I NTERMEDIATE SIZE 768 33.4 8.7 25.1 19.3
QF I NTERMEDIATE RELU 32.8 8.7 26.7 19.8
QF I NTERMEDIATE SILU 33.2 8.7 26.5 19.6
COMPOSITE :NOQF LAYER 32.8 8.5 23.8 19.7
COMPOSITE : QF ONLY 32.2 8.0 26.3 18.8
COMPOSITE : QF AND VISION ONLY 33.7 8.7 24.8 20.4
COMPOSITE : QF AND LANGUAGE ONLY 33.6 8.9 24.9 19.4
NO RESIDUAL CONNECTION IN QF INTERMEDIATE 32 8.4 26.4 19
DROPOUT 0INQF LAYER 33 8.2 26.9 19.6
DROPOUT 0.1 INQF LAYER 30.3 8.1 25.5 19
Table 4. QF Layer Ablations. Validation Accuracy per Skill Class (algebra, measure, spatial, pattern) per Architectural, Optimization and
Hyperparameter Choices. The fused vision encoder is DinoV2+SigLIP and the text encoder is SigLIP. From CometML multimodalAI.
CHOICES ALGEBRA MEASURE SPATIAL PATTERN
SMARTEST VLM 11.2 10.4 26.8 27
1 MHA HEADS 10.5 10.8 23.2 22.7
3 MHA HEADS 11.1 11.3 26.8 27.0
4 MHA HEADS 11.2 10.6 27.8 26.6
8 MHA HEADS 11.6 10.4 27.9 25.8
QF I NTERMEDIATE SIZE 128 11.1 10.2 26.9 27.1
QF I NTERMEDIATE SIZE 512 11.3 10.4 27.8 26.4
QF I NTERMEDIATE SIZE 768 11.3 11.5 27 26.7
QF I NTERMEDIATE RELU 10.8 9.9 28.1 25.5
QF I NTERMEDIATE SILU 10.9 9.5 27.4 26
COMPOSITE :NOQF 11.5 10.3 25.6 25.4
COMPOSITE : QF ONLY 11.3 10.7 27.4 25.7
COMPOSITE : QF AND VISION ONLY 11.3 11.5 27.3 27. future works. In a related vein, (Wu &
Xie, 2023) and (Tong et al., 2024), proffer the reasoning ca-pabilities of multimodal large language models and explores
their visual representation learning abilities.
2.1. Benchmark, Dataset, and Challenges
So how can we help (deep) artificial neural networks rea-
son better? In (Cherian et al., 2022) experiments show
that the visual signal is very important in solving complex
multi-reasoning skill puzzles and, despite being very large,
language-only models lag behind visual language models in
terms of performance. Conversely, in (Zhang et al., 2024b)
the conclusion appears to be that large multimodal models
cannot truly understand the visual diagrams for mathemati-
cal reasoning, along the line of weak visual grounding and
poor attention to visual detail in (Tong et al., 2024) and
(Wu & Xie, 2023) for large multimodal models for math,
question answering, and other reasoning tasks. The Simple
Multimodal Algorithmic Reasoning Task (SMART) intro-
duced in (Cherian et al., 2022) contains puzzles that measure
intelligence across eight different reasoning skill classes:
counting, math, logic, path, measure, logic, and pattern.
Problems include an image and a text question and are for-
mulated as multiple choice. We can see a few examples of
problems in Figure 2. Baseline models trained in (Cherian
et al., 2022) struggle to solve this task, especially when
employing transformers. In the past, specialized neural net-
works such as (Mikuła et al., 2023) have been developed to
solve specific reasoning tasks, specifically premise selection
in automated theorem proving. In this article, we investigate
how we can craft and train deep neural networks which em-
ploy several types of deep learning blocks and multimodal
inputs from deep frozen transformers to reason better across
the eight meta reasoning axes in the SMART task.
The SMART reasoning task and baselines . A set of vision-
language models are trained as benchmarks in (Cherian
et",False
2fb1e8e4e7c044ab018b903eb62e5ffdb089546a,Progress or Regress? Self-Improvement Reversal in Post-training,"['Ting Wu', 'Xuefeng Li', 'Pengfei Liu']",https://openreview.net/pdf/2fb1e8e4e7c044ab018b903eb62e5ffdb089546a.pdf,"Progress or Regress? Self-Improvement Reversal in Post-training A comprehensive evaluative framework to scrutinize the underlying mechanisms and outcomes of post-training self-improvement. Self-improvement through post-training methods such as iterative preference learning has been acclaimed for enhancing the problem-solving capabilities~(e.g., mathematical reasoning) of Large Language Models~(LLMs) without human intervention. However, as exploration deepens, it becomes crucial to assess whether these improvements genuinely signify progress in solving more challenging problems or if they could lead to unintended regressions. To address this, we propose a comprehensive evaluative framework that goes beyond the superficial pass@1 metric to scrutinize the underlying enhancements of post-training paradigms for self-improvement. Through rigorous experimentation and analysis across diverse problem-solving tasks, the empirical results point out the phenomenon of \emph{self-improvement reversal}, where models showing improved performance across benchmarks will paradoxically exhibit declines in broader, essential capabilities, like output diversity and out-of-distribution~(OOD) generalization. These findings indicate that current self-improvement practices through post-training are inadequate for equipping models to tackle more complex problems. Furthermore, they underscore the necessity of our critical evaluation metrics in discerning the \emph{progress or regress} dichotomy for self-improving LLMs.",2fb1e8e4e7c044ab018b903eb62e5ffdb089546a.pdf,"methodology to CSQA, GSM8K,
MATH and MBPP datasets with three post-training meth-
ods. Generation sampling Nvaries from 21to26with the
temperature set as 0.75.
Reversal Observation As depicted in Figure 3, contrary
to prior assumptions, the rapid increase in pass@N accu-
racy with increasing Nchallenges the notion of progres-
sively harder problem-solving. Specifically, as Ngrows,
M1achieves near-perfect pass@N accuracy on IS(t) across
all evaluated datasets, suggesting its inherent capacity to
tackle the deemed improvement problems .
Selection Optimization for Answer Alignment The em-
pirical findings depicted in Figure 3 offer a critical insight:
iterative self-improvement hardly entails the acquisition of
new problem-solving abilities, but rather the enhancement
of the model’s correct answer selection within its generation
space.
7
Progress or Regress? Self-Improvement Reversal in Post-training
5.2. Solutions Diversity
While pass@1 accuracy measures the correctness of the
final answer, it does not capture the diversity of solutions
a model can generate. We posit that a model’s capacity
to produce diverse solutions is indicative of its robustness
and flexibility in problem-solving. To thoroughly under-
stand the evolution of answer diversity during the process
of iterative self-improvement, we employ a combination of
Distinct N-grams (Li et al., 2016) and Sentence-BERT
embedding cosine similarity (Reimers & Gurevych, 2019)
to measure mod diversity. These metrics have been shown
to correlate well with human assessments of diversity (Tevet
& Berant, 2021). Additionally, for mathematical reasoning,
we introduce Distinct Equations to measure the diversity of
mathematical answers by analyzing the variety of equations
in the generated solutions.
Each diversity metric Div takes a set of Nmodel outputs,
and produces a scalar score representing how diverse the
set is. Distinct N-grams measures syntactic diversity by
counting the number of unique n-grams (averaged over
n= 1","Conclusion
In this paper, we foster a comprehensive understanding
of the current landscape of post-training practices in self-
improvement. Our evaluation, beyond simple pass@1 ac-
curacy, utilizing multifaceted metrics such as improvementproblems, solutions diversity and OOD generalization, un-
derscores the necessity for a critical examination of both the
progressive and regressive effects in current self-improving
post-training methods. By broadening the scope of our
analysis, we provide deeper insights into the true nature of
iterative self-improvement with post-training, paving the
way for more robust and genuinely self-improving LLMs.
Impact Statement
The current landscape of post-training practices for self-
improvement in large language models (LLMs) necessitates
a thorough understanding to address both their progressive
and regressive effects. In this work, we conduct an in-depth
evaluation beyond simple pass@1 accuracy, utilizing mul-
tifaceted metrics such as improvement problems, solutions
diversity, and OOD generalization. Our findings under-
score the critical need for a comprehensive examination
of these methods. By expanding our analytical scope, we
provide deeper insights into the true nature of iterative self-
improvement through post-training, paving the way for more
robust and genuinely self-improving LLMs. This research
aims to empower scholars and engineers to develop more
reliable and effective post-training strategies, ultimately ad-
vancing the field of LLMs and pushing the boundaries of
model capabilities to tackle more complex challenges.
References
AI@Meta. Introducing meta llama 3: The most capable
openly available llm to date. 2024. URL https://ai.
meta.com/blog/meta-llama-3/ .
Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski,
H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.
Program synthesis with large language models. arXiv
preprint arXiv:2108.07732 , 2021.
Chen, Z., Deng, Y ., Yuan, H., Ji, K., and Gu, Q. Self-
play","methodology to CSQA, GSM8K, MATH and MBPP datasets with three post-training meth- ods. Generation sampling Nvaries from 21to26with the temperature set as 0.75. Reversal Observation As depicted in Figure 3, contrary to prior assumptions, the rapid increase in pass@N accu- racy with increasing Nchallenges the notion of progres- sively harder problem-solving. Specifically, as Ngrows, M1achieves near-perfect pass@N accuracy on IS(t) across all evaluated datasets, suggesting its inherent capacity to tackle the deemed improvement problems . Selection Optimization for Answer Alignment The em- pirical findings depicted in Figure 3 offer a critical insight: iterative self-improvement hardly entails the acquisition of new problem-solving abilities, but rather the enhancement of the model s correct answer selection within its generation space. 7 Progress or Regress? Self-Improvement Reversal in Post-training 5.2. Solutions Diversity While pass@1 accuracy measures the correctness of the final answer, it does not capture the diversity of solutions a model can generate. We posit that a model s capacity to produce diverse solutions is indicative of its robustness and flexibility in problem-solving. To thoroughly under- stand the evolution of answer diversity during the process of iterative self-improvement, we employ a combination of Distinct N-grams (Li et al., 2016) and Sentence-BERT embedding cosine similarity (Reimers & Gurevych, 2019) to measure mod diversity. These metrics have been shown to correlate well with human assessments of diversity (Tevet & Berant, 2021). Additionally, for mathematical reasoning, we introduce Distinct Equations to measure the diversity of mathematical answers by analyzing the variety of equations in the generated solutions. Each diversity metric Div takes a set of Nmodel outputs, and produces a scalar score representing how diverse the set is. Distinct N-grams measures syntactic diversity by counting the number of unique n-grams (averaged over n= 1","Conclusion In this paper, we foster a comprehensive understanding of the current landscape of post-training practices in self- improvement. Our evaluation, beyond simple pass@1 ac- curacy, utilizing multifaceted metrics such as improvementproblems, solutions diversity and OOD generalization, un- derscores the necessity for a critical examination of both the progressive and regressive effects in current self-improving post-training methods. By broadening the scope of our analysis, we provide deeper insights into the true nature of iterative self-improvement with post-training, paving the way for more robust and genuinely self-improving LLMs. Impact Statement The current landscape of post-training practices for self- improvement in large language models (LLMs) necessitates a thorough understanding to address both their progressive and regressive effects. In this work, we conduct an in-depth evaluation beyond simple pass@1 accuracy, utilizing mul- tifaceted metrics such as improvement problems, solutions diversity, and OOD generalization. Our findings under- score the critical need for a comprehensive examination of these methods. By expanding our analytical scope, we provide deeper insights into the true nature of iterative self- improvement through post-training, paving the way for more robust and genuinely self-improving LLMs. This research aims to empower scholars and engineers to develop more reliable and effective post-training strategies, ultimately ad- vancing the field of LLMs and pushing the boundaries of model capabilities to tackle more complex challenges.","Progress or Regress? Self-Improvement Reversal in Post-training A comprehensive evaluative framework to scrutinize the underlying mechanisms and outcomes of post-training self-improvement. Self-improvement through post-training methods such as iterative preference learning has been acclaimed for enhancing the problem-solving capabilities~(e.g., mathematical reasoning) of Large Language Models~(LLMs) without human intervention. However, as exploration deepens, it becomes crucial to assess whether these improvements genuinely signify progress in solving more challenging problems or if they could lead to unintended regressions. To address this, we propose a comprehensive evaluative framework that goes beyond the superficial pass@1 metric to scrutinize the underlying enhancements of post-training paradigms for self-improvement. Through rigorous experimentation and analysis across diverse problem-solving tasks, the empirical results point out the phenomenon of \emph{self-improvement reversal}, where models showing improved performance across benchmarks will paradoxically exhibit declines in broader, essential capabilities, like output diversity and out-of-distribution~(OOD) generalization. These findings indicate that current self-improvement practices through post-training are inadequate for equipping models to tackle more complex problems. Furthermore, they underscore the necessity of our critical evaluation metrics in discerning the \emph{progress or regress} dichotomy for self-improving LLMs. Conclusion In this paper, we foster a comprehensive understanding of the current landscape of post-training practices in self- improvement. Our evaluation, beyond simple pass@1 ac- curacy, utilizing multifaceted metrics such as improvementproblems, solutions diversity and OOD generalization, un- derscores the necessity for a critical examination of both the progressive and regressive effects in current self-improving post-training methods. By broadening the scope of our analysis, we provide deeper insights into the true nature of iterative self-improvement with post-training, paving the way for more robust and genuinely self-improving LLMs. Impact Statement The current landscape of post-training practices for self- improvement in large language models (LLMs) necessitates a thorough understanding to address both their progressive and regressive effects. In this work, we conduct an in-depth evaluation beyond simple pass@1 accuracy, utilizing mul- tifaceted metrics such as improvement problems, solutions diversity, and OOD generalization. Our findings under- score the critical need for a comprehensive examination of these methods. By expanding our analytical scope, we provide deeper insights into the true nature of iterative self- improvement through post-training, paving the way for more robust and genuinely self-improving LLMs. This research aims to empower scholars and engineers to develop more reliable and effective post-training strategies, ultimately ad- vancing the field of LLMs and pushing the boundaries of model capabilities to tackle more complex challenges. methodology to CSQA, GSM8K, MATH and MBPP datasets with three post-training meth- ods. Generation sampling Nvaries from 21to26with the temperature set as 0.75. Reversal Observation As depicted in Figure 3, contrary to prior assumptions, the rapid increase in pass@N accu- racy with increasing Nchallenges the notion of progres- sively harder problem-solving. Specifically, as Ngrows, M1achieves near-perfect pass@N accuracy on IS(t) across all evaluated datasets, suggesting its inherent capacity to tackle the deemed improvement problems . Selection Optimization for Answer Alignment The em- pirical findings depicted in Figure 3 offer a critical insight: iterative self-improvement hardly entails the acquisition of new problem-solving abilities, but rather the enhancement of the model s correct answer selection within its generation space. 7 Progress or Regress? Self-Improvement Reversal in Post-training 5.2. Solutions Diversity While pass@1 accuracy measures the correctness of the final answer, it does not capture the diversity of solutions a model can generate. We posit that a model s capacity to produce diverse solutions is indicative of its robustness and flexibility in problem-solving. To thoroughly under- stand the evolution of answer diversity during the process of iterative self-improvement, we employ a combination of Distinct N-grams (Li et al., 2016) and Sentence-BERT embedding cosine similarity (Reimers & Gurevych, 2019) to measure mod diversity. These metrics have been shown to correlate well with human assessments of diversity (Tevet & Berant, 2021). Additionally, for mathematical reasoning, we introduce Distinct Equations to measure the diversity of mathematical answers by analyzing the variety of equations in the generated solutions. Each diversity metric Div takes a set of Nmodel outputs, and produces a scalar score representing how diverse the set is. Distinct N-grams measures syntactic diversity by counting the number of unique n-grams (averaged over n= 1","Progress or Regress? Self-Improvement Reversal in Post-training A comprehensive evaluative framework to scrutinize the underlying mechanisms and outcomes of post-training self-improvement. Self-improvement through post-training methods such as iterative preference learning has been acclaimed for enhancing the problem-solving capabilities~(e.g., mathematical reasoning) of Large Language Models~(LLMs) without human intervention. However, as exploration deepens, it becomes crucial to assess whether these improvements genuinely signify progress in solving more challenging problems or if they could lead to unintended regressions. To address this, we propose a comprehensive evaluative framework that goes beyond the superficial pass@1 metric to scrutinize the underlying enhancements of post-training paradigms for self-improvement. Through rigorous experimentation and analysis across diverse problem-solving tasks, the empirical results point out the phenomenon of \emph{self-improvement reversal}, where models showing improved performance across benchmarks will paradoxically exhibit declines in broader, essential capabilities, like output diversity and out-of-distribution~(OOD) generalization. These findings indicate that current self-improvement practices through post-training are inadequate for equipping models to tackle more complex problems. Furthermore, they underscore the necessity of our critical evaluation metrics in discerning the \emph{progress or regress} dichotomy for self-improving LLMs.",0,"methodology to CSQA, GSM8K,
MATH and MBPP datasets with three post-training meth-
ods. Generation sampling Nvaries from 21to26with the
temperature set as 0.75.
Reversal Observation As depicted in Figure 3, contrary
to prior assumptions, the rapid increase in pass@N accu-
racy with increasing Nchallenges the notion of progres-
sively harder problem-solving. Specifically, as Ngrows,
M1achieves near-perfect pass@N accuracy on IS(t) across
all evaluated datasets, suggesting its inherent capacity to
tackle the deemed improvement problems .
Selection Optimization for Answer Alignment The em-
pirical findings depicted in Figure 3 offer a critical insight:
iterative self-improvement hardly entails the acquisition of
new problem-solving abilities, but rather the enhancement
of the model’s correct answer selection within its generation
space.
7
Progress or Regress? Self-Improvement Reversal in Post-training
5.2. Solutions Diversity
While pass@1 accuracy measures the correctness of the
final answer, it does not capture the diversity of solutions
a model can generate. We posit that a model’s capacity
to produce diverse solutions is indicative of its robustness
and flexibility in problem-solving. To thoroughly under-
stand the evolution of answer diversity during the process
of iterative self-improvement, we employ a combination of
Distinct N-grams (Li et al., 2016) and Sentence-BERT
embedding cosine similarity (Reimers & Gurevych, 2019)
to measure mod diversity. These metrics have been shown
to correlate well with human assessments of diversity (Tevet
& Berant, 2021). Additionally, for mathematical reasoning,
we introduce Distinct Equations to measure the diversity of
mathematical answers by analyzing the variety of equations
in the generated solutions.
Each diversity metric Div takes a set of Nmodel outputs,
and produces a scalar score representing how diverse the
set is. Distinct N-grams measures syntactic diversity by
counting the number of unique n-grams (averaged over
n= 1 Conclusion
In this paper, we foster a comprehensive understanding
of the current landscape of post-training practices in self-
improvement. Our evaluation, beyond simple pass@1 ac-
curacy, utilizing multifaceted metrics such as improvementproblems, solutions diversity and OOD generalization, un-
derscores the necessity for a critical examination of both the
progressive and regressive effects in current self-improving
post-training methods. By broadening the scope of our
analysis, we provide deeper insights into the true nature of
iterative self-improvement with post-training, paving the
way for more robust and genuinely self-improving LLMs.
Impact Statement
The current landscape of post-training practices for self-
improvement in large language models (LLMs) necessitates
a thorough understanding to address both their progressive
and regressive effects. In this work, we conduct an in-depth
evaluation beyond simple pass@1 accuracy, utilizing mul-
tifaceted metrics such as improvement problems, solutions
diversity, and OOD generalization. Our findings under-
score the critical need for a comprehensive examination
of these methods. By expanding our analytical scope, we
provide deeper insights into the true nature of iterative self-
improvement through post-training, paving the way for more
robust and genuinely self-improving LLMs. This research
aims to empower scholars and engineers to develop more
reliable and effective post-training strategies, ultimately ad-
vancing the field of LLMs and pushing the boundaries of
model capabilities to tackle more complex challenges.
References
AI@Meta. Introducing meta llama 3: The most capable
openly available llm to date. 2024. URL https://ai.
meta.com/blog/meta-llama-3/ .
Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski,
H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.
Program synthesis with large language models. arXiv
preprint arXiv:2108.07732 , 2021.
Chen, Z., Deng, Y ., Yuan, H., Ji, K., and Gu, Q. Self-
play",False
4c53285eb3ec1484b1b90f4da18df757f653be53,Pre-Calc: Learning to Use the Calculator Improves Numeracy in Language Models,"['Vishruth Veerendranath', 'Vishwa Shah', 'Kshitish Ghate']",https://openreview.net/pdf/4c53285eb3ec1484b1b90f4da18df757f653be53.pdf,"Pre-Calc: Learning to Use the Calculator Improves Numeracy in Language Models We propose Pre-Calc, our method to teach smaller language models how to use calculators. By pre-finetuning BERT, RoBERTa, and Flan-T5 on calculator use tasks, we improved these models' performance on tasks requiring numerical understanding. Quantitative and numerical comprehension in language is an important task in many fields like education and finance, but still remains a challenging task for language models. While tool and calculator usage has shown to be helpful to improve mathematical reasoning in large pretrained decoder-only language models, this remains unexplored for smaller language models with encoders. In this paper, we propose Pre-Calc, a simple pre-finetuning objective of learning to use the calculator for both encoder-only and encoder-decoder architectures, formulated as a discriminative and generative task respectively. We pre-train BERT and RoBERTa for discriminative calculator use and Flan-T5 for generative calculator use on the MAWPS, SVAMP, and AsDiv-A datasets, which improves performance on downstream tasks that require numerical understanding.",4c53285eb3ec1484b1b90f4da18df757f653be53.pdf,"methodology.
3. Pre-Calc Methodology
We posit that learning to use a calculator requires under-
standing of numbers and ways in which numbers can be
combined. This is used to formulate the Pre-Calc objec-
tives described below.
3.1. Encoder-Only
3.1.1. D ATA PREPROCESSING
We preprocess Calc-MAWPS, Calc-SV AMP and Calc-
AsDiv-A (from the Calc-X collection) (Kadl ˇc´ık et al., 2023)and add 2 new features required for Pre-Calc. First is the
operand tag sequence , which is a sequence of binary tags
that is 1 if the original token it corresponds to is an operand
and 0 if it isn’t. Secondly we extract the Operation , which
is the operation among {+ (add), - (subtract), * (multiply),
/ (divide) }that is required for the question. We extract the
operation either directly from the equation or the reasoning
chain in Calc-X and generate the operand tag sequence, by
first extracting the operands and then tagging the occurances
of the operands in the binary sequence with a 1. As part
of this process we also filter out instances where there are
more than one distinct operations as part of the equation.
3.1.2. P RE-CALC METHOD
An illustration of the Pre-Calc method for Encoder-only
model can be seen in Fig 1. This is decomposed into two
tasks as a dual-objective.
Firstly, we use the pretrained Encoder-only language model
for the task of Operand Identification , which is a token-level
classification task. The tags possible for each token are 1
and 0.
Secondly, we perform the task of Operation Classficiation
by adding a special [OP] token at the end of each sequence
and using this [OP] token’s final layer representation to
classify the operation required in this sequence (+, -, *, /).
Hence, this is essentially a sequence-level classification task
similar to classifying from the representation of a [CLS]
token. However, we do not use the [CLS] token at the
start of the sequence, to enable this objective even in non-
bidirectional models with an autoregressive attention mask
(like de","Future Work
In this work, we improve the numeracy in language models
on the QNLI and QQA tasks which involve textual and com-
putational quantitative reasoning. We do so by proposing
calculator usage as a pre-finetuning task in a discrimina-
tive and generative fashion for encoder-only and encoder-
decoder models respectively. This improves encoder-only
models across various downstream tasks and improves
encoder-decoder models on tasks that require explicit com-
putation.
Future work can address the balance between textual under-
standing and numerical reasoning, by refining regularization
strategies to maintain the language model’s core strengths
while enhancing its computational abilities. Tool-use in
encoder-only models could also be extended to more com-
plex tools similar to decoder-only models.
Acknowledgments
We thank Robert Lo for the helpful discussions.References
Aghajanyan, A., Gupta, A., Shrivastava, A., Chen, X.,
Zettlemoyer, L., and Gupta, S. Muppet: Massive multi-
task representations with pre-finetuning. arXiv preprint
arXiv:2101.11038 , 2021.
Chen, C.-C., Huang, H.-H., Takamura, H., and Chen, H.-
H. Numeracy-600k: Learning numeracy for detecting
exaggerated information in market comments. In Pro-
ceedings of the 57th Annual Meeting of the Association
for Computational Linguistics , pp. 6307–6313, 2019.
Chen, C.-C., Huang, H.-H., and Chen, H.-H. Nquad:
70,000+ questions for machine comprehension of the
numerals in text. In Proceedings of the 30th ACM Inter-
national Conference on Information & Knowledge Man-
agement , pp. 2925–2929, 2021.
Chen, C.-C., Takamura, H., Kobayashi, I., and Miyao,
Y . Improving numeracy by input reframing and quan-
titative pre-finetuning task. In Findings of the Associ-
ation for Computational Linguistics: EACL 2023 , pp.
69–77, Dubrovnik, Croatia, May 2023. Association for
Computational Linguistics. doi: 10.18653/v1/2023.
findings-eacl.4. URL https://aclanthology.
org/2023.findings-eacl.4 .
Devlin, J., Chang, M.-W., Lee,","methodology. 3. Pre-Calc Methodology We posit that learning to use a calculator requires under- standing of numbers and ways in which numbers can be combined. This is used to formulate the Pre-Calc objec- tives described below. 3.1. Encoder-Only 3.1.1. D ATA PREPROCESSING We preprocess Calc-MAWPS, Calc-SV AMP and Calc- AsDiv-A (from the Calc-X collection) (Kadl c k et al., 2023)and add 2 new features required for Pre-Calc. First is the operand tag sequence , which is a sequence of binary tags that is 1 if the original token it corresponds to is an operand and 0 if it isn t. Secondly we extract the Operation , which is the operation among {+ (add), - (subtract), * (multiply), / (divide) }that is required for the question. We extract the operation either directly from the equation or the reasoning chain in Calc-X and generate the operand tag sequence, by first extracting the operands and then tagging the occurances of the operands in the binary sequence with a 1. As part of this process we also filter out instances where there are more than one distinct operations as part of the equation. 3.1.2. P RE-CALC METHOD An illustration of the Pre-Calc method for Encoder-only model can be seen in Fig 1. This is decomposed into two tasks as a dual-objective. Firstly, we use the pretrained Encoder-only language model for the task of Operand Identification , which is a token-level classification task. The tags possible for each token are 1 and 0. Secondly, we perform the task of Operation Classficiation by adding a special [OP] token at the end of each sequence and using this [OP] token s final layer representation to classify the operation required in this sequence (+, -, *, /). Hence, this is essentially a sequence-level classification task similar to classifying from the representation of a [CLS] token. However, we do not use the [CLS] token at the start of the sequence, to enable this objective even in non- bidirectional models with an autoregressive attention mask (like de","Future Work In this work, we improve the numeracy in language models on the QNLI and QQA tasks which involve textual and com- putational quantitative reasoning. We do so by proposing calculator usage as a pre-finetuning task in a discrimina- tive and generative fashion for encoder-only and encoder- decoder models respectively. This improves encoder-only models across various downstream tasks and improves encoder-decoder models on tasks that require explicit com- putation. Future work can address the balance between textual under- standing and numerical reasoning, by refining regularization strategies to maintain the language model s core strengths while enhancing its computational abilities. Tool-use in encoder-only models could also be extended to more com- plex tools similar to decoder-only models. Acknowledgments We thank Robert Lo for the helpful discussions.","Pre-Calc: Learning to Use the Calculator Improves Numeracy in Language Models We propose Pre-Calc, our method to teach smaller language models how to use calculators. By pre-finetuning BERT, RoBERTa, and Flan-T5 on calculator use tasks, we improved these models' performance on tasks requiring numerical understanding. Quantitative and numerical comprehension in language is an important task in many fields like education and finance, but still remains a challenging task for language models. While tool and calculator usage has shown to be helpful to improve mathematical reasoning in large pretrained decoder-only language models, this remains unexplored for smaller language models with encoders. In this paper, we propose Pre-Calc, a simple pre-finetuning objective of learning to use the calculator for both encoder-only and encoder-decoder architectures, formulated as a discriminative and generative task respectively. We pre-train BERT and RoBERTa for discriminative calculator use and Flan-T5 for generative calculator use on the MAWPS, SVAMP, and AsDiv-A datasets, which improves performance on downstream tasks that require numerical understanding. Future Work In this work, we improve the numeracy in language models on the QNLI and QQA tasks which involve textual and com- putational quantitative reasoning. We do so by proposing calculator usage as a pre-finetuning task in a discrimina- tive and generative fashion for encoder-only and encoder- decoder models respectively. This improves encoder-only models across various downstream tasks and improves encoder-decoder models on tasks that require explicit com- putation. Future work can address the balance between textual under- standing and numerical reasoning, by refining regularization strategies to maintain the language model s core strengths while enhancing its computational abilities. Tool-use in encoder-only models could also be extended to more com- plex tools similar to decoder-only models. Acknowledgments We thank Robert Lo for the helpful discussions. methodology. 3. Pre-Calc Methodology We posit that learning to use a calculator requires under- standing of numbers and ways in which numbers can be combined. This is used to formulate the Pre-Calc objec- tives described below. 3.1. Encoder-Only 3.1.1. D ATA PREPROCESSING We preprocess Calc-MAWPS, Calc-SV AMP and Calc- AsDiv-A (from the Calc-X collection) (Kadl c k et al., 2023)and add 2 new features required for Pre-Calc. First is the operand tag sequence , which is a sequence of binary tags that is 1 if the original token it corresponds to is an operand and 0 if it isn t. Secondly we extract the Operation , which is the operation among {+ (add), - (subtract), * (multiply), / (divide) }that is required for the question. We extract the operation either directly from the equation or the reasoning chain in Calc-X and generate the operand tag sequence, by first extracting the operands and then tagging the occurances of the operands in the binary sequence with a 1. As part of this process we also filter out instances where there are more than one distinct operations as part of the equation. 3.1.2. P RE-CALC METHOD An illustration of the Pre-Calc method for Encoder-only model can be seen in Fig 1. This is decomposed into two tasks as a dual-objective. Firstly, we use the pretrained Encoder-only language model for the task of Operand Identification , which is a token-level classification task. The tags possible for each token are 1 and 0. Secondly, we perform the task of Operation Classficiation by adding a special [OP] token at the end of each sequence and using this [OP] token s final layer representation to classify the operation required in this sequence (+, -, *, /). Hence, this is essentially a sequence-level classification task similar to classifying from the representation of a [CLS] token. However, we do not use the [CLS] token at the start of the sequence, to enable this objective even in non- bidirectional models with an autoregressive attention mask (like de","Pre-Calc: Learning to Use the Calculator Improves Numeracy in Language Models We propose Pre-Calc, our method to teach smaller language models how to use calculators. By pre-finetuning BERT, RoBERTa, and Flan-T5 on calculator use tasks, we improved these models' performance on tasks requiring numerical understanding. Quantitative and numerical comprehension in language is an important task in many fields like education and finance, but still remains a challenging task for language models. While tool and calculator usage has shown to be helpful to improve mathematical reasoning in large pretrained decoder-only language models, this remains unexplored for smaller language models with encoders. In this paper, we propose Pre-Calc, a simple pre-finetuning objective of learning to use the calculator for both encoder-only and encoder-decoder architectures, formulated as a discriminative and generative task respectively. We pre-train BERT and RoBERTa for discriminative calculator use and Flan-T5 for generative calculator use on the MAWPS, SVAMP, and AsDiv-A datasets, which improves performance on downstream tasks that require numerical understanding.",0,"methodology.
3. Pre-Calc Methodology
We posit that learning to use a calculator requires under-
standing of numbers and ways in which numbers can be
combined. This is used to formulate the Pre-Calc objec-
tives described below.
3.1. Encoder-Only
3.1.1. D ATA PREPROCESSING
We preprocess Calc-MAWPS, Calc-SV AMP and Calc-
AsDiv-A (from the Calc-X collection) (Kadl ˇc´ık et al., 2023)and add 2 new features required for Pre-Calc. First is the
operand tag sequence , which is a sequence of binary tags
that is 1 if the original token it corresponds to is an operand
and 0 if it isn’t. Secondly we extract the Operation , which
is the operation among {+ (add), - (subtract), * (multiply),
/ (divide) }that is required for the question. We extract the
operation either directly from the equation or the reasoning
chain in Calc-X and generate the operand tag sequence, by
first extracting the operands and then tagging the occurances
of the operands in the binary sequence with a 1. As part
of this process we also filter out instances where there are
more than one distinct operations as part of the equation.
3.1.2. P RE-CALC METHOD
An illustration of the Pre-Calc method for Encoder-only
model can be seen in Fig 1. This is decomposed into two
tasks as a dual-objective.
Firstly, we use the pretrained Encoder-only language model
for the task of Operand Identification , which is a token-level
classification task. The tags possible for each token are 1
and 0.
Secondly, we perform the task of Operation Classficiation
by adding a special [OP] token at the end of each sequence
and using this [OP] token’s final layer representation to
classify the operation required in this sequence (+, -, *, /).
Hence, this is essentially a sequence-level classification task
similar to classifying from the representation of a [CLS]
token. However, we do not use the [CLS] token at the
start of the sequence, to enable this objective even in non-
bidirectional models with an autoregressive attention mask
(like de Future Work
In this work, we improve the numeracy in language models
on the QNLI and QQA tasks which involve textual and com-
putational quantitative reasoning. We do so by proposing
calculator usage as a pre-finetuning task in a discrimina-
tive and generative fashion for encoder-only and encoder-
decoder models respectively. This improves encoder-only
models across various downstream tasks and improves
encoder-decoder models on tasks that require explicit com-
putation.
Future work can address the balance between textual under-
standing and numerical reasoning, by refining regularization
strategies to maintain the language model’s core strengths
while enhancing its computational abilities. Tool-use in
encoder-only models could also be extended to more com-
plex tools similar to decoder-only models.
Acknowledgments
We thank Robert Lo for the helpful discussions.References
Aghajanyan, A., Gupta, A., Shrivastava, A., Chen, X.,
Zettlemoyer, L., and Gupta, S. Muppet: Massive multi-
task representations with pre-finetuning. arXiv preprint
arXiv:2101.11038 , 2021.
Chen, C.-C., Huang, H.-H., Takamura, H., and Chen, H.-
H. Numeracy-600k: Learning numeracy for detecting
exaggerated information in market comments. In Pro-
ceedings of the 57th Annual Meeting of the Association
for Computational Linguistics , pp. 6307–6313, 2019.
Chen, C.-C., Huang, H.-H., and Chen, H.-H. Nquad:
70,000+ questions for machine comprehension of the
numerals in text. In Proceedings of the 30th ACM Inter-
national Conference on Information & Knowledge Man-
agement , pp. 2925–2929, 2021.
Chen, C.-C., Takamura, H., Kobayashi, I., and Miyao,
Y . Improving numeracy by input reframing and quan-
titative pre-finetuning task. In Findings of the Associ-
ation for Computational Linguistics: EACL 2023 , pp.
69–77, Dubrovnik, Croatia, May 2023. Association for
Computational Linguistics. doi: 10.18653/v1/2023.
findings-eacl.4. URL https://aclanthology.
org/2023.findings-eacl.4 .
Devlin, J., Chang, M.-W., Lee,",False
017a4ef5896e1abf13238bf6c96df91a351c84ea,Learning Efficient Recursive Numeral Systems via Reinforcement Learning,"['Jonathan David Thomas', 'Andrea Silvi', 'Devdatt Dubhashi', 'Emil Carlsson', 'Moa Johansson']",https://openreview.net/pdf/017a4ef5896e1abf13238bf6c96df91a351c84ea.pdf,"Learning Efficient Recursive Numeral Systems via Reinforcement Learning How did recursive numeral systems emerge? We take steps towards a mechanistic explanation via a Reinforcement Learning approach which optimizes a lexicon under a given meta-grammar. The emergence of mathematical concepts, such as number systems, is an understudied area in AI for mathematics and reasoning. It has previously been shown (Carlsson et al., 2021) that by using reinforcement learning (RL), agents can derive simple approximate and exact-restricted numeral systems. However, it is a major challenge to show how more complex recursive numeral systems, similar to the one utilised in English, could arise via a simple learning mechanism such as RL. Here, we introduce an approach towards deriving a mechanistic explanation of the emergence of recursive number systems where we consider an RL agent which directly optimizes a lexicon under a given meta-grammar. Utilising a slightly modified version of the seminal meta-grammar of (Hurford, 1975), we demonstrate that our RL agent can effectively modify the lexicon towards Pareto-optimal configurations which are comparable to those observed within human numeral systems.",017a4ef5896e1abf13238bf6c96df91a351c84ea.pdf,"approach towards deriv-
ing a mechanistic explanation of the emergence
of recursive number systems where we consider
an RL agent which directly optimizes a lexicon
under a given meta-grammar. Utilising a slightly
modified version of the seminal meta-grammar
of (Hurford, 1975), we demonstrate that our RL
agent can effectively modify the lexicon towards
Pareto-optimal configurations which are compa-
rable to those observed within human numeral
systems.
1. Introduction
While there is evidence to suggest that animals, young in-
fants and adult humans possess a biologically determined,
domain-specific representation of numbers and elementary
arithmetic operations, only humans have a capacity for gen-
erating an infinite set of natural numbers, while all other
species seem to lack such a capacity (Hauser et al., 2002;
Chomsky, 1982;9; Dehaene & Changeux, 1993; Dehaene,
1997). This unique capacity is central to many aspects of
human cognition, including, of course, the development of
sophisticated mathematics. The fundamental mechanism
underlying this is the use of a finite symbolic system to
represent arbitrarily large discrete numerical magnitudes
i.e. the positive integers. However, the work within AI on
developing and changing representations of mathematical
1Chalmers University of Technology, Gothenburg,
Sweden. Correspondence to: Jonathan D. Thomas
<jonathan.thomas@chalmers.se >.concepts, such as number systems, is limited, and primarily
concerns revising representations of logical theories (Bundy
& Li, 2023).
In cognitive science, a recent influential body of work sug-
gests language is shaped by a pressure for efficient communi-
cation which involves an information-theoretic trade-off be-
tween cognitive load and informativeness (Kemp & Regier,
2012; Gibson et al., 2017; Zaslavsky et al., 2019). This
means that language is under pressure to be simultaneously
informative, in order to support effective communication,
while also being simple, to minimize the cognitive","future work.
We note that human languages do not tend to cluster to a
single point of the Pareto frontier like the lexicons our agent
finds, but instead present a sizable variety in terms of lex-
icons size and even morphemes that are lexicalized. We
hypothesize that this might be due to the current limitations
of our RL-based approach which does not capture the dif-
ferent influences that may shape the evolution of language.
Furthermore, our current approach does not allow for the
emergence of some classes of numeral systems e.g. Type
2-Russian (as can be found in (Deni ´c & Szymanik, 2024)).
We are unable to achieve this representation as it includes
numerals within DandMwhich are non-sequential which
is unachievable with our current action space.
6. Conclusions and Future Work
Our work serves as an exploration of RL as a method for
the direct optimization of a lexicon within the context of
numeral systems. We have demonstrated that this is possible
and that an RL agent can learn to manipulate its lexicalisa-
tions in order to find an optimal trade-off between average
morphosyntactic complexity and lexicon size. The resultant
lexicons are comparable to those which we observe within
human numeral systems. A key enabler of this was a mi-
nor modification to a well-established meta-grammar for
expressing numeral systems. Optimization of the existing
meta-grammar produces systems which do not possess reg-
ular recursive structures or forms. Our modification avoids
the aforementioned issues and enables an RL agent to opti-
mize several lexicons towards the Pareto frontier found via
this new meta-grammar. An interesting direction we intend
4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5
Lexicon Size1.01.21.41.61.82.02.22.4Average Morphosyntactic ComplexityPareto Frontier
Path 1
Path 2
Path 3
Final ConfigurationFigure 6. Myopic RL-based Agent optimising configurations over
a fixed length episode (8-steps). Each path is the median of the
trajectories that the agent follows when starting","approach towards deriv- ing a mechanistic explanation of the emergence of recursive number systems where we consider an RL agent which directly optimizes a lexicon under a given meta-grammar. Utilising a slightly modified version of the seminal meta-grammar of (Hurford, 1975), we demonstrate that our RL agent can effectively modify the lexicon towards Pareto-optimal configurations which are compa- rable to those observed within human numeral systems. 1. Introduction While there is evidence to suggest that animals, young in- fants and adult humans possess a biologically determined, domain-specific representation of numbers and elementary arithmetic operations, only humans have a capacity for gen- erating an infinite set of natural numbers, while all other species seem to lack such a capacity (Hauser et al., 2002; Chomsky, 1982;9; Dehaene & Changeux, 1993; Dehaene, 1997). This unique capacity is central to many aspects of human cognition, including, of course, the development of sophisticated mathematics. The fundamental mechanism underlying this is the use of a finite symbolic system to represent arbitrarily large discrete numerical magnitudes i.e. the positive integers. However, the work within AI on developing and changing representations of mathematical 1Chalmers University of Technology, Gothenburg, Sweden. Correspondence to: Jonathan D. Thomas <jonathan.thomas@chalmers.se >.concepts, such as number systems, is limited, and primarily concerns revising representations of logical theories (Bundy & Li, 2023). In cognitive science, a recent influential body of work sug- gests language is shaped by a pressure for efficient communi- cation which involves an information-theoretic trade-off be- tween cognitive load and informativeness (Kemp & Regier, 2012; Gibson et al., 2017; Zaslavsky et al., 2019). This means that language is under pressure to be simultaneously informative, in order to support effective communication, while also being simple, to minimize the cognitive","future work. We note that human languages do not tend to cluster to a single point of the Pareto frontier like the lexicons our agent finds, but instead present a sizable variety in terms of lex- icons size and even morphemes that are lexicalized. We hypothesize that this might be due to the current limitations of our RL-based approach which does not capture the dif- ferent influences that may shape the evolution of language. Furthermore, our current approach does not allow for the emergence of some classes of numeral systems e.g. Type 2-Russian (as can be found in (Deni c & Szymanik, 2024)). We are unable to achieve this representation as it includes numerals within DandMwhich are non-sequential which is unachievable with our current action space. 6. Conclusions and Future Work Our work serves as an exploration of RL as a method for the direct optimization of a lexicon within the context of numeral systems. We have demonstrated that this is possible and that an RL agent can learn to manipulate its lexicalisa- tions in order to find an optimal trade-off between average morphosyntactic complexity and lexicon size. The resultant lexicons are comparable to those which we observe within human numeral systems. A key enabler of this was a mi- nor modification to a well-established meta-grammar for expressing numeral systems. Optimization of the existing meta-grammar produces systems which do not possess reg- ular recursive structures or forms. Our modification avoids the aforementioned issues and enables an RL agent to opti- mize several lexicons towards the Pareto frontier found via this new meta-grammar. An interesting direction we intend 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 Lexicon Size1.01.21.41.61.82.02.22.4Average Morphosyntactic ComplexityPareto Frontier Path 1 Path 2 Path 3 Final ConfigurationFigure 6. Myopic RL-based Agent optimising configurations over a fixed length episode (8-steps). Each path is the median of the trajectories that the agent follows when starting","Learning Efficient Recursive Numeral Systems via Reinforcement Learning How did recursive numeral systems emerge? We take steps towards a mechanistic explanation via a Reinforcement Learning approach which optimizes a lexicon under a given meta-grammar. The emergence of mathematical concepts, such as number systems, is an understudied area in AI for mathematics and reasoning. It has previously been shown (Carlsson et al., 2021) that by using reinforcement learning (RL), agents can derive simple approximate and exact-restricted numeral systems. However, it is a major challenge to show how more complex recursive numeral systems, similar to the one utilised in English, could arise via a simple learning mechanism such as RL. Here, we introduce an approach towards deriving a mechanistic explanation of the emergence of recursive number systems where we consider an RL agent which directly optimizes a lexicon under a given meta-grammar. Utilising a slightly modified version of the seminal meta-grammar of (Hurford, 1975), we demonstrate that our RL agent can effectively modify the lexicon towards Pareto-optimal configurations which are comparable to those observed within human numeral systems. future work. We note that human languages do not tend to cluster to a single point of the Pareto frontier like the lexicons our agent finds, but instead present a sizable variety in terms of lex- icons size and even morphemes that are lexicalized. We hypothesize that this might be due to the current limitations of our RL-based approach which does not capture the dif- ferent influences that may shape the evolution of language. Furthermore, our current approach does not allow for the emergence of some classes of numeral systems e.g. Type 2-Russian (as can be found in (Deni c & Szymanik, 2024)). We are unable to achieve this representation as it includes numerals within DandMwhich are non-sequential which is unachievable with our current action space. 6. Conclusions and Future Work Our work serves as an exploration of RL as a method for the direct optimization of a lexicon within the context of numeral systems. We have demonstrated that this is possible and that an RL agent can learn to manipulate its lexicalisa- tions in order to find an optimal trade-off between average morphosyntactic complexity and lexicon size. The resultant lexicons are comparable to those which we observe within human numeral systems. A key enabler of this was a mi- nor modification to a well-established meta-grammar for expressing numeral systems. Optimization of the existing meta-grammar produces systems which do not possess reg- ular recursive structures or forms. Our modification avoids the aforementioned issues and enables an RL agent to opti- mize several lexicons towards the Pareto frontier found via this new meta-grammar. An interesting direction we intend 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 Lexicon Size1.01.21.41.61.82.02.22.4Average Morphosyntactic ComplexityPareto Frontier Path 1 Path 2 Path 3 Final ConfigurationFigure 6. Myopic RL-based Agent optimising configurations over a fixed length episode (8-steps). Each path is the median of the trajectories that the agent follows when starting approach towards deriv- ing a mechanistic explanation of the emergence of recursive number systems where we consider an RL agent which directly optimizes a lexicon under a given meta-grammar. Utilising a slightly modified version of the seminal meta-grammar of (Hurford, 1975), we demonstrate that our RL agent can effectively modify the lexicon towards Pareto-optimal configurations which are compa- rable to those observed within human numeral systems. 1. Introduction While there is evidence to suggest that animals, young in- fants and adult humans possess a biologically determined, domain-specific representation of numbers and elementary arithmetic operations, only humans have a capacity for gen- erating an infinite set of natural numbers, while all other species seem to lack such a capacity (Hauser et al., 2002; Chomsky, 1982;9; Dehaene & Changeux, 1993; Dehaene, 1997). This unique capacity is central to many aspects of human cognition, including, of course, the development of sophisticated mathematics. The fundamental mechanism underlying this is the use of a finite symbolic system to represent arbitrarily large discrete numerical magnitudes i.e. the positive integers. However, the work within AI on developing and changing representations of mathematical 1Chalmers University of Technology, Gothenburg, Sweden. Correspondence to: Jonathan D. Thomas <jonathan.thomas@chalmers.se >.concepts, such as number systems, is limited, and primarily concerns revising representations of logical theories (Bundy & Li, 2023). In cognitive science, a recent influential body of work sug- gests language is shaped by a pressure for efficient communi- cation which involves an information-theoretic trade-off be- tween cognitive load and informativeness (Kemp & Regier, 2012; Gibson et al., 2017; Zaslavsky et al., 2019). This means that language is under pressure to be simultaneously informative, in order to support effective communication, while also being simple, to minimize the cognitive","Learning Efficient Recursive Numeral Systems via Reinforcement Learning How did recursive numeral systems emerge? We take steps towards a mechanistic explanation via a Reinforcement Learning approach which optimizes a lexicon under a given meta-grammar. The emergence of mathematical concepts, such as number systems, is an understudied area in AI for mathematics and reasoning. It has previously been shown (Carlsson et al., 2021) that by using reinforcement learning (RL), agents can derive simple approximate and exact-restricted numeral systems. However, it is a major challenge to show how more complex recursive numeral systems, similar to the one utilised in English, could arise via a simple learning mechanism such as RL. Here, we introduce an approach towards deriving a mechanistic explanation of the emergence of recursive number systems where we consider an RL agent which directly optimizes a lexicon under a given meta-grammar. Utilising a slightly modified version of the seminal meta-grammar of (Hurford, 1975), we demonstrate that our RL agent can effectively modify the lexicon towards Pareto-optimal configurations which are comparable to those observed within human numeral systems.",0,"approach towards deriv-
ing a mechanistic explanation of the emergence
of recursive number systems where we consider
an RL agent which directly optimizes a lexicon
under a given meta-grammar. Utilising a slightly
modified version of the seminal meta-grammar
of (Hurford, 1975), we demonstrate that our RL
agent can effectively modify the lexicon towards
Pareto-optimal configurations which are compa-
rable to those observed within human numeral
systems.
1. Introduction
While there is evidence to suggest that animals, young in-
fants and adult humans possess a biologically determined,
domain-specific representation of numbers and elementary
arithmetic operations, only humans have a capacity for gen-
erating an infinite set of natural numbers, while all other
species seem to lack such a capacity (Hauser et al., 2002;
Chomsky, 1982;9; Dehaene & Changeux, 1993; Dehaene,
1997). This unique capacity is central to many aspects of
human cognition, including, of course, the development of
sophisticated mathematics. The fundamental mechanism
underlying this is the use of a finite symbolic system to
represent arbitrarily large discrete numerical magnitudes
i.e. the positive integers. However, the work within AI on
developing and changing representations of mathematical
1Chalmers University of Technology, Gothenburg,
Sweden. Correspondence to: Jonathan D. Thomas
<jonathan.thomas@chalmers.se >.concepts, such as number systems, is limited, and primarily
concerns revising representations of logical theories (Bundy
& Li, 2023).
In cognitive science, a recent influential body of work sug-
gests language is shaped by a pressure for efficient communi-
cation which involves an information-theoretic trade-off be-
tween cognitive load and informativeness (Kemp & Regier,
2012; Gibson et al., 2017; Zaslavsky et al., 2019). This
means that language is under pressure to be simultaneously
informative, in order to support effective communication,
while also being simple, to minimize the cognitive future work.
We note that human languages do not tend to cluster to a
single point of the Pareto frontier like the lexicons our agent
finds, but instead present a sizable variety in terms of lex-
icons size and even morphemes that are lexicalized. We
hypothesize that this might be due to the current limitations
of our RL-based approach which does not capture the dif-
ferent influences that may shape the evolution of language.
Furthermore, our current approach does not allow for the
emergence of some classes of numeral systems e.g. Type
2-Russian (as can be found in (Deni ´c & Szymanik, 2024)).
We are unable to achieve this representation as it includes
numerals within DandMwhich are non-sequential which
is unachievable with our current action space.
6. Conclusions and Future Work
Our work serves as an exploration of RL as a method for
the direct optimization of a lexicon within the context of
numeral systems. We have demonstrated that this is possible
and that an RL agent can learn to manipulate its lexicalisa-
tions in order to find an optimal trade-off between average
morphosyntactic complexity and lexicon size. The resultant
lexicons are comparable to those which we observe within
human numeral systems. A key enabler of this was a mi-
nor modification to a well-established meta-grammar for
expressing numeral systems. Optimization of the existing
meta-grammar produces systems which do not possess reg-
ular recursive structures or forms. Our modification avoids
the aforementioned issues and enables an RL agent to opti-
mize several lexicons towards the Pareto frontier found via
this new meta-grammar. An interesting direction we intend
4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5
Lexicon Size1.01.21.41.61.82.02.22.4Average Morphosyntactic ComplexityPareto Frontier
Path 1
Path 2
Path 3
Final ConfigurationFigure 6. Myopic RL-based Agent optimising configurations over
a fixed length episode (8-steps). Each path is the median of the
trajectories that the agent follows when starting",False
648d1ef62a6f6ea84faa78689bbf3a7ceec0fc9a,"More Details, Please: Improving Autoformalization with More Detailed Proofs","['Guillem Tarrach', 'Albert Q. Jiang', 'Daniel Raggi', 'Wenda Li', 'Mateja Jamnik']",https://openreview.net/pdf/648d1ef62a6f6ea84faa78689bbf3a7ceec0fc9a.pdf,"More Details, Please: Improving Autoformalization with More Detailed Proofs We propose SPADeR, an approach to autoformalization that uses language models to infer and explicitly incorporate implicit details from informal proofs The formalization of mathematical theorems and their proofs is a time-consuming and tedious process which, despite recent advances in the reasoning capabilities of AI systems, remains a challenging task for computers. Existing attempts to automate the process with language models struggle with the difference in level of detail between formal and informal proofs. Successful autoformalization requires models to understand and be able to explain the nuances of logical arguments, a critical aspect of reasoning that is often overlooked in existing research. In this work, we introduce Sketch, Prove, Add Detail & Repeat (SPADeR), an approach that enhances proof autoformalizers by using language models to infer and explicitly incorporate implicit details from informal proofs. With the same number of autoformalization attempts, our method increases the percentage of successfully formalized problems in the miniF2F test dataset from 34.8% to 38.1%.",648d1ef62a6f6ea84faa78689bbf3a7ceec0fc9a.pdf,"approach that enhances proof aut-
oformalizers by using language models to infer
and explicitly incorporate implicit details from
informal proofs. With the same number of auto-
formalization attempts, our method increases the
percentage of successfully formalized problems
in the miniF2F test dataset from 34.8% to 38.1%.
1. Introduction
A significant body of recent work has investigated the rea-
soning capabilities of Large Language Models (LLMs), par-
ticularly in the context of solving mathematical problems.
One frequently studied task is Automated Theorem Prov-
ing (ATP), which involves automatically generating formal
proofs of mathematical theorems. However, few studies
have investigated the ability of LLMs to understand and
explain mathematical arguments. In this work, we introduce
an approach that leverages this capability to construct more
detailed informal mathematical proofs, thereby improving
the process of autoformalization – the translation of infor-
mal proofs into formally verifiable formal proofs. Informal
proofs lack many details that are necessary to verify their
correctness. While formal proofs do not suffer from this
1University of Cambridge2University of Edinburgh. Corre-
spondence to: Guillem Tarrach <guillem.tarrach@gmail.com >.
AI for MATH Workshop at ICML 2024 , Vienna, Austria. Copyright
2024 by the author(s).issue, in practice the focus on low-level details makes for-
mal automated theorem provers less successful at high-level
planning. As a result, autoformalization systems struggle
with the discrepancy in the level of detail in formal and in-
formal proofs (Jiang et al., 2023, Section 5.2 and Appendix
C). Our approach uses LLMs to explain informal proofs by
inferring and incorporating implicit details, thereby bridging
the gap between informal and formal proofs.
To plan ahead and focus on the overall proof strategy, math-
ematicians usually write proofs in a non-linear, hierarchical
manner: They start by writing a high-level proof draft an","conclusion, we make the following contributions:
•We propose a method for using LLMs to construct
more detailed proofs by inferring implicit details in
informal mathematical proofs.
•We demonstrate the usefulness of the presented method
for autoformalization.
Our work shows that LLMs can provide detailed mathemat-
ical proofs by inferring and explaining implicit reasoning
steps. This ability helps bridge the gap between informal
and formal mathematical proofs and enables LLM-based
autoformalization systems to verify more theorems.2. Background and Related Work
2.1. Mathematical Reasoning with Language Models
With recent advances in language models, particularly the
introduction of LLMs, there has been an increase in research
into their reasoning capabilities, particularly in the context
of mathematical problem-solving (Hendrycks et al., 2021;
Drori et al., 2022; Welleck et al., 2021). While alterna-
tive prompting methods (Wei et al., 2022; Yao et al., 2023;
Zheng et al., 2023a) help improve the accuracy of reasoning
arguments, language models still frequently make mistakes.
These challenges highlight the need for robust verification
methods to complement informal reasoning.
Furthermore, the ability of LLMs to understand and explain
existing arguments remains largely unexplored. In this
work, we investigate these abilities and their evaluation
2
More Details, Please: Improving Autoformalization with More Detailed Proofs
through autoformalization and formal verification.
2.2. Autoformalization
To address the limitations of reasoning with language mod-
els, recent work has explored the combination of informal
reasoning with formal verification through autoformaliza-
tion. While early approaches to autoformalization with deep
learning took inspiration from Neural Machine Translation
(Wang et al., 2018), it has been observed (Wu et al., 2022)
that LLMs are better suited for this task because of their in-
context few-shot learning capabilities (Brown et al., 2020)
and th","approach that enhances proof aut- oformalizers by using language models to infer and explicitly incorporate implicit details from informal proofs. With the same number of auto- formalization attempts, our method increases the percentage of successfully formalized problems in the miniF2F test dataset from 34.8% to 38.1%. 1. Introduction A significant body of recent work has investigated the rea- soning capabilities of Large Language Models (LLMs), par- ticularly in the context of solving mathematical problems. One frequently studied task is Automated Theorem Prov- ing (ATP), which involves automatically generating formal proofs of mathematical theorems. However, few studies have investigated the ability of LLMs to understand and explain mathematical arguments. In this work, we introduce an approach that leverages this capability to construct more detailed informal mathematical proofs, thereby improving the process of autoformalization the translation of infor- mal proofs into formally verifiable formal proofs. Informal proofs lack many details that are necessary to verify their correctness. While formal proofs do not suffer from this 1University of Cambridge2University of Edinburgh. Corre- spondence to: Guillem Tarrach <guillem.tarrach@gmail.com >. AI for MATH Workshop at ICML 2024 , Vienna, Austria. Copyright 2024 by the author(s).issue, in practice the focus on low-level details makes for- mal automated theorem provers less successful at high-level planning. As a result, autoformalization systems struggle with the discrepancy in the level of detail in formal and in- formal proofs (Jiang et al., 2023, Section 5.2 and Appendix C). Our approach uses LLMs to explain informal proofs by inferring and incorporating implicit details, thereby bridging the gap between informal and formal proofs. To plan ahead and focus on the overall proof strategy, math- ematicians usually write proofs in a non-linear, hierarchical manner: They start by writing a high-level proof draft an","conclusion, we make the following contributions: We propose a method for using LLMs to construct more detailed proofs by inferring implicit details in informal mathematical proofs. We demonstrate the usefulness of the presented method for autoformalization. Our work shows that LLMs can provide detailed mathemat- ical proofs by inferring and explaining implicit reasoning steps. This ability helps bridge the gap between informal and formal mathematical proofs and enables LLM-based autoformalization systems to verify more theorems.2. Background and Related Work 2.1. Mathematical Reasoning with Language Models With recent advances in language models, particularly the introduction of LLMs, there has been an increase in research into their reasoning capabilities, particularly in the context of mathematical problem-solving (Hendrycks et al., 2021; Drori et al., 2022; Welleck et al., 2021). While alterna- tive prompting methods (Wei et al., 2022; Yao et al., 2023; Zheng et al., 2023a) help improve the accuracy of reasoning arguments, language models still frequently make mistakes. These challenges highlight the need for robust verification methods to complement informal reasoning. Furthermore, the ability of LLMs to understand and explain existing arguments remains largely unexplored. In this work, we investigate these abilities and their evaluation 2 More Details, Please: Improving Autoformalization with More Detailed Proofs through autoformalization and formal verification. 2.2. Autoformalization To address the limitations of reasoning with language mod- els, recent work has explored the combination of informal reasoning with formal verification through autoformaliza- tion. While early approaches to autoformalization with deep learning took inspiration from Neural Machine Translation (Wang et al., 2018), it has been observed (Wu et al., 2022) that LLMs are better suited for this task because of their in- context few-shot learning capabilities (Brown et al., 2020) and th","More Details, Please: Improving Autoformalization with More Detailed Proofs We propose SPADeR, an approach to autoformalization that uses language models to infer and explicitly incorporate implicit details from informal proofs The formalization of mathematical theorems and their proofs is a time-consuming and tedious process which, despite recent advances in the reasoning capabilities of AI systems, remains a challenging task for computers. Existing attempts to automate the process with language models struggle with the difference in level of detail between formal and informal proofs. Successful autoformalization requires models to understand and be able to explain the nuances of logical arguments, a critical aspect of reasoning that is often overlooked in existing research. In this work, we introduce Sketch, Prove, Add Detail & Repeat (SPADeR), an approach that enhances proof autoformalizers by using language models to infer and explicitly incorporate implicit details from informal proofs. With the same number of autoformalization attempts, our method increases the percentage of successfully formalized problems in the miniF2F test dataset from 34.8% to 38.1%. conclusion, we make the following contributions: We propose a method for using LLMs to construct more detailed proofs by inferring implicit details in informal mathematical proofs. We demonstrate the usefulness of the presented method for autoformalization. Our work shows that LLMs can provide detailed mathemat- ical proofs by inferring and explaining implicit reasoning steps. This ability helps bridge the gap between informal and formal mathematical proofs and enables LLM-based autoformalization systems to verify more theorems.2. Background and Related Work 2.1. Mathematical Reasoning with Language Models With recent advances in language models, particularly the introduction of LLMs, there has been an increase in research into their reasoning capabilities, particularly in the context of mathematical problem-solving (Hendrycks et al., 2021; Drori et al., 2022; Welleck et al., 2021). While alterna- tive prompting methods (Wei et al., 2022; Yao et al., 2023; Zheng et al., 2023a) help improve the accuracy of reasoning arguments, language models still frequently make mistakes. These challenges highlight the need for robust verification methods to complement informal reasoning. Furthermore, the ability of LLMs to understand and explain existing arguments remains largely unexplored. In this work, we investigate these abilities and their evaluation 2 More Details, Please: Improving Autoformalization with More Detailed Proofs through autoformalization and formal verification. 2.2. Autoformalization To address the limitations of reasoning with language mod- els, recent work has explored the combination of informal reasoning with formal verification through autoformaliza- tion. While early approaches to autoformalization with deep learning took inspiration from Neural Machine Translation (Wang et al., 2018), it has been observed (Wu et al., 2022) that LLMs are better suited for this task because of their in- context few-shot learning capabilities (Brown et al., 2020) and th approach that enhances proof aut- oformalizers by using language models to infer and explicitly incorporate implicit details from informal proofs. With the same number of auto- formalization attempts, our method increases the percentage of successfully formalized problems in the miniF2F test dataset from 34.8% to 38.1%. 1. Introduction A significant body of recent work has investigated the rea- soning capabilities of Large Language Models (LLMs), par- ticularly in the context of solving mathematical problems. One frequently studied task is Automated Theorem Prov- ing (ATP), which involves automatically generating formal proofs of mathematical theorems. However, few studies have investigated the ability of LLMs to understand and explain mathematical arguments. In this work, we introduce an approach that leverages this capability to construct more detailed informal mathematical proofs, thereby improving the process of autoformalization the translation of infor- mal proofs into formally verifiable formal proofs. Informal proofs lack many details that are necessary to verify their correctness. While formal proofs do not suffer from this 1University of Cambridge2University of Edinburgh. Corre- spondence to: Guillem Tarrach <guillem.tarrach@gmail.com >. AI for MATH Workshop at ICML 2024 , Vienna, Austria. Copyright 2024 by the author(s).issue, in practice the focus on low-level details makes for- mal automated theorem provers less successful at high-level planning. As a result, autoformalization systems struggle with the discrepancy in the level of detail in formal and in- formal proofs (Jiang et al., 2023, Section 5.2 and Appendix C). Our approach uses LLMs to explain informal proofs by inferring and incorporating implicit details, thereby bridging the gap between informal and formal proofs. To plan ahead and focus on the overall proof strategy, math- ematicians usually write proofs in a non-linear, hierarchical manner: They start by writing a high-level proof draft an","More Details, Please: Improving Autoformalization with More Detailed Proofs We propose SPADeR, an approach to autoformalization that uses language models to infer and explicitly incorporate implicit details from informal proofs The formalization of mathematical theorems and their proofs is a time-consuming and tedious process which, despite recent advances in the reasoning capabilities of AI systems, remains a challenging task for computers. Existing attempts to automate the process with language models struggle with the difference in level of detail between formal and informal proofs. Successful autoformalization requires models to understand and be able to explain the nuances of logical arguments, a critical aspect of reasoning that is often overlooked in existing research. In this work, we introduce Sketch, Prove, Add Detail & Repeat (SPADeR), an approach that enhances proof autoformalizers by using language models to infer and explicitly incorporate implicit details from informal proofs. With the same number of autoformalization attempts, our method increases the percentage of successfully formalized problems in the miniF2F test dataset from 34.8% to 38.1%.",0,"approach that enhances proof aut-
oformalizers by using language models to infer
and explicitly incorporate implicit details from
informal proofs. With the same number of auto-
formalization attempts, our method increases the
percentage of successfully formalized problems
in the miniF2F test dataset from 34.8% to 38.1%.
1. Introduction
A significant body of recent work has investigated the rea-
soning capabilities of Large Language Models (LLMs), par-
ticularly in the context of solving mathematical problems.
One frequently studied task is Automated Theorem Prov-
ing (ATP), which involves automatically generating formal
proofs of mathematical theorems. However, few studies
have investigated the ability of LLMs to understand and
explain mathematical arguments. In this work, we introduce
an approach that leverages this capability to construct more
detailed informal mathematical proofs, thereby improving
the process of autoformalization – the translation of infor-
mal proofs into formally verifiable formal proofs. Informal
proofs lack many details that are necessary to verify their
correctness. While formal proofs do not suffer from this
1University of Cambridge2University of Edinburgh. Corre-
spondence to: Guillem Tarrach <guillem.tarrach@gmail.com >.
AI for MATH Workshop at ICML 2024 , Vienna, Austria. Copyright
2024 by the author(s).issue, in practice the focus on low-level details makes for-
mal automated theorem provers less successful at high-level
planning. As a result, autoformalization systems struggle
with the discrepancy in the level of detail in formal and in-
formal proofs (Jiang et al., 2023, Section 5.2 and Appendix
C). Our approach uses LLMs to explain informal proofs by
inferring and incorporating implicit details, thereby bridging
the gap between informal and formal proofs.
To plan ahead and focus on the overall proof strategy, math-
ematicians usually write proofs in a non-linear, hierarchical
manner: They start by writing a high-level proof draft an conclusion, we make the following contributions:
•We propose a method for using LLMs to construct
more detailed proofs by inferring implicit details in
informal mathematical proofs.
•We demonstrate the usefulness of the presented method
for autoformalization.
Our work shows that LLMs can provide detailed mathemat-
ical proofs by inferring and explaining implicit reasoning
steps. This ability helps bridge the gap between informal
and formal mathematical proofs and enables LLM-based
autoformalization systems to verify more theorems.2. Background and Related Work
2.1. Mathematical Reasoning with Language Models
With recent advances in language models, particularly the
introduction of LLMs, there has been an increase in research
into their reasoning capabilities, particularly in the context
of mathematical problem-solving (Hendrycks et al., 2021;
Drori et al., 2022; Welleck et al., 2021). While alterna-
tive prompting methods (Wei et al., 2022; Yao et al., 2023;
Zheng et al., 2023a) help improve the accuracy of reasoning
arguments, language models still frequently make mistakes.
These challenges highlight the need for robust verification
methods to complement informal reasoning.
Furthermore, the ability of LLMs to understand and explain
existing arguments remains largely unexplored. In this
work, we investigate these abilities and their evaluation
2
More Details, Please: Improving Autoformalization with More Detailed Proofs
through autoformalization and formal verification.
2.2. Autoformalization
To address the limitations of reasoning with language mod-
els, recent work has explored the combination of informal
reasoning with formal verification through autoformaliza-
tion. While early approaches to autoformalization with deep
learning took inspiration from Neural Machine Translation
(Wang et al., 2018), it has been observed (Wu et al., 2022)
that LLMs are better suited for this task because of their in-
context few-shot learning capabilities (Brown et al., 2020)
and th",True
0587d424a1a041845ea791237cb2754d46157cc3,Technical Report for ICML 2024 Automated Math Reasoning Challenge: Solving Optimization Problems with Open Source Large Language Model,"['Duc M. Nguyen', 'Sungahn Ko']",https://openreview.net/pdf/0587d424a1a041845ea791237cb2754d46157cc3.pdf,"Technical Report for ICML 2024 Automated Math Reasoning Challenge: Solving Optimization Problems with Open Source Large Language Model We investigated open-source Large Language Model's capabilities to solve optimization problem This technical report presents an approach utilizing open-source Large Language Models for Automated Optimization Problem-solving With Code Challenge at the ICML 2024 AI4Math Workshop. This challenge emphasizes the ability of Large Language Models (LLMs) to handle complex mathematical reasoning from formulating to solving the problem at hand. By exploring different prompting techniques, such as few-shot, self-consistency, chain-of-thought, and tree-of-thought, we aim to explore the current state-of-the-art LLMs' mathematical reasoning abilities.",0587d424a1a041845ea791237cb2754d46157cc3.pdf,"approach uti-
lizing open-source Large Language Models for
Automated Optimization Problem-solving With
Code Challenge at the ICML 2024 AI4Math
Workshop. This challenge emphasizes the abil-
ity of Large Language Models (LLMs) to handle
complex mathematical reasoning from formulat-
ing to solving the problem at hand. By exploring
different prompting techniques, such as few-shot,
self-consistency, chain-of-thought, and tree-of-
thought, we aim to explore the current state-of-
the-art LLMs’ mathematical reasoning abilities.
1. Introduction
Recent research has highlighted the remarkable potential
of state-of-the-art Large Language Models like GPT-4
(Achiam et al., 2023) showcasing their promising abilities in
reasoning across diverse fields, encompassing tasks such as
solving mathematical word problems and proving theorems
(Huang et al., 2024). Automated mathematical reasoning,
which requires sophisticated multi-step planning and rea-
soning, has attracted active research to evaluate and develop
intelligent agents capable of obtaining advanced forms of
human intelligence such as mathematical reasoning.
In this technical report, we investigate the ability to for-
mulate and solve optimization problems, which is critical
across various domains, ranging from operations research
and engineering to finance and machine learning, by Open
Source Large Language Models. Traditionally, solving opti-
mization problems has required human expertise in math-
ematical modeling and algorithm design. However, the
rise of LLMs presents an opportunity to automate this pro-
cess, enabling machines to understand, interpret, and solve
optimization problems expressed in natural language. To
1Department of Computer Science and Engineering, Ulsan
National Institute of Science and Technology, Ulsan, Republic of
Korea. Correspondence to: Sungahn Ko <sako@unist.ac.kr >.
The first AI for Math Workshop at the 41stInternational Confer-
ence on Machine Learning , Vienna, Austria. Copyright 2024 by
the a","Future Work
Even though open-source LLMs demonstrate good exper-
imental results in the challenge, there are important areas
needing further investigation.
•Improvement of Evaluation Mechanisms . The un-
derperformance of ToT prompting suggests a need for
better evaluation mechanisms within LLMs. Future
work can focus on improving the self-evaluation capa-
bility of LLMs by training with synthetic data similar
to Chen et al. (2024) or utilizing RAG-based mecha-
nism (Wei et al., 2022b).
•Fine-tuning on the training data set . The perfor-
mance can be further enhanced by fine-tuning an LLM
or a small model on the competition training data set,
which we have omitted in this technical report due to
the lack of resources.
•Incorporate more difficult questions in the data set .
Most of the questions in the data set are Linear Pro-
gramming problems, lacking diversity in difficulties.
Future work could focus on building a more diverse
data set consisting of other types of problems in opti-
mization such as Convex Optimization, Dynamic Pro-
gramming, or Stochastic Optimal Control.
6. Conclusion
This technical report has investigated the capabilities of
open-source Large Language Models (LLMs) in formu-
lating and solving optimization problems through various
prompting techniques. The exploration of methods such as
few-shot prompting, self-consistency prompting, chain-of-
thought (CoT) prompting, and tree-of-thought (ToT) prompt-
ing has provided valuable insights into how LLMs can be
effectively utilized for complex mathematical reasoning
tasks.
6
Solving Optimization Problems with Open Source Large Language Model
References
Introducing meta llama 3: The most capable openly avail-
able llm to date. URL https://ai.meta.com/
blog/meta-llama-3/ .
Achiam, O. J., Adler, S., Agarwal, S., Ahmad, L., Akkaya,
I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman,
S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Bal-
com, V ., Baltescu, P., Bao, H., Bavarian, M., Belgum,
J","approach uti- lizing open-source Large Language Models for Automated Optimization Problem-solving With Code Challenge at the ICML 2024 AI4Math Workshop. This challenge emphasizes the abil- ity of Large Language Models (LLMs) to handle complex mathematical reasoning from formulat- ing to solving the problem at hand. By exploring different prompting techniques, such as few-shot, self-consistency, chain-of-thought, and tree-of- thought, we aim to explore the current state-of- the-art LLMs mathematical reasoning abilities. 1. Introduction Recent research has highlighted the remarkable potential of state-of-the-art Large Language Models like GPT-4 (Achiam et al., 2023) showcasing their promising abilities in reasoning across diverse fields, encompassing tasks such as solving mathematical word problems and proving theorems (Huang et al., 2024). Automated mathematical reasoning, which requires sophisticated multi-step planning and rea- soning, has attracted active research to evaluate and develop intelligent agents capable of obtaining advanced forms of human intelligence such as mathematical reasoning. In this technical report, we investigate the ability to for- mulate and solve optimization problems, which is critical across various domains, ranging from operations research and engineering to finance and machine learning, by Open Source Large Language Models. Traditionally, solving opti- mization problems has required human expertise in math- ematical modeling and algorithm design. However, the rise of LLMs presents an opportunity to automate this pro- cess, enabling machines to understand, interpret, and solve optimization problems expressed in natural language. To 1Department of Computer Science and Engineering, Ulsan National Institute of Science and Technology, Ulsan, Republic of Korea. Correspondence to: Sungahn Ko <sako@unist.ac.kr >. The first AI for Math Workshop at the 41stInternational Confer- ence on Machine Learning , Vienna, Austria. Copyright 2024 by the a","Future Work Even though open-source LLMs demonstrate good exper- imental results in the challenge, there are important areas needing further investigation. Improvement of Evaluation Mechanisms . The un- derperformance of ToT prompting suggests a need for better evaluation mechanisms within LLMs. Future work can focus on improving the self-evaluation capa- bility of LLMs by training with synthetic data similar to Chen et al. (2024) or utilizing RAG-based mecha- nism (Wei et al., 2022b). Fine-tuning on the training data set . The perfor- mance can be further enhanced by fine-tuning an LLM or a small model on the competition training data set, which we have omitted in this technical report due to the lack of resources. Incorporate more difficult questions in the data set . Most of the questions in the data set are Linear Pro- gramming problems, lacking diversity in difficulties. Future work could focus on building a more diverse data set consisting of other types of problems in opti- mization such as Convex Optimization, Dynamic Pro- gramming, or Stochastic Optimal Control. 6. Conclusion This technical report has investigated the capabilities of open-source Large Language Models (LLMs) in formu- lating and solving optimization problems through various prompting techniques. The exploration of methods such as few-shot prompting, self-consistency prompting, chain-of- thought (CoT) prompting, and tree-of-thought (ToT) prompt- ing has provided valuable insights into how LLMs can be effectively utilized for complex mathematical reasoning tasks. 6 Solving Optimization Problems with Open Source Large Language Model","Technical Report for ICML 2024 Automated Math Reasoning Challenge: Solving Optimization Problems with Open Source Large Language Model We investigated open-source Large Language Model's capabilities to solve optimization problem This technical report presents an approach utilizing open-source Large Language Models for Automated Optimization Problem-solving With Code Challenge at the ICML 2024 AI4Math Workshop. This challenge emphasizes the ability of Large Language Models (LLMs) to handle complex mathematical reasoning from formulating to solving the problem at hand. By exploring different prompting techniques, such as few-shot, self-consistency, chain-of-thought, and tree-of-thought, we aim to explore the current state-of-the-art LLMs' mathematical reasoning abilities. Future Work Even though open-source LLMs demonstrate good exper- imental results in the challenge, there are important areas needing further investigation. Improvement of Evaluation Mechanisms . The un- derperformance of ToT prompting suggests a need for better evaluation mechanisms within LLMs. Future work can focus on improving the self-evaluation capa- bility of LLMs by training with synthetic data similar to Chen et al. (2024) or utilizing RAG-based mecha- nism (Wei et al., 2022b). Fine-tuning on the training data set . The perfor- mance can be further enhanced by fine-tuning an LLM or a small model on the competition training data set, which we have omitted in this technical report due to the lack of resources. Incorporate more difficult questions in the data set . Most of the questions in the data set are Linear Pro- gramming problems, lacking diversity in difficulties. Future work could focus on building a more diverse data set consisting of other types of problems in opti- mization such as Convex Optimization, Dynamic Pro- gramming, or Stochastic Optimal Control. 6. Conclusion This technical report has investigated the capabilities of open-source Large Language Models (LLMs) in formu- lating and solving optimization problems through various prompting techniques. The exploration of methods such as few-shot prompting, self-consistency prompting, chain-of- thought (CoT) prompting, and tree-of-thought (ToT) prompt- ing has provided valuable insights into how LLMs can be effectively utilized for complex mathematical reasoning tasks. 6 Solving Optimization Problems with Open Source Large Language Model approach uti- lizing open-source Large Language Models for Automated Optimization Problem-solving With Code Challenge at the ICML 2024 AI4Math Workshop. This challenge emphasizes the abil- ity of Large Language Models (LLMs) to handle complex mathematical reasoning from formulat- ing to solving the problem at hand. By exploring different prompting techniques, such as few-shot, self-consistency, chain-of-thought, and tree-of- thought, we aim to explore the current state-of- the-art LLMs mathematical reasoning abilities. 1. Introduction Recent research has highlighted the remarkable potential of state-of-the-art Large Language Models like GPT-4 (Achiam et al., 2023) showcasing their promising abilities in reasoning across diverse fields, encompassing tasks such as solving mathematical word problems and proving theorems (Huang et al., 2024). Automated mathematical reasoning, which requires sophisticated multi-step planning and rea- soning, has attracted active research to evaluate and develop intelligent agents capable of obtaining advanced forms of human intelligence such as mathematical reasoning. In this technical report, we investigate the ability to for- mulate and solve optimization problems, which is critical across various domains, ranging from operations research and engineering to finance and machine learning, by Open Source Large Language Models. Traditionally, solving opti- mization problems has required human expertise in math- ematical modeling and algorithm design. However, the rise of LLMs presents an opportunity to automate this pro- cess, enabling machines to understand, interpret, and solve optimization problems expressed in natural language. To 1Department of Computer Science and Engineering, Ulsan National Institute of Science and Technology, Ulsan, Republic of Korea. Correspondence to: Sungahn Ko <sako@unist.ac.kr >. The first AI for Math Workshop at the 41stInternational Confer- ence on Machine Learning , Vienna, Austria. Copyright 2024 by the a","Technical Report for ICML 2024 Automated Math Reasoning Challenge: Solving Optimization Problems with Open Source Large Language Model We investigated open-source Large Language Model's capabilities to solve optimization problem This technical report presents an approach utilizing open-source Large Language Models for Automated Optimization Problem-solving With Code Challenge at the ICML 2024 AI4Math Workshop. This challenge emphasizes the ability of Large Language Models (LLMs) to handle complex mathematical reasoning from formulating to solving the problem at hand. By exploring different prompting techniques, such as few-shot, self-consistency, chain-of-thought, and tree-of-thought, we aim to explore the current state-of-the-art LLMs' mathematical reasoning abilities.",0,"approach uti-
lizing open-source Large Language Models for
Automated Optimization Problem-solving With
Code Challenge at the ICML 2024 AI4Math
Workshop. This challenge emphasizes the abil-
ity of Large Language Models (LLMs) to handle
complex mathematical reasoning from formulat-
ing to solving the problem at hand. By exploring
different prompting techniques, such as few-shot,
self-consistency, chain-of-thought, and tree-of-
thought, we aim to explore the current state-of-
the-art LLMs’ mathematical reasoning abilities.
1. Introduction
Recent research has highlighted the remarkable potential
of state-of-the-art Large Language Models like GPT-4
(Achiam et al., 2023) showcasing their promising abilities in
reasoning across diverse fields, encompassing tasks such as
solving mathematical word problems and proving theorems
(Huang et al., 2024). Automated mathematical reasoning,
which requires sophisticated multi-step planning and rea-
soning, has attracted active research to evaluate and develop
intelligent agents capable of obtaining advanced forms of
human intelligence such as mathematical reasoning.
In this technical report, we investigate the ability to for-
mulate and solve optimization problems, which is critical
across various domains, ranging from operations research
and engineering to finance and machine learning, by Open
Source Large Language Models. Traditionally, solving opti-
mization problems has required human expertise in math-
ematical modeling and algorithm design. However, the
rise of LLMs presents an opportunity to automate this pro-
cess, enabling machines to understand, interpret, and solve
optimization problems expressed in natural language. To
1Department of Computer Science and Engineering, Ulsan
National Institute of Science and Technology, Ulsan, Republic of
Korea. Correspondence to: Sungahn Ko <sako@unist.ac.kr >.
The first AI for Math Workshop at the 41stInternational Confer-
ence on Machine Learning , Vienna, Austria. Copyright 2024 by
the a Future Work
Even though open-source LLMs demonstrate good exper-
imental results in the challenge, there are important areas
needing further investigation.
•Improvement of Evaluation Mechanisms . The un-
derperformance of ToT prompting suggests a need for
better evaluation mechanisms within LLMs. Future
work can focus on improving the self-evaluation capa-
bility of LLMs by training with synthetic data similar
to Chen et al. (2024) or utilizing RAG-based mecha-
nism (Wei et al., 2022b).
•Fine-tuning on the training data set . The perfor-
mance can be further enhanced by fine-tuning an LLM
or a small model on the competition training data set,
which we have omitted in this technical report due to
the lack of resources.
•Incorporate more difficult questions in the data set .
Most of the questions in the data set are Linear Pro-
gramming problems, lacking diversity in difficulties.
Future work could focus on building a more diverse
data set consisting of other types of problems in opti-
mization such as Convex Optimization, Dynamic Pro-
gramming, or Stochastic Optimal Control.
6. Conclusion
This technical report has investigated the capabilities of
open-source Large Language Models (LLMs) in formu-
lating and solving optimization problems through various
prompting techniques. The exploration of methods such as
few-shot prompting, self-consistency prompting, chain-of-
thought (CoT) prompting, and tree-of-thought (ToT) prompt-
ing has provided valuable insights into how LLMs can be
effectively utilized for complex mathematical reasoning
tasks.
6
Solving Optimization Problems with Open Source Large Language Model
References
Introducing meta llama 3: The most capable openly avail-
able llm to date. URL https://ai.meta.com/
blog/meta-llama-3/ .
Achiam, O. J., Adler, S., Agarwal, S., Ahmad, L., Akkaya,
I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman,
S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Bal-
com, V ., Baltescu, P., Bao, H., Bavarian, M., Belgum,
J",True
cdf640fc47f06c403bd674317fc554bcacd8d5e9,Advancing LLM Reasoning Generalists with Preference Trees,"['Lifan Yuan', 'Ganqu Cui', 'Hanbin Wang', 'Ning Ding', 'Xingyao Wang', 'Jia Deng', 'Boji Shan', 'Huimin Chen', 'Ruobing Xie', 'Yankai Lin', 'Zhenghao Liu', 'Bowen Zhou', 'Hao Peng', 'Zhiyuan Liu', 'Maosong Sun']",https://openreview.net/pdf/cdf640fc47f06c403bd674317fc554bcacd8d5e9.pdf,"Advancing LLM Reasoning Generalists with Preference Trees We present Eurus, state-of-the-art open LLM reasoning generalists and its recipe. We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of EURUS can be primarily attributed to ULTRAINTERACT, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. ULTRAINTERACT can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting of (1) reasoning chains with diverse planning strategies in a unified format, (2) multi-turn interaction trajectories with the environment and the critique, and (3) pairwise data to facilitate preference learning. ULTRAINTERACT allows us to conduct an in-depth exploration of preference learning for reasoning tasks. Our investigation reveals that some well-established preference learning algorithms may be less suitable for reasoning tasks compared to their effectiveness in general conversations. Inspired by this, we derive a novel reward modeling objective which, together with ULTRAINTERACT, leads to a strong reward model. All artifacts produced during this research will be made public.",cdf640fc47f06c403bd674317fc554bcacd8d5e9.pdf,"approach in the open-source community (Tun-
stall et al., 2023; Bai et al., 2023) with the proposal of
DPO (Rafailov et al., 2023) and high-quality preference
datasets (Cui et al., 2023; Zhu et al., 2023). Different from
open-domain chatbots, preference learning is largely under-
explored in complex reasoning. Recent research showed
performance degradation when applying DPO on reasoning
tasks, but some newly proposed algorithms demonstrated a
positive effect (Ethayarajh et al., 2024; Chen et al., 2024a;
Mitra et al., 2024; Shao et al., 2024). However, a deep un-
derstanding of preference learning, specifically its efficacy
on complex reasoning, is not yet established.
8. Conclusion
We strive to narrow the huge gap between open-source mod-
els and proprietary models from the perspective of align-
ment. Our work pushes the boundaries of open-source rea-
soning generalists by (1) releasing a high-quality multi-turn
reasoning dataset ULTRA INTERACT with preference trees,
(2) introducing EURUS -series LLMs which achieve new
SOTA on challenging reasoning benchmarks and (3) provid-
ing insights on preference learning for reasoning through
analysis, leading to new reward modeling objectives as well
as a powerful reward model for reasoning.
8
Advancing LLM Reasoning Generalists with Preference Trees
References
Amini, A., Gabriel, S., Lin, S., Koncel-Kedziorski, R., Choi,
Y ., and Hajishirzi, H. MathQA: Towards interpretable
math word problem solving with operation-based for-
malisms. In Proc. ofNAACL-HLT, 2019.
Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski,
H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.
Program synthesis with large language models. ArXiv
preprint, abs/2108.07732, 2021.
Bai, J., Bai, S., Chu, Y ., Cui, Z., Dang, K., Deng, X., Fan,
Y ., Ge, W., Han, Y ., Huang, F., Hui, B., Ji, L., Li, M.,
Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., Ma, J.,
Men, R., Ren, X., Ren, X., Tan, C., Tan, S., Tu, J., Wang,
P., Wang, S., Wang, W., Wu, S.,","conclusions of concurrent work (Chen et al.,
2024b; Wang et al., 2024). Training only on open-source
data without U LTRA INTERACT greatly hurts the reasoning
performance, confirming the effectiveness of ULTRA IN-
TERACT . Meanwhile, training only on ULTRA INTERACT
suffers a performance drop except for BBH, especially in
instruction following. We attribute the performance drop
to a worse instruction-following ability. This suggests the
necessity of mixing ULTRA INTERACT with other alignment
data for better all-around supervised fine-tuning.
7. Related Work
Open LLMs in Reasoning. Open-source LLMs have
shown remarkable progress in building specialists that ex-
cel in mathematics reasoning (Luo et al., 2023a; Yue et al.,
2023; Toshniwal et al., 2024) or coding abilities (Roziere
et al., 2023; Wei et al., 2023; Guo et al., 2024a; Zheng et al.,
2024). On the contrary, mastering general reasoning capabil-
ities still challenges open models, while the most advanced
ones (DeepSeek-AI, 2024; Bai et al., 2023; Touvron et al.,
2023; Jiang et al., 2024) are well behind proprietary mod-
els. More, these cutting-edge open general-purpose models
maintain their alignment recipes confidential, which fur-
ther hinders the replication and development of open-sourcereasoning models.
Preference Learning for Reasoning. Aligning language
models from human or AI preferences has emerged as a
prevalent approach in the open-source community (Tun-
stall et al., 2023; Bai et al., 2023) with the proposal of
DPO (Rafailov et al., 2023) and high-quality preference
datasets (Cui et al., 2023; Zhu et al., 2023). Different from
open-domain chatbots, preference learning is largely under-
explored in complex reasoning. Recent research showed
performance degradation when applying DPO on reasoning
tasks, but some newly proposed algorithms demonstrated a
positive effect (Ethayarajh et al., 2024; Chen et al., 2024a;
Mitra et al., 2024; Shao et al., 2024). However, a deep un-
derstanding of preference learn","approach in the open-source community (Tun- stall et al., 2023; Bai et al., 2023) with the proposal of DPO (Rafailov et al., 2023) and high-quality preference datasets (Cui et al., 2023; Zhu et al., 2023). Different from open-domain chatbots, preference learning is largely under- explored in complex reasoning. Recent research showed performance degradation when applying DPO on reasoning tasks, but some newly proposed algorithms demonstrated a positive effect (Ethayarajh et al., 2024; Chen et al., 2024a; Mitra et al., 2024; Shao et al., 2024). However, a deep un- derstanding of preference learning, specifically its efficacy on complex reasoning, is not yet established. 8. Conclusion We strive to narrow the huge gap between open-source mod- els and proprietary models from the perspective of align- ment. Our work pushes the boundaries of open-source rea- soning generalists by (1) releasing a high-quality multi-turn reasoning dataset ULTRA INTERACT with preference trees, (2) introducing EURUS -series LLMs which achieve new SOTA on challenging reasoning benchmarks and (3) provid- ing insights on preference learning for reasoning through analysis, leading to new reward modeling objectives as well as a powerful reward model for reasoning. 8 Advancing LLM Reasoning Generalists with Preference Trees","conclusions of concurrent work (Chen et al., 2024b; Wang et al., 2024). Training only on open-source data without U LTRA INTERACT greatly hurts the reasoning performance, confirming the effectiveness of ULTRA IN- TERACT . Meanwhile, training only on ULTRA INTERACT suffers a performance drop except for BBH, especially in instruction following. We attribute the performance drop to a worse instruction-following ability. This suggests the necessity of mixing ULTRA INTERACT with other alignment data for better all-around supervised fine-tuning. 7. Related Work Open LLMs in Reasoning. Open-source LLMs have shown remarkable progress in building specialists that ex- cel in mathematics reasoning (Luo et al., 2023a; Yue et al., 2023; Toshniwal et al., 2024) or coding abilities (Roziere et al., 2023; Wei et al., 2023; Guo et al., 2024a; Zheng et al., 2024). On the contrary, mastering general reasoning capabil- ities still challenges open models, while the most advanced ones (DeepSeek-AI, 2024; Bai et al., 2023; Touvron et al., 2023; Jiang et al., 2024) are well behind proprietary mod- els. More, these cutting-edge open general-purpose models maintain their alignment recipes confidential, which fur- ther hinders the replication and development of open-sourcereasoning models. Preference Learning for Reasoning. Aligning language models from human or AI preferences has emerged as a prevalent approach in the open-source community (Tun- stall et al., 2023; Bai et al., 2023) with the proposal of DPO (Rafailov et al., 2023) and high-quality preference datasets (Cui et al., 2023; Zhu et al., 2023). Different from open-domain chatbots, preference learning is largely under- explored in complex reasoning. Recent research showed performance degradation when applying DPO on reasoning tasks, but some newly proposed algorithms demonstrated a positive effect (Ethayarajh et al., 2024; Chen et al., 2024a; Mitra et al., 2024; Shao et al., 2024). However, a deep un- derstanding of preference learn","Advancing LLM Reasoning Generalists with Preference Trees We present Eurus, state-of-the-art open LLM reasoning generalists and its recipe. We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of EURUS can be primarily attributed to ULTRAINTERACT, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. ULTRAINTERACT can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting of (1) reasoning chains with diverse planning strategies in a unified format, (2) multi-turn interaction trajectories with the environment and the critique, and (3) pairwise data to facilitate preference learning. ULTRAINTERACT allows us to conduct an in-depth exploration of preference learning for reasoning tasks. Our investigation reveals that some well-established preference learning algorithms may be less suitable for reasoning tasks compared to their effectiveness in general conversations. Inspired by this, we derive a novel reward modeling objective which, together with ULTRAINTERACT, leads to a strong reward model. All artifacts produced during this research will be made public. conclusions of concurrent work (Chen et al., 2024b; Wang et al., 2024). Training only on open-source data without U LTRA INTERACT greatly hurts the reasoning performance, confirming the effectiveness of ULTRA IN- TERACT . Meanwhile, training only on ULTRA INTERACT suffers a performance drop except for BBH, especially in instruction following. We attribute the performance drop to a worse instruction-following ability. This suggests the necessity of mixing ULTRA INTERACT with other alignment data for better all-around supervised fine-tuning. 7. Related Work Open LLMs in Reasoning. Open-source LLMs have shown remarkable progress in building specialists that ex- cel in mathematics reasoning (Luo et al., 2023a; Yue et al., 2023; Toshniwal et al., 2024) or coding abilities (Roziere et al., 2023; Wei et al., 2023; Guo et al., 2024a; Zheng et al., 2024). On the contrary, mastering general reasoning capabil- ities still challenges open models, while the most advanced ones (DeepSeek-AI, 2024; Bai et al., 2023; Touvron et al., 2023; Jiang et al., 2024) are well behind proprietary mod- els. More, these cutting-edge open general-purpose models maintain their alignment recipes confidential, which fur- ther hinders the replication and development of open-sourcereasoning models. Preference Learning for Reasoning. Aligning language models from human or AI preferences has emerged as a prevalent approach in the open-source community (Tun- stall et al., 2023; Bai et al., 2023) with the proposal of DPO (Rafailov et al., 2023) and high-quality preference datasets (Cui et al., 2023; Zhu et al., 2023). Different from open-domain chatbots, preference learning is largely under- explored in complex reasoning. Recent research showed performance degradation when applying DPO on reasoning tasks, but some newly proposed algorithms demonstrated a positive effect (Ethayarajh et al., 2024; Chen et al., 2024a; Mitra et al., 2024; Shao et al., 2024). However, a deep un- derstanding of preference learn approach in the open-source community (Tun- stall et al., 2023; Bai et al., 2023) with the proposal of DPO (Rafailov et al., 2023) and high-quality preference datasets (Cui et al., 2023; Zhu et al., 2023). Different from open-domain chatbots, preference learning is largely under- explored in complex reasoning. Recent research showed performance degradation when applying DPO on reasoning tasks, but some newly proposed algorithms demonstrated a positive effect (Ethayarajh et al., 2024; Chen et al., 2024a; Mitra et al., 2024; Shao et al., 2024). However, a deep un- derstanding of preference learning, specifically its efficacy on complex reasoning, is not yet established. 8. Conclusion We strive to narrow the huge gap between open-source mod- els and proprietary models from the perspective of align- ment. Our work pushes the boundaries of open-source rea- soning generalists by (1) releasing a high-quality multi-turn reasoning dataset ULTRA INTERACT with preference trees, (2) introducing EURUS -series LLMs which achieve new SOTA on challenging reasoning benchmarks and (3) provid- ing insights on preference learning for reasoning through analysis, leading to new reward modeling objectives as well as a powerful reward model for reasoning. 8 Advancing LLM Reasoning Generalists with Preference Trees","Advancing LLM Reasoning Generalists with Preference Trees We present Eurus, state-of-the-art open LLM reasoning generalists and its recipe. We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of EURUS can be primarily attributed to ULTRAINTERACT, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. ULTRAINTERACT can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting of (1) reasoning chains with diverse planning strategies in a unified format, (2) multi-turn interaction trajectories with the environment and the critique, and (3) pairwise data to facilitate preference learning. ULTRAINTERACT allows us to conduct an in-depth exploration of preference learning for reasoning tasks. Our investigation reveals that some well-established preference learning algorithms may be less suitable for reasoning tasks compared to their effectiveness in general conversations. Inspired by this, we derive a novel reward modeling objective which, together with ULTRAINTERACT, leads to a strong reward model. All artifacts produced during this research will be made public.",0,"approach in the open-source community (Tun-
stall et al., 2023; Bai et al., 2023) with the proposal of
DPO (Rafailov et al., 2023) and high-quality preference
datasets (Cui et al., 2023; Zhu et al., 2023). Different from
open-domain chatbots, preference learning is largely under-
explored in complex reasoning. Recent research showed
performance degradation when applying DPO on reasoning
tasks, but some newly proposed algorithms demonstrated a
positive effect (Ethayarajh et al., 2024; Chen et al., 2024a;
Mitra et al., 2024; Shao et al., 2024). However, a deep un-
derstanding of preference learning, specifically its efficacy
on complex reasoning, is not yet established.
8. Conclusion
We strive to narrow the huge gap between open-source mod-
els and proprietary models from the perspective of align-
ment. Our work pushes the boundaries of open-source rea-
soning generalists by (1) releasing a high-quality multi-turn
reasoning dataset ULTRA INTERACT with preference trees,
(2) introducing EURUS -series LLMs which achieve new
SOTA on challenging reasoning benchmarks and (3) provid-
ing insights on preference learning for reasoning through
analysis, leading to new reward modeling objectives as well
as a powerful reward model for reasoning.
8
Advancing LLM Reasoning Generalists with Preference Trees
References
Amini, A., Gabriel, S., Lin, S., Koncel-Kedziorski, R., Choi,
Y ., and Hajishirzi, H. MathQA: Towards interpretable
math word problem solving with operation-based for-
malisms. In Proc. ofNAACL-HLT, 2019.
Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski,
H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.
Program synthesis with large language models. ArXiv
preprint, abs/2108.07732, 2021.
Bai, J., Bai, S., Chu, Y ., Cui, Z., Dang, K., Deng, X., Fan,
Y ., Ge, W., Han, Y ., Huang, F., Hui, B., Ji, L., Li, M.,
Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., Ma, J.,
Men, R., Ren, X., Ren, X., Tan, C., Tan, S., Tu, J., Wang,
P., Wang, S., Wang, W., Wu, S., conclusions of concurrent work (Chen et al.,
2024b; Wang et al., 2024). Training only on open-source
data without U LTRA INTERACT greatly hurts the reasoning
performance, confirming the effectiveness of ULTRA IN-
TERACT . Meanwhile, training only on ULTRA INTERACT
suffers a performance drop except for BBH, especially in
instruction following. We attribute the performance drop
to a worse instruction-following ability. This suggests the
necessity of mixing ULTRA INTERACT with other alignment
data for better all-around supervised fine-tuning.
7. Related Work
Open LLMs in Reasoning. Open-source LLMs have
shown remarkable progress in building specialists that ex-
cel in mathematics reasoning (Luo et al., 2023a; Yue et al.,
2023; Toshniwal et al., 2024) or coding abilities (Roziere
et al., 2023; Wei et al., 2023; Guo et al., 2024a; Zheng et al.,
2024). On the contrary, mastering general reasoning capabil-
ities still challenges open models, while the most advanced
ones (DeepSeek-AI, 2024; Bai et al., 2023; Touvron et al.,
2023; Jiang et al., 2024) are well behind proprietary mod-
els. More, these cutting-edge open general-purpose models
maintain their alignment recipes confidential, which fur-
ther hinders the replication and development of open-sourcereasoning models.
Preference Learning for Reasoning. Aligning language
models from human or AI preferences has emerged as a
prevalent approach in the open-source community (Tun-
stall et al., 2023; Bai et al., 2023) with the proposal of
DPO (Rafailov et al., 2023) and high-quality preference
datasets (Cui et al., 2023; Zhu et al., 2023). Different from
open-domain chatbots, preference learning is largely under-
explored in complex reasoning. Recent research showed
performance degradation when applying DPO on reasoning
tasks, but some newly proposed algorithms demonstrated a
positive effect (Ethayarajh et al., 2024; Chen et al., 2024a;
Mitra et al., 2024; Shao et al., 2024). However, a deep un-
derstanding of preference learn",False
3ba5283059bb755e01651618340073d09b23f233,GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning,"['Mehran Kazemi', 'Hamidreza Alvari', 'Ankit Anand', 'Jialin Wu', 'Xi Chen', 'Radu Soricut']",https://openreview.net/pdf/3ba5283059bb755e01651618340073d09b23f233.pdf,"GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning We created a dataset of geometry problems and conducted a systematic evaluation of large models. Large language models have shown impressive results for multi-hop mathematical reasoning when the input question is only textual. Many mathematical reasoning problems, however, contain both text and image. With the ever-increasing adoption of vision language models (VLMs), understanding their reasoning abilities for such problems is crucial. In this paper, we evaluate the reasoning capabilities of VLMs along various axes through the lens of geometry problems. We procedurally create a synthetic dataset of geometry questions with controllable difficulty levels along multiple axes, thus enabling a systematic evaluation. The empirical results obtained using our benchmark for state-of-the-art VLMs indicate that these models are not as capable in subjects like geometry (and, by generalization, other topics requiring similar reasoning) as suggested by previous benchmarks. This is made especially clear by the construction of our benchmark at various depth levels, since solving higher-depth problems requires long chains of reasoning rather than additional memorized knowledge.",3ba5283059bb755e01651618340073d09b23f233.pdf,"Approaches: Some of the ap-
proaches for improving the multi-hop reasoning of LLMs
and VLMs range from pre-training on relevant data
(Hendrycks et al., 2021; Lewkowycz et al., 2022), fine-
tuning with (Nye et al., 2022; Dalvi et al., 2021; Zelikman
et al., 2022; Kazemi et al., 2023a) and without (Clark et al.,
2021; Betz et al., 2021) explicitly generating the solution,
in-context learning with solutions (Wei et al., 2022), decom-
posing the problem into smaller pieces and solving them
separately (Zhou et al., 2023; Khot et al., 2023) and using
2
A Systematic Evaluation of Large Models for Geometric Reasoning
Dataset →
Feature ↓
ProofWriter
(Tafjord et al., 2021)
BoardgameQA
(Kazemi et al., 2023a)
AR-LSAT
(Zhong et al., 2021)
AQUA
(Ling et al., 2017)
GSM8k
(Cobbe et al., 2021)
CLEVR-Math
(Lindstr ¨om & Abraham, 2022)
ChartQA
(Masry et al., 2022)
GeoS
(Seo et al., 2015)
GeoQA
(Chen et al., 2021)
Geometry3k
(Lu et al., 2021)
UniGeo
(Chen et al., 2022a)
PGPS9K
(Zhang et al., 2023)
GeomVerse
Textual
Understanding✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
Visual
Understanding✗ ✗ ✗ ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
Mathematical
Reasoning✗ ∼ ✗ ✓ ✓ ∼ ∼ ✓ ✓ ✓ ✓ ✓ ✓
Automatic
Difficulty
Control✓ ✓ ✗ ✗ ✗ ∼ ∼ ✗ ✗ ✗ ✗ ✗ ✓
Table 1: A comparison of GeomVerse with some of the recent and/or widely-used multi-hop (logical or mathematical)
reasoning datasets. We use ∼when a dataset contains a property to a limited extent.
LLMs/VLMs as tools within classical algorithms (Kazemi
et al., 2023b; Creswell et al., 2023). In the realm of reason-
ing about geometry problems, existing work typically de-
velops specialized models or tools (e.g, (Trinh et al., 2024))
or resorts to distillation strategies (e.g., (Gao et al., 2023));
measuring the reasoning ability of general-purpose VLMs
is less studied.
3. The GeomVerse Dataset
We start with some preliminaries and terminologies. Then,
we explain how GeomVerse is created. The dataset will be
publicly available upon the acceptance of the paper.
3.1. Multi-Hop Logical Reasoning
A","Future work can verify the merit of finetuning
models on synthetic geometry problems for improving their
performance on real datasets. In an initial experiment, we
measured the performance of the PaLI 5B model on Ge-
ometry3k with and without finetuning on GeomVerse and
observed modest improvements (from almost 0 to almost 2
percent accuracy). We believe this is due to the difference
in the visual and textual features of the Geomety3k and
GeomVerse , as well as the poor generalization of PaLI to
geometry problems beyond its training distribution. Bet-
ter aligning the textual and visual features and using more
powerful models can yield more gains.
References
Abdelghani, R., Wang, Y .-H., Yuan, X., Wang, T., Lu-
cas, P., Sauz ´eon, H., and Oudeyer, P.-Y . Gpt-3-driven
pedagogical agents to train children’s curious question-
asking skills. International Journal of Artificial Intelli-
gence in Education , June 2023. ISSN 1560-4306. doi:
10.1007/s40593-023-00340-7. URL http://dx.doi.
org/10.1007/s40593-023-00340-7 .
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,
Hasson, Y ., Lenc, K., Mensch, A., Millican, K., Reynolds,
M., et al. Flamingo: a visual language model for few-shot
learning. Advances in Neural Information Processing
Systems , 35:23716–23736, 2022.
Allaway, E., Hwang, J. D., Bhagavatula, C., McKeown, K.,
Downey, D., and Choi, Y . Penguins don’t fly: Reason-
ing about generics through instantiations and exceptions.
arXiv preprint arXiv:2205.11658 , 2022.
8
A Systematic Evaluation of Large Models for Geometric Reasoning
Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin,
D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen,
Z., et al. Palm 2 technical report. arXiv preprint
arXiv:2305.10403 , 2023.
Betz, G., V oigt, C., and Richardson, K. Critical think-
ing for language models. In Proceedings of the 14th
International Conference on Computational Semantics
(IWCS) , pp. 63–75, Groningen, The Netherlands (on-
line), June 2021. Association for","Approaches: Some of the ap- proaches for improving the multi-hop reasoning of LLMs and VLMs range from pre-training on relevant data (Hendrycks et al., 2021; Lewkowycz et al., 2022), fine- tuning with (Nye et al., 2022; Dalvi et al., 2021; Zelikman et al., 2022; Kazemi et al., 2023a) and without (Clark et al., 2021; Betz et al., 2021) explicitly generating the solution, in-context learning with solutions (Wei et al., 2022), decom- posing the problem into smaller pieces and solving them separately (Zhou et al., 2023; Khot et al., 2023) and using 2 A Systematic Evaluation of Large Models for Geometric Reasoning Dataset Feature ProofWriter (Tafjord et al., 2021) BoardgameQA (Kazemi et al., 2023a) AR-LSAT (Zhong et al., 2021) AQUA (Ling et al., 2017) GSM8k (Cobbe et al., 2021) CLEVR-Math (Lindstr om & Abraham, 2022) ChartQA (Masry et al., 2022) GeoS (Seo et al., 2015) GeoQA (Chen et al., 2021) Geometry3k (Lu et al., 2021) UniGeo (Chen et al., 2022a) PGPS9K (Zhang et al., 2023) GeomVerse Textual Understanding Visual Understanding Mathematical Reasoning Automatic Difficulty Control Table 1: A comparison of GeomVerse with some of the recent and/or widely-used multi-hop (logical or mathematical) reasoning datasets. We use when a dataset contains a property to a limited extent. LLMs/VLMs as tools within classical algorithms (Kazemi et al., 2023b; Creswell et al., 2023). In the realm of reason- ing about geometry problems, existing work typically de- velops specialized models or tools (e.g, (Trinh et al., 2024)) or resorts to distillation strategies (e.g., (Gao et al., 2023)); measuring the reasoning ability of general-purpose VLMs is less studied. 3. The GeomVerse Dataset We start with some preliminaries and terminologies. Then, we explain how GeomVerse is created. The dataset will be publicly available upon the acceptance of the paper. 3.1. Multi-Hop Logical Reasoning A","Future work can verify the merit of finetuning models on synthetic geometry problems for improving their performance on real datasets. In an initial experiment, we measured the performance of the PaLI 5B model on Ge- ometry3k with and without finetuning on GeomVerse and observed modest improvements (from almost 0 to almost 2 percent accuracy). We believe this is due to the difference in the visual and textual features of the Geomety3k and GeomVerse , as well as the poor generalization of PaLI to geometry problems beyond its training distribution. Bet- ter aligning the textual and visual features and using more powerful models can yield more gains.","GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning We created a dataset of geometry problems and conducted a systematic evaluation of large models. Large language models have shown impressive results for multi-hop mathematical reasoning when the input question is only textual. Many mathematical reasoning problems, however, contain both text and image. With the ever-increasing adoption of vision language models (VLMs), understanding their reasoning abilities for such problems is crucial. In this paper, we evaluate the reasoning capabilities of VLMs along various axes through the lens of geometry problems. We procedurally create a synthetic dataset of geometry questions with controllable difficulty levels along multiple axes, thus enabling a systematic evaluation. The empirical results obtained using our benchmark for state-of-the-art VLMs indicate that these models are not as capable in subjects like geometry (and, by generalization, other topics requiring similar reasoning) as suggested by previous benchmarks. This is made especially clear by the construction of our benchmark at various depth levels, since solving higher-depth problems requires long chains of reasoning rather than additional memorized knowledge. Future work can verify the merit of finetuning models on synthetic geometry problems for improving their performance on real datasets. In an initial experiment, we measured the performance of the PaLI 5B model on Ge- ometry3k with and without finetuning on GeomVerse and observed modest improvements (from almost 0 to almost 2 percent accuracy). We believe this is due to the difference in the visual and textual features of the Geomety3k and GeomVerse , as well as the poor generalization of PaLI to geometry problems beyond its training distribution. Bet- ter aligning the textual and visual features and using more powerful models can yield more gains. Approaches: Some of the ap- proaches for improving the multi-hop reasoning of LLMs and VLMs range from pre-training on relevant data (Hendrycks et al., 2021; Lewkowycz et al., 2022), fine- tuning with (Nye et al., 2022; Dalvi et al., 2021; Zelikman et al., 2022; Kazemi et al., 2023a) and without (Clark et al., 2021; Betz et al., 2021) explicitly generating the solution, in-context learning with solutions (Wei et al., 2022), decom- posing the problem into smaller pieces and solving them separately (Zhou et al., 2023; Khot et al., 2023) and using 2 A Systematic Evaluation of Large Models for Geometric Reasoning Dataset Feature ProofWriter (Tafjord et al., 2021) BoardgameQA (Kazemi et al., 2023a) AR-LSAT (Zhong et al., 2021) AQUA (Ling et al., 2017) GSM8k (Cobbe et al., 2021) CLEVR-Math (Lindstr om & Abraham, 2022) ChartQA (Masry et al., 2022) GeoS (Seo et al., 2015) GeoQA (Chen et al., 2021) Geometry3k (Lu et al., 2021) UniGeo (Chen et al., 2022a) PGPS9K (Zhang et al., 2023) GeomVerse Textual Understanding Visual Understanding Mathematical Reasoning Automatic Difficulty Control Table 1: A comparison of GeomVerse with some of the recent and/or widely-used multi-hop (logical or mathematical) reasoning datasets. We use when a dataset contains a property to a limited extent. LLMs/VLMs as tools within classical algorithms (Kazemi et al., 2023b; Creswell et al., 2023). In the realm of reason- ing about geometry problems, existing work typically de- velops specialized models or tools (e.g, (Trinh et al., 2024)) or resorts to distillation strategies (e.g., (Gao et al., 2023)); measuring the reasoning ability of general-purpose VLMs is less studied. 3. The GeomVerse Dataset We start with some preliminaries and terminologies. Then, we explain how GeomVerse is created. The dataset will be publicly available upon the acceptance of the paper. 3.1. Multi-Hop Logical Reasoning A","GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning We created a dataset of geometry problems and conducted a systematic evaluation of large models. Large language models have shown impressive results for multi-hop mathematical reasoning when the input question is only textual. Many mathematical reasoning problems, however, contain both text and image. With the ever-increasing adoption of vision language models (VLMs), understanding their reasoning abilities for such problems is crucial. In this paper, we evaluate the reasoning capabilities of VLMs along various axes through the lens of geometry problems. We procedurally create a synthetic dataset of geometry questions with controllable difficulty levels along multiple axes, thus enabling a systematic evaluation. The empirical results obtained using our benchmark for state-of-the-art VLMs indicate that these models are not as capable in subjects like geometry (and, by generalization, other topics requiring similar reasoning) as suggested by previous benchmarks. This is made especially clear by the construction of our benchmark at various depth levels, since solving higher-depth problems requires long chains of reasoning rather than additional memorized knowledge.",0,"Approaches: Some of the ap-
proaches for improving the multi-hop reasoning of LLMs
and VLMs range from pre-training on relevant data
(Hendrycks et al., 2021; Lewkowycz et al., 2022), fine-
tuning with (Nye et al., 2022; Dalvi et al., 2021; Zelikman
et al., 2022; Kazemi et al., 2023a) and without (Clark et al.,
2021; Betz et al., 2021) explicitly generating the solution,
in-context learning with solutions (Wei et al., 2022), decom-
posing the problem into smaller pieces and solving them
separately (Zhou et al., 2023; Khot et al., 2023) and using
2
A Systematic Evaluation of Large Models for Geometric Reasoning
Dataset →
Feature ↓
ProofWriter
(Tafjord et al., 2021)
BoardgameQA
(Kazemi et al., 2023a)
AR-LSAT
(Zhong et al., 2021)
AQUA
(Ling et al., 2017)
GSM8k
(Cobbe et al., 2021)
CLEVR-Math
(Lindstr ¨om & Abraham, 2022)
ChartQA
(Masry et al., 2022)
GeoS
(Seo et al., 2015)
GeoQA
(Chen et al., 2021)
Geometry3k
(Lu et al., 2021)
UniGeo
(Chen et al., 2022a)
PGPS9K
(Zhang et al., 2023)
GeomVerse
Textual
Understanding✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
Visual
Understanding✗ ✗ ✗ ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
Mathematical
Reasoning✗ ∼ ✗ ✓ ✓ ∼ ∼ ✓ ✓ ✓ ✓ ✓ ✓
Automatic
Difficulty
Control✓ ✓ ✗ ✗ ✗ ∼ ∼ ✗ ✗ ✗ ✗ ✗ ✓
Table 1: A comparison of GeomVerse with some of the recent and/or widely-used multi-hop (logical or mathematical)
reasoning datasets. We use ∼when a dataset contains a property to a limited extent.
LLMs/VLMs as tools within classical algorithms (Kazemi
et al., 2023b; Creswell et al., 2023). In the realm of reason-
ing about geometry problems, existing work typically de-
velops specialized models or tools (e.g, (Trinh et al., 2024))
or resorts to distillation strategies (e.g., (Gao et al., 2023));
measuring the reasoning ability of general-purpose VLMs
is less studied.
3. The GeomVerse Dataset
We start with some preliminaries and terminologies. Then,
we explain how GeomVerse is created. The dataset will be
publicly available upon the acceptance of the paper.
3.1. Multi-Hop Logical Reasoning
A Future work can verify the merit of finetuning
models on synthetic geometry problems for improving their
performance on real datasets. In an initial experiment, we
measured the performance of the PaLI 5B model on Ge-
ometry3k with and without finetuning on GeomVerse and
observed modest improvements (from almost 0 to almost 2
percent accuracy). We believe this is due to the difference
in the visual and textual features of the Geomety3k and
GeomVerse , as well as the poor generalization of PaLI to
geometry problems beyond its training distribution. Bet-
ter aligning the textual and visual features and using more
powerful models can yield more gains.
References
Abdelghani, R., Wang, Y .-H., Yuan, X., Wang, T., Lu-
cas, P., Sauz ´eon, H., and Oudeyer, P.-Y . Gpt-3-driven
pedagogical agents to train children’s curious question-
asking skills. International Journal of Artificial Intelli-
gence in Education , June 2023. ISSN 1560-4306. doi:
10.1007/s40593-023-00340-7. URL http://dx.doi.
org/10.1007/s40593-023-00340-7 .
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,
Hasson, Y ., Lenc, K., Mensch, A., Millican, K., Reynolds,
M., et al. Flamingo: a visual language model for few-shot
learning. Advances in Neural Information Processing
Systems , 35:23716–23736, 2022.
Allaway, E., Hwang, J. D., Bhagavatula, C., McKeown, K.,
Downey, D., and Choi, Y . Penguins don’t fly: Reason-
ing about generics through instantiations and exceptions.
arXiv preprint arXiv:2205.11658 , 2022.
8
A Systematic Evaluation of Large Models for Geometric Reasoning
Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin,
D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen,
Z., et al. Palm 2 technical report. arXiv preprint
arXiv:2305.10403 , 2023.
Betz, G., V oigt, C., and Richardson, K. Critical think-
ing for language models. In Proceedings of the 14th
International Conference on Computational Semantics
(IWCS) , pp. 63–75, Groningen, The Netherlands (on-
line), June 2021. Association for",False
b658ad52e686b34e585fbe860bd4a1bbf08341ab,Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving,"['Aniket Rajiv Didolkar', 'Anirudh Goyal', 'Nan Rosemary Ke', 'Siyuan Guo', 'Michal Valko', 'Timothy P Lillicrap', 'Danilo Jimenez Rezende', 'Yoshua Bengio', 'Michael Curtis Mozer', 'Sanjeev Arora']",https://openreview.net/pdf/b658ad52e686b34e585fbe860bd4a1bbf08341ab.pdf,"Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving Extracting metacognitive knowledge from LLMs and using it to improve math reasoning. \emph{Metacognitive knowledge} refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans. To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH. (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label. This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic, even though this article applies it to math problems.",b658ad52e686b34e585fbe860bd4a1bbf08341ab.pdf,"methodology presented is domain-agnostic,
even though this article applies it to math prob-
lems.
*Equal contribution1Mila, University of Mon-
treal2Google Deepmind3The University of Cambridge
4Princeton University. Correspondence to: Aniket Didolkar
<adidolkar123@gmail.com >, Anirudh Goyal <anirud-
hgoyal9119@gmail.com >.
The first AI for MATH Workshop at the 41st International Confer-
ence on Machine Learning, Vienna, Austria. Copyright 2024 by
the author(s).1. Introduction
Large language models (LLMs) have demonstrated remark-
able advancements in recent years at natural language in-
ference tasks ( 1–7), as well as scientific and mathematical
problems ( 8–11), although their limitations on mathematical
problems are also well-documented (12–17).
A core concept in human pedagogy is Metacognition (18),
sometimes described as thinking about thinking . It refers
to ability to reason about one’s own cognitive processes as
well as about learning-relevant properties of information
or data. Metacognitive Knowledge refers to the learner’s
accumulated knowledge of this type. Pedagogy research
shows that improving learners’ metacognitive knowledge
can improve their capabilities, for example on math ( 19;20).
The current paper raises the question “Do LLMs also have
metacognitive knowledge?” And if yes, Can we bootstrap
such knowledge to further improve LLM capabilities?
At first glance, this quest seems difficult. Deciphering
LLMs’ inner working from their huge set of parameters
–all results of non-linear optimization— is notoriously hard.
Furthermore, scientists lack parameter access to most lead-
ing AI models. But there are still reasons to hope we can
understand metacognition by interacting with LLMs. They
display some human tics, such as ability to improve their
math reasoning via Chain of Thought (CoT) (21) and also
the “Let’s think step by step” prompt ( 22). These were gen-
erally perceived as convenient tricks to get around the limi-
tations imposed by the LLM’s aut","future work.
Paper organization and main results: Section 3 describes
the method and Section 4 describes experiments. Using
a strong LLM - GPT-4 - to identify skills, we validate
the usefulness of these skills by demonstrates a significant
11.6% enhancement over CoT on the MATH Dataset us-
ing the method described in Section 3. Furthermore, the
identified skills also improve the generation of code-based
solutions for the problems within the MATH dataset giv-
ing a 7.52% improvement over the baseline PAL approach
(24), which also instructs the model to generate code. Sec-
tion 4.3 shows that the the skill exemplar repository created
for MATH noticeably improved in-context performance for
weaker LLMs on the same dataset and that the repository for
GSM8K helped improve in-context performance for other
math datasets. This shows that a powerful LLM can be used
for deeper understanding of skills that translates across other
LLMs and related datasets.
2. Related Works
For human learning, statistical methods can infer latent skills
from data and use the inferred skills to more accurately
forecast student learning ( 25;26). In machine learning,
works that study learning via skill induction include ( 27–30).
These start with some definition of skills in terms of model
parameters, whereas we use a powerful LLM in a black
box way to identify and consolidate skills. A discussion
of various prompting strategies is covered in Section 4 and
Appendix Section 8.
3. Automated Skill Discovery
We describe an automated process for categorizing mathe-
matical questions according to specific skills needed to solve
them. See Figure 1. Recent works relating skills and LLMs
(31;32) were an inspiration. Conceptually, the strategy in-
volves the creation of a detailed skill exemplar repository,
which contains a compilation of skill names alongside re-
spective illustrative examples (comprising both questions
2
Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving
Da","methodology presented is domain-agnostic, even though this article applies it to math prob- lems. *Equal contribution1Mila, University of Mon- treal2Google Deepmind3The University of Cambridge 4Princeton University. Correspondence to: Aniket Didolkar <adidolkar123@gmail.com >, Anirudh Goyal <anirud- hgoyal9119@gmail.com >. The first AI for MATH Workshop at the 41st International Confer- ence on Machine Learning, Vienna, Austria. Copyright 2024 by the author(s).1. Introduction Large language models (LLMs) have demonstrated remark- able advancements in recent years at natural language in- ference tasks ( 1 7), as well as scientific and mathematical problems ( 8 11), although their limitations on mathematical problems are also well-documented (12 17). A core concept in human pedagogy is Metacognition (18), sometimes described as thinking about thinking . It refers to ability to reason about one s own cognitive processes as well as about learning-relevant properties of information or data. Metacognitive Knowledge refers to the learner s accumulated knowledge of this type. Pedagogy research shows that improving learners metacognitive knowledge can improve their capabilities, for example on math ( 19;20). The current paper raises the question Do LLMs also have metacognitive knowledge? And if yes, Can we bootstrap such knowledge to further improve LLM capabilities? At first glance, this quest seems difficult. Deciphering LLMs inner working from their huge set of parameters all results of non-linear optimization is notoriously hard. Furthermore, scientists lack parameter access to most lead- ing AI models. But there are still reasons to hope we can understand metacognition by interacting with LLMs. They display some human tics, such as ability to improve their math reasoning via Chain of Thought (CoT) (21) and also the Let s think step by step prompt ( 22). These were gen- erally perceived as convenient tricks to get around the limi- tations imposed by the LLM s aut","future work. Paper organization and main results: Section 3 describes the method and Section 4 describes experiments. Using a strong LLM - GPT-4 - to identify skills, we validate the usefulness of these skills by demonstrates a significant 11.6% enhancement over CoT on the MATH Dataset us- ing the method described in Section 3. Furthermore, the identified skills also improve the generation of code-based solutions for the problems within the MATH dataset giv- ing a 7.52% improvement over the baseline PAL approach (24), which also instructs the model to generate code. Sec- tion 4.3 shows that the the skill exemplar repository created for MATH noticeably improved in-context performance for weaker LLMs on the same dataset and that the repository for GSM8K helped improve in-context performance for other math datasets. This shows that a powerful LLM can be used for deeper understanding of skills that translates across other LLMs and related datasets. 2. Related Works For human learning, statistical methods can infer latent skills from data and use the inferred skills to more accurately forecast student learning ( 25;26). In machine learning, works that study learning via skill induction include ( 27 30). These start with some definition of skills in terms of model parameters, whereas we use a powerful LLM in a black box way to identify and consolidate skills. A discussion of various prompting strategies is covered in Section 4 and Appendix Section 8. 3. Automated Skill Discovery We describe an automated process for categorizing mathe- matical questions according to specific skills needed to solve them. See Figure 1. Recent works relating skills and LLMs (31;32) were an inspiration. Conceptually, the strategy in- volves the creation of a detailed skill exemplar repository, which contains a compilation of skill names alongside re- spective illustrative examples (comprising both questions 2 Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving Da","Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving Extracting metacognitive knowledge from LLMs and using it to improve math reasoning. \emph{Metacognitive knowledge} refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans. To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH. (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label. This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic, even though this article applies it to math problems. future work. Paper organization and main results: Section 3 describes the method and Section 4 describes experiments. Using a strong LLM - GPT-4 - to identify skills, we validate the usefulness of these skills by demonstrates a significant 11.6% enhancement over CoT on the MATH Dataset us- ing the method described in Section 3. Furthermore, the identified skills also improve the generation of code-based solutions for the problems within the MATH dataset giv- ing a 7.52% improvement over the baseline PAL approach (24), which also instructs the model to generate code. Sec- tion 4.3 shows that the the skill exemplar repository created for MATH noticeably improved in-context performance for weaker LLMs on the same dataset and that the repository for GSM8K helped improve in-context performance for other math datasets. This shows that a powerful LLM can be used for deeper understanding of skills that translates across other LLMs and related datasets. 2. Related Works For human learning, statistical methods can infer latent skills from data and use the inferred skills to more accurately forecast student learning ( 25;26). In machine learning, works that study learning via skill induction include ( 27 30). These start with some definition of skills in terms of model parameters, whereas we use a powerful LLM in a black box way to identify and consolidate skills. A discussion of various prompting strategies is covered in Section 4 and Appendix Section 8. 3. Automated Skill Discovery We describe an automated process for categorizing mathe- matical questions according to specific skills needed to solve them. See Figure 1. Recent works relating skills and LLMs (31;32) were an inspiration. Conceptually, the strategy in- volves the creation of a detailed skill exemplar repository, which contains a compilation of skill names alongside re- spective illustrative examples (comprising both questions 2 Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving Da methodology presented is domain-agnostic, even though this article applies it to math prob- lems. *Equal contribution1Mila, University of Mon- treal2Google Deepmind3The University of Cambridge 4Princeton University. Correspondence to: Aniket Didolkar <adidolkar123@gmail.com >, Anirudh Goyal <anirud- hgoyal9119@gmail.com >. The first AI for MATH Workshop at the 41st International Confer- ence on Machine Learning, Vienna, Austria. Copyright 2024 by the author(s).1. Introduction Large language models (LLMs) have demonstrated remark- able advancements in recent years at natural language in- ference tasks ( 1 7), as well as scientific and mathematical problems ( 8 11), although their limitations on mathematical problems are also well-documented (12 17). A core concept in human pedagogy is Metacognition (18), sometimes described as thinking about thinking . It refers to ability to reason about one s own cognitive processes as well as about learning-relevant properties of information or data. Metacognitive Knowledge refers to the learner s accumulated knowledge of this type. Pedagogy research shows that improving learners metacognitive knowledge can improve their capabilities, for example on math ( 19;20). The current paper raises the question Do LLMs also have metacognitive knowledge? And if yes, Can we bootstrap such knowledge to further improve LLM capabilities? At first glance, this quest seems difficult. Deciphering LLMs inner working from their huge set of parameters all results of non-linear optimization is notoriously hard. Furthermore, scientists lack parameter access to most lead- ing AI models. But there are still reasons to hope we can understand metacognition by interacting with LLMs. They display some human tics, such as ability to improve their math reasoning via Chain of Thought (CoT) (21) and also the Let s think step by step prompt ( 22). These were gen- erally perceived as convenient tricks to get around the limi- tations imposed by the LLM s aut","Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving Extracting metacognitive knowledge from LLMs and using it to improve math reasoning. \emph{Metacognitive knowledge} refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans. To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH. (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label. This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic, even though this article applies it to math problems.",0,"methodology presented is domain-agnostic,
even though this article applies it to math prob-
lems.
*Equal contribution1Mila, University of Mon-
treal2Google Deepmind3The University of Cambridge
4Princeton University. Correspondence to: Aniket Didolkar
<adidolkar123@gmail.com >, Anirudh Goyal <anirud-
hgoyal9119@gmail.com >.
The first AI for MATH Workshop at the 41st International Confer-
ence on Machine Learning, Vienna, Austria. Copyright 2024 by
the author(s).1. Introduction
Large language models (LLMs) have demonstrated remark-
able advancements in recent years at natural language in-
ference tasks ( 1–7), as well as scientific and mathematical
problems ( 8–11), although their limitations on mathematical
problems are also well-documented (12–17).
A core concept in human pedagogy is Metacognition (18),
sometimes described as thinking about thinking . It refers
to ability to reason about one’s own cognitive processes as
well as about learning-relevant properties of information
or data. Metacognitive Knowledge refers to the learner’s
accumulated knowledge of this type. Pedagogy research
shows that improving learners’ metacognitive knowledge
can improve their capabilities, for example on math ( 19;20).
The current paper raises the question “Do LLMs also have
metacognitive knowledge?” And if yes, Can we bootstrap
such knowledge to further improve LLM capabilities?
At first glance, this quest seems difficult. Deciphering
LLMs’ inner working from their huge set of parameters
–all results of non-linear optimization— is notoriously hard.
Furthermore, scientists lack parameter access to most lead-
ing AI models. But there are still reasons to hope we can
understand metacognition by interacting with LLMs. They
display some human tics, such as ability to improve their
math reasoning via Chain of Thought (CoT) (21) and also
the “Let’s think step by step” prompt ( 22). These were gen-
erally perceived as convenient tricks to get around the limi-
tations imposed by the LLM’s aut future work.
Paper organization and main results: Section 3 describes
the method and Section 4 describes experiments. Using
a strong LLM - GPT-4 - to identify skills, we validate
the usefulness of these skills by demonstrates a significant
11.6% enhancement over CoT on the MATH Dataset us-
ing the method described in Section 3. Furthermore, the
identified skills also improve the generation of code-based
solutions for the problems within the MATH dataset giv-
ing a 7.52% improvement over the baseline PAL approach
(24), which also instructs the model to generate code. Sec-
tion 4.3 shows that the the skill exemplar repository created
for MATH noticeably improved in-context performance for
weaker LLMs on the same dataset and that the repository for
GSM8K helped improve in-context performance for other
math datasets. This shows that a powerful LLM can be used
for deeper understanding of skills that translates across other
LLMs and related datasets.
2. Related Works
For human learning, statistical methods can infer latent skills
from data and use the inferred skills to more accurately
forecast student learning ( 25;26). In machine learning,
works that study learning via skill induction include ( 27–30).
These start with some definition of skills in terms of model
parameters, whereas we use a powerful LLM in a black
box way to identify and consolidate skills. A discussion
of various prompting strategies is covered in Section 4 and
Appendix Section 8.
3. Automated Skill Discovery
We describe an automated process for categorizing mathe-
matical questions according to specific skills needed to solve
them. See Figure 1. Recent works relating skills and LLMs
(31;32) were an inspiration. Conceptually, the strategy in-
volves the creation of a detailed skill exemplar repository,
which contains a compilation of skill names alongside re-
spective illustrative examples (comprising both questions
2
Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving
Da",False
