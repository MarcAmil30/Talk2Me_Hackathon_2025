name,abstract,tags,pdf_link,title,type
"['George Tsoukalas', 'Jasper Lee', 'John Jennings', 'Jimmy Xin', 'Michelle Ding', 'Michael Jennings', 'Amitayush Thakur', 'Swarat Chaudhuri']","We present PutnamBench, a new multilingual evaluation benchmark for formal theorem-proving. PutnamBench consists of formalizations of problems sourced from the William Lowell Putnam Mathematical Competition, the premier undergraduate-level mathematics competition in North America. All the problem statements come with formalizations in Lean 4 and Isabelle; a substantial subset have Coq formalizations as well. PutnamBench consists of 1337 hand-written formalizations across the three proof assistants, and aims to benchmark the next generation of theorem-proving algorithms for competition mathematics.  Proving the theorems requires significant problem-solving ability and proficiency in a broad range of topics taught in undergraduate mathematics courses. We evaluate several established neural and symbolic theorem provers using PutnamBench. These approaches can only solve a handful of the problems, establishing our benchmark as a difficult open challenge for research on formal theorem-proving. PutnamBench is available at https://github.com/trishullab/PUTNAM.","['theorem proving', 'automated mathematical reasoning', 'AI for MATH', 'Lean 4', 'Coq', 'Isabelle']",https://openreview.net/pdf/f3aad42fb7a93a0a80417c4bf1486c090f4651a0.pdf,PutnamBench: A Multilingual Competition-Mathematics Benchmark for Formal Theorem-Proving,AI4MATH
"['Amrith Setlur', 'Saurabh Garg', 'Xinyang Geng', 'Naman Garg', 'Virginia Smith', 'Aviral Kumar']","Training on model-generated synthetic data is a promising approach for finetuning LLMs, but it remains unclear when it helps or hurts. In this paper, we investigate this for reasoning problems via an empirical study, followed by a theoretical formalization. 
First, we find that while the typical approach of finetuning a model on synthetic correct or *positive* problem-solution pairs generated by capable models offers modest performance gains, sampling more correct solutions from the finetuned learner **doubles** the sample efficiency of synthetic data. At the same time, training on model-generated positives can amplify  spurious  correlations, resulting in flat or even inverse scaling trends as the amount of data increases. Surprisingly, we find that several of these issues can be addressed if we also utilize *negative* responses, \ie model-generated responses that are deemed incorrect via final answer checking. Crucially, these negatives must be constructed such that the training can appropriately recover the utility or credit of each intermediate step in the negative response. With this \emph{per-step} scheme, we are able to attain consistent gains over only positive data, attaining performance similar to amplifying the amount of synthetic data by **8x**. We show that training on per-step negatives can help to unlearn spurious correlations in the positive data, and is equivalent to advantage-weighted reinforcement learning (RL), implying that it inherits benefits of RL over imitating positive data alone.","['synthetic data', 'large language models', 'math reasoning', 'reinforcement learning']",https://openreview.net/pdf/aa31d8eb1de7fb2f55359bbe2174056de902fe7d.pdf,Learning to Reason by Failing: Offline RL on Sub-optimal Rollouts Scales Synthetic Data by 8x,AI4MATH
"['Vasilii Nesterov', 'Yermek Kapushev', 'Mikhail Burtsev']","Integrating large language models as proof assistants with theorem provers has shown great promise. However, one of the major challenges in this field is the scarcity of training data. To address this, we release a new open-source tool, *Lean4trace*, for training data extraction from Lean 4 sources. Unlike previous approaches, *Lean4trace* is deeply integrated into the Lean elaborator, allowing us to modify proofs on-the-fly. Leveraging this feature, we propose two methods of data augmentation in Lean: (1) decomposing composite proof steps into multiple simpler steps;  (2) testing existing proof automation tactics at each proof state and collecting the successful ones. Models trained on this augmented data are capable of proving 58.0% of theorems from a hold-out subset of Mathlib and 35.6% of the test subset of the MiniF2F benchmark.","['Theorem Proving', 'Automated theorem proving', 'Data augmentation', 'AI for math']",https://openreview.net/pdf/7bc6e168a9fe100095a13b5221a503d8d173f3ab.pdf,Lean4trace: Data augmentation for neural theorem proving in Lean,AI4MATH
"['Max Vladymyrov', 'Johannes von Oswald', 'Nolan Andrew Miller', 'Mark Sandler']","This paper investigates the potential of linear Transformers as solvers for systems of linear equations. We propose a novel approach where the Transformer encodes each equation as a separate token, allowing the model to process the system in a permutation-invariant manner. To enhance generalizability and reduce the parameter count, we introduce a block-wise re-parameterization technique for the attention weight matrices. This technique decouples the problem dimension from the model's parameter count, enabling the Transformer to effectively handle systems of varying sizes. Our experiments demonstrate the Transformer's competitive performance compared to established classical methods such as Conjugate Gradient, especially for systems with smaller sizes. We further explore the model's ability to extrapolate to larger systems, providing evidence for its potential as a versatile and efficient solver for linear equations.","['linear attention', 'numerical methods', 'linear system of equations']",https://openreview.net/pdf/70ff41de083363020856e5381537edb482990dd0.pdf,Efficient Linear System Solver with Transformers,AI4MATH
"['Zhenyu Wu', 'Qingkai Zeng', 'Zhihan Zhang', 'Zhaoxuan Tan', 'Chao Shen', 'Meng Jiang']","Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple yet effective verification method can unleash inherent capabilities of the LLMs. That is to mask a key condition in the question, add the current response to construct a verification question, and predict the condition to verify the response. The condition can be an entity in an open-domain question or a numeric value in a math question, which requires minimal effort (via prompting) to identify. We propose an iterative verify-then-correct framework to progressively identify and correct (probably) false responses, named ProCo. We conduct experiments on three reasoning tasks. On average, ProCo, with GPT-3.5-Turbo as the backend LLM, yields $+6.8$ exact match on four open-domain question answering datasets, $+14.1$ accuracy on three arithmetic reasoning datasets, and $+9.6$ accuracy on a commonsense reasoning dataset, compared to Self-Correct.","['Self-Correct', 'Multi-step Reasoning', 'Prompting', 'Chain-of-Thought', 'Large Language Models']",https://openreview.net/pdf/0afed850bcc759f25fe19106cf23af0f53a7d57d.pdf,Large Language Models Can Self-Correct with Minimal Effort,AI4MATH
"['Alexander Havrilla', 'Yuqing Du', 'Sharath Chandra Raparthy', 'Christoforos Nalmpantis', 'Jane Dwivedi-Yu', 'Eric Hambro', 'Sainbayar Sukhbaatar', 'Roberta Raileanu']","Reinforcement Learning from Human Feedback (\textbf{RLHF}) has emerged as a dominant approach for aligning LLM outputs with human preferences. Inspired by the success of RLHF, we study the performance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization (\textbf{PPO}), Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model. We additionally start from multiple initializations with and without supervised fine-tuning (\textbf{SFT}) data.  Overall, we find models fine-tuned with Expert Iteration to consistently achieve the highest task accuracy with PPO and RCRL close behind. Surprisingly, the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of $10^6$ samples to converge from a pretrained checkpoint. We investigate why this is the case, concluding that during RL training models fail to explore significantly beyond solutions already produced by SFT models. Additionally, we discuss a trade off between maj@1 and pass@96 metric performance during SFT training and how conversely RL training improves both simultaneously. We then conclude by discussing the implications of our findings for RLHF and the future role of RL in LLM fine-tuning.","['Reasoning', 'RL', 'LLM']",https://openreview.net/pdf/8b3bf32d71f7ec91ebdcfd798094623345755e15.pdf,Teaching Large Language Models to Reason with Reinforcement Learning,AI4MATH
"['Koji Hashimoto', 'Koshiro Matsuo', 'Masaki Murata', 'Gakuto Ogiwara', 'Daichi Takeda']","Mathematical inverse problems of determining a governing differential equation for given solution data remain a fundamental challenge.
To find a working example of AI for math, we provide a concrete example using a physical setup of a quantum gravity problem.
We present a novel sparse Neural Network (NN) model which is interpretable, to solve the inverse problem: the AdS/CFT correspondence.
According to the conjectured correspondence, a special condensed matter system on a ring is equivalent to a gravity system on a bulk disk. The inverse problem is to reconstruct the higher-dimensional gravity metric from the data of the condensed matter system. 
We use the response functions of a condensed matter system as our data, and 
by supervised machine learning, we successfully train the neural network which is equivalent to a scalar field equation on an emergent geometry of the bulk spacetime. 
The developed method may work as a ground for generic bulk reconstruction, i.e. a solution to the inverse problem of the AdS/CFT correspondence.
From a technical perspective, to achieve better numerical control, our neural network model incorporates a novel layer that implements the Runge-Kutta method.","['Machine Learning', 'ICML', 'AdS/CFT', 'Runge-Kutta Method']",https://openreview.net/pdf/1843f03c96256409a143dfdb23c8318c0813895d.pdf,AI for an inverse problem: Physical model solving quantum gravity,AI4MATH
"['George Granberry', 'Wolfgang Ahrendt', 'Moa Johansson']","Formal specifications are supposed to unambigu-
ously describe the behaviour of (parts of) pro-
grams and are usually provided as extra annota-
tions of the program code. The intention is both
to document the code and to be able to automati-
cally check compliance of programs using formal
methods tools. Writing good specifications can
however be both difficult and time-consuming for
the programmer. In this case-study, we investigate
how GPT-4 can help with the task. We propose
a neuro-symbolic integration, by which we aug-
ment the LLM prompts with outputs from two
formal methods tools in the Frama-C ecosystem
(Pathcrawler and EVA), and produce C program
annotations in the specifications language ACSL.
We demonstrate how this impacts the quality of
annotations: information about input/output ex-
amples from Pathcrawler produce more context-
aware annotations, while the inclusion of EVA
reports yields annotations more attuned to run-
time errors.","['Specification', 'Synthesis', 'Formal Methods', 'LLM']",https://openreview.net/pdf/92d4eaa2df1188934d60f18bffc337a55e590e99.pdf,Specify What? A Case-Study using GPT-4 and Formal Methods For Specification Synthesis,AI4MATH
"['Xueqing Wu', 'Rui Zheng', 'Jingzhen Sha', 'Te-Lin Wu', 'Hanyu Zhou', 'Tang Mohan', 'Kai-Wei Chang', 'Nanyun Peng', 'Haoran Huang']","Data analysis is a crucial analytical process to generate in-depth studies and conclusive insights to comprehensively answer a given user query for tabular data. In this work, we aim to propose new resources and benchmarks to inspire future research on this crucial yet challenging and under-explored task. However, collecting data analysis annotations curated by experts can be prohibitively expensive. We propose to automatically generate high-quality answer annotations leveraging the code-generation capabilities of LLMs with a multi-turn prompting technique. We construct the **DACO dataset**, containing (1) 440 databases (of tabular data) collected from real-world scenarios, (2) ~2k query-answer pairs that can serve as weak supervision for model training, and (3) a concentrated but high-quality test set with human refined annotations that serves as our main evaluation benchmark. We train a 6B supervised fine-tuning (SFT) model on DACO dataset, and find that the SFT model learns reasonable data analysis capabilities. To further align the models with human preference, we use reinforcement learning to encourage generating analysis perceived by human as helpful, and design a set of dense rewards to propagate the sparse human preference reward to intermediate code generation steps. Our DACO-RL algorithm is evaluated by human annotators to produce more helpful answers than SFT model in 57.72% cases, validating the effectiveness of our proposed algorithm.","['data analysis', 'tabular data', 'code generation']",https://openreview.net/pdf/46caee17a170a1a8ce500d42f3be4659ebc8f34b.pdf,DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation,AI4MATH
"['Denis Tarasov', 'Kumar Shridhar']","Large Language Models (LLMs) have demonstrated proficiency in their reasoning abilities, yet their large size presents scalability challenges and limits any further customization. In contrast, compact models offer customized training but often fall short in solving complex reasoning tasks. This study focuses on distilling the LLMs' decomposition skills into compact models using offline reinforcement learning. We leverage the advancements in the LLM`s capabilities to provide feedback and generate a specialized task-specific dataset for training compact models. The development of an AI-generated dataset and the establishment of baselines constitute the primary contributions of our work, underscoring the potential of compact models in replicating complex problem-solving skills.","['reasoning', 'reinforcement learning', 'dataset', 'benchmark']",https://openreview.net/pdf/b5ceba0148c6dcdfa3dd15ada8c3e29bc88ec2c8.pdf,Distilling LLMs’ Decomposition Abilities into Compact Language Models,AI4MATH
"['Chuanyang Zheng', 'Zhengying Liu', 'Enze Xie', 'Zhenguo Li', 'Yu Li']","The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that en- hance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic mul- tiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experi- ments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text- davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sam- ple paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% → 91.9%), GSM8K (92% → 95.5%), AQuA (76.4% → 79.9%) and MATH (50.3% → 53.9%).","['Language models', 'natural language processing', 'reasoning']",https://openreview.net/pdf/e6106df113d9ae7f3ed2d023ebc8c8f9d8c3a3f5.pdf,Progressive-Hint Prompting Improves Reasoning in Large Language Models,AI4MATH
"['Vernon Toh Yan Han', 'Ratish Puduppully', 'Nancy F. Chen']","Large Language Models (LLMs), combined with program-based solving techniques, are increasingly demonstrating proficiency in mathematical reasoning. For example, closed-source models such as OpenAI GPT-4 and Claude show excellent results in solving math word problems. However, progress in math word problem-solving for open-source LLMs is limited, and the challenges these models face are not well-studied. In this paper, we study the performance of strong open-source LLMs, including Llama 2 (7B), Code Llama (7B), and Mistral (7B) on math word problems using program-based solving techniques. Specifically, we analyze the outputs of these models when applied to math word problems and identify a category of problems that pose a significant challenge, particularly those involving quantities spanning multiple units. To address this issue, we propose a systematic approach by defining the units for each quantity and ensuring the consistency of these units during mathematical operations. We developed Unit Consistency Programs (UCPs), an annotated dataset of math word problems, each paired with programs containing unit specifications and unit verification routines. We fine-tuned Llama 2 (7B), Code Llama (7B), and Mistral (7B) models with UCPs to produce their VerityMath variants. Our findings indicate that our approach, which incorporates unit consistency, currently slightly underperforms compared to an approach that does not. To understand the reasons behind this, we conducted an in-depth error analysis and suggested options for future improvements.","['math word problem solving', 'python programs', 'dataset']",https://openreview.net/pdf/2e116b08e4c0d273b422594f60a3c16662b3d034.pdf,VerityMath: Advancing Mathematical Reasoning by Self-Verification Through Unit Consistency,AI4MATH
"['Denisa Roberts', 'Lucas Roberts']","In this article, we investigate vision-language models (VLM) as reasoners. The ability to form abstractions underlies mathematical reasoning, problem-solving, and other Math AI tasks. Several formalisms have been given to these underlying abstractions and skills utilized by humans and intelligent systems for reasoning. Furthermore, human reasoning is inherently multimodal, and as such, we focus our investigations on multimodal AI. In this article, we employ the abstractions given in the SMART task (Simple Multimodal Algorithmic Reasoning Task) introduced in \cite{cherian2022deep} as meta-reasoning and problem-solving skills along eight axes: math, counting, path, measure, logic, spatial, and pattern. We investigate the ability of vision-language models to reason along these axes and seek avenues of improvement. Including composite representations with vision-language cross-attention enabled learning multimodal representations adaptively from fused frozen pretrained backbones for better visual grounding. Furthermore, proper hyperparameter and other training choices led to strong improvements (up to $48\%$ gain in accuracy) on the SMART task, further underscoring the power of deep multimodal learning. The smartest VLM, which includes a novel QF multimodal layer, improves upon the best previous baselines in every one of the eight fundamental reasoning skills. End-to-end code is available at \href{https://github.com/D-Roberts/smarter}{github.com/D-Roberts/smarter}.","['Reasoning', 'Vision-Language', 'Deep Learning', 'Neural Networks']",https://openreview.net/pdf/e71382c2b2bdd9fa5eaeae740459879c5613a036.pdf,Smart Vision-Language Reasoners,AI4MATH
"['Ting Wu', 'Xuefeng Li', 'Pengfei Liu']","Self-improvement through post-training methods such as iterative preference learning has been acclaimed for enhancing the problem-solving capabilities~(e.g., mathematical reasoning) of Large Language Models~(LLMs) without human intervention. However, as exploration deepens, it becomes crucial to assess whether these improvements genuinely signify progress in solving more challenging problems or if they could lead to unintended regressions. To address this, we propose a comprehensive evaluative framework that goes beyond the superficial pass@1 metric to scrutinize the underlying enhancements of post-training paradigms for self-improvement. Through rigorous experimentation and analysis across diverse problem-solving tasks, the empirical results point out the phenomenon of \emph{self-improvement reversal}, where models showing improved performance across benchmarks will paradoxically exhibit declines in broader, essential capabilities, like output diversity and out-of-distribution~(OOD) generalization. These findings indicate that current self-improvement practices through post-training are inadequate for equipping models to tackle more complex problems. Furthermore, they underscore the necessity of our critical evaluation metrics in discerning the \emph{progress or regress} dichotomy for self-improving LLMs.","['Large Language Model', 'Self-improvement', 'Evaluation']",https://openreview.net/pdf/2fb1e8e4e7c044ab018b903eb62e5ffdb089546a.pdf,Progress or Regress? Self-Improvement Reversal in Post-training,AI4MATH
"['Vishruth Veerendranath', 'Vishwa Shah', 'Kshitish Ghate']","Quantitative and numerical comprehension in language is an important task in many fields like education and finance, but still remains a challenging task for language models. While tool and calculator usage has shown to be helpful to improve mathematical reasoning in large pretrained decoder-only language models, this remains unexplored for smaller language models with encoders. In this paper, we propose Pre-Calc, a simple pre-finetuning objective of learning to use the calculator for both encoder-only and encoder-decoder architectures, formulated as a discriminative and generative task respectively. We pre-train BERT and RoBERTa for discriminative calculator use and Flan-T5 for generative calculator use on the MAWPS, SVAMP, and AsDiv-A datasets, which improves performance on downstream tasks that require numerical understanding.","['MathNLI', 'Mathematical Reasoning']",https://openreview.net/pdf/4c53285eb3ec1484b1b90f4da18df757f653be53.pdf,Pre-Calc: Learning to Use the Calculator Improves Numeracy in Language Models,AI4MATH
"['Jonathan David Thomas', 'Andrea Silvi', 'Devdatt Dubhashi', 'Emil Carlsson', 'Moa Johansson']","The emergence of mathematical concepts, such as number systems, is an understudied area in AI for mathematics and reasoning. It has previously been shown (Carlsson et al., 2021) that by using reinforcement learning (RL), agents can derive simple approximate and exact-restricted numeral systems. However, it is a major challenge to show how more complex recursive numeral systems, similar to the one utilised in English, could arise via a simple learning mechanism such as RL. Here, we introduce an approach towards deriving a mechanistic explanation of the emergence of recursive number systems where we consider an RL agent which directly optimizes a lexicon under a given meta-grammar. Utilising a slightly modified version of the seminal meta-grammar of (Hurford, 1975), we demonstrate that our RL agent can effectively modify the lexicon towards Pareto-optimal configurations which are comparable to those observed within human numeral systems.","['Reinforcement Learning', 'Recursive Numeral Systems', 'Representation Learning', 'Language Evolution']",https://openreview.net/pdf/017a4ef5896e1abf13238bf6c96df91a351c84ea.pdf,Learning Efficient Recursive Numeral Systems via Reinforcement Learning,AI4MATH
"['Guillem Tarrach', 'Albert Q. Jiang', 'Daniel Raggi', 'Wenda Li', 'Mateja Jamnik']","The formalization of mathematical theorems and their proofs is a time-consuming and tedious process which, despite recent advances in the reasoning capabilities of AI systems, remains a challenging task for computers. Existing attempts to automate the process with language models struggle with the difference in level of detail between formal and informal proofs. Successful autoformalization requires models to understand and be able to explain the nuances of logical arguments, a critical aspect of reasoning that is often overlooked in existing research. In this work, we introduce Sketch, Prove, Add Detail & Repeat (SPADeR), an approach that enhances proof autoformalizers by using language models to infer and explicitly incorporate implicit details from informal proofs. With the same number of autoformalization attempts, our method increases the percentage of successfully formalized problems in the miniF2F test dataset from 34.8% to 38.1%.","['autoformalization', 'formalization', 'automated theorem proving', 'large language models', 'proof verification', 'mathematical reasoning']",https://openreview.net/pdf/648d1ef62a6f6ea84faa78689bbf3a7ceec0fc9a.pdf,"More Details, Please: Improving Autoformalization with More Detailed Proofs",AI4MATH
"['Duc M. Nguyen', 'Sungahn Ko']","This technical report presents an approach utilizing open-source Large Language Models for Automated Optimization Problem-solving With Code Challenge at the ICML 2024 AI4Math Workshop. This challenge emphasizes the ability of Large Language Models (LLMs) to handle complex mathematical reasoning from formulating to solving the problem at hand. By exploring different prompting techniques, such as few-shot, self-consistency, chain-of-thought, and tree-of-thought, we aim to explore the current state-of-the-art LLMs' mathematical reasoning abilities.","['Open Source', 'Large Language Model', 'Automated Math Reasoning', 'Prompt Engineering']",https://openreview.net/pdf/0587d424a1a041845ea791237cb2754d46157cc3.pdf,Technical Report for ICML 2024 Automated Math Reasoning Challenge: Solving Optimization Problems with Open Source Large Language Model,AI4MATH
"['Lifan Yuan', 'Ganqu Cui', 'Hanbin Wang', 'Ning Ding', 'Xingyao Wang', 'Jia Deng', 'Boji Shan', 'Huimin Chen', 'Ruobing Xie', 'Yankai Lin', 'Zhenghao Liu', 'Bowen Zhou', 'Hao Peng', 'Zhiyuan Liu', 'Maosong Sun']","We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of EURUS can be primarily attributed to ULTRAINTERACT, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. ULTRAINTERACT can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting of (1) reasoning chains with diverse planning strategies in a unified format, (2) multi-turn interaction trajectories with the environment and the critique, and (3) pairwise data to facilitate preference learning. ULTRAINTERACT allows us to conduct an in-depth exploration of preference learning for reasoning tasks. Our investigation reveals that some well-established preference learning algorithms may be less suitable for reasoning tasks compared to their effectiveness in general conversations. Inspired by this, we derive a novel reward modeling objective which, together with ULTRAINTERACT, leads to a strong reward model. All artifacts produced during this research will be made public.","['Alignment', 'Data', 'Reasoning', 'Preference Tree']",https://openreview.net/pdf/cdf640fc47f06c403bd674317fc554bcacd8d5e9.pdf,Advancing LLM Reasoning Generalists with Preference Trees,AI4MATH
"['Mehran Kazemi', 'Hamidreza Alvari', 'Ankit Anand', 'Jialin Wu', 'Xi Chen', 'Radu Soricut']","Large language models have shown impressive results for multi-hop mathematical reasoning when the input question is only textual. Many mathematical reasoning problems, however, contain both text and image. With the ever-increasing adoption of vision language models (VLMs), understanding their reasoning abilities for such problems is crucial. 
In this paper, we evaluate the reasoning capabilities of VLMs along various axes through the lens of geometry problems. 
We procedurally create a synthetic dataset of geometry questions with controllable difficulty levels along multiple axes, thus enabling a systematic evaluation.
The empirical results obtained using our benchmark for state-of-the-art VLMs indicate that these models are not as capable in subjects like geometry (and, by generalization, other topics requiring similar reasoning) as suggested by previous benchmarks. This is made especially clear by the construction of our benchmark at various depth levels, since solving higher-depth problems requires long chains of reasoning rather than additional memorized knowledge.","['Large Language Models', 'Vision Language Models', 'LLM Reasoning', 'Geometric Reasoning']",https://openreview.net/pdf/3ba5283059bb755e01651618340073d09b23f233.pdf,GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning,AI4MATH
"['Aniket Rajiv Didolkar', 'Anirudh Goyal', 'Nan Rosemary Ke', 'Siyuan Guo', 'Michal Valko', 'Timothy P Lillicrap', 'Danilo Jimenez Rezende', 'Yoshua Bengio', 'Michael Curtis Mozer', 'Sanjeev Arora']","\emph{Metacognitive knowledge} refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also  have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure  to get a powerful  LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans.

To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH.  (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label.  This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic,  even though this article applies it to math problems.","['metacognitive knowledge', 'math reasoning']",https://openreview.net/pdf/b658ad52e686b34e585fbe860bd4a1bbf08341ab.pdf,Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving,AI4MATH
"['Majdi Hassan', 'Nikhil Shenoy', 'Jungyoon Lee', 'Hannes Stark', 'Stephan Thaler', 'Dominique Beaini']","Predicting low-energy molecular conformations given a molecular graph is an important but challenging task in computational drug discovery. Existing state-of-the-art approaches either resort to large scale transformer-based models that diffuse over conformer fields, or use computationally expensive methods to generate initial structures and diffuse over torsion angles. In this work, we introduce Equivariant Transformer Flow (ET-Flow). We showcase that a well-designed flow matching approach with equivariance and harmonic prior alleviates the need for complex internal geometry calculations and large architectures, contrary to the prevailing methods in the field. Our approach results in a straightforward and scalable method that directly operates on all-atom coordinates with minimal assumptions. ET-Flow outperforms or matches the previous state-of-the-art in molecular conformer generation benchmarks with significantly fewer parameters, no dependence on internal geometry, and fast inference.","['flow matching', 'molecular conformers', 'equivariant graph networks']",https://openreview.net/pdf/65288dfe6291289a7a0dcf5c03978ad5a57b5d76.pdf,Equivariant Flow Matching for Molecular Conformer Generation,ML4LMS
"['Panukorn Taleongpong', 'Brooks Paige']","Deep molecular generative models have shown promising results and paved a new way for drug discovery. Their ability to explore the molecular space, estimated to be 1060, is significantly greater than traditional methods used for the virtual screening of existing databases. We introduce a novel fragmentation algorithm particularly suitable for use in deep generative models. In contrast to existing fragmentation algorithms, our procedure sequentially breaks a molecule along BRIC bonds in such a manner that the linearization of fragments is directly invertible, guaranteed to be able to reconstruct the original molecule from the fragment sequence. This makes it appropriate for use in deep generative models trained with sequential models as likelihoods. We compare with previous fragment-based SMILES VAE methods and observe that our approach significantly enhances coverage of the molecular space and outperforms on distribution learning benchmarks.","['Deep generative models', 'molecules', 'VAE', 'SMILES', 'fragment-based']",https://openreview.net/pdf/f8bc0ace232c735192259521a6074bf07bad0c27.pdf,Improving Fragment-Based Deep Molecular Generative Models,ML4LMS
"['Rıza Özçelik', 'Sarah de Ruiter', 'Emanuele Criscuolo', 'Francesca Grisoni']","Generative deep learning is reshaping drug design. Chemical language models (CLMs) — which generate molecules in the form of molecular strings — bear particular promise for this endeavor. Here, we introduce a recent deep learning architecture, termed Structured State-Space Sequence (S4) model, into de novo drug design. In addition to its unprecedented performance in various fields, S4 has shown remarkable capabilities to learn the global properties of sequences. This aspect is intriguing in chemical language modeling, where complex molecular properties like bioactivity can 'emerge' from separated portions in the molecular string. This observation gives rise to the following question: Can S4 advance chemical language modeling for de novo design? To provide an answer, we systematically benchmark S4 with state-of-the-art CLMs on an array of drug discovery tasks, such as the identification of bioactive compounds, and the design of drug-like molecules and natural products. S4 showed a superior capacity to learn complex molecular properties, while at the same time exploring diverse scaffolds. Finally, when applied prospectively to kinase inhibition, S4 designed eight of out ten molecules that were predicted as highly active by molecular dynamics simulations. Taken together, these findings advocate for the introduction of S4 into chemical language modeling — uncovering its untapped potential in the molecular sciences.","['drug discovery', 'de novo design', 'chemical language modeling', 'state space models', 's4']",https://openreview.net/pdf/132106c8f143e55bf5334c1f9ac7fe16f07ab109.pdf,Chemical Language Modeling with Structured State Spaces,ML4LMS
"['Samuel Don Stanton', 'Robert G Alberstein', 'Nathan C. Frey', 'Andrew Martin Watkins', 'Kyunghyun Cho']","There is a growing body of work seeking to replicate the success of machine learning (ML) on domains like computer vision (CV) and natural language processing (NLP) to applications involving biophysical data. One of the key ingredients of prior successes in CV and NLP was the broad acceptance of difficult benchmarks that distilled key subproblems into approachable tasks that any junior researcher could investigate, but good benchmarks for biophysical domains are rare. This scarcity is partially due to a narrow focus on benchmarks which simulate biophysical data; we propose instead to carefully abstract biophysical problems into simpler ones with key geometric similarities. In particular we propose a new class of closed-form test functions for biophysical sequence optimization, which we call Ehrlich functions. We provide empirical results demonstrating these functions are interesting objects of study and can be non-trivial to solve with a standard genetic optimization baseline.","['black-box-optimization', 'discrete-optimization', 'benchmarking', 'drug-discovery', 'protein-design']",https://openreview.net/pdf/2fdd2a925ef8a06cae0a2d977c6c95f7f093492f.pdf,Closed-Form Test Functions for Biophysical Sequence Optimization Algorithms,ML4LMS
"['Saleem Abdul Fattah Ahmed Al Dajani', 'Frédéric Laquai']","Wearable sensors have revolutionized modern health towards precision medicine. Many measurements taken by wearables are not currently used for medical purposes, and their physical roots are not understood. Here we show the origin of inferring oxygen saturation from measuring estimated oxygen variation while preparing for a marathon via spectrophotometric pulsatile blood flow transcutaneous reflectance oximetry, building six years of data tracking the stages of running a marathon. Based on inputs from quantum mechanical simulations into classical wave and electromagnetic theory models, the imaginary part of the complex dielectric function of the active site of human hemoglobin is altered upon oxygen binding to the heme group, changing the extinction coefficient and selectively absorbing wavelengths of light in a way that enables its detection from the ratio of red to infrared absorbance. A fundamental nominal max oxygen consumption is inferred from quantum mechanical absorbance spectra with and without oxygen binding by training a machine learning model on 1.5 years of daily wearable reflective pulse oximetry data with an $R^2$ of 0.84. Reflectance oximetry is strongly correlated ($R^2$ of 0.96) to standard pulse oximetry, such as through earlobe or fingertip, enabling accurate, rapid, regular, and automated monitoring of vital signs with wearable sensors on smart watches and fitness trackers, and supplying artificial intelligence inferences of function-symptom relationships for precision medicine.","['Precision medicine', 'artificial intelligence', 'wearable sensors', 'hemoglobin', 'max oxygen consumption rate', 'pulsatile blood flow transcutaneous reflectance oximetry', 'density functional theory', 'random forest regression', 'machine learning']",https://openreview.net/pdf/3cb4aa8d6633f76486002b32ff3c2fb113d95457.pdf,Machine learning nominal max oxygen consumption from wearable reflective pulse oximetry with density functional theory,ML4LMS
"['Xiang Fu', 'Andrew Scott Rosen', 'Kyle Bystrom', 'Rui Wang', 'Albert Musaelian', 'Boris Kozinsky', 'Tess Smidt', 'Tommi Jaakkola']","In density functional theory, charge density is the core attribute of atomic systems from which all chemical properties can be derived. Machine learning methods are promising in significantly accelerating charge density prediction, yet existing approaches either lack accuracy or scalability. We propose a recipe that can achieve both. In particular, we identify three key ingredients: (1) representing the charge density with atomic and virtual orbitals (spherical fields centered at atom/virtual coordinates); (2) using expressive and learnable orbital basis sets (basis function for the spherical fields); and (3) using high-capacity equivariant neural network architecture. Our method achieves state-of-the-art accuracy while being more than an order of magnitude faster than existing methods. Furthermore, our method enables flexible efficiency-accuracy trade-offs by adjusting the model/basis sizes.","['density functional theory', 'charge density', 'equivariant', 'molecule']",https://openreview.net/pdf/ce1a97b65d6b64209d253d03f99032c1589715fc.pdf,A Recipe for Charge Density Prediction,ML4LMS
"['Jonas Teufel', 'Pascal Friederich']","AI models have advanced various branches of society, industry, and science. In the domains of chemistry and material science, for example, graph neural networks have proven a useful tool for molecular and material property predictions. Despite their superior predictive performance, the inner workings of most complex AI models remain elusive to human observers. Methods of explainable AI (xAI) can be used to increase the transparency of these predictions to gain a better understanding not only of the model's behavior but also of the underlying task itself. We introduce a method to extract structure-property relationships for molecular property predictions from global concept explanations. By clustering in a latent space of subgraph embeddings, we discover molecules with similar subgraph motifs. For each cluster of similar substructures we can compute an average contribution towards the model's target prediction - therefore reconstructing the general structure-property relationships on which the model's decisions are based on. Finally, a language model can be prompted with all information about the observed structural motifs to provide a hypothesis for a causal explanation of the method. We validate our method on various synthetic and real-world graph property prediction tasks and find that it is able to reproduce known chemical rules of thumb.","['Graph Neural Networks', 'Explainable AI', 'Concept Explanations', 'Molecular Property Prediction']",https://openreview.net/pdf/7752d2747bf1cf1a34717bfc6acdd769e8bd1a42.pdf,Finding Structure-Property Relationships for Molecular Property Predictions with Globally Explainable AI,ML4LMS
"['Vineeth Dorna', 'D. Subhalingam', 'Keshav Kolluru', 'Shreshth Tuli', 'Mrityunjay Singh', 'Saurabh Singal', 'N M Anoop Krishnan', 'Sayan Ranu']","3D generative models have shown significant promise in *structure-based drug design (SBDD)*, particularly in discovering ligands tailored to specific target binding sites. Existing algorithms often focus primarily on ligand-target binding, characterized by binding affinity. Moreover, models trained solely on target-ligand distribution may fall short in addressing the broader objectives of drug discovery, such as the development of novel ligands with desired properties like drug-likeness, and synthesizability, underscoring the multifaceted nature of the drug design process.
To overcome these challenges, we decouple the problem into molecular generation and property prediction. The latter synergistically *guides* the diffusion sampling process, facilitating guided diffusion and resulting in the creation of meaningful molecules with the desired properties. We call this guided molecular generation process as TAGMol.
Through experiments on benchmark datasets, TAGMol demonstrates superior performance compared to state-of-the-art baselines, achieving a 22% improvement in average Vina Score and yielding favorable outcomes in essential auxiliary properties. This establishes TAGMol as a comprehensive framework for drug generation. The code is available at https://github.com/moleculeai/TAGMol.","['Structure-Based Drug Design', 'Drug-Target Binding', 'Property prediction', 'Conditional Diffusion', 'Property guidance.']",https://openreview.net/pdf/11b2bf2ea710bb34cc6d5631e1aca78d4dc7aeaa.pdf,TAGMol: Target-Aware Gradient-guided Molecule Generation,ML4LMS
"['Noor Youssef', 'Sarah Gurev', 'Hannah Rivka Pierce-Hoffman', 'Alexander A Cohen', 'Luis F Caldera', 'Pamela J Bjorkman', 'Debora Susan Marks']","Mosaic nanoparticle vaccines incorporating naturally diverse sarbecovirus receptor binding domains (RBDs) represent a promising approach for pan-coronavirus vaccines. Mosaic nanoparticles elicit broad, cross-reactive immune responses, likely because elicited antibodies utilize avidity effects to preferentially bind conserved regions across neighboring RBDs. However, the diversity in natural RBDs is limited, leading to ‘off-target’ antibodies that bind to conserved regions across the selected RBDs but which are likely to mutate in the future.  We therefore develop a novel future-proof vaccine design method, building upon a probabilistic generative model of antibody escape, to computationally design RBDs with further diversity. This approach aims to focus antibody responses to regions that are (1) neutralizing, (2) accessible and (3) unlikely to mutate during future viral evolution. The designs will be assessed by immunizing mice and testing the breadth of neutralizability of the sera compared to a nanoparticle composed of naturally diverse strains.","['Future-proof vaccine design', 'nanoparticle vaccines', 'viral evolution', 'antibody', 'protein fitness models', 'protein language models', 'protein design']",https://openreview.net/pdf/e35754f20a409293df6bbfe47f01c1fbb9c2e12b.pdf,Future-proof vaccine design with a generative model of antibody cross-reactivity,ML4LMS
"['Yining Huang', 'Zuobai Zhang', 'Jian Tang', 'Debora Susan Marks', 'Pascal Notin']","Multiple Sequence Alignments (MSAs) are crucial in protein sequence analysis for identifying homologous proteins sharing a common evolutionary origin. However, traditional MSA search tools struggle to recover distantly related sequences that, despite low sequence similarity, exhibit high structural and functional resemblance—often missing in the so-called ‘midnight zone’ of protein similarity. To overcome these limitations, we propose the integration of structure similarity search tools to enhance the identification of homologous proteins. This approach utilizes Foldseek to search the AlphaFold database, aligning structurally similar proteins to construct Multiple Structure Alignments (MStructAs) alongside traditional MSAs. By combining these alignments, we develop family-specific generative models for protein fitness prediction, using diverse assays from the ProteinGym benchmarks. Our findings reveal that incorporating structure-based retrieval into MSAs significantly improves the performance of alignment-based methods, suggesting a robust hybrid retrieval strategy that harnesses both sequence and structure similarities.",['Multiple sequence alignment; protein structure retrieval; protein fitness prediction'],https://openreview.net/pdf/356c6dd9812b78992aa24f0a9c1aa6d3247e3248.pdf,Augmenting Evolutionary Models with Structure-based Retrieval,ML4LMS
"['Saleem Abdul Fattah Ahmed Al Dajani', 'David Keyes']","Deep AndersoNN is a framework for accelerating AI by taking the continuum limit as the number of explicit layers in a neural network approaches infinity, and can be taken as a single implicit layer, known as a deep equilibrium model. Solving for parameters of a deep equilibrium model reduces to a nonlinear fixed point iteration problem, enabling use of vector-to-vector iterative solvers and windowing techniques, such as Anderson extrapolation, for accelerating convergence to the fixed point deep equilibrium. Here we show that Deep AndersoNN achieves up to an order of magnitude of speed-up in training and inference. The method is demonstrated on density functional theory results for industrial applications by constructing artificial life and materials `scientists' capable of classifying biomolecules, drugs, and compounds as strongly or weakly polar, sorting metal-organic frameworks by pore size, and classifying crystalline materials as metals, semiconductors, and insulators, using graph images of node-neighbor representations transformed from atom-bond networks. Results exhibit accuracy up to 98\% and showcase synergy between Deep AndersoNN and machine learning capabilities of modern computing architectures, e.g. GPUs, for accelerated computational life and materials science by quickly identifying structure-property relationships. This paves the way for saving up to 90\% of compute required for AI, reducing its carbon footprint by up to 60 gigatons per year by 2030, and scaling above memory limitations of explicit neural networks in life and materials science, and beyond.","['Artificial life scientist', 'artificial material scientist', 'artificial intelligence', 'Anderson extrapolation', 'deep equilibrium', 'high performance computing', 'density functional theory', 'drug discovery']",https://openreview.net/pdf/599513927d64a2657f889362b864f93504c03a0e.pdf,Constructing artificial life and materials scientists with accelerated AI using Deep AndersoNN,ML4LMS
"['Chantriolnt-Andreas Kapourani', 'Alice Del Vecchio', 'Agnieszka Dobrowolska', 'Andrew Anighoro', 'Edith M. Hessel', 'Lindsay Edwards', 'Cristian Regep']","DNA models hold significant potential for linking genetic variation to transcriptional regulation, which is crucial for understanding disease mechanisms at the genetic and molecular level and developing targeted therapies. Supervised approaches, such as Enformer and Basenji, have shown promising results in predicting causal variants. Recently, self-supervised models like Nucleotide Transformer and HyenaDNA have made remarkable advancements, with variant-centric benchmarks suggesting competitive performance on the variant effect prediction task. In this study, we propose to evaluate models also on gene-centric benchmarks, which often are of higher relevance to the genetics community for mapping causal variants to affected genes.","['genetic variants', 'deep learning', 'genomics', 'gene expression', 'eqtl']",https://openreview.net/pdf/f9fdc8d3f9f30a57632294fc68dabff99b7a8d3d.pdf,Gene-centric evaluation of causal variant prediction for DNA models,ML4LMS
"['Fabian C Spoendlin', 'Wing Ki Wong', 'Guy Georges', 'Alexander Bujotzek', 'Charlotte Deane']","Proteins are highly flexible macromolecules and the ability to adapt their shape is fundamental to many functional properties. While a single, 'static' protein structure can be predicted at high accuracy, current methods are severely limited at predicting structural flexibility. A major factor limiting such predictions is the scarcity of suitable training data. Here, we focus on the functionally important antibody CDRs and related loop motifs. We implement a strategy to create a large dataset of evidence for conformational flexibility and develop AbFlex, a method able to predict CDR flexibility with high accuracy.","['antibodies', 'protein structure prediction', 'dataset', 'drug discovery', 'geometric deep learning']",https://openreview.net/pdf/b3cfe2153823aaf1c08c7f7291b8f3d9e87bae03.pdf,AbFlex: Predicting the conformational flexibility of antibody CDRs,ML4LMS
"['Alvaro Ciudad Serrano', 'Adrian Morales-Pastor', 'Laura Malo', 'Isaac Filella-Merce', 'VICTOR GUALLAR', 'Alexis Molina']","In this study, we present ScoreFormer, a novel graph transformer model designed to accurately predict molecular docking scores, thereby optimizing high-throughput virtual screening (HTVS) in drug discovery. The architecture integrates Principal Neighborhood Aggregation (PNA) and Learnable Random Walk Positional Encodings (LRWPE), enhancing the model's ability to understand complex molecular structures and their relationship with their respective docking scores. This approach significantly surpasses traditional HTVS methods and recent Graph Neural Network (GNN) models in both recovery and efficiency due to a wider coverage of the chemical space and enhanced performance. Our results demonstrate that ScoreFormer achieves competitive performance in docking score prediction and offers a substantial 1.65-fold reduction in inference time compared to existing models. We evaluated ScoreFormer across multiple datasets under various conditions, confirming its robustness and reliability in identifying potential drug candidates rapidly.","['graph transformer', 'surrogate model', 'docking', 'HTVS']",https://openreview.net/pdf/fcf73b558f85e013ecedc671ee9ed5441dc9730f.pdf,Scoreformer: A Surrogate Model For Large-Scale Prediction of Docking Scores,ML4LMS
"['Michael Jones', 'Smayan Khanna', 'Andrew Ferguson']","Coarse-grained models have become ubiquitous in biomolecular modeling tasks aimed at studying slow dynamical processes such as protein folding and DNA hybridization. Although these models considerably accelerate sampling, it remains challenging to recover an ensemble of all-atom structures corresponding to coarse-grained simulations. In this work, we introduce a generative approach called FlowBack that uses a flow-matching objective to map samples from a coarse-grained prior distribution to an all-atom data distribution. We construct our prior distribution to be amenable to any coarse-grained map and any type of macromolecule, and we find that generated structures are more robust and contain less steric clashes than those generated by previous approaches. We train a protein-specific model on structures from the Protein Data Bank which achieve state-of-the-art results on bond quality on clash score. Furthermore, we train a model on DNA-protein data which achieves excellent reconstruction and generative capabilities on complexes from the PDB as well as on coarse-grained simulations of DNA-protein binding.","['Backmapping', 'Generative', 'Flow-matching', 'Transferable', 'DNA-protein']",https://openreview.net/pdf/a71f0cb12f12209579ede380b6511c341f9a4c2f.pdf,FlowBack: A Flow-matching Approach for Generative Backmapping of Macromolecules,ML4LMS
"['Soorin Yim', 'Doyeong Hwang', 'Kiyoung Kim', 'Sehui Han']","Enzymes are biological catalysts with numerous
industrial applications, and they are categorized
by the Enzyme Commission (EC) number system
based on their catalytic activities. With over
200 million protein sequences identified, experimental
characterization of enzymes is impractical,
necessitating computational methods. Current approaches
face challenges with class imbalance and
intrinsic hierarchy of the EC number system. This
study employs hierarchical contrastive learning
for EC number prediction, effectively integrating
the EC number hierarchy into the model. Our
approach addresses severe class imbalance and
improves prediction performance, particularly for
higher hierarchical levels and previously unseen
EC numbers, demonstrating enhanced robustness
and outperforming existing methods.","['Enzymatic Reaction', 'Protein Function Prediction', 'EC Number Prediction', 'Supervised Contrastive Learning']",https://openreview.net/pdf/28281eeddd53eea7d8378c26bebc6163e4a00b0e.pdf,Hierarchical Contrastive Learning for Enzyme Function Prediction,ML4LMS
"['Alberto Cattaneo', 'Thomas Martynec', 'Stephen Bonner', 'Carlo Luschi', 'Daniel Justus']","Knowledge Graph Completion has been increasingly adopted as a useful method for several tasks in biomedical research, like drug repurposing or drug-target identification. To that end, a variety of datasets and Knowledge Graph Embedding models has been proposed over the years. However, little is known about the properties that render a dataset useful for a given task and, even though theoretical properties of Knowledge Graph Embedding models are well understood, their practical utility in this field remains controversial.
We conduct a comprehensive investigation into the topological properties of publicly available biomedical Knowledge Graphs and establish links to the accuracy observed in real-world applications. By releasing all model predictions and a new suite of analysis tools we invite the community to build upon our work and continue improving the understanding of these crucial applications.","['Machine Learning', 'Knowledge Graphs', 'Knowledge Graph Completion', 'Graph Topology', 'Drug Discovery', 'Biomedical Research']",https://openreview.net/pdf/81a11360cd14b58c8da298b3311f37d9f94859d8.pdf,Towards Linking Graph Topology to Model Performance for Biomedical Knowledge Graph Completion,ML4LMS
"['Dimitar Georgiev', 'Simon Vilms Pedersen', 'Ruoxiao Xie', 'Álvaro Fernández-Galiana', 'Molly M. Stevens', 'Mauricio Barahona']","Raman spectroscopy is a non-destructive and label-free chemical analysis technique, which plays a key role in the analysis and discovery cycle of various branches of life and material sciences. Recently, there has been a marked increase in the adoption of machine learning techniques in Raman spectroscopic analysis. Nonetheless, progress in the area is still impeded by the lack of software, methodological and data standardisation, and the ensuing fragmentation and lack of reproducibility of analysis workflows thereof. To address these issues, we introduce *RamanSPy*, an open-source Python package for Raman spectroscopic data analysis, which supports day-to-day tasks, integrative analyses, the development of methods and protocols, and the integration of advanced data analytics. *RamanSPy* is highly modular, not tied to a particular technology or data format, and can be readily interfaced with the burgeoning ecosystem for data science, statistical analysis and machine learning in Python. *RamanSPy* is hosted at https://github.com/barahona-research-group/RamanSPy, supplemented with extended online documentation, available at https://ramanspy.readthedocs.io, that includes tutorials, example applications, and details about the real-world research applications presented in this paper.","['Raman spectroscopy', 'spectral analysis', 'chemometrics', 'spectral preprocessing', 'artificial intelligence', 'machine learning', 'Python package']",https://openreview.net/pdf/8400461e17b6fc2393358d1525de96e5a3c33344.pdf,RamanSPy: Augmenting Raman Spectroscopy Data Analysis with AI,ML4LMS
"['Benedikt Kantz', 'Clemens Staudinger', 'Christoph Feilmayr', 'Johannes Wachlmayr', 'Alexander Haberl', 'Stefan Schuster', 'Franz Pernkopf']","eXplainable Artificial Intelligence (XAI) aims at providing understandable explanations of black box models. In this paper, we evaluate current XAI methods by scoring them based on ground truth simulations and sensitivity analysis. To this end, we used an Electric Arc Furnace (EAF) model to better understand the limits and robustness characteristics of XAI methods such as SHapley Additive exPlanations (SHAP), Local Interpretable Model-agnostic Explanations (LIME), as well as Averaged Local Effects (ALE) or Smooth Gradients (SG) in a highly topical setting. These XAI methods were applied to various types of black-box models and then scored based on their correctness compared to the ground-truth sensitivity of the data-generating processes using a novel scoring evaluation methodology over a range of simulated additive noise. The resulting evaluation shows that the capability of the Machine Learning (ML) models to capture the process accurately is, indeed, coupled with the correctness of the explainability of the underlying data-generating process. We furthermore show the differences between XAI methods in their ability to correctly predict the true sensitivity of the modeled industrial process.","['eXplainable Artificial Intelligence', 'Robustness', 'Noise Analysis', 'Machine Learning', 'Industrial Process Modelling', 'Evaluation Methodology']",https://openreview.net/pdf/a476b1c9e6e324bdebce6109f91bc6486db309b2.pdf,Robustness of Explainable Artificial Intelligence in Industrial Process Modelling,ML4LMS
"['Dan Ofer', 'Michal Linial']","Viruses elude the immune system through molecular mimicry, adopting their hosts biophysical characteristics. We adapt protein language models (PLMs) to differenti-ate between human and viral proteins. Understanding where the immune system and our models make mistakes could reveal viral immune escape mechanisms. We ap-plied pretrained PLMs to predict viral from human pro-teins. achieving a state-of-the-art results (99.7% ROCAUC). We use interpretable models to characterize viral escapers. Altogether, mistakes account for 3.9% of the sequences with viral proteins being disproportionally misclassified. Errors often involve proteins with low im-munogenic potential, human specific viruses, and reverse transcriptases. Viral families causing chronic infections and immune evasion are further enriched. Biological and ML models make similar mistakes. Integrating PLMs with explainable AI, we provide novel insights into viral im-mune escape mechanisms, enhancing strategies for vac-cine development and antiviral research.","['Machine learning', 'Viruses', 'Protein Language models', 'deep learning', 'explainable AI', 'immune escape']",https://openreview.net/pdf/b09603cf77e16bc99e1c029cd1c2d1b7012a84da.pdf,Protein language models expose viral mimicry and immune escape,ML4LMS
"['Ningning Chen', 'Wenkai Han', 'Sai T. Reddy']","Machine learning guided protein engineering, which consists of high-throughput screening and deep sequencing of protein mutagenesis libraries combined with machine learning is a powerful approach for engineering proteins and interrogating their ﬁtness landscapes. Uncertainty quantiﬁcation enhances the trustworthiness of model predictions by indicating reliability and thus can be used to guide downstream experimental work. Aleatoric uncertainty identifying inherent observational noise in protein properties and epistemic uncertainty revealing gaps in the model’s knowledge based on the amount of training data. Although uncertainty quantiﬁcation has been investigated in the application of protein engineering, systematic benchmarks for probabilistic machine learning model selection and the beneﬁts of different types of uncertainty in protein ﬁtness predictions are lacking. Addressing this gap, our study benchmarks six advanced probabilistic modeling techniques across eleven diverse protein-ﬁtness datasets, employing evaluation metrics on prediction accuracy and uncertainty quality to assess performance for both in-distribution and out-ofdistribution scenarios. Our ﬁndings offer valuable insights into the application of uncertaintyaware machine learning in high-throughput protein screening experiments. Our study supports more robust, efﬁcient experimental processes and enhances the practical usability of machine learning models in real-word protein ﬁtness related tasks such as therapeutic antibody optimization and viral evolution.","['protein fitness prediction', 'probabilistic modeling', 'uncertainty quantification']",https://openreview.net/pdf/ea8dc3f4f9437c6ad83f3a1ab1baf06247932801.pdf,Benchmarking probabilistic machine learning in protein ﬁtness landscape predictions,ML4LMS
"['Tashin Ahmed', 'Md Habibur Rahman Sifat']","This study presents a proof of concept for utilizing Graph Kolmogorov Arnold Networks (GraphKAN/GKAN) in predicting the binding affinity of small molecules to protein targets. Working with three protein targets, we explored the potential of GraphKAN to infer binding affinities. We compared the performance of GraphKAN with MLP-based graph neural networks, 1D convolutional neural networks (1D CNN), and machine learning algorithms like random forests. Although the model did not achieve state-of-the-art performance, our results demonstrate its feasibility and highlight its promise as a novel approach in computational drug discovery. This work opens new research directions, suggesting that further refinement and exploration of GraphKAN could significantly impact the efficiency and accuracy of binding affinity predictions, ultimately aiding in the discovery of new therapeutic agents. Source code is available at - https://github.com/TashinAhmed/ferroin.","['GraphKAN', 'KAN', 'GNN', 'Activation functions']",https://openreview.net/pdf/15240fc1de5f04e093eb31e2ae98aa5a437d424f.pdf,GraphKAN: Graph Kolmogorov Arnold Network for Small Molecule-Protein Interaction Predictions,ML4LMS
"['Nikola Kolev', 'Emily Hofmann', 'Geoff Thornton', 'Max Trouton', 'Filippo Federici', 'David Gao', 'Steven Schofield', 'Taylor Stock', 'Neil Curson']","Scanning tunneling microscopy (STM) is a powerful technique for imaging surfaces with atomic resolution, providing invaluable insights into surface structure and physical and chemical processes occurring on surfaces. A regular task of STM image analysis is detecting and labelling features of interest against the background of the unperturbed surface. Performing this segmentation manually is a labor-intensive task, requiring significant human effort. ​In this paper, we propose an automated approach to the segmentation of STM images that leverages few-shot learning and unsupervised learning to remove the requirement for large manually annotated datasets. Our technique offers greater flexibility compared to previous supervised methods, being easier to adapt to an unseen surface while maintaining high accuracy, reaching up to 90%. We demonstrate the effectiveness of our approach on two distinct surfaces: Si(001), TiO2(110). We show that our model exhibits strong generalization capabilities, adapting well to unseen surfaces with only as little as one additional labeled data point after initial training. This work represents a significant step towards more efficient and adaptable segmentation of STM images.","['Scanning Tunelling Microscope', 'unsupervised learning', 'few shot learning', 'segmentation', 'surface science', 'computer vision']",https://openreview.net/pdf/545770acf29134decd458b6d8e3e8f274cf26688.pdf,Scanning Tunneling Microscopy (STM) Image Segmentation Using Unsupervised and Few-shot Learning,ML4LMS
"['Meng Liu', 'Saee Gopal Paliwal']","Accurate prediction of protein-ligand binding affinities is crucial for drug development. Recent advances in machine learning show promising results on this task. However, these methods typically rely heavily on labeled data, which can be scarce or unreliable, or they rely on assumptions like Boltzmann-distributed data that may not hold true in practice. Here, we present DualBind, a novel framework that integrates supervised mean squared error (MSE) with unsupervised denoising score matching (DSM) to accurately learn the binding energy function. DualBind not only addresses the limitations of DSM-only models by providing more accurate absolute affinity predictions but also improves generalizability and reduces reliance on labeled data compared to MSE-only models. Our experimental results demonstrate that DualBind excels in predicting binding affinities and can effectively utilize both labeled and unlabeled data to enhance performance.","['Binding Affinity Prediction', 'Dual-Loss Framework', 'Denoising Score Matching']",https://openreview.net/pdf/21a500542c9db53d3b5b4801aa155b5718d2bd5c.pdf,DualBind: A Dual-Loss Framework for Protein-Ligand Binding Affinity Prediction,ML4LMS
"['Juno Nam', 'Sulin Liu', 'Gavin Winter', 'Rafael Gomez-Bombarelli']","We introduce LiFlow, a generative acceleration framework designed for efficiently simulating diffusive dynamics in solids, particularly lithium-based solid-state electrolytes (SSEs). LiFlow consists of two components: Propagator and Corrector, which utilize a conditional flow matching scheme to predict atomic displacements and perform denoising, respectively. Our model achieves a Spearman's rank correlation of approximately 0.7 for the lithium mean squared displacement (MSD) on test set based on composition and temperature splits and offers a substantial speedup compared to reference molecular dynamics (MD) simulations using machine learning interatomic potentials (MLIPs). This framework facilitates high-throughput virtual screening for electrolyte materials and holds promise for the optimization of the kinetic properties of crystalline solids.","['molecular dynamics', 'generative model', 'conditional flow matching', 'solid-state electrolytes']",https://openreview.net/pdf/148d89e0577cf3ddab63cab758f1d177c25f2783.pdf,Generative acceleration of molecular dynamics simulations for solid-state electrolytes,ML4LMS
"['Rebecca Birolo', 'Rıza Özçelik', 'Andrea Aramini', 'Michele R. Chierotti', 'Roberto Gobetto', 'Francesca Grisoni']","Approximately 40% of marketed drugs exhibit suboptimal pharmacokinetic profiles. Co-crystallization, where pairs of molecules form a multicomponent crystal, constitutes a promising strategy to enhance physicochemical properties without compromising the pharmacological activity.  However, finding promising co-crystal pairs is resource-intensive, due to the vast number of possible combinations. We present DeepCocrystal, a novel deep learning approach designed to predict co-crystal formation by processing the `chemical language' from a supramolecular vantage point. Rigorous validation of DeepCocrystal showed a balanced accuracy of 78% in realistic scenarios, outperforming existing models. By leveraging properties of molecular string representations, DeepCocrystal can also estimate the uncertainty of its predictions. We harness this capability in a challenging prospective study, and successfully discovered two novel co-crystal of diflunisal, an anti-inflammatory drug. This study underscores the potential of deep learning -- and in particular of chemical language processing -- to accelerate co-crystallization, and ultimately drug development, in both academic and industrial contexts.","['Deep Learning', 'chemical language', 'co-crystal prediction']",https://openreview.net/pdf/5a743d28e1336cdcd511412bffa9854afbd5b9ce.pdf,Deep Supramolecular Language Processing for Co-crystal Prediction,ML4LMS
"['Tuan Le', 'Julian Cremer', 'Djork-Arné Clevert', 'Kristof T Schütt']","We propose PoLiGenX for de novo ligand design using latent-conditioned, target-aware equivariant diffusion. Our model leverages the conditioning of the generation process on reference molecules within a protein pocket to produce shape-similar de novo ligands that can be used for target-aware hit expansion and hit optimization.
The results of our study showcase the efficacy of PoLiGenX in ligand design. Docking scores indicate that the generated ligands exhibit superior binding affinity compared to the reference molecule while preserving the shape. At the same time, our model maintains chemical diversity, ensuring the exploration of diverse chemical space. The evaluation of Lipinski's rule of five suggests that the sampled molecules possess a higher drug-likeness than the reference data. This constitutes an important step towards the controlled generation of therapeutically relevant de novo ligands tailored to specific protein targets.","['Equivariance', 'Diffusion', 'Generative Chemistry', 'Molecule Design', 'Structure-based Drug Discovery', 'Graph Neural Networks']",https://openreview.net/pdf/807a265abb4706580089e96a8e3ee62d509718d9.pdf,Latent-Guided Equivariant Diffusion for Controlled Structure-Based De Novo Ligand Generation,ML4LMS
"['Sei Chang', 'Zaiqian Chen', 'Bianca Dumitrascu', 'David A. Knowles']","RNA velocity-based methods estimate cellular dynamics and cell developmental trajectories based on spliced and unspliced RNA counts. Although numerous methods have been proposed, RNA velocity-based models vary greatly in their biophysical assumptions, architectures, and use cases. In this work, we introduce a new architecture, CellFlows, which incorporates self-supervised neural dimensionality reduction with the flexibility of neural-based latent time estimation into a mechanistic model, improving model interpretability and accuracy. CellFlows models splicing dynamics to infer gene and context-specific kinetic rates at single-cell resolution and correctly identifies both linear and branching cellular differentiation pathways originating from mouse embryonic stem cells.","['Single-cell transcriptomics', 'RNA velocity', 'Neural ODE', 'deep generative modeling', 'pseudotime', 'trajectory inference']",https://openreview.net/pdf/dea9eb2b229805646eb0f2581984ac8b3fc941d9.pdf,CellFlows: Inferring Splicing Kinetics from Latent and Mechanistic Cellular Dynamics,ML4LMS
"['Jérémie DONA', 'Arthur Flajolet', 'Andrei Marginean', 'Antoine Cully', 'Thomas PIERROT']","Designing sequences with desired properties is a common problem in Biology. Relying exclusively on wet-lab experiments to select sequences is costly and time-consuming so in-silico design is often used as a preliminary step.  The latter is hard for three reasons.  First, the search space is discrete and large. Second, scoring functions quantifying target properties may be inaccurate, especially if fitted on a limited dataset. Third, not all properties can be modeled in silico or measured in vitro, thus requiring in-vivo experiments for evaluation. Strategies have been developed in the literature to address the first two challenges. As for the third one, there is a consensus that concurrently evaluating batches of sequences, supposedly high-performing and diverse, is a good strategy to maximize the chances that at least one design will meet all desidera. Ideally, this is achieved in one shot. We develop a Quality Diversity approach, to guarantee diversity for any batch size.  We show that our method outperforms existing ones in terms of diversity, performance, and hyperparameter sensitivity on three datasets from the literature.",['Sequence-design; quality-diversity'],https://openreview.net/pdf/735f60043bf3cd6e067f0ecdf6a647142d451a78.pdf,Quality-Diversity for One-Shot Biological Sequence Design,ML4LMS
"['Udit Surya Saha', 'Michele Vendruscolo', 'Anne E Carpenter', 'Shantanu Singh', 'Andreas Bender', 'Srijit Seal']","Recent advances in machine learning for materials science have significantly improved the prediction of novel materials. Building on these methods, we have adapted them for drug discovery, specifically focusing on assessing performance on out-of-distribution data. We found this approach more effective than conventional cross-validation methods by employing k-fold n-step forward cross-validation (SFCV) for predicting small molecules. Additionally, we introduced two new metrics: discovery yield and novelty error. These metrics provide deeper insights into model applicability and prediction accuracy for drug-like molecules. Based on our findings, we recommend incorporating these metrics into state-of-the-art bioactivity prediction models for drug discovery.","['Drug Discovery', 'Validation', 'Bioacitivity', 'Out-of-Distribution', 'Protein Target']",https://openreview.net/pdf/daa98278d16a9d575661b7f03924be50f9afef3a.pdf,Out-of-Distribution Validation for Bioactivity Prediction  in Drug Discovery: Lessons from Materials Science,ML4LMS
"['Sai Krishna Sirumalla', 'David Stephen Farina Jr', 'Zhuoran Qiao', 'Daniele Alessandro Di Cesare', 'Felipe Costas Farias', 'Michael Bernard O’Connor', 'Peter John Bygrave', 'Feizhi Ding', 'Thomas Dresselhaus', 'Marcelo Gomes Pereira de Lacerda', 'Jason Matthew Swails', 'Daniel Miles', 'Matthew Welborn', 'Fred Manby', 'Thomas Miller']","We introduce a 1B-parameter transformer model pre-trained from scratch on 2.25T tokens from a massive mixture of datasets centered around drug discovery. These datasets are heterogeneous, coming from dozens of sources and covering 15 data modalities. We demonstrate the model’s capability on various molecular assay prediction tasks, including public benchmarks and internally generated holdouts from real-world drug discovery programs. Following parameter-efficient fine-tuning, the multi-modal transformer excels at multi-task predictions compared to strong molecular property prediction baselines including XGBoost and Chemprop.","['Drug discovery', 'Transformers', 'Multi-modal', 'Multi-task', 'Molecular property prediction']",https://openreview.net/pdf/e9a0c02ea3a486fa2c74a8d3caa2bd9fb190af57.pdf,Multi-Modal and Multi-Task Transformer for Small Molecule Drug Discovery,ML4LMS
"['Ruochi Zhang', 'Ningning Chen', 'Fengfeng Zhou', 'Xin Gao']","Artiﬁcial intelligence has emerged as an epicenter of global attention, given the rapid proliferation of cutting-edge AI tools. One promising avenue of application is the leveraging of deep learning methodologies to resolve complex biological conundrums. However, an essential question arises about the reliability and utility of deep learning models in the context of biosciences, where experimental data are often limited, especially in comparison to the vast data troves available in other domains. In this work, we focus on the task of identifying the change of binding afﬁnity (∆∆G) induced by mutations in protein-protein interaction, exploring the impact of the data bias, the methods of model evaluation and interpretation. Surprisingly, we ﬁnd that deep learning models may only learn the unintentional bias in the dataset instead of intrinsic principles, therefore proper data analysis and model evaluation should be applied not just focusing on improving the evaluation metrics. Our work provides a guideline to navigate the trustworthiness challenges in deep learning in bioscience and brings forth suggestions for future improvements.","['Protein-protein interactions', 'PPI', 'Data Bias', 'model evaluation and interpretation']",https://openreview.net/pdf/09f7e6a51ed8e01872f4b297a78c78105d242e94.pdf,"Navigating Trustworthiness of Deep Learning in ∆∆G prediction : Addressing Data Bias, Model Evaluation, and Interpretation",ML4LMS
"['Batuhan Koyuncu', 'Aleyna Dilan Kıran', 'Katja Heilmann', 'Laith Hamid', 'Anja Buder', 'Veronika Engert', 'Martin Walter', 'Isabel Valera']","Accurate prediction of stress in everyday life is essential to prevent chronic stress and maintain health and well-being through early and personalized intervention. With the goal of enabling reliable prediction suitable for everyday life, we present MuStP, a two-stage machine learning pipeline designed to predict stress from low-resolution heart rate (HR) and high-resolution electrocardiography (ECG) measurements from commercial smartwatches. Our model is pre-trained with labeled data collected in a controlled laboratory stress study. Subsequently, we transfer the model for everyday use, enabling it to operate with everyday smartwatch data in various environments. The model transfer strategy effectively addresses the domain shift from laboratory data to highly imbalanced smartwatch data and allows personalization. The empirical results on smartwatch data show that MuStP can predict stress everyday with an F1 score of $0.52$, despite the measurements having sparse labels for stress.","['Stress Prediction', 'Machine Learning', 'Preventive Healtcare']",https://openreview.net/pdf/0850980b2bbdf94f1c479f4dc9b98e8fe7707066.pdf,From Laboratory to Everyday Life: Personalized Stress Prediction via Smartwatches,ML4LMS
"['Indra Priyadarsini', 'Vidushi Sharma', 'Seiji Takeda', 'Akihiro Kishimoto', 'Lisa Hamada', 'Hajime Shinohara']","Development of efficient and high-performing electrolytes is crucial for advancing energy storage technologies, particularly in batteries. Predicting the performance of battery electrolytes rely on complex interactions between the individual constituents. Consequently, a strategy that adeptly captures these relationships and forms a robust representation of the formulation is essential for integrating with machine learning models to predict properties accurately. In this paper, we introduce a novel approach leveraging a transformer-based molecular representation model to effectively and efficiently capture the representation of electrolyte formulations. The performance of the proposed approach is evaluated on two battery property prediction tasks and the results show superior performance compared to the state-of-the-art methods.","['electrolyte', 'transformer', 'molecular representation']",https://openreview.net/pdf/eda336669cdc68ca4ecaa9542326e504191a71b6.pdf,Improving Performance Prediction of Electrolyte Formulations with Transformer-based Molecular Representation Model,ML4LMS
"['Dorina Weichert', 'Alexander Kister', 'Sebastian Houben', 'Gunar Ernis', 'Tim Wirtz']","In materials science and engineering, the lifetime of materials and products is tested by costly manual characterization procedures that are standardized only in certain cases. In this paper, we investigate a modular Bayesian approach to lifetime testing that can reduce the number of experiments and, thus, the overall cost of experiments. The approach is based on the correct definition of the probability of the outcome of an experiment, e.g., its likelihood. Since this is usually unknown, we extend it to the adversarial setting, finding an experimental procedure that is robust to a given set of probabilities in the worst case. By simulations, we empirically show the advantages of this procedure over the state-of-the-art and the basic approach, potentially reducing the number of costly experiments.","['Life tests', 'Bayesian Inference', 'Active Learning']",https://openreview.net/pdf/93ddd5c97e40a5766f88dcdd97495389a518990c.pdf,A Bayesian Approach to Adversarially Robust Life Testing,ML4LMS
"['Hannes Stark', 'Umesh Padia', 'Julia Balla', 'Cameron Diao']","Generating protein sequences conditioned on protein structures is an impactful technique for protein engineering. When synthesizing engineered proteins, they are commonly translated into DNA and expressed in an organism such as yeast. One difficulty in this process is that the expression rates can be low due to suboptimal codon sequences for expressing a protein in a host organism. We propose CodonMPNN, which generates a codon sequence conditioned on a protein backbone structure and an organism label. If naturally occurring DNA sequences are close to codon optimality, CodonMPNN could learn to generate codon sequences with higher expression yields than heuristic codon choices for generated amino acid sequences. Experiments show that CodonMPNN retains the performance of previous inverse folding approaches and recovers wild-type codons more frequently than baselines. Furthermore, CodonMPNN has a higher likelihood of generating high-fitness codon sequences than low-fitness codon sequences for the same protein sequence.","['proteins', 'design', 'codon', 'codon optimization', 'inverse folding', 'graph neural network', 'protein structure']",https://openreview.net/pdf/8b542cadc616220ef6b49aa0fda49f6663720811.pdf,CodonMPNN for Organism Specific and Codon Optimal Inverse Folding,ML4LMS
"['Ankit Bhardwaj', 'Joshua Weiner', 'Preetha Balasubramanian', 'Lakshmi Subramanian']","Zero-imputation methods are widely applied to address non-biological zeros in scRNA-seq data. However, these methods can introduce artificial signals, skewing the results of downstream analysis to match initial assumptions rather than emulate the underlying biological processes. This paper makes a simple but surprising observation:
we demonstrate that several popular zero imputation techniques provide significantly varied results on the downstream network inference tasks over the same real-world scRNA datasets. Benchmarking their performance on synthetically controlled simulated scRNA datasets using the SERGIO simulator and the GENIE3 network inference algorithm, we observed poor metrics across the board. A key takeaway from our analysis is both unearthing the unreliability of existing imputation techniques and the inability to define a uniform gold-standard for zero imputation.","['Zero Imputation', 'Network Inference']",https://openreview.net/pdf/c02342d63d04171f07a8ea50b1321b2b52f8be4e.pdf,Limitations of scRNA-seq Zero-Imputation Methods for Network Inference,ML4LMS
"['Tatiana Malygina', 'Olga Kalinina']","Many organisms, such as bacteria, fungi, and plants, produce intricate chemicals that are not needed for their growth and reproduction, and thus are called secondary metabolites or natural products (NPs). 
NPs are a rich source of drugs, with most antibiotics being derivatives of NPs. 
In a producer organism, NPs are synthesized by a set of enzymes encoded by genes that often lie near each other on the chromosome and are called a biosynthetic gene cluster (BGC).

In this work, we explore the capability of protein language models (PLMs) to produce meaningful representations of BGCs.
We employ transfer learning to train models to predict the chemical class of the produced compound and explore the topological properties of the produced embeddings.

The code is available at project's GitHub repository:  https://github.com/kalininalab/NaturalPPLuM.","['biosynthetic gene cluster', 'natural product discovery', 'protein language model', 'transfer learning', 'contrastive learning', 'natural language processing']",https://openreview.net/pdf/ed545c1c3af1003877fe505eb36ddb7bf58cde7b.pdf,Exploring sequence landscape of biosynthetic gene clusters with protein language models,ML4LMS
"['Stephen Zhewen Lu', 'Ziqing Lu', 'Ehsan Hajiramezanali', 'Tommaso Biancalani', 'Yoshua Bengio', 'Gabriele Scalia', 'Michał Koziarski']","High-content phenotypic screening, including high-content imaging (HCI), has gained popularity in the last few years for its ability to characterize novel therapeutics without prior knowledge of the protein target. This work focuses on the novel task of HCI-guided molecular design. We consider an approach in which we leverage an unsupervised multimodal joint embedding to define a latent similarity as a reward for GFlowNets. The proposed model learns to generate new molecules that could produce phenotypic effects similar to those of the given image target, without relying on pre-annotated phenotypic labels.  We demonstrate that our method generates molecules with high morphological and structural similarity to the target, increasing the likelihood of similar biological activity.","['drug discovery', 'generative models', 'GFlowNets', 'contrastive learning', 'multimodal data']",https://openreview.net/pdf/1caf4827280a8542994f400472d4b40284b243cd.pdf,Cell Morphology-Guided Small Molecule Generation with GFlowNets,ML4LMS
"['Gabriele Corso', 'Vignesh Ram Somnath', 'Noah Getz', 'Regina Barzilay', 'Tommi Jaakkola', 'Andreas Krause']","Diffusion models have emerged as a recent successful paradigm for molecular docking. However, these methods treat the protein either as a rigid structure, or force the model to fold proteins from unstructured noise. In this work, we instead focus on flexible docking, leveraging the unbound distribution of proteins to model the precise effect(s) of ligand binding. While Flow Matching (FM) presents an attractive option for this task, we show that a naive application of flow matching results in a complex learning task with poor performance. We thus propose Unbalanced Flow Matching, a generalization of flow matching that allows us to tradeoff sample efficiency with approximation accuracy by relaxing the marginal constraints. Empirically, we validate our framework on flexible docking, demonstrating strong improvements in protein conformation prediction while retaining comparable docking accuracy.","['protein-ligand docking', 'unbalanced transport', 'flow matching']",https://openreview.net/pdf/50130326aeb9657fc4d250893f22aa563d87ac42.pdf,Flexible Docking via Unbalanced Flow Matching,ML4LMS
"['Julian Cremer', 'Tuan Le', 'Frank Noe', 'Djork-Arné Clevert', 'Kristof T Schütt']","The generation of ligands that both are tailored to a given protein pocket and exhibit a range of desired chemical properties is a major challenge in structure-based drug design.
Here, we propose an in-silico approach for the de novo generation of 3D ligand structures using the equivariant diffusion model PILOT, combining pocket conditioning with a large-scale pre-training and property guidance.
Its multi-objective trajectory-based importance sampling strategy is designed to direct the model towards molecules that not only exhibit desired characteristics such as increased binding affinity for a given protein pocket but also maintains high synthetic accessibility.
This ensures the practicality of sampled molecules, thus maximizing their potential for the drug discovery pipeline.
PILOT significantly outperforms existing methods across various metrics on the common benchmark dataset CrossDocked2020. 
Moreover, we employ PILOT to generate novel ligands for unseen protein pockets from the Kinodata-3D dataset, which encompasses a substantial portion of the human kinome.
The generated structures exhibit predicted IC50 values indicative of potent biological activity, which highlights the potential of PILOT as a powerful tool for structure-based drug design.","['Diffusion', 'Equivariance', 'Generative Chemistry', 'Multi-objective Guidance', 'Molecule Design', 'Structure-based Drug Discovery']",https://openreview.net/pdf/fcb112534337fb21bfbf3c9272afde3a382068f6.pdf,Multi-Objective Guidance via Importance Sampling for Target-Aware Diffusion-based De Novo Ligand Generation,ML4LMS
"['Piersilvio De Bartolomeis', 'Javier Abad', 'Konstantin Donhauser', 'Fanny Yang']","Randomized trials are considered the gold standard for making informed decisions in medicine, yet they often lack generalizability to the patient populations in clinical practice. Observational studies, on the other hand, cover a broader patient population but are prone to various biases. Thus, before using an observational study for decision-making, it is crucial to benchmark its treatment effect estimates against those derived from a randomized trial. We propose a novel strategy to benchmark observational studies beyond the average treatment effect. First, we design a statistical test for the null hypothesis that the treatment effects estimated from the two studies, conditioned on a set of relevant features, differ up to some tolerance. We then estimate an asymptotically valid lower bound on the maximum bias strength for any subgroup in the observational study. Finally, we validate our benchmarking strategy in a real-world setting and show that it leads to conclusions that align with established medical knowledge.","['treatment effect', 'causal inference', 'confounding']",https://openreview.net/pdf/20d8fcfa598dcb32afebb7f86d46e01b83efe8d8.pdf,Detecting critical treatment effect bias in small subgroups,ML4LMS
"['Julien Delile', 'Srayanta Mukherjee', 'Anton Van Pamel', 'Leonid Zhukov']","Large language models (LLMs) are transforming the way information is retrieved with vast amounts of knowledge being summarized and presented via natural language conversations. Yet, LLMs are prone to highlight the most frequently seen pieces of information from the training set and to neglect the rare ones. In biomedical research, latest discoveries are key to academic and industrial actors and are obscured by the abundance of an ever-increasing literature corpus (the information overload problem). Surfacing new associations between biomedical entities, e.g., drugs, genes, diseases, with LLMs becomes a challenge of capturing the long-tail knowledge of the biomedical scientific production.  In this study, we show that typical RAG methods may leave out a significant proportion of relevant information due to clusters of over-represented concepts in the biomedical literature. We introduce a novel method that leverages a knowledge graph to down-sample these clusters and mitigate the information overload problem. Its retrieval performance is about twice better than embedding similarity alternatives on both precision and recall. Finally, we demonstrate that both embedding similarity and knowledge graph retrieval methods can be combined into a hybrid model that outperforms both, enabling potential improvements to biomedical question-answering models.","['RAG LLM', 'Information retrieval', 'knowledge graphs', 'biomedical literature']",https://openreview.net/pdf/ed6cad328f2a543a392506dd263a2d51cacdb756.pdf,Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge,ML4LMS
"['Simon L. Dürr', 'Ursula Rothlisberger']","Metals play important roles for  enzyme function and many therapeutically relevant proteins. Despite the fact that the first drugs developed via computer aided drug design were metalloprotein inhibitors, many computational pipelines still discard metalloproteins due to the difficulties of modelling them computationally. New ""cofolding"" methods such as AlphaFold3 (AF3) and RoseTTAfold-AllAtom (RFAA) promise to improve this issue by being able to dock small molecules in presence of multiple complex cofactors including metals or covalent modifications. Here, we analyze the current status for metal ion prediction using these methods. We find that currently only AF3 provides realistic predictions for metal ions, RFAA in contrast does perform worse than more specialized models such as AllMetal3D in predicting the location of metal ions accurately. We find that AF3 predictions are consistent with expected physico-chemical trends/intuition whereas RFAA often also predicts unrealistic metal ion locations.","['metalloproteins', 'cofolding', 'alphafold', 'protein-ligand interactions', 'docking', 'benchmark']",https://openreview.net/pdf/1f184552c1866dee70bbd9601eb158140fc7649c.pdf,Predicting metal-protein interactions using cofolding methods: Status quo,ML4LMS
"['Asif Khan', 'Giuseppe Torrisi', 'Luciana Luque', 'Claudia Owczarek', 'Maddy Parsons', 'Chris Sander', 'Linus Schumacher']","Triple-negative breast cancer (TNBC) is a particularly aggressive subtype of breast cancer that is usually treated with  chemotherapy. However, the effectiveness of the treatment can vary widely. Accurate prediction of the response to chemotherapy is crucial in preparing effective personalized treatment. This paper introduces a machine learning framework that uses imaging mass cytometry (IMC) data from clinical trials to train graph neural networks (GNNs) to predict whether a patient will respond to chemotherapy. Our approach combines single-cell protein expression and spatial cell-cell contact information extracted from IMC images. To account for staining variability known as batch effects, we introduce a surrogate loss function that enables learning of a representation space predictive of response, yet invariant to batch artefacts. We investigate different graph construction methods (k-nearest neighbors, k-atmost neighbors, Delaunay triangulation) to capture cell-cell contact delineating tumor microenvironment. Our framework demonstrates improved predictive performance through batch effect correction and effective integration of protein expression with spatial cellular relationships.","['Graph neural networks (GNNs)', 'Machine Learning for Spatial Analysis', 'Single-cell data analysis', 'Imaging mass cytometry (IMC)']",https://openreview.net/pdf/c453ff086a56b4a28950335cb48519a976c255f2.pdf,Batch-effect invariant graph neural networks for predicting chemotherapy response in triple-negative breast cancer patients,ML4LMS
"['Yuanqi Du', 'Michael Plainer', 'Rob Brekelmans', 'Chenru Duan', 'Frank Noe', 'Carla P Gomes', 'Alan Aspuru-Guzik', 'Kirill Neklyudov']","Rare event sampling in dynamical systems is a fundamental problem arising in the natural sciences, which poses significant computational challenges due to an exponentially large space of trajectories. For settings where the dynamical system of interest follows a Brownian motion with known drift, the question of conditioning the process to reach a given endpoint or desired rare event is definitively answered by Doob's $h$-transform. However, the naive simulation of this transform is infeasible, as it requires sufficiently many forward trajectories to estimate rare event probabilities. In this work, we propose a variational formulation of Doob's $h$-transform --- an optimization problem over trajectories between a given initial point and the desired ending point. To solve this optimization, we propose a simulation-free training objective with a model parameterization that imposes the desired boundary conditions by design. Our approach significantly reduces the search space over trajectories and avoids expensive trajectory simulation and inefficient importance sampling estimators which are required in existing methods. We demonstrate the ability of our method to find feasible transition paths on real-world molecular simulation and protein folding tasks.","['Transition Path Sampling', 'Protein Folding', 'Schrödinger Bridge']",https://openreview.net/pdf/b2eb11c3441d940de862e7e81dff696a779cb5db.pdf,Doob's Lagrangian: A Sample-Efficient Variational Approach to Transition Path Sampling,ML4LMS
['Danny Reidenbach'],"Structure-based Drug Design (SBDD), the task of designing 3D molecules (ligands) to bind with a target protein pocket, is a fundamental task in drug discovery. Recent geometric deep learning methods for SBDD fail to accurately generate valid docked structures without relying on physics-based post-processing (ie AutoDock Vina redocking), which resamples all the important geometric qualities of the molecule. Without 3D structure information or additional training on protein-ligand complexes as required by prior methods, EvoSBDD attains a state-of-the-art success rate of 86.4%, an average binding affinity of -10.27 kcal/mol, and demonstrates speed improvements up to 25.6x compared to the prior best method. EvoSBDD is the first method to maintain 100% generated molecule validity, novelty, and uniqueness and also excels in real-world off-target(s) binding prevention.","['black box optimization', 'structure-based drug design', 'multi-objective molecule optimization', 'molecule design', 'pretrained autoencoder', 'protein ligand binding', 'ligand']",https://openreview.net/pdf/0d7e6a3239499e6b00d6be3584512dacfe9ff0c3.pdf,EvoSBDD: Latent Evolution for Accurate and Efficient Structure-Based Drug Design,ML4LMS
"['Alex Hawkins-Hooker', 'Jakub Kmec', 'Oliver Bent', 'Paul Duckworth']","Although various schemes have been proposed for exploiting the distributional knowledge captured by protein language models (PLMs) to enhance supervised fitness prediction and design, lack of head-to-head comparison across different prediction strategies and different classes of PLM has made it challenging to identify the best-performing methods, and to understand the factors contributing to performance. Here, we extend previously proposed ranking-based loss functions to adapt the likelihoods of family-based and masked protein language models, and demonstrate that the best configurations outperform state-of-the-art approaches based on frozen embeddings in the low-data setting. Furthermore, we propose ensembling strategies that exploit the strong dependence of the mutational distributions learned by PLMs on sequence context, showing that they can be used to guide efficient optimisation strategies over fitness landscapes.","['protein design', 'fine-tuning', 'protein language models', 'bayesian optimisation']",https://openreview.net/pdf/d8364397676bcb675e6b342a0c9c639b9ac1a62b.pdf,Likelihood-based fine-tuning of protein language models for few-shot fitness prediction and design,ML4LMS
"['Bowen Jing', 'Hannes Stark', 'Tommi Jaakkola', 'Bonnie Berger']","Molecular dynamics (MD) is a powerful technique for studying microscopic phenomena, but its computational cost has driven significant interest in the development of deep learning-based surrogate models. We introduce generative modeling of molecular trajectories as a paradigm for learning flexible multi-task surrogate models of MD from data. By conditioning on appropriately chosen frames of the trajectory, we show such generative models can be adapted to diverse tasks such as forward simulation, transition path sampling, and trajectory upsampling. By alternatively conditioning on part of the molecular system and inpainting the rest, we also demonstrate the first steps towards dynamics-conditioned molecular design. We validate the full set of these capabilities on tetrapeptide simulations and show that our model can produce reasonable ensembles of protein monomers. Altogether, our work illustrates how generative modeling can unlock value from MD data towards diverse downstream tasks that are not straightforward to address with existing methods or even MD itself.","['molecular dynamics', 'molecular simulation', 'transition paths', 'Boltzmann distribution', 'proteins']",https://openreview.net/pdf/0bc1083ba49f7c63d995796df8a0d770e9f0f983.pdf,Generative Modeling of Molecular Dynamics Trajectories,ML4LMS
"['Dinkar Juyal', 'Harshith Padigela', 'Chintan Shah', 'Daniel Shenker', 'Natalia Harguindeguy', 'Yi Liu', 'Blake Martin', 'Yibo Zhang', 'Michael Nercessian', 'Miles Markey', 'Isaac Finberg', 'Kelsey Luu', 'Daniel Borders', 'Syed Ashar Javed', 'Emma Krause', 'Raymond Biju', 'Aashish Sood', 'Allen Ma', 'Jackson Nyman', 'John Shamshoian', 'Guillaume Chhor', 'Darpan Sanghavi', 'Marc Thibault', 'Limin Yu', 'Fedaa Najdawi', 'Jennifer A. Hipp', 'Darren Fahy', 'Benjamin Glass', 'Eric Walk', 'John Abel', 'Harsha Vardhan pokkalla', 'Andrew H. Beck', 'Sean Grullon']","Pathology images provide a unique challenge for computer-vision-based analysis: a single pathology Whole Slide Image (WSI) is gigapixel-sized and often contains hundreds of thousands to millions of objects of interest across multiple resolutions. In this work, we propose PathoLogy Universal TransfOrmer (PLUTO): a light-weight pathology FM that is pre-trained on a diverse dataset of 195 million image tiles collected from multiple sites. We design task-specific adaptation heads that utilize PLUTO's output embeddings for tasks that span pathology scales ranging from subcellular to slide-scale, including instance segmentation, tile classification, and slide-level prediction. We find that PLUTO matches or outperforms existing task-specific baselines and pathology-specific foundation models, some of which use orders-of-magnitude larger datasets and model sizes when compared to PLUTO. Our findings present a path towards a universal embedding to power pathology image analysis, and motivate further exploration around pathology foundation models in terms of data diversity, architectural improvements, sample efficiency, and practical deployability in real-world applications.","['Foundation model', 'Digital Pathology']",https://openreview.net/pdf/ba854d94e289a97781bf8dc00557fd1dbea6c3a9.pdf,PLUTO: Pathology-Universal Transformer,ML4LMS
"['Michał Koziarski', 'Andrei Rekesh', 'Dmytro Shevchuk', 'Almer M. van der Sloot', 'Piotr Gaiński', 'Yoshua Bengio', 'Cheng-Hao Liu', 'Mike Tyers', 'Robert A. Batey']","In this paper, we propose an extension of the GFlowNet framework that operates directly in the space of chemical reactions, offering out-of-the-box synthesizability, while maintaining comparable quality of generated candidates. We demonstrate that with the proposed set of reactions and fragments, it is possible to obtain a search space of molecules orders of magnitude larger than existing screening libraries while offering low costs of synthesis. We also show that the approach scales to very large fragment libraries, further increasing the number of potential molecules. Our experiments showcase the effectiveness of the proposed approach across a range of oracle models.","['drug discovery', 'generative models', 'GFlowNets', 'synthesizability']",https://openreview.net/pdf/8a755bddac38be9ab5d992e0f95823826096f6d9.pdf,RGFN: Synthesizable Molecular Generation Using GFlowNets,ML4LMS
"['Vincent Zaballa', 'Elliot E Hui']","Systems biology models are useful models of complex biological systems that may require a large amount of experimental data to fit each model's parameters or to approximate a likelihood function. These models range from a few to thousands of parameters depending on the complexity of the biological system modeled, potentially making the task of fitting parameters to the model difficult - especially when new experimental data cannot be gathered. We demonstrate a method that uses structural biology predictions to augment systems biology models to improve systems biology models' predictions without having to gather more experimental data. Additionally, we show how systems biology models' predictions can help evaluate novel structural biology hypotheses, which may also be expensive or infeasible to validate.","['Sysems biology', 'structural biology', 'probabilistic machine learning']",https://openreview.net/pdf/5a06827b76a02ccc57c496400516cfbabcac12a2.pdf,Reducing Uncertainty through Mutual Information in Structural and Systems Biology,ML4LMS
"['Mikhail Andronov', 'Natalia Andronova', 'Michael Wand', 'Djork-Arné Clevert', 'Jürgen Schmidhuber']","Template-free SMILES-to-SMILES translation models for reaction prediction and single-step retrosynthesis are of interest for industrial applications in computer-aided synthesis planning systems due to their state-of-the-art accuracy. However, they suffer from slow inference speed. We present a method to accelerate inference in autoregressive SMILES generators through speculative decoding by copying query string subsequences into target strings in the right places. We apply our method to the molecular transformer implemented in Pytorch Lightning and achieve over 3X faster inference in reaction prediction and single-step retrosynthesis.","['cheminformatics', 'reaction prediction', 'synthesis', 'transformers', 'efficient inference']",https://openreview.net/pdf/38d6ff840ad277c924bb5d6652e3269a8d01cc1a.pdf,Accelerating the inference of string generation-based chemical reaction models for industrial applications,ML4LMS
"['Dimitar Georgiev', 'Álvaro Fernández-Galiana', 'Simon Vilms Pedersen', 'Georgios Papadopoulos', 'Ruoxiao Xie', 'Molly M. Stevens', 'Mauricio Barahona']","Raman spectroscopy is widely used across life and material sciences to characterize the chemical composition of samples in a nondestructive, label-free manner. Many applications entail the unmixing of signals from mixtures of molecular species to identify the individual components present and their proportions, yet conventional methods for chemometrics often struggle with complex mixture scenarios encountered in practice. Here, we develop autoencoder neural network models for hyperspectral unmixing of Raman spectroscopy data, which we systematically validate using synthetic and experimental benchmark datasets we created in-house. Our results demonstrate that autoencoders provide improved accuracy, robustness and efficiency compared to standard unmixing methods. We also showcase the applicability of our approach to complex biological settings by showing improved biochemical characterization of volumetric Raman imaging data from a human leukemia monocytic cell line.","['Raman spectroscopy', 'hyperspectral unmixing', 'chemometrics', 'autoencoders', 'machine learning']",https://openreview.net/pdf/68ab8729b87a4a9823720791f34d3e14ec967242.pdf,Hyperspectral Unmixing for Raman Spectroscopy via Physics-Constrained Autoencoders,ML4LMS
"['Tommaso Rodani', 'Alessio ansuini', 'Alberto Cazzaniga']","We address the issue of multi-tip artifacts in Scanning Tunneling Microscopy (STM) images by applying the fast Fourier transform (FFT) as a feature engineering method. We fine-tune various neural network architectures using a synthetic dataset, including Vision Transformers (ViT). The FFT-based preprocessing significantly improves the performance of ViT models compared to using only the grayscale channel. Ablation experiments highlight the optimal conditions for synthetic dataset generation. Unlike traditional methods that are challenging to implement for large datasets and used offline, our method enables on-the-fly classification at scale. Our findings demonstrate the efficacy of combining the Fourier transform with deep learning for enhanced artifact detection in STM images, contributing to more accurate analysis in material science research.","['Machine Learning', 'ICML', 'STM images', 'material science', 'FFT', 'Fourier']",https://openreview.net/pdf/fc9573475550b12bd11247f63ffda756f1b3fb20.pdf,Enhancing Multi-Tip Artifact Detection in STM Images Using Fourier Transform and Vision Transformers,ML4LMS
"['Sanjay Nagaraj', 'Jiaqi Han', 'Aksh Garg', 'Minkai Xu']","Molecule inverse design is of critical significance in drug discovery which requires molecules to be generated based on certain chemical properties or structural compositions. Generative models, most popularly diffusion models, have shown great promise in performing inverse design through conditioning techniques and/or explicit energy guidance during sampling. In this work, we propose a novel guidance framework, Energy-Free Guidance for Geometric Diffusion Models, that effectively boosts the utility of molecule inverse design without any auxiliary energy head for guidance. The key innovation lies in the joint training strategy for the conditional and unconditional score models via random masking, which are then composed during sampling in an SE(3)-equivariant fashion, ensuring the critical physical symmetry of the geometric distribution. This feature alleviates practitioners from needing additional efforts in training energy prediction heads and avoids the adversarial gradient coming from them. We conduct experiments on a diverse range of inverse design tasks on QM9, showing that our approach achieves state-of-the-art on 4 out of 6 design targets without leveraging any external energy gradients.","['Inverse Molecular Design', 'Geometric Diffusion Models', 'SE(3) Equivariance', 'Molecular Generation', 'Classifier-Free Guidance']",https://openreview.net/pdf/882a126c0529a61a8d75874de2487adf45aa7fc4.pdf,Energy-Free Guidance of Geometric Diffusion Models for 3D Molecule Inverse Design,ML4LMS
"['Russell Maguire', 'Kotryna Bloznelyte', 'Fikayo Adepoju', 'Matthew Armean-Jones', 'Shafiat Dewan', 'Akash Gupta', 'Frances Patricia Jones', 'Preet Lalli', 'Anna Schooneveld', 'Sean Thompson', 'Ece Ebrahimi', 'Stella Fozzard', 'David Berman', 'Luca Rossoni', 'Will Addison', 'Ian Taylor']","The dominant paradigms for integrating machine-learning into protein engineering are de novo protein design and guided directed evolution. Guiding directed evolution requires a model of protein fitness, but most models are only evaluated in silico on datasets comprising few mutations. Due to the limited number of mutations in these datasets, it is unclear how well these models can guide directed evolution efforts. We demonstrate in vitro how zero-shot and few-shot protein language models of fitness can be used to guide two rounds of directed evolution with simulated annealing. Our few-shot simulated annealing approach recommended enzyme variants with 1.62 × improved PET degradation over 72 h period, outperforming the top engineered variant from the literature, which was 1.40 × fitter than wild-type. In the second round, 240 in vitro examples were used for training, 32 homologous sequences were used for evolutionary context and 176 variants were evaluated for improved PET degradation, achieving a hit-rate of 39 % of variants fitter than wild-type.","['protein', 'language models', 'machine learning', 'directed evolution', 'few-shot', 'zero-shot', 'low-n']",https://openreview.net/pdf/00c3a11ae5cca4970e9dc093d22eb65943492602.pdf,Protein Language Models in Directed Evolution,ML4LMS
"['Michael Backenköhler', 'Joschka Groß', 'Paula Linh Kramer', 'Verena Wolf', 'Andrea Volkamer']","Drug discovery can benefit from machine learning on 3D structural data of protein-ligand (PL) complexes, but a shortage of such data limits model training. For kinase targets, we generated a large in-silico dataset, kinodata-3D, using template docking. This dataset improved binding affinity predictions. Using an E(3)-invariant GNN model, we investigated learned protein-ligand interactions by removing spatial edges between protein and ligand atoms. Significant prediction changes in known binding regions confirmed the model's understanding of binding mechanisms. This approach aims to enhance explainable AI (XAI) methods, aiding the discovery of novel kinase binding mechanisms and improving model transparency.","['Binding affinity prediction', 'explainable AI', 'GNN', 'docking']",https://openreview.net/pdf/48be9473ce485049b45cddc7f2807c2e389e9efc.pdf,Structural activity prediction models recover known binding modes (Poster abstract),ML4LMS
"['Paula Torren-Peraire', 'Jonas Verhoeven', 'Dorota Herman', 'Hugo Ceulemans', 'Igor V. Tetko', 'Jörg K. Wegner']","Computer-aided synthesis planning approaches have allowed a greater exploration of potential synthesis routes, however, these methods are generally developed to produce linear routes from a singular product to a set of proposed building blocks and are not designed to leverage potential shared paths between targets. These convergent routes allow the simultaneous synthesis of compounds, reducing the time and cost of synthesis across compound libraries. We introduce a novel planning approach to develop convergent synthesis routes, which can search multiple products and intermediates simultaneously, enhancing the overall efficiency and practical applicability of retrosynthetic planning. We evaluate the multi-step synthesis planning approach using extracted convergent routes from Johnson \& Johnson Electronic Laboratory Notebooks (J\&J ELN) and publicly available datasets and observe that solvability is generally very high across those routes, being able to identify a convergent route for over 90\% of the test routes and showing an individual compound solvability of over 98\%.","['Computer-aided synthesis planning', 'Convergent routes', 'Retrosynthesis']",https://openreview.net/pdf/cfdc1c63041a51cec19c92d002d30819204c0600.pdf,Improving Route Development Using Convergent Retrosynthesis Planning,ML4LMS
"['Alex T. Müller', 'Kenneth Atz', 'Michael Reutlinger', 'Nicolas Zorn']","Finding a meaningful molecular representation that can be leveraged for a variety of tasks in chemical sciences and drug discovery is of wide interest, and new representation learning techniques are continuously being explored. Here, we investigate the fusion of graph attention neural networks with recurrent neural networks within a variational autoencoder framework for molecular representation learning. This combination leverages the strengths of both architectures to capture properties of molecular structures, enabling more effective encoding and flexible decoding processes. With the resulting representation, we observe competitive performance in quantitative structure-activity relationship (QSAR) benchmarks, a high validity and drug-likeness of randomly sampled molecules and robustness for linear latent space interpolation between two molecules. Our approach holds promise for facilitating downstream tasks such as clustering, QSAR, virtual screening and generative molecular design, all unified in one molecular representation.","['Molecular Representation Learning', 'Graph Neural Networks', 'LSTM', 'Variational Autoencoder']",https://openreview.net/pdf/5a11cd20141665cda78dc4355c7ed31a3e0eef99.pdf,Combining Graph Attention and Recurrent Neural Networks in a Variational Autoencoder for Molecular Representation Learning and Drug Design,ML4LMS
"['Janani Durairaj', 'Yusuf Adeshina', 'Zhonglin Cao', 'Xuejin Zhang', 'Vladas Oleinikovas', 'Thomas Duignan', 'Zachary McClure', 'Xavier Robin', 'Emanuele Rossi', 'Guoqing Zhou', 'Srimukh Prasad Veccham', 'Clemens Isert', 'Yuxing Peng', 'Prabindh Sundareson', 'Mehmet Akdel', 'Gabriele Corso', 'Hannes Stark', 'Zachary Wayne Carpenter', 'Michael M. Bronstein', 'Emine Kucukbenli', 'Torsten Schwede', 'Luca Naef']","Protein-ligand interactions (PLI) are foundational to small molecule drug design. With computational methods striving towards experimental accuracy, there is a critical demand for a well-curated and diverse PLI dataset. Existing datasets are often limited in size and diversity, and commonly used evaluation sets suffer from training information leakage, hindering the realistic assessment of method generalization capabilities. To address these shortcomings, we present PLINDER, the largest and most annotated dataset to date, comprising 449,383 PLI systems, each with over 500 annotations, similarity metrics at protein, pocket, interaction and ligand levels, and paired unbound (apo) and predicted structures. We propose an approach to generate training and evaluation splits that minimizes task-specific leakage and maximizes test set quality, and compare the resulting performance of DiffDock when re-trained with different kinds of splits.","['Protein-Ligand', 'Dataset', 'Interactions', 'Machine Learning', 'Docking']",https://openreview.net/pdf/c196965f12b8e599abc7f2ccf18b9cd1a8ae492a.pdf,PLINDER: The protein-ligand interactions dataset and evaluation resource,ML4LMS
"['Jose Arjona-Medina', 'Ramil Nugmanov']","Despite the rapid and significant advancements in deep learning for Quantitative Structure-Activity Relationship (QSAR) models, the challenge of learning robust molecular representations that effectively generalize in real-world scenarios to novel compounds remains an elusive and unresolved task.

This study examines how atom-level pretraining with quantum mechanics (QM) data can mitigate violations of assumptions regarding the distributional similarity between training and test data and therefore improve performance and generalization in downstream tasks. 

In the public dataset Therapeutics Data Commons (TDC), we show how pretraining on atom-level QM improves performance overall and makes the activation of the features distributes more Gaussian-like which results in a representation that is more robust to distribution shifts. 

To the best of our knowledge, this is the first time that hidden state molecular representations are analyzed to compare the effects of molecule-level and atom-level pretraining on QM data.","['Drug Discovery', 'Graph Neural Networks', 'Molecular property predictions', 'QSAR']",https://openreview.net/pdf/6c50293803a507ee2e09b4353f2586c99c64def1.pdf,Analysis of Atom-level pretraining with QM data for Graph Neural Networks Molecular property models,ML4LMS
"['Ali Ramlaoui', 'Théo Saulus', 'Basile Terver', 'Victor Schmidt', 'David Rolnick', 'Fragkiskos D. Malliaros', 'Alexandre AGM Duval']","Rapid advancements in machine learning (ML) are transforming materials science by significantly speeding up material property calculations. However, the proliferation of ML approaches has made it challenging for scientists to keep up with the most promising techniques. This paper presents an empirical study on Geometric Graph Neural Networks for 3D atomic systems, focusing on the impact of different (1) canonicalization methods, (2) graph creation strategies, and (3) auxiliary tasks, on performance, scalability and symmetry enforcement. Our findings and insights aim to guide researchers in selecting optimal modeling components for molecular modeling tasks.","['equivariance', 'geometry', 'materials', 'gnn', 'deep_learning', 'oc20']",https://openreview.net/pdf/d0ddd6249b228279e1f0c84c2f985497ad67f317.pdf,Improving Molecular Modeling with Geometric GNNs: an Empirical Study,ML4LMS
